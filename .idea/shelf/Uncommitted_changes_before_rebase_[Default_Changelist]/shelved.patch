Index: client/nats/nats_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2012-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage nats\r\n\r\n////////////////////////////////////////////////////////////////////////////////\r\n// Package scoped specific tests here..\r\n////////////////////////////////////////////////////////////////////////////////\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"reflect\"\r\n\t\"regexp\"\r\n\t\"runtime\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/server\"\r\n\tnatsserver \"github.com/kubemq-io/broker/server/gnatsd/test\"\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nfunc TestVersion(t *testing.T) {\r\n\t// Semantic versioning\r\n\tverRe := regexp.MustCompile(`\\d+.\\d+.\\d+(-\\S+)?`)\r\n\tif !verRe.MatchString(Version) {\r\n\t\tt.Fatalf(\"Version not compatible with semantic versioning: %q\", Version)\r\n\t}\r\n}\r\n\r\n// Dumb wait program to sync on callbacks, etc... Will timeout\r\nfunc Wait(ch chan bool) error {\r\n\treturn WaitTime(ch, 5*time.Second)\r\n}\r\n\r\nfunc WaitTime(ch chan bool, timeout time.Duration) error {\r\n\tselect {\r\n\tcase <-ch:\r\n\t\treturn nil\r\n\tcase <-time.After(timeout):\r\n\t}\r\n\treturn errors.New(\"timeout\")\r\n}\r\n\r\nfunc stackFatalf(t *testing.T, f string, args ...interface{}) {\r\n\tlines := make([]string, 0, 32)\r\n\tmsg := fmt.Sprintf(f, args...)\r\n\tlines = append(lines, msg)\r\n\r\n\t// Generate the Stack of callers: Skip us and verify* frames.\r\n\tfor i := 1; true; i++ {\r\n\t\t_, file, line, ok := runtime.Caller(i)\r\n\t\tif !ok {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tmsg := fmt.Sprintf(\"%d - %s:%d\", i, file, line)\r\n\t\tlines = append(lines, msg)\r\n\t}\r\n\tt.Fatalf(\"%s\", strings.Join(lines, \"\\n\"))\r\n}\r\n\r\n// Check the error channel for an error and if one is present,\r\n// calls t.Fatal(e.Error()). Note that this supports tests that\r\n// send nil to the error channel and so report error only if\r\n// e is != nil.\r\nfunc checkErrChannel(t *testing.T, errCh chan error) {\r\n\tt.Helper()\r\n\tselect {\r\n\tcase e := <-errCh:\r\n\t\tif e != nil {\r\n\t\t\tt.Fatal(e.Error())\r\n\t\t}\r\n\tdefault:\r\n\t}\r\n}\r\n\r\nfunc TestVersionMatchesTag(t *testing.T) {\r\n\ttag := os.Getenv(\"TRAVIS_TAG\")\r\n\tif tag == \"\" {\r\n\t\tt.SkipNow()\r\n\t}\r\n\t// We expect a tag of the form vX.Y.Z. If that's not the case,\r\n\t// we need someone to have a look. So fail if first letter is not\r\n\t// a `v`\r\n\tif tag[0] != 'v' {\r\n\t\tt.Fatalf(\"Expect tag to start with `v`, tag is: %s\", tag)\r\n\t}\r\n\t// Strip the `v` from the tag for the version comparison.\r\n\tif Version != tag[1:] {\r\n\t\tt.Fatalf(\"Version (%s) does not match tag (%s)\", Version, tag[1:])\r\n\t}\r\n}\r\n\r\nfunc TestExpandPath(t *testing.T) {\r\n\tif runtime.GOOS == \"windows\" {\r\n\t\torigUserProfile := os.Getenv(\"USERPROFILE\")\r\n\t\torigHomeDrive, origHomePath := os.Getenv(\"HOMEDRIVE\"), os.Getenv(\"HOMEPATH\")\r\n\t\tdefer func() {\r\n\t\t\tos.Setenv(\"USERPROFILE\", origUserProfile)\r\n\t\t\tos.Setenv(\"HOMEDRIVE\", origHomeDrive)\r\n\t\t\tos.Setenv(\"HOMEPATH\", origHomePath)\r\n\t\t}()\r\n\r\n\t\tcases := []struct {\r\n\t\t\tpath        string\r\n\t\t\tuserProfile string\r\n\t\t\thomeDrive   string\r\n\t\t\thomePath    string\r\n\r\n\t\t\twantPath string\r\n\t\t\twantErr  bool\r\n\t\t}{\r\n\t\t\t// Missing HOMEDRIVE and HOMEPATH.\r\n\t\t\t{path: \"/Foo/Bar\", userProfile: `C:\\Foo\\Bar`, wantPath: \"/Foo/Bar\"},\r\n\t\t\t{path: \"Foo/Bar\", userProfile: `C:\\Foo\\Bar`, wantPath: \"Foo/Bar\"},\r\n\t\t\t{path: \"~/Fizz\", userProfile: `C:\\Foo\\Bar`, wantPath: `C:\\Foo\\Bar\\Fizz`},\r\n\t\t\t{path: `${HOMEDRIVE}${HOMEPATH}\\Fizz`, userProfile: `C:\\Foo\\Bar`, wantPath: `C:\\Foo\\Bar\\Fizz`},\r\n\r\n\t\t\t// Missing USERPROFILE.\r\n\t\t\t{path: \"~/Fizz\", homeDrive: \"X:\", homePath: `\\Foo\\Bar`, wantPath: `X:\\Foo\\Bar\\Fizz`},\r\n\r\n\t\t\t// Set all environment variables. HOMEDRIVE and HOMEPATH take\r\n\t\t\t// precedence.\r\n\t\t\t{path: \"~/Fizz\", userProfile: `C:\\Foo\\Bar`,\r\n\t\t\t\thomeDrive: \"X:\", homePath: `\\Foo\\Bar`, wantPath: `X:\\Foo\\Bar\\Fizz`},\r\n\r\n\t\t\t// Missing all environment variables.\r\n\t\t\t{path: \"~/Fizz\", wantErr: true},\r\n\t\t}\r\n\t\tfor i, c := range cases {\r\n\t\t\tt.Run(fmt.Sprintf(\"windows case %d\", i), func(t *testing.T) {\r\n\t\t\t\tos.Setenv(\"USERPROFILE\", c.userProfile)\r\n\t\t\t\tos.Setenv(\"HOMEDRIVE\", c.homeDrive)\r\n\t\t\t\tos.Setenv(\"HOMEPATH\", c.homePath)\r\n\r\n\t\t\t\tgotPath, err := expandPath(c.path)\r\n\t\t\t\tif !c.wantErr && err != nil {\r\n\t\t\t\t\tt.Fatalf(\"unexpected error: got=%v; want=%v\", err, nil)\r\n\t\t\t\t} else if c.wantErr && err == nil {\r\n\t\t\t\t\tt.Fatalf(\"unexpected success: got=%v; want=%v\", nil, \"err\")\r\n\t\t\t\t}\r\n\r\n\t\t\t\tif gotPath != c.wantPath {\r\n\t\t\t\t\tt.Fatalf(\"unexpected path: got=%v; want=%v\", gotPath, c.wantPath)\r\n\t\t\t\t}\r\n\t\t\t})\r\n\t\t}\r\n\r\n\t\treturn\r\n\t}\r\n\r\n\t// Unix tests\r\n\r\n\torigHome := os.Getenv(\"HOME\")\r\n\tdefer os.Setenv(\"HOME\", origHome)\r\n\r\n\tcases := []struct {\r\n\t\tpath    string\r\n\t\thome    string\r\n\t\ttestEnv string\r\n\r\n\t\twantPath string\r\n\t\twantErr  bool\r\n\t}{\r\n\t\t{path: \"/foo/bar\", home: \"/fizz/buzz\", wantPath: \"/foo/bar\"},\r\n\t\t{path: \"foo/bar\", home: \"/fizz/buzz\", wantPath: \"foo/bar\"},\r\n\t\t{path: \"~/fizz\", home: \"/foo/bar\", wantPath: \"/foo/bar/fizz\"},\r\n\t\t{path: \"$HOME/fizz\", home: \"/foo/bar\", wantPath: \"/foo/bar/fizz\"},\r\n\r\n\t\t// missing HOME env var\r\n\t\t{path: \"~/fizz\", wantErr: true},\r\n\t}\r\n\tfor i, c := range cases {\r\n\t\tt.Run(fmt.Sprintf(\"unix case %d\", i), func(t *testing.T) {\r\n\t\t\tos.Setenv(\"HOME\", c.home)\r\n\r\n\t\t\tgotPath, err := expandPath(c.path)\r\n\t\t\tif !c.wantErr && err != nil {\r\n\t\t\t\tt.Fatalf(\"unexpected error: got=%v; want=%v\", err, nil)\r\n\t\t\t} else if c.wantErr && err == nil {\r\n\t\t\t\tt.Fatalf(\"unexpected success: got=%v; want=%v\", nil, \"err\")\r\n\t\t\t}\r\n\r\n\t\t\tif gotPath != c.wantPath {\r\n\t\t\t\tt.Fatalf(\"unexpected path: got=%v; want=%v\", gotPath, c.wantPath)\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n}\r\n\r\n////////////////////////////////////////////////////////////////////////////////\r\n// Reconnect tests\r\n////////////////////////////////////////////////////////////////////////////////\r\n\r\nconst TEST_PORT = 8368\r\n\r\nvar reconnectOpts = Options{\r\n\tUrl:            fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT),\r\n\tAllowReconnect: true,\r\n\tMaxReconnect:   10,\r\n\tReconnectWait:  100 * time.Millisecond,\r\n\tTimeout:        DefaultTimeout,\r\n}\r\n\r\nfunc RunServerOnPort(port int) *server.Server {\r\n\topts := natsserver.DefaultTestOptions\r\n\topts.Port = port\r\n\treturn RunServerWithOptions(&opts)\r\n}\r\n\r\nfunc RunServerWithOptions(opts *server.Options) *server.Server {\r\n\treturn natsserver.RunServer(opts)\r\n}\r\n\r\nfunc TestReconnectServerStats(t *testing.T) {\r\n\tts := RunServerOnPort(TEST_PORT)\r\n\r\n\topts := reconnectOpts\r\n\tnc, _ := opts.Connect()\r\n\tdefer nc.Close()\r\n\tnc.Flush()\r\n\r\n\tts.Shutdown()\r\n\t// server is stopped here...\r\n\r\n\tts = RunServerOnPort(TEST_PORT)\r\n\tdefer ts.Shutdown()\r\n\r\n\tif err := nc.FlushTimeout(5 * time.Second); err != nil {\r\n\t\tt.Fatalf(\"Error on Flush: %v\", err)\r\n\t}\r\n\r\n\t// Make sure the server who is reconnected has the reconnects stats reset.\r\n\tnc.mu.Lock()\r\n\t_, cur := nc.currentServer()\r\n\tnc.mu.Unlock()\r\n\r\n\tif cur.reconnects != 0 {\r\n\t\tt.Fatalf(\"Current Server's reconnects should be 0 vs %d\\n\", cur.reconnects)\r\n\t}\r\n}\r\n\r\n////////////////////////////////////////////////////////////////////////////////\r\n// ServerPool tests\r\n////////////////////////////////////////////////////////////////////////////////\r\n\r\nvar testServers = []string{\r\n\t\"nats://localhost:1222\",\r\n\t\"nats://localhost:1223\",\r\n\t\"nats://localhost:1224\",\r\n\t\"nats://localhost:1225\",\r\n\t\"nats://localhost:1226\",\r\n\t\"nats://localhost:1227\",\r\n\t\"nats://localhost:1228\",\r\n}\r\n\r\nfunc TestSimplifiedURLs(t *testing.T) {\r\n\topts := GetDefaultOptions()\r\n\topts.NoRandomize = true\r\n\topts.Servers = []string{\r\n\t\t\"nats://host1:1234\",\r\n\t\t\"nats://host2:\",\r\n\t\t\"nats://host3\",\r\n\t\t\"host4:1234\",\r\n\t\t\"host5:\",\r\n\t\t\"host6\",\r\n\t\t\"nats://[1:2:3:4]:1234\",\r\n\t\t\"nats://[5:6:7:8]:\",\r\n\t\t\"nats://[9:10:11:12]\",\r\n\t\t\"[13:14:15:16]:\",\r\n\t\t\"[17:18:19:20]:1234\",\r\n\t}\r\n\r\n\t// We expect the result in the server pool to be:\r\n\texpected := []string{\r\n\t\t\"nats://host1:1234\",\r\n\t\t\"nats://host2:4222\",\r\n\t\t\"nats://host3:4222\",\r\n\t\t\"nats://host4:1234\",\r\n\t\t\"nats://host5:4222\",\r\n\t\t\"nats://host6:4222\",\r\n\t\t\"nats://[1:2:3:4]:1234\",\r\n\t\t\"nats://[5:6:7:8]:4222\",\r\n\t\t\"nats://[9:10:11:12]:4222\",\r\n\t\t\"nats://[13:14:15:16]:4222\",\r\n\t\t\"nats://[17:18:19:20]:1234\",\r\n\t}\r\n\r\n\tnc := &Conn{Opts: opts}\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\r\n\t}\r\n\t// Check server pool directly\r\n\tfor i, u := range nc.srvPool {\r\n\t\tif u.url.String() != expected[i] {\r\n\t\t\tt.Fatalf(\"Expected url %q, got %q\", expected[i], u.url.String())\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestServersRandomize(t *testing.T) {\r\n\topts := GetDefaultOptions()\r\n\topts.Servers = testServers\r\n\tnc := &Conn{Opts: opts}\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\r\n\t}\r\n\t// Build []string from srvPool\r\n\tclientServers := []string{}\r\n\tfor _, s := range nc.srvPool {\r\n\t\tclientServers = append(clientServers, s.url.String())\r\n\t}\r\n\t// In theory this could happen..\r\n\tif reflect.DeepEqual(testServers, clientServers) {\r\n\t\tt.Fatalf(\"ServerPool list not randomized\\n\")\r\n\t}\r\n\r\n\t// Now test that we do not randomize if proper flag is set.\r\n\topts = GetDefaultOptions()\r\n\topts.Servers = testServers\r\n\topts.NoRandomize = true\r\n\tnc = &Conn{Opts: opts}\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\r\n\t}\r\n\t// Build []string from srvPool\r\n\tclientServers = []string{}\r\n\tfor _, s := range nc.srvPool {\r\n\t\tclientServers = append(clientServers, s.url.String())\r\n\t}\r\n\tif !reflect.DeepEqual(testServers, clientServers) {\r\n\t\tt.Fatalf(\"ServerPool list should not be randomized\\n\")\r\n\t}\r\n\r\n\t// Although the original intent was that if Opts.Url is\r\n\t// set, Opts.Servers is not (and vice versa), the behavior\r\n\t// is that Opts.Url is always first, even when randomization\r\n\t// is enabled. So make sure that this is still the case.\r\n\topts = GetDefaultOptions()\r\n\topts.Url = DefaultURL\r\n\topts.Servers = testServers\r\n\tnc = &Conn{Opts: opts}\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\r\n\t}\r\n\t// Build []string from srvPool\r\n\tclientServers = []string{}\r\n\tfor _, s := range nc.srvPool {\r\n\t\tclientServers = append(clientServers, s.url.String())\r\n\t}\r\n\t// In theory this could happen..\r\n\tif reflect.DeepEqual(testServers, clientServers) {\r\n\t\tt.Fatalf(\"ServerPool list not randomized\\n\")\r\n\t}\r\n\tif clientServers[0] != DefaultURL {\r\n\t\tt.Fatalf(\"Options.Url should be first in the array, got %v\", clientServers[0])\r\n\t}\r\n}\r\n\r\nfunc TestSelectNextServer(t *testing.T) {\r\n\topts := GetDefaultOptions()\r\n\topts.Servers = testServers\r\n\topts.NoRandomize = true\r\n\tnc := &Conn{Opts: opts}\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\r\n\t}\r\n\tif nc.current != nc.srvPool[0] {\r\n\t\tt.Fatalf(\"Wrong default selection: %v\\n\", nc.current.url)\r\n\t}\r\n\r\n\tsel, err := nc.selectNextServer()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Got an err: %v\\n\", err)\r\n\t}\r\n\t// Check that we are now looking at #2, and current is now last.\r\n\tif len(nc.srvPool) != len(testServers) {\r\n\t\tt.Fatalf(\"List is incorrect size: %d vs %d\\n\", len(nc.srvPool), len(testServers))\r\n\t}\r\n\tif nc.current.url.String() != testServers[1] {\r\n\t\tt.Fatalf(\"Selection incorrect: %v vs %v\\n\", nc.current.url, testServers[1])\r\n\t}\r\n\tif nc.srvPool[len(nc.srvPool)-1].url.String() != testServers[0] {\r\n\t\tt.Fatalf(\"Did not push old to last position\\n\")\r\n\t}\r\n\tif sel != nc.srvPool[0] {\r\n\t\tt.Fatalf(\"Did not return correct server: %v vs %v\\n\", sel.url, nc.srvPool[0].url)\r\n\t}\r\n\r\n\t// Test that we do not keep servers where we have tried to reconnect past our limit.\r\n\tnc.srvPool[0].reconnects = int(opts.MaxReconnect)\r\n\tif _, err := nc.selectNextServer(); err != nil {\r\n\t\tt.Fatalf(\"Got an err: %v\\n\", err)\r\n\t}\r\n\t// Check that we are now looking at #3, and current is not in the list.\r\n\tif len(nc.srvPool) != len(testServers)-1 {\r\n\t\tt.Fatalf(\"List is incorrect size: %d vs %d\\n\", len(nc.srvPool), len(testServers)-1)\r\n\t}\r\n\tif nc.current.url.String() != testServers[2] {\r\n\t\tt.Fatalf(\"Selection incorrect: %v vs %v\\n\", nc.current.url, testServers[2])\r\n\t}\r\n\tif nc.srvPool[len(nc.srvPool)-1].url.String() == testServers[1] {\r\n\t\tt.Fatalf(\"Did not throw away the last server correctly\\n\")\r\n\t}\r\n}\r\n\r\n// This will test that comma separated url strings work properly for\r\n// the Connect() command.\r\nfunc TestUrlArgument(t *testing.T) {\r\n\tcheck := func(url string, expected []string) {\r\n\t\tif !reflect.DeepEqual(processUrlString(url), expected) {\r\n\t\t\tt.Fatalf(\"Got wrong response processing URL: %q, RES: %#v\\n\", url, processUrlString(url))\r\n\t\t}\r\n\t}\r\n\t// This is normal case\r\n\toneExpected := []string{\"nats://localhost:1222\"}\r\n\r\n\tcheck(\"nats://localhost:1222\", oneExpected)\r\n\tcheck(\"nats://localhost:1222 \", oneExpected)\r\n\tcheck(\" nats://localhost:1222\", oneExpected)\r\n\tcheck(\" nats://localhost:1222 \", oneExpected)\r\n\r\n\tvar multiExpected = []string{\r\n\t\t\"nats://localhost:1222\",\r\n\t\t\"nats://localhost:1223\",\r\n\t\t\"nats://localhost:1224\",\r\n\t}\r\n\r\n\tcheck(\"nats://localhost:1222,nats://localhost:1223,nats://localhost:1224\", multiExpected)\r\n\tcheck(\"nats://localhost:1222, nats://localhost:1223, nats://localhost:1224\", multiExpected)\r\n\tcheck(\" nats://localhost:1222, nats://localhost:1223, nats://localhost:1224 \", multiExpected)\r\n\tcheck(\"nats://localhost:1222,   nats://localhost:1223  ,nats://localhost:1224\", multiExpected)\r\n}\r\n\r\nfunc TestParserPing(t *testing.T) {\r\n\tc := &Conn{}\r\n\tfake := &bytes.Buffer{}\r\n\tc.bw = bufio.NewWriterSize(fake, c.Opts.ReconnectBufSize)\r\n\r\n\tc.ps = &parseState{}\r\n\r\n\tif c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\r\n\t}\r\n\tping := []byte(\"PING\\r\\n\")\r\n\terr := c.parse(ping[:1])\r\n\tif err != nil || c.ps.state != OP_P {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(ping[1:2])\r\n\tif err != nil || c.ps.state != OP_PI {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(ping[2:3])\r\n\tif err != nil || c.ps.state != OP_PIN {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(ping[3:4])\r\n\tif err != nil || c.ps.state != OP_PING {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(ping[4:5])\r\n\tif err != nil || c.ps.state != OP_PING {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(ping[5:6])\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(ping)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\t// Should tolerate spaces\r\n\tping = []byte(\"PING  \\r\")\r\n\terr = c.parse(ping)\r\n\tif err != nil || c.ps.state != OP_PING {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tping = []byte(\"PING  \\r  \\n\")\r\n\terr = c.parse(ping)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n}\r\n\r\nfunc TestParserErr(t *testing.T) {\r\n\tc := &Conn{}\r\n\tc.status = CLOSED\r\n\tfake := &bytes.Buffer{}\r\n\tc.bw = bufio.NewWriterSize(fake, c.Opts.ReconnectBufSize)\r\n\r\n\tc.ps = &parseState{}\r\n\r\n\t// This test focuses on the parser only, not how the error is\r\n\t// actually processed by the upper layer.\r\n\r\n\tif c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\r\n\t}\r\n\r\n\texpectedError := \"'Any kind of error'\"\r\n\terrProto := []byte(\"-ERR  \" + expectedError + \"\\r\\n\")\r\n\terr := c.parse(errProto[:1])\r\n\tif err != nil || c.ps.state != OP_MINUS {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[1:2])\r\n\tif err != nil || c.ps.state != OP_MINUS_E {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[2:3])\r\n\tif err != nil || c.ps.state != OP_MINUS_ER {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[3:4])\r\n\tif err != nil || c.ps.state != OP_MINUS_ERR {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[4:5])\r\n\tif err != nil || c.ps.state != OP_MINUS_ERR_SPC {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[5:6])\r\n\tif err != nil || c.ps.state != OP_MINUS_ERR_SPC {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\r\n\t// Check with split arg buffer\r\n\terr = c.parse(errProto[6:7])\r\n\tif err != nil || c.ps.state != MINUS_ERR_ARG {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[7:10])\r\n\tif err != nil || c.ps.state != MINUS_ERR_ARG {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[10 : len(errProto)-2])\r\n\tif err != nil || c.ps.state != MINUS_ERR_ARG {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\tif c.ps.argBuf == nil {\r\n\t\tt.Fatal(\"ArgBuf should not be nil\")\r\n\t}\r\n\ts := string(c.ps.argBuf)\r\n\tif s != expectedError {\r\n\t\tt.Fatalf(\"Expected %v, got %v\", expectedError, s)\r\n\t}\r\n\terr = c.parse(errProto[len(errProto)-2:])\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\r\n\t// Check without split arg buffer\r\n\terrProto = []byte(\"-ERR 'Any error'\\r\\n\")\r\n\terr = c.parse(errProto)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n}\r\n\r\nfunc TestParserOK(t *testing.T) {\r\n\tc := &Conn{}\r\n\tc.ps = &parseState{}\r\n\r\n\tif c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\r\n\t}\r\n\terrProto := []byte(\"+OKay\\r\\n\")\r\n\terr := c.parse(errProto[:1])\r\n\tif err != nil || c.ps.state != OP_PLUS {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[1:2])\r\n\tif err != nil || c.ps.state != OP_PLUS_O {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[2:3])\r\n\tif err != nil || c.ps.state != OP_PLUS_OK {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(errProto[3:])\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n}\r\n\r\nfunc TestParserShouldFail(t *testing.T) {\r\n\tc := &Conn{}\r\n\tc.ps = &parseState{}\r\n\r\n\tif err := c.parse([]byte(\" PING\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"POO\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"Px\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"PIx\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"PINx\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\t// Stop here because 'PING' protos are tolerant for anything between PING and \\n\r\n\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"POx\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"PONx\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\t// Stop here because 'PONG' protos are tolerant for anything between PONG and \\n\r\n\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"ZOO\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"Mx\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSx\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSGx\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSG  foo\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSG \\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSG foo 1\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSG foo bar 1\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSG foo bar 1 baz\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"MSG foo 1 bar baz\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"+x\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"+Ox\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"-x\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"-Ex\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"-ERx\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n\tc.ps.state = OP_START\r\n\tif err := c.parse([]byte(\"-ERRx\\r\\n\")); err == nil {\r\n\t\tt.Fatal(\"Should have received a parse error\")\r\n\t}\r\n}\r\n\r\nfunc TestParserSplitMsg(t *testing.T) {\r\n\tnc := &Conn{}\r\n\tnc.ps = &parseState{}\r\n\r\n\tbuf := []byte(\"MSG a\\r\\n\")\r\n\terr := nc.parse(buf)\r\n\tif err == nil {\r\n\t\tt.Fatal(\"Expected an error\")\r\n\t}\r\n\tnc.ps = &parseState{}\r\n\r\n\tbuf = []byte(\"MSG a b c\\r\\n\")\r\n\terr = nc.parse(buf)\r\n\tif err == nil {\r\n\t\tt.Fatal(\"Expected an error\")\r\n\t}\r\n\tnc.ps = &parseState{}\r\n\r\n\texpectedCount := uint64(1)\r\n\texpectedSize := uint64(3)\r\n\r\n\tbuf = []byte(\"MSG a\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif nc.ps.argBuf == nil {\r\n\t\tt.Fatal(\"Arg buffer should have been created\")\r\n\t}\r\n\r\n\tbuf = []byte(\" 1 3\\r\\nf\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif nc.ps.ma.size != 3 {\r\n\t\tt.Fatalf(\"Wrong msg size: %d instead of 3\", nc.ps.ma.size)\r\n\t}\r\n\tif nc.ps.ma.sid != 1 {\r\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\r\n\t}\r\n\tif string(nc.ps.ma.subject) != \"a\" {\r\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\r\n\t}\r\n\tif nc.ps.msgBuf == nil {\r\n\t\tt.Fatal(\"Msg buffer should have been created\")\r\n\t}\r\n\r\n\tbuf = []byte(\"oo\\r\\n\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\r\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\r\n\t}\r\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\r\n\t\tt.Fatal(\"Buffers should be nil now\")\r\n\t}\r\n\r\n\tbuf = []byte(\"MSG a 1 3\\r\\nfo\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif nc.ps.ma.size != 3 {\r\n\t\tt.Fatalf(\"Wrong msg size: %d instead of 3\", nc.ps.ma.size)\r\n\t}\r\n\tif nc.ps.ma.sid != 1 {\r\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\r\n\t}\r\n\tif string(nc.ps.ma.subject) != \"a\" {\r\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\r\n\t}\r\n\tif nc.ps.argBuf == nil {\r\n\t\tt.Fatal(\"Arg buffer should have been created\")\r\n\t}\r\n\tif nc.ps.msgBuf == nil {\r\n\t\tt.Fatal(\"Msg buffer should have been created\")\r\n\t}\r\n\r\n\texpectedCount++\r\n\texpectedSize += 3\r\n\r\n\tbuf = []byte(\"o\\r\\n\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\r\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\r\n\t}\r\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\r\n\t\tt.Fatal(\"Buffers should be nil now\")\r\n\t}\r\n\r\n\tbuf = []byte(\"MSG a 1 6\\r\\nfo\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif nc.ps.ma.size != 6 {\r\n\t\tt.Fatalf(\"Wrong msg size: %d instead of 3\", nc.ps.ma.size)\r\n\t}\r\n\tif nc.ps.ma.sid != 1 {\r\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\r\n\t}\r\n\tif string(nc.ps.ma.subject) != \"a\" {\r\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\r\n\t}\r\n\tif nc.ps.argBuf == nil {\r\n\t\tt.Fatal(\"Arg buffer should have been created\")\r\n\t}\r\n\tif nc.ps.msgBuf == nil {\r\n\t\tt.Fatal(\"Msg buffer should have been created\")\r\n\t}\r\n\r\n\tbuf = []byte(\"ob\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\r\n\texpectedCount++\r\n\texpectedSize += 6\r\n\r\n\tbuf = []byte(\"ar\\r\\n\")\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\r\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\r\n\t}\r\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\r\n\t\tt.Fatal(\"Buffers should be nil now\")\r\n\t}\r\n\r\n\t// Let's have a msg that is bigger than the parser's scratch size.\r\n\t// Since we prepopulate the msg with 'foo', adding 3 to the size.\r\n\tmsgSize := cap(nc.ps.scratch) + 100 + 3\r\n\tbuf = []byte(fmt.Sprintf(\"MSG a 1 b %d\\r\\nfoo\", msgSize))\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif nc.ps.ma.size != msgSize {\r\n\t\tt.Fatalf(\"Wrong msg size: %d instead of %d\", nc.ps.ma.size, msgSize)\r\n\t}\r\n\tif nc.ps.ma.sid != 1 {\r\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\r\n\t}\r\n\tif string(nc.ps.ma.subject) != \"a\" {\r\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\r\n\t}\r\n\tif string(nc.ps.ma.reply) != \"b\" {\r\n\t\tt.Fatalf(\"Wrong reply: '%s' instead of 'b'\", string(nc.ps.ma.reply))\r\n\t}\r\n\tif nc.ps.argBuf == nil {\r\n\t\tt.Fatal(\"Arg buffer should have been created\")\r\n\t}\r\n\tif nc.ps.msgBuf == nil {\r\n\t\tt.Fatal(\"Msg buffer should have been created\")\r\n\t}\r\n\r\n\texpectedCount++\r\n\texpectedSize += uint64(msgSize)\r\n\r\n\tbufSize := msgSize - 3\r\n\r\n\tbuf = make([]byte, bufSize)\r\n\tfor i := 0; i < bufSize; i++ {\r\n\t\tbuf[i] = byte('a' + (i % 26))\r\n\t}\r\n\r\n\terr = nc.parse(buf)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Parser error: %v\", err)\r\n\t}\r\n\tif nc.ps.state != MSG_PAYLOAD {\r\n\t\tt.Fatalf(\"Wrong state: %v instead of %v\", nc.ps.state, MSG_PAYLOAD)\r\n\t}\r\n\tif nc.ps.ma.size != msgSize {\r\n\t\tt.Fatalf(\"Wrong (ma) msg size: %d instead of %d\", nc.ps.ma.size, msgSize)\r\n\t}\r\n\tif len(nc.ps.msgBuf) != msgSize {\r\n\t\tt.Fatalf(\"Wrong msg size: %d instead of %d\", len(nc.ps.msgBuf), msgSize)\r\n\t}\r\n\t// Check content:\r\n\tif string(nc.ps.msgBuf[0:3]) != \"foo\" {\r\n\t\tt.Fatalf(\"Wrong msg content: %s\", string(nc.ps.msgBuf))\r\n\t}\r\n\tfor k := 3; k < nc.ps.ma.size; k++ {\r\n\t\tif nc.ps.msgBuf[k] != byte('a'+((k-3)%26)) {\r\n\t\t\tt.Fatalf(\"Wrong msg content: %s\", string(nc.ps.msgBuf))\r\n\t\t}\r\n\t}\r\n\r\n\tbuf = []byte(\"\\r\\n\")\r\n\tif err := nc.parse(buf); err != nil {\r\n\t\tt.Fatalf(\"Unexpected error during parsing: %v\", err)\r\n\t}\r\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\r\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\r\n\t}\r\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\r\n\t\tt.Fatal(\"Buffers should be nil now\")\r\n\t}\r\n\tif nc.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Wrong state: %v\", nc.ps.state)\r\n\t}\r\n}\r\n\r\nfunc TestNormalizeError(t *testing.T) {\r\n\texpected := \"Typical Error\"\r\n\tif s := normalizeErr(\"-ERR '\" + expected + \"'\"); s != expected {\r\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\r\n\t}\r\n\r\n\texpected = \"Trim Surrounding Spaces\"\r\n\tif s := normalizeErr(\"-ERR    '\" + expected + \"'   \"); s != expected {\r\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\r\n\t}\r\n\r\n\texpected = \"Trim Surrounding Spaces Without Quotes\"\r\n\tif s := normalizeErr(\"-ERR    \" + expected + \"   \"); s != expected {\r\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\r\n\t}\r\n\r\n\texpected = \"Error Without Quotes\"\r\n\tif s := normalizeErr(\"-ERR \" + expected); s != expected {\r\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\r\n\t}\r\n\r\n\texpected = \"Error With Quote Only On Left\"\r\n\tif s := normalizeErr(\"-ERR '\" + expected); s != expected {\r\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\r\n\t}\r\n\r\n\texpected = \"Error With Quote Only On Right\"\r\n\tif s := normalizeErr(\"-ERR \" + expected + \"'\"); s != expected {\r\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\r\n\t}\r\n}\r\n\r\nfunc TestAsyncINFO(t *testing.T) {\r\n\topts := GetDefaultOptions()\r\n\tc := &Conn{Opts: opts}\r\n\r\n\tc.ps = &parseState{}\r\n\r\n\tif c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\r\n\t}\r\n\r\n\tinfo := []byte(\"INFO {}\\r\\n\")\r\n\tif c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\r\n\t}\r\n\terr := c.parse(info[:1])\r\n\tif err != nil || c.ps.state != OP_I {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(info[1:2])\r\n\tif err != nil || c.ps.state != OP_IN {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(info[2:3])\r\n\tif err != nil || c.ps.state != OP_INF {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(info[3:4])\r\n\tif err != nil || c.ps.state != OP_INFO {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(info[4:5])\r\n\tif err != nil || c.ps.state != OP_INFO_SPC {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\terr = c.parse(info[5:])\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\r\n\t// All at once\r\n\terr = c.parse(info)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\r\n\t// Server pool needs to be setup\r\n\tc.setupServerPool()\r\n\r\n\t// Partials requiring argBuf\r\n\texpectedServer := serverInfo{\r\n\t\tID:           \"test\",\r\n\t\tHost:         \"localhost\",\r\n\t\tPort:         4222,\r\n\t\tVersion:      \"1.2.3\",\r\n\t\tAuthRequired: true,\r\n\t\tTLSRequired:  true,\r\n\t\tMaxPayload:   2 * 1024 * 1024,\r\n\t\tConnectURLs:  []string{\"localhost:5222\", \"localhost:6222\"},\r\n\t}\r\n\t// Set NoRandomize so that the check with expectedServer info\r\n\t// matches.\r\n\tc.Opts.NoRandomize = true\r\n\r\n\tb, _ := json.Marshal(expectedServer)\r\n\tinfo = []byte(fmt.Sprintf(\"INFO %s\\r\\n\", b))\r\n\tif c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\r\n\t}\r\n\terr = c.parse(info[:9])\r\n\tif err != nil || c.ps.state != INFO_ARG || c.ps.argBuf == nil {\r\n\t\tt.Fatalf(\"Unexpected: %d err: %v argBuf: %v\\n\", c.ps.state, err, c.ps.argBuf)\r\n\t}\r\n\terr = c.parse(info[9:11])\r\n\tif err != nil || c.ps.state != INFO_ARG || c.ps.argBuf == nil {\r\n\t\tt.Fatalf(\"Unexpected: %d err: %v argBuf: %v\\n\", c.ps.state, err, c.ps.argBuf)\r\n\t}\r\n\terr = c.parse(info[11:])\r\n\tif err != nil || c.ps.state != OP_START || c.ps.argBuf != nil {\r\n\t\tt.Fatalf(\"Unexpected: %d err: %v argBuf: %v\\n\", c.ps.state, err, c.ps.argBuf)\r\n\t}\r\n\tif !reflect.DeepEqual(c.info, expectedServer) {\r\n\t\tt.Fatalf(\"Expected server info to be: %v, got: %v\", expectedServer, c.info)\r\n\t}\r\n\r\n\t// Good INFOs\r\n\tgood := []string{\"INFO {}\\r\\n\", \"INFO  {}\\r\\n\", \"INFO {} \\r\\n\", \"INFO { \\\"server_id\\\": \\\"test\\\"  }   \\r\\n\", \"INFO {\\\"connect_urls\\\":[]}\\r\\n\"}\r\n\tfor _, gi := range good {\r\n\t\tc.ps = &parseState{}\r\n\t\terr = c.parse([]byte(gi))\r\n\t\tif err != nil || c.ps.state != OP_START {\r\n\t\t\tt.Fatalf(\"Protocol %q should be fine. Err=%v state=%v\", gi, err, c.ps.state)\r\n\t\t}\r\n\t}\r\n\r\n\t// Wrong INFOs\r\n\twrong := []string{\"IxNFO {}\\r\\n\", \"INxFO {}\\r\\n\", \"INFxO {}\\r\\n\", \"INFOx {}\\r\\n\", \"INFO{}\\r\\n\", \"INFO {}\"}\r\n\tfor _, wi := range wrong {\r\n\t\tc.ps = &parseState{}\r\n\t\terr = c.parse([]byte(wi))\r\n\t\tif err == nil && c.ps.state == OP_START {\r\n\t\t\tt.Fatalf(\"Protocol %q should have failed\", wi)\r\n\t\t}\r\n\t}\r\n\r\n\tcheckPool := func(urls ...string) {\r\n\t\t// Check both pool and urls map\r\n\t\tif len(c.srvPool) != len(urls) {\r\n\t\t\tstackFatalf(t, \"Pool should have %d elements, has %d\", len(urls), len(c.srvPool))\r\n\t\t}\r\n\t\tif len(c.urls) != len(urls) {\r\n\t\t\tstackFatalf(t, \"Map should have %d elements, has %d\", len(urls), len(c.urls))\r\n\t\t}\r\n\t\tfor _, url := range urls {\r\n\t\t\tif _, present := c.urls[url]; !present {\r\n\t\t\t\tstackFatalf(t, \"Pool should have %q\", url)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Now test the decoding of \"connect_urls\"\r\n\r\n\t// Reset the pool\r\n\tc.setupServerPool()\r\n\t// Reinitialize the parser\r\n\tc.ps = &parseState{}\r\n\r\n\tinfo = []byte(\"INFO {\\\"connect_urls\\\":[\\\"localhost:4222\\\", \\\"localhost:5222\\\"]}\\r\\n\")\r\n\terr = c.parse(info)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\t// Pool now should contain 127.0.0.1:4222 (the default URL), localhost:4222 and localhost:5222\r\n\tcheckPool(\"127.0.0.1:4222\", \"localhost:4222\", \"localhost:5222\")\r\n\r\n\t// Make sure that if client receives the same, it is not added again.\r\n\terr = c.parse(info)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\t// Pool should still contain 127.0.0.1:4222 (the default URL), localhost:4222 and localhost:5222\r\n\tcheckPool(\"127.0.0.1:4222\", \"localhost:4222\", \"localhost:5222\")\r\n\r\n\t// Receive a new URL\r\n\tinfo = []byte(\"INFO {\\\"connect_urls\\\":[\\\"localhost:4222\\\", \\\"localhost:5222\\\", \\\"localhost:6222\\\"]}\\r\\n\")\r\n\terr = c.parse(info)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\t// Pool now should contain 127.0.0.1:4222 (the default URL), localhost:4222, localhost:5222 and localhost:6222\r\n\tcheckPool(\"127.0.0.1:4222\", \"localhost:4222\", \"localhost:5222\", \"localhost:6222\")\r\n\r\n\t// Check that pool may be randomized on setup, but new URLs are always\r\n\t// added at end of pool.\r\n\tc.Opts.NoRandomize = false\r\n\tc.Opts.Servers = testServers\r\n\t// Reset the pool\r\n\tc.setupServerPool()\r\n\t// Reinitialize the parser\r\n\tc.ps = &parseState{}\r\n\t// Capture the pool sequence after randomization\r\n\turlsAfterPoolSetup := make([]string, 0, len(c.srvPool))\r\n\tfor _, srv := range c.srvPool {\r\n\t\turlsAfterPoolSetup = append(urlsAfterPoolSetup, srv.url.Host)\r\n\t}\r\n\tcheckNewURLsAddedRandomly := func() {\r\n\t\tt.Helper()\r\n\t\tvar ok bool\r\n\t\tfor i := 0; i < len(urlsAfterPoolSetup); i++ {\r\n\t\t\tif c.srvPool[i].url.Host != urlsAfterPoolSetup[i] {\r\n\t\t\t\tok = true\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\t\tif !ok {\r\n\t\t\tt.Fatalf(\"New URLs were not added randmonly: %q\", c.Servers())\r\n\t\t}\r\n\t}\r\n\t// Add new urls\r\n\tnewURLs := \"\\\"impA:4222\\\", \\\"impB:4222\\\", \\\"impC:4222\\\", \" +\r\n\t\t\"\\\"impD:4222\\\", \\\"impE:4222\\\", \\\"impF:4222\\\", \\\"impG:4222\\\", \" +\r\n\t\t\"\\\"impH:4222\\\", \\\"impI:4222\\\", \\\"impJ:4222\\\"\"\r\n\tinfo = []byte(\"INFO {\\\"connect_urls\\\":[\" + newURLs + \"]}\\r\\n\")\r\n\terr = c.parse(info)\r\n\tif err != nil || c.ps.state != OP_START {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\tcheckNewURLsAddedRandomly()\r\n\t// Check that we have not moved the first URL\r\n\tif u := c.srvPool[0].url.Host; u != urlsAfterPoolSetup[0] {\r\n\t\tt.Fatalf(\"Expected first URL to be %q, got %q\", urlsAfterPoolSetup[0], u)\r\n\t}\r\n}\r\n\r\nfunc TestConnServers(t *testing.T) {\r\n\topts := GetDefaultOptions()\r\n\tc := &Conn{Opts: opts}\r\n\tc.ps = &parseState{}\r\n\tc.setupServerPool()\r\n\r\n\tvalidateURLs := func(serverUrls []string, expectedUrls ...string) {\r\n\t\tvar found bool\r\n\t\tif len(serverUrls) != len(expectedUrls) {\r\n\t\t\tstackFatalf(t, \"Array should have %d elements, has %d\", len(expectedUrls), len(serverUrls))\r\n\t\t}\r\n\r\n\t\tfor _, ev := range expectedUrls {\r\n\t\t\tfound = false\r\n\t\t\tfor _, av := range serverUrls {\r\n\t\t\t\tif ev == av {\r\n\t\t\t\t\tfound = true\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif !found {\r\n\t\t\t\tstackFatalf(t, \"array is missing %q in %v\", ev, serverUrls)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// check the default url\r\n\tvalidateURLs(c.Servers(), \"nats://127.0.0.1:4222\")\r\n\tif len(c.DiscoveredServers()) != 0 {\r\n\t\tt.Fatalf(\"Expected no discovered servers\")\r\n\t}\r\n\r\n\t// Add a new URL\r\n\terr := c.parse([]byte(\"INFO {\\\"connect_urls\\\":[\\\"localhost:5222\\\"]}\\r\\n\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\r\n\t}\r\n\t// Server list should now contain both the default and the new url.\r\n\tvalidateURLs(c.Servers(), \"nats://127.0.0.1:4222\", \"nats://localhost:5222\")\r\n\t// Discovered servers should only contain the new url.\r\n\tvalidateURLs(c.DiscoveredServers(), \"nats://localhost:5222\")\r\n\r\n\t// verify user credentials are stripped out.\r\n\topts.Servers = []string{\"nats://user:pass@localhost:4333\", \"nats://token@localhost:4444\"}\r\n\tc = &Conn{Opts: opts}\r\n\tc.ps = &parseState{}\r\n\tc.setupServerPool()\r\n\r\n\tvalidateURLs(c.Servers(), \"nats://localhost:4333\", \"nats://localhost:4444\")\r\n}\r\n\r\nfunc TestConnAsyncCBDeadlock(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\tch := make(chan bool)\r\n\to := GetDefaultOptions()\r\n\to.Url = fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT)\r\n\to.ClosedCB = func(_ *Conn) {\r\n\t\tch <- true\r\n\t}\r\n\to.AsyncErrorCB = func(nc *Conn, sub *Subscription, err error) {\r\n\t\t// do something with nc that requires locking behind the scenes\r\n\t\t_ = nc.LastError()\r\n\t}\r\n\tnc, err := o.Connect()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Should have connected ok: %v\", err)\r\n\t}\r\n\r\n\ttotal := 300\r\n\twg := &sync.WaitGroup{}\r\n\twg.Add(total)\r\n\tfor i := 0; i < total; i++ {\r\n\t\tgo func() {\r\n\t\t\t// overwhelm asyncCB with errors\r\n\t\t\tnc.processErr(AUTHORIZATION_ERR)\r\n\t\t\twg.Done()\r\n\t\t}()\r\n\t}\r\n\twg.Wait()\r\n\r\n\tnc.Close()\r\n\tif e := Wait(ch); e != nil {\r\n\t\tt.Fatal(\"Deadlock\")\r\n\t}\r\n}\r\n\r\nfunc TestPingTimerLeakedOnClose(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n\t// There was a bug (issue #338) that if connection\r\n\t// was created and closed quickly, the pinger would\r\n\t// be created from a go-routine and would cause the\r\n\t// connection object to be retained until the ping\r\n\t// timer fired.\r\n\t// Wait a little bit and check if the timer is set.\r\n\t// With the defect it would be.\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\tnc.mu.Lock()\r\n\tpingTimerSet := nc.ptmr != nil\r\n\tnc.mu.Unlock()\r\n\tif pingTimerSet {\r\n\t\tt.Fatal(\"Pinger timer should not be set\")\r\n\t}\r\n}\r\n\r\nfunc TestNoEcho(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT)\r\n\r\n\tnc, err := Connect(url, NoEcho())\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tr := int32(0)\r\n\t_, err = nc.Subscribe(\"foo\", func(m *Msg) {\r\n\t\tatomic.AddInt32(&r, 1)\r\n\t})\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\terr = nc.Publish(\"foo\", []byte(\"Hello World\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\tnc.Flush()\r\n\r\n\tif nr := atomic.LoadInt32(&r); nr != 0 {\r\n\t\tt.Fatalf(\"Expected no messages echoed back, received %d\\n\", nr)\r\n\t}\r\n}\r\n\r\nfunc TestNoEchoOldServer(t *testing.T) {\r\n\topts := GetDefaultOptions()\r\n\topts.Url = DefaultURL\r\n\topts.NoEcho = true\r\n\r\n\tnc := &Conn{Opts: opts}\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\r\n\t}\r\n\r\n\t// Old style with no proto, meaning 0. We need Proto:1 for NoEcho support.\r\n\toldInfo := \"{\\\"server_id\\\":\\\"22\\\",\\\"version\\\":\\\"1.1.0\\\",\\\"go\\\":\\\"go1.10.2\\\",\\\"port\\\":4222,\\\"max_payload\\\":1048576}\"\r\n\r\n\terr := nc.processInfo(oldInfo)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing old style INFO: %v\\n\", err)\r\n\t}\r\n\r\n\t// Make sure connectProto generates an error.\r\n\t_, err = nc.connectProto()\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expected an error but got none\\n\")\r\n\t}\r\n}\r\n\r\n// Trust Server Tests\r\n\r\nvar (\r\n\toSeed = []byte(\"SOAL7GTNI66CTVVNXBNQMG6V2HTDRWC3HGEP7D2OUTWNWSNYZDXWFOX4SU\")\r\n\taSeed = []byte(\"SAAASUPRY3ONU4GJR7J5RUVYRUFZXG56F4WEXELLLORQ65AEPSMIFTOJGE\")\r\n\tuSeed = []byte(\"SUAMK2FG4MI6UE3ACF3FK3OIQBCEIEZV7NSWFFEW63UXMRLFM2XLAXK4GY\")\r\n\r\n\taJWT = \"eyJ0eXAiOiJqd3QiLCJhbGciOiJlZDI1NTE5In0.eyJqdGkiOiJLWjZIUVRXRlY3WkRZSFo3NklRNUhPM0pINDVRNUdJS0JNMzJTSENQVUJNNk5PNkU3TUhRIiwiaWF0IjoxNTQ0MDcxODg5LCJpc3MiOiJPRDJXMkk0TVZSQTVUR1pMWjJBRzZaSEdWTDNPVEtGV1FKRklYNFROQkVSMjNFNlA0NlMzNDVZWSIsInN1YiI6IkFBUFFKUVVQS1ZYR1c1Q1pINUcySEZKVUxZU0tERUxBWlJWV0pBMjZWRFpPN1dTQlVOSVlSRk5RIiwidHlwZSI6ImFjY291bnQiLCJuYXRzIjp7ImxpbWl0cyI6eyJzdWJzIjotMSwiY29ubiI6LTEsImltcG9ydHMiOi0xLCJleHBvcnRzIjotMSwiZGF0YSI6LTEsInBheWxvYWQiOi0xLCJ3aWxkY2FyZHMiOnRydWV9fX0.8o35JPQgvhgFT84Bi2Z-zAeSiLrzzEZn34sgr1DIBEDTwa-EEiMhvTeos9cvXxoZVCCadqZxAWVwS6paAMj8Bg\"\r\n\r\n\tuJWT = \"eyJ0eXAiOiJqd3QiLCJhbGciOiJlZDI1NTE5In0.eyJqdGkiOiJBSFQzRzNXRElDS1FWQ1FUWFJUTldPRlVVUFRWNE00RFZQV0JGSFpJQUROWEZIWEpQR0FBIiwiaWF0IjoxNTQ0MDcxODg5LCJpc3MiOiJBQVBRSlFVUEtWWEdXNUNaSDVHMkhGSlVMWVNLREVMQVpSVldKQTI2VkRaTzdXU0JVTklZUkZOUSIsInN1YiI6IlVBVDZCV0NTQ1dMVUtKVDZLNk1CSkpPRU9UWFo1QUpET1lLTkVWUkZDN1ZOTzZPQTQzTjRUUk5PIiwidHlwZSI6InVzZXIiLCJuYXRzIjp7InB1YiI6e30sInN1YiI6e319fQ._8A1XM88Q2kp7XVJZ42bQuO9E3QPsNAGKtVjAkDycj8A5PtRPby9UpqBUZzBwiJQQO3TUcD5GGqSvsMm6X8hCQ\"\r\n\r\n\tchained = `\r\n-----BEGIN NATS USER JWT-----\r\neyJ0eXAiOiJqd3QiLCJhbGciOiJlZDI1NTE5In0.eyJqdGkiOiJBSFQzRzNXRElDS1FWQ1FUWFJUTldPRlVVUFRWNE00RFZQV0JGSFpJQUROWEZIWEpQR0FBIiwiaWF0IjoxNTQ0MDcxODg5LCJpc3MiOiJBQVBRSlFVUEtWWEdXNUNaSDVHMkhGSlVMWVNLREVMQVpSVldKQTI2VkRaTzdXU0JVTklZUkZOUSIsInN1YiI6IlVBVDZCV0NTQ1dMVUtKVDZLNk1CSkpPRU9UWFo1QUpET1lLTkVWUkZDN1ZOTzZPQTQzTjRUUk5PIiwidHlwZSI6InVzZXIiLCJuYXRzIjp7InB1YiI6e30sInN1YiI6e319fQ._8A1XM88Q2kp7XVJZ42bQuO9E3QPsNAGKtVjAkDycj8A5PtRPby9UpqBUZzBwiJQQO3TUcD5GGqSvsMm6X8hCQ\r\n------END NATS USER JWT------\r\n\r\n************************* IMPORTANT *************************\r\nNKEY Seed printed below can be used to sign and prove identity.\r\nNKEYs are sensitive and should be treated as secrets.\r\n\r\n-----BEGIN USER NKEY SEED-----\r\nSUAMK2FG4MI6UE3ACF3FK3OIQBCEIEZV7NSWFFEW63UXMRLFM2XLAXK4GY\r\n------END USER NKEY SEED------\r\n`\r\n)\r\n\r\nfunc runTrustServer() *server.Server {\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\topts := natsserver.DefaultTestOptions\r\n\topts.Port = TEST_PORT\r\n\topts.TrustedKeys = []string{string(pub)}\r\n\ts := RunServerWithOptions(&opts)\r\n\tmr := &server.MemAccResolver{}\r\n\takp, _ := nkeys.FromSeed(aSeed)\r\n\tapub, _ := akp.PublicKey()\r\n\tmr.Store(string(apub), aJWT)\r\n\ts.SetAccountResolver(mr)\r\n\treturn s\r\n}\r\n\r\nfunc TestBasicUserJWTAuth(t *testing.T) {\r\n\tif server.VERSION[0] == '1' {\r\n\t\tt.Skip()\r\n\t}\r\n\tts := runTrustServer()\r\n\tdefer ts.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT)\r\n\t_, err := Connect(url)\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expecting an error on connect\")\r\n\t}\r\n\r\n\tjwtCB := func() (string, error) {\r\n\t\treturn uJWT, nil\r\n\t}\r\n\tsigCB := func(nonce []byte) ([]byte, error) {\r\n\t\tkp, _ := nkeys.FromSeed(uSeed)\r\n\t\tsig, _ := kp.Sign(nonce)\r\n\t\treturn sig, nil\r\n\t}\r\n\r\n\t// Try with user jwt but no sig\r\n\t_, err = Connect(url, UserJWT(jwtCB, nil))\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expecting an error on connect\")\r\n\t}\r\n\r\n\t// Try with user callback\r\n\t_, err = Connect(url, UserJWT(nil, sigCB))\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expecting an error on connect\")\r\n\t}\r\n\r\n\tnc, err := Connect(url, UserJWT(jwtCB, sigCB))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t}\r\n\tnc.Close()\r\n}\r\n\r\nfunc TestUserCredentialsTwoFiles(t *testing.T) {\r\n\tif server.VERSION[0] == '1' {\r\n\t\tt.Skip()\r\n\t}\r\n\tts := runTrustServer()\r\n\tdefer ts.Shutdown()\r\n\r\n\tuserJWTFile := createTmpFile(t, []byte(uJWT))\r\n\tdefer os.Remove(userJWTFile)\r\n\tuserSeedFile := createTmpFile(t, uSeed)\r\n\tdefer os.Remove(userSeedFile)\r\n\r\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT)\r\n\tnc, err := Connect(url, UserCredentials(userJWTFile, userSeedFile))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t}\r\n\tnc.Close()\r\n}\r\n\r\nfunc TestUserCredentialsChainedFile(t *testing.T) {\r\n\tif server.VERSION[0] == '1' {\r\n\t\tt.Skip()\r\n\t}\r\n\tts := runTrustServer()\r\n\tdefer ts.Shutdown()\r\n\r\n\tchainedFile := createTmpFile(t, []byte(chained))\r\n\tdefer os.Remove(chainedFile)\r\n\r\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT)\r\n\tnc, err := Connect(url, UserCredentials(chainedFile))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t}\r\n\tnc.Close()\r\n}\r\n\r\nfunc createNewUserKeys() (string, []byte) {\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tpriv, _ := kp.Seed()\r\n\treturn pub, priv\r\n}\r\n\r\nfunc TestExpiredUserCredentials(t *testing.T) {\r\n\t// The goal of this test was to check how a client with an expiring JWT\r\n\t// behaves. It should receive an async -ERR indicating that the auth\r\n\t// has expired, which will trigger reconnects. There, the lib should\r\n\t// received -ERR for auth violation in response to the CONNECT (instead\r\n\t// of the PONG). The library should close the connection after receiving\r\n\t// twice the same auth error.\r\n\t// If we use an actual JWT that expires, the way the JWT library expires\r\n\t// a JWT cause the server to send the async -ERR first but then accepts\r\n\t// the CONNECT (since JWT lib does not say that it has expired), but\r\n\t// when the server sets up the expire callback, that callback fires right\r\n\t// away and so client receives async -ERR again.\r\n\t// So for a deterministic test, we won't use an actual NATS Server.\r\n\t// Instead, we will use a mock that simply returns appropriate -ERR and\r\n\t// ensure the client behaves as expected.\r\n\tl, e := net.Listen(\"tcp\", \"127.0.0.1:0\")\r\n\tif e != nil {\r\n\t\tt.Fatal(\"Could not listen on an ephemeral port\")\r\n\t}\r\n\ttl := l.(*net.TCPListener)\r\n\tdefer tl.Close()\r\n\r\n\taddr := tl.Addr().(*net.TCPAddr)\r\n\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(1)\r\n\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tconnect := 0\r\n\t\tfor {\r\n\t\t\tconn, err := l.Accept()\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tdefer conn.Close()\r\n\r\n\t\t\tinfo := \"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"nonce\\\":\\\"anonce\\\"}\\r\\n\"\r\n\t\t\tconn.Write([]byte(info))\r\n\r\n\t\t\t// Read connect and ping commands sent from the client\r\n\t\t\tbr := bufio.NewReaderSize(conn, 10*1024)\r\n\t\t\tbr.ReadLine()\r\n\t\t\tbr.ReadLine()\r\n\r\n\t\t\tif connect++; connect == 1 {\r\n\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"%s%s\", _PONG_OP_, _CRLF_)))\r\n\t\t\t\ttime.Sleep(300 * time.Millisecond)\r\n\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", AUTHENTICATION_EXPIRED_ERR)))\r\n\t\t\t} else {\r\n\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", AUTHORIZATION_ERR)))\r\n\t\t\t}\r\n\t\t\tconn.Close()\r\n\t\t}\r\n\t}()\r\n\r\n\tch := make(chan bool)\r\n\terrCh := make(chan error, 10)\r\n\r\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port)\r\n\tnc, err := Connect(url,\r\n\t\tReconnectWait(25*time.Millisecond),\r\n\t\tReconnectJitter(0, 0),\r\n\t\tMaxReconnects(-1),\r\n\t\tErrorHandler(func(_ *Conn, _ *Subscription, e error) {\r\n\t\t\tselect {\r\n\t\t\tcase errCh <- e:\r\n\t\t\tdefault:\r\n\t\t\t}\r\n\t\t}),\r\n\t\tClosedHandler(func(nc *Conn) {\r\n\t\t\tch <- true\r\n\t\t}),\r\n\t)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// We should give up since we get the same error on both tries.\r\n\tif err := WaitTime(ch, 2*time.Second); err != nil {\r\n\t\tt.Fatal(\"Should have closed after multiple failed attempts.\")\r\n\t}\r\n\tif stats := nc.Stats(); stats.Reconnects > 2 {\r\n\t\tt.Fatalf(\"Expected at most 2 reconnects, got %d\", stats.Reconnects)\r\n\t}\r\n\t// We expect 3 errors, an AUTHENTICATION_EXPIRED_ERR, then 2 AUTHORIZATION_ERR\r\n\t// before the connection is closed.\r\n\tfor i := 0; i < 3; i++ {\r\n\t\tselect {\r\n\t\tcase e := <-errCh:\r\n\t\t\tif i == 0 && e != ErrAuthExpired {\r\n\t\t\t\tt.Fatalf(\"Expected error %q, got %q\", ErrAuthExpired, e)\r\n\t\t\t} else if i > 0 && e != ErrAuthorization {\r\n\t\t\t\tt.Fatalf(\"Expected error %q, got %q\", ErrAuthorization, e)\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif i == 0 {\r\n\t\t\t\tt.Fatalf(\"Missing %q error\", ErrAuthExpired)\r\n\t\t\t} else {\r\n\t\t\t\tt.Fatalf(\"Missing %q error\", ErrAuthorization)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t// We should not have any more error\r\n\tselect {\r\n\tcase e := <-errCh:\r\n\t\tt.Fatalf(\"Extra error: %v\", e)\r\n\tdefault:\r\n\t}\r\n\t// Close the listener and wait for go routine to end.\r\n\tl.Close()\r\n\twg.Wait()\r\n}\r\n\r\nfunc TestExpiredUserCredentialsRenewal(t *testing.T) {\r\n\tif server.VERSION[0] == '1' {\r\n\t\tt.Skip()\r\n\t}\r\n\tts := runTrustServer()\r\n\tdefer ts.Shutdown()\r\n\r\n\t// Create user credentials that will expire in a short timeframe.\r\n\tpub, priv := createNewUserKeys()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tnuc.Expires = time.Now().Add(time.Second).Unix()\r\n\takp, _ := nkeys.FromSeed(aSeed)\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error encoding user jwt: %v\", err)\r\n\t}\r\n\tcreds, err := jwt.FormatUserConfig(ujwt, priv)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error encoding credentials: %v\", err)\r\n\t}\r\n\tchainedFile := createTmpFile(t, creds)\r\n\tdefer os.Remove(chainedFile)\r\n\r\n\trch := make(chan bool)\r\n\r\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", TEST_PORT)\r\n\tnc, err := Connect(url,\r\n\t\tUserCredentials(chainedFile),\r\n\t\tReconnectWait(25*time.Millisecond),\r\n\t\tReconnectJitter(0, 0),\r\n\t\tMaxReconnects(2),\r\n\t\tReconnectHandler(func(nc *Conn) {\r\n\t\t\trch <- true\r\n\t\t}),\r\n\t)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Place new credentials underneath.\r\n\tnuc.Expires = time.Now().Add(30 * time.Second).Unix()\r\n\tujwt, err = nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error encoding user jwt: %v\", err)\r\n\t}\r\n\tcreds, err = jwt.FormatUserConfig(ujwt, priv)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error encoding credentials: %v\", err)\r\n\t}\r\n\tif err := ioutil.WriteFile(chainedFile, creds, 0666); err != nil {\r\n\t\tt.Fatalf(\"Error writing conf file: %v\", err)\r\n\t}\r\n\r\n\t// Make sure we get disconnected and reconnected first.\r\n\tif err := WaitTime(rch, 2*time.Second); err != nil {\r\n\t\tt.Fatal(\"Should have reconnected.\")\r\n\t}\r\n\r\n\t// We should not have been closed.\r\n\tif nc.IsClosed() {\r\n\t\tt.Fatal(\"Got disconnected when we should have reconnected.\")\r\n\t}\r\n\r\n\t// Check that we clear the lastErr that can cause the disconnect.\r\n\t// Our reconnect CB will happen before the clear. So check after a bit.\r\n\ttime.Sleep(50 * time.Millisecond)\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tif nc.current.lastErr != nil {\r\n\t\tt.Fatalf(\"Expected lastErr to be cleared, got %q\", nc.current.lastErr)\r\n\t}\r\n}\r\n\r\n// If we are using TLS and have multiple servers we try to match the IP\r\n// from a discovered server with the expected hostname for certs without IP\r\n// designations. In certain cases where there is a not authorized error and\r\n// we were trying the second server with the IP only and getting an error\r\n// that was hard to understand for the end user. This did require\r\n// Opts.Secure = false, but the fix removed the check on Opts.Secure to decide\r\n// if we need to save off the hostname that we connected to first.\r\nfunc TestUserCredentialsChainedFileNotFoundError(t *testing.T) {\r\n\tif server.VERSION[0] == '1' {\r\n\t\tt.Skip()\r\n\t}\r\n\t// Setup opts for both servers.\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\topts := natsserver.DefaultTestOptions\r\n\topts.Port = -1\r\n\topts.Cluster.Port = -1\r\n\topts.TrustedKeys = []string{string(pub)}\r\n\ttc := &server.TLSConfigOpts{\r\n\t\tCertFile: \"./test/configs/certs/server_noip.pem\",\r\n\t\tKeyFile:  \"./test/configs/certs/key_noip.pem\",\r\n\t}\r\n\tvar err error\r\n\tif opts.TLSConfig, err = server.GenTLSConfig(tc); err != nil {\r\n\t\tpanic(\"Can't build TLCConfig\")\r\n\t}\r\n\r\n\t// copy the opts for the second server.\r\n\topts2 := opts\r\n\r\n\tsa := RunServerWithOptions(&opts)\r\n\tdefer sa.Shutdown()\r\n\r\n\trouteAddr := fmt.Sprintf(\"nats-route://%s:%d\", opts.Cluster.Host, opts.Cluster.Port)\r\n\trurl, _ := url.Parse(routeAddr)\r\n\topts2.Routes = []*url.URL{rurl}\r\n\r\n\tsb := RunServerWithOptions(&opts2)\r\n\tdefer sb.Shutdown()\r\n\r\n\twait := time.Now().Add(2 * time.Second)\r\n\tfor time.Now().Before(wait) {\r\n\t\tsanr := sa.NumRoutes()\r\n\t\tsbnr := sb.NumRoutes()\r\n\t\tif sanr == 1 && sbnr == 1 {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\ttime.Sleep(50 * time.Millisecond)\r\n\t}\r\n\r\n\t// Make sure we get the right error here.\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://localhost:%d\", opts.Port),\r\n\t\tRootCAs(\"./test/configs/certs/ca.pem\"),\r\n\t\tUserCredentials(\"filenotfound.creds\"))\r\n\r\n\tif err == nil {\r\n\t\tnc.Close()\r\n\t\tt.Fatalf(\"Expected an error on missing credentials file\")\r\n\t}\r\n\tif !strings.Contains(err.Error(), \"no such file or directory\") {\r\n\t\tt.Fatalf(\"Expected a missing file error, got %q\", err)\r\n\t}\r\n}\r\n\r\nfunc TestNkeyAuth(t *testing.T) {\r\n\tif server.VERSION[0] == '1' {\r\n\t\tt.Skip()\r\n\t}\r\n\r\n\tseed := []byte(\"SUAKYRHVIOREXV7EUZTBHUHL7NUMHPMAS7QMDU3GTIUWEI5LDNOXD43IZY\")\r\n\tkp, _ := nkeys.FromSeed(seed)\r\n\tpub, _ := kp.PublicKey()\r\n\r\n\tsopts := natsserver.DefaultTestOptions\r\n\tsopts.Port = TEST_PORT\r\n\tsopts.Nkeys = []*server.NkeyUser{&server.NkeyUser{Nkey: string(pub)}}\r\n\tts := RunServerWithOptions(&sopts)\r\n\tdefer ts.Shutdown()\r\n\r\n\topts := reconnectOpts\r\n\tif _, err := opts.Connect(); err == nil {\r\n\t\tt.Fatalf(\"Expected to fail with no nkey auth defined\")\r\n\t}\r\n\topts.Nkey = string(pub)\r\n\tif _, err := opts.Connect(); err != ErrNkeyButNoSigCB {\r\n\t\tt.Fatalf(\"Expected to fail with nkey defined but no signature callback, got %v\", err)\r\n\t}\r\n\tbadSign := func(nonce []byte) ([]byte, error) {\r\n\t\treturn []byte(\"VALID?\"), nil\r\n\t}\r\n\topts.SignatureCB = badSign\r\n\tif _, err := opts.Connect(); err == nil {\r\n\t\tt.Fatalf(\"Expected to fail with nkey and bad signature callback\")\r\n\t}\r\n\tgoodSign := func(nonce []byte) ([]byte, error) {\r\n\t\tsig, err := kp.Sign(nonce)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Failed signing nonce: %v\", err)\r\n\t\t}\r\n\t\treturn sig, nil\r\n\t}\r\n\topts.SignatureCB = goodSign\r\n\tnc, err := opts.Connect()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to succeed but got %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Now disconnect by killing the server and restarting.\r\n\tts.Shutdown()\r\n\tts = RunServerWithOptions(&sopts)\r\n\tdefer ts.Shutdown()\r\n\r\n\tif err := nc.FlushTimeout(5 * time.Second); err != nil {\r\n\t\tt.Fatalf(\"Error on Flush: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc createTmpFile(t *testing.T, content []byte) string {\r\n\tt.Helper()\r\n\tconf, err := ioutil.TempFile(\"\", \"\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating conf file: %v\", err)\r\n\t}\r\n\tfName := conf.Name()\r\n\tconf.Close()\r\n\tif err := ioutil.WriteFile(fName, content, 0666); err != nil {\r\n\t\tos.Remove(fName)\r\n\t\tt.Fatalf(\"Error writing conf file: %v\", err)\r\n\t}\r\n\treturn fName\r\n}\r\n\r\nfunc TestNKeyOptionFromSeed(t *testing.T) {\r\n\tif _, err := NkeyOptionFromSeed(\"file_that_does_not_exist\"); err == nil {\r\n\t\tt.Fatal(\"Expected error got none\")\r\n\t}\r\n\r\n\tseedFile := createTmpFile(t, []byte(`\r\n\t\t# No seed\r\n\t\tTHIS_NOT_A_NKEY_SEED\r\n\t`))\r\n\tdefer os.Remove(seedFile)\r\n\tif _, err := NkeyOptionFromSeed(seedFile); err == nil || !strings.Contains(err.Error(), \"seed found\") {\r\n\t\tt.Fatalf(\"Expected error about seed not found, got %v\", err)\r\n\t}\r\n\tos.Remove(seedFile)\r\n\r\n\tseedFile = createTmpFile(t, []byte(`\r\n\t\t# Invalid seed\r\n\t\tSUBADSEED\r\n\t`))\r\n\t// Make sure that we detect SU (trim space) but it still fails because\r\n\t// this is not a valid NKey.\r\n\tif _, err := NkeyOptionFromSeed(seedFile); err == nil || strings.Contains(err.Error(), \"seed found\") {\r\n\t\tt.Fatalf(\"Expected error about invalid key, got %v\", err)\r\n\t}\r\n\tos.Remove(seedFile)\r\n\r\n\tkp, _ := nkeys.CreateUser()\r\n\tseed, _ := kp.Seed()\r\n\tseedFile = createTmpFile(t, seed)\r\n\topt, err := NkeyOptionFromSeed(seedFile)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error: %v\", err)\r\n\t}\r\n\r\n\tl, e := net.Listen(\"tcp\", \"127.0.0.1:0\")\r\n\tif e != nil {\r\n\t\tt.Fatal(\"Could not listen on an ephemeral port\")\r\n\t}\r\n\ttl := l.(*net.TCPListener)\r\n\tdefer tl.Close()\r\n\r\n\taddr := tl.Addr().(*net.TCPAddr)\r\n\r\n\tch := make(chan bool, 1)\r\n\terrCh := make(chan error, 1)\r\n\trs := func(ch chan bool) {\r\n\t\tconn, err := l.Accept()\r\n\t\tif err != nil {\r\n\t\t\terrCh <- fmt.Errorf(\"error accepting client connection: %v\", err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\tdefer conn.Close()\r\n\t\tinfo := \"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"nonce\\\":\\\"anonce\\\"}\\r\\n\"\r\n\t\tconn.Write([]byte(info))\r\n\r\n\t\t// Read connect and ping commands sent from the client\r\n\t\tbr := bufio.NewReaderSize(conn, 10*1024)\r\n\t\tline, _, err := br.ReadLine()\r\n\t\tif err != nil {\r\n\t\t\terrCh <- fmt.Errorf(\"expected CONNECT and PING from client, got: %s\", err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// If client got an error reading the seed, it will not send it\r\n\t\tif bytes.Contains(line, []byte(`\"sig\":`)) {\r\n\t\t\tconn.Write([]byte(\"PONG\\r\\n\"))\r\n\t\t} else {\r\n\t\t\tconn.Write([]byte(`-ERR go away\\r\\n`))\r\n\t\t\tconn.Close()\r\n\t\t}\r\n\t\t// Now wait to be notified that we can finish\r\n\t\t<-ch\r\n\t\terrCh <- nil\r\n\t}\r\n\tgo rs(ch)\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port), opt)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n\tclose(ch)\r\n\r\n\tcheckErrChannel(t, errCh)\r\n\r\n\t// Now that option is already created, change content of file\r\n\tioutil.WriteFile(seedFile, []byte(`xxxxx`), 0666)\r\n\tch = make(chan bool, 1)\r\n\tgo rs(ch)\r\n\r\n\tif _, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port), opt); err == nil {\r\n\t\tt.Fatal(\"Expected error, got none\")\r\n\t}\r\n\tclose(ch)\r\n\tcheckErrChannel(t, errCh)\r\n}\r\n\r\nfunc TestLookupHostResultIsRandomized(t *testing.T) {\r\n\torgAddrs, err := net.LookupHost(\"localhost\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error looking up host: %v\", err)\r\n\t}\r\n\r\n\t// We actually want the IPv4 and IPv6 addresses, so lets make sure.\r\n\tif !reflect.DeepEqual(orgAddrs, []string{\"::1\", \"127.0.0.1\"}) {\r\n\t\tt.Skip(\"Was looking for IPv4 and IPv6 addresses for localhost to perform test\")\r\n\t}\r\n\r\n\topts := natsserver.DefaultTestOptions\r\n\topts.Host = \"127.0.0.1\"\r\n\topts.Port = TEST_PORT\r\n\ts1 := RunServerWithOptions(&opts)\r\n\tdefer s1.Shutdown()\r\n\r\n\topts.Host = \"::1\"\r\n\ts2 := RunServerWithOptions(&opts)\r\n\tdefer s2.Shutdown()\r\n\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tnc, err := Connect(fmt.Sprintf(\"localhost:%d\", TEST_PORT))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\tif ncls := s1.NumClients(); ncls < 35 || ncls > 65 {\r\n\t\tt.Fatalf(\"Does not seem balanced between multiple servers: s1:%d, s2:%d\", s1.NumClients(), s2.NumClients())\r\n\t}\r\n}\r\n\r\nfunc TestLookupHostResultIsNotRandomizedWithNoRandom(t *testing.T) {\r\n\torgAddrs, err := net.LookupHost(\"localhost\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error looking up host: %v\", err)\r\n\t}\r\n\r\n\t// We actually want the IPv4 and IPv6 addresses, so lets make sure.\r\n\tif !reflect.DeepEqual(orgAddrs, []string{\"::1\", \"127.0.0.1\"}) {\r\n\t\tt.Skip(\"Was looking for IPv4 and IPv6 addresses for localhost to perform test\")\r\n\t}\r\n\r\n\topts := natsserver.DefaultTestOptions\r\n\topts.Host = orgAddrs[0]\r\n\topts.Port = TEST_PORT\r\n\ts1 := RunServerWithOptions(&opts)\r\n\tdefer s1.Shutdown()\r\n\r\n\topts.Host = orgAddrs[1]\r\n\ts2 := RunServerWithOptions(&opts)\r\n\tdefer s2.Shutdown()\r\n\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tnc, err := Connect(fmt.Sprintf(\"localhost:%d\", TEST_PORT), DontRandomize())\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\tif ncls := s1.NumClients(); ncls != 100 {\r\n\t\tt.Fatalf(\"Expected all clients on first server, only got %d of 100\", ncls)\r\n\t}\r\n}\r\n\r\nfunc TestConnectedAddr(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\tvar nc *Conn\r\n\tif addr := nc.ConnectedAddr(); addr != _EMPTY_ {\r\n\t\tt.Fatalf(\"Expected empty result for nil connection, got %q\", addr)\r\n\t}\r\n\tnc, err := Connect(fmt.Sprintf(\"localhost:%d\", TEST_PORT))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error connecting: %v\", err)\r\n\t}\r\n\texpected := s.Addr().String()\r\n\tif addr := nc.ConnectedAddr(); addr != expected {\r\n\t\tt.Fatalf(\"Expected address %q, got %q\", expected, addr)\r\n\t}\r\n\tnc.Close()\r\n\tif addr := nc.ConnectedAddr(); addr != _EMPTY_ {\r\n\t\tt.Fatalf(\"Expected empty result for closed connection, got %q\", addr)\r\n\t}\r\n}\r\n\r\nfunc TestSubscribeSyncRace(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"127.0.0.1:%d\", TEST_PORT))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tgo func() {\r\n\t\ttime.Sleep(time.Millisecond)\r\n\t\tnc.Close()\r\n\t}()\r\n\r\n\tsubj := \"foo.sync.race\"\r\n\tfor i := 0; i < 10000; i++ {\r\n\t\tif _, err := nc.SubscribeSync(subj); err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tif _, err := nc.QueueSubscribeSync(subj, \"gc\"); err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestBadSubjectsAndQueueNames(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"127.0.0.1:%d\", TEST_PORT))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error connecting: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Make sure we get errors on bad subjects (spaces, etc)\r\n\t// We want the client to protect the user.\r\n\tbadSubs := []string{\"foo bar\", \"foo..bar\", \".foo\", \"bar.baz.\", \"baz\\t.foo\"}\r\n\tfor _, subj := range badSubs {\r\n\t\tif _, err := nc.SubscribeSync(subj); err != ErrBadSubject {\r\n\t\t\tt.Fatalf(\"Expected an error of ErrBadSubject for %q, got %v\", subj, err)\r\n\t\t}\r\n\t}\r\n\r\n\tbadQueues := []string{\"foo group\", \"group\\t1\", \"g1\\r\\n2\"}\r\n\tfor _, q := range badQueues {\r\n\t\tif _, err := nc.QueueSubscribeSync(\"foo\", q); err != ErrBadQueueName {\r\n\t\t\tt.Fatalf(\"Expected an error of ErrBadQueueName for %q, got %v\", q, err)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc BenchmarkNextMsgNoTimeout(b *testing.B) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\tncp, err := Connect(fmt.Sprintf(\"127.0.0.1:%d\", TEST_PORT))\r\n\tif err != nil {\r\n\t\tb.Fatalf(\"Error connecting: %v\", err)\r\n\t}\r\n\tncs, err := Connect(fmt.Sprintf(\"127.0.0.1:%d\", TEST_PORT), SyncQueueLen(b.N))\r\n\tif err != nil {\r\n\t\tb.Fatalf(\"Error connecting: %v\", err)\r\n\t}\r\n\r\n\t// Test processing speed so no long subject or payloads.\r\n\tsubj := \"a\"\r\n\r\n\tsub, err := ncs.SubscribeSync(subj)\r\n\tif err != nil {\r\n\t\tb.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tncs.Flush()\r\n\r\n\t// Set it up so we can internally queue all the messages.\r\n\tsub.SetPendingLimits(b.N, b.N*1000)\r\n\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\tncp.Publish(subj, nil)\r\n\t}\r\n\tncp.Flush()\r\n\r\n\t// Wait for them to all be queued up, testing NextMsg not server here.\r\n\t// Only wait at most one second.\r\n\twait := time.Now().Add(time.Second)\r\n\tfor time.Now().Before(wait) {\r\n\t\tnm, _, err := sub.Pending()\r\n\t\tif err != nil {\r\n\t\t\tb.Fatalf(\"Error on Pending() - %v\", err)\r\n\t\t}\r\n\t\tif nm >= b.N {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\ttime.Sleep(10 * time.Millisecond)\r\n\t}\r\n\r\n\tb.ResetTimer()\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\tif _, err := sub.NextMsg(10 * time.Millisecond); err != nil {\r\n\t\t\tb.Fatalf(\"Error getting message[%d]: %v\", i, err)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestAuthErrorOnReconnect(t *testing.T) {\r\n\t// This is a bit of an artificial test, but it is to demonstrate\r\n\t// that if the client is disconnected from a server (not due to an auth error),\r\n\t// it will still correctly stop the reconnection logic if it gets twice an\r\n\t// auth error from the same server.\r\n\r\n\to1 := natsserver.DefaultTestOptions\r\n\to1.Port = -1\r\n\ts1 := RunServerWithOptions(&o1)\r\n\tdefer s1.Shutdown()\r\n\r\n\to2 := natsserver.DefaultTestOptions\r\n\to2.Port = -1\r\n\to2.Username = \"ivan\"\r\n\to2.Password = \"pwd\"\r\n\ts2 := RunServerWithOptions(&o2)\r\n\tdefer s2.Shutdown()\r\n\r\n\tdch := make(chan bool)\r\n\tcch := make(chan bool)\r\n\r\n\turls := fmt.Sprintf(\"nats://%s:%d, nats://%s:%d\", o1.Host, o1.Port, o2.Host, o2.Port)\r\n\tnc, err := Connect(urls,\r\n\t\tReconnectWait(25*time.Millisecond),\r\n\t\tReconnectJitter(0, 0),\r\n\t\tMaxReconnects(-1),\r\n\t\tDontRandomize(),\r\n\t\tDisconnectErrHandler(func(_ *Conn, e error) {\r\n\t\t\tdch <- true\r\n\t\t}),\r\n\t\tClosedHandler(func(_ *Conn) {\r\n\t\t\tcch <- true\r\n\t\t}))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect, got err: %v\\n\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\ts1.Shutdown()\r\n\r\n\t// wait for disconnect\r\n\tif e := WaitTime(dch, 5*time.Second); e != nil {\r\n\t\tt.Fatal(\"Did not receive a disconnect callback message\")\r\n\t}\r\n\r\n\t// Wait for ClosedCB\r\n\tif e := WaitTime(cch, 5*time.Second); e != nil {\r\n\t\treconnects := nc.Stats().Reconnects\r\n\t\tt.Fatalf(\"Did not receive a closed callback message, #reconnects: %v\", reconnects)\r\n\t}\r\n\r\n\t// We should have stopped after 2 reconnects.\r\n\tif reconnects := nc.Stats().Reconnects; reconnects != 2 {\r\n\t\tt.Fatalf(\"Expected 2 reconnects, got %v\", reconnects)\r\n\t}\r\n\r\n\t// Expect connection to be closed...\r\n\tif !nc.IsClosed() {\r\n\t\tt.Fatalf(\"Wrong status: %d\\n\", nc.Status())\r\n\t}\r\n}\r\n\r\nfunc TestStatsRace(t *testing.T) {\r\n\to := natsserver.DefaultTestOptions\r\n\to.Port = -1\r\n\ts := RunServerWithOptions(&o)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://%s:%d\", o.Host, o.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(1)\r\n\tch := make(chan bool)\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\tselect {\r\n\t\t\tcase <-ch:\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t\tnc.Stats()\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\tnc.Subscribe(\"foo\", func(_ *Msg) {})\r\n\tfor i := 0; i < 1000; i++ {\r\n\t\tnc.Publish(\"foo\", []byte(\"hello\"))\r\n\t}\r\n\r\n\tclose(ch)\r\n\twg.Wait()\r\n}\r\n\r\nfunc TestRequestLeaksMapEntries(t *testing.T) {\r\n\to := natsserver.DefaultTestOptions\r\n\to.Port = -1\r\n\ts := RunServerWithOptions(&o)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://%s:%d\", o.Host, o.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tresponse := []byte(\"I will help you\")\r\n\tnc.Subscribe(\"foo\", func(m *Msg) {\r\n\t\tnc.Publish(m.Reply, response)\r\n\t})\r\n\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tmsg, err := nc.Request(\"foo\", nil, 500*time.Millisecond)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Received an error on Request test: %s\", err)\r\n\t\t}\r\n\t\tif !bytes.Equal(msg.Data, response) {\r\n\t\t\tt.Fatalf(\"Received invalid response\")\r\n\t\t}\r\n\t}\r\n\tnc.mu.Lock()\r\n\tnum := len(nc.respMap)\r\n\tnc.mu.Unlock()\r\n\tif num != 0 {\r\n\t\tt.Fatalf(\"Expected 0 entries in response map, got %d\", num)\r\n\t}\r\n}\r\n\r\nfunc TestRequestMultipleReplies(t *testing.T) {\r\n\to := natsserver.DefaultTestOptions\r\n\to.Port = -1\r\n\ts := RunServerWithOptions(&o)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://%s:%d\", o.Host, o.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tresponse := []byte(\"I will help you\")\r\n\tnc.Subscribe(\"foo\", func(m *Msg) {\r\n\t\tm.Respond(response)\r\n\t\tm.Respond(response)\r\n\t})\r\n\tnc.Flush()\r\n\r\n\tnc2, err := Connect(fmt.Sprintf(\"nats://%s:%d\", o.Host, o.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\terrCh := make(chan error, 1)\r\n\t// Send a request on bar and expect nothing\r\n\tgo func() {\r\n\t\tif m, err := nc2.Request(\"bar\", nil, 500*time.Millisecond); m != nil || err == nil {\r\n\t\t\terrCh <- fmt.Errorf(\"Expected no reply, got m=%+v err=%v\", m, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\terrCh <- nil\r\n\t}()\r\n\r\n\t// Send a request on foo, we use only one of the 2 replies\r\n\tif _, err := nc2.Request(\"foo\", nil, time.Second); err != nil {\r\n\t\tt.Fatalf(\"Received an error on Request test: %s\", err)\r\n\t}\r\n\tif e := <-errCh; e != nil {\r\n\t\tt.Fatal(e.Error())\r\n\t}\r\n}\r\n\r\nfunc TestRequestInit(t *testing.T) {\r\n\to := natsserver.DefaultTestOptions\r\n\to.Port = -1\r\n\ts := RunServerWithOptions(&o)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(s.ClientURL())\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tif _, err := nc.Subscribe(\"foo\", func(m *Msg) {\r\n\t\tm.Respond([]byte(\"reply\"))\r\n\t}); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\t// Artificially change the status to something that would make the internal subscribe\r\n\t// call fail. Don't use CLOSED because then there is a risk that the flusher() goes away\r\n\t// and so the rest of the test would fail.\r\n\tnc.mu.Lock()\r\n\torgStatus := nc.status\r\n\tnc.status = DRAINING_SUBS\r\n\tnc.mu.Unlock()\r\n\r\n\tif _, err := nc.Request(\"foo\", []byte(\"request\"), 50*time.Millisecond); err == nil {\r\n\t\tt.Fatal(\"Expected error, got none\")\r\n\t}\r\n\r\n\tnc.mu.Lock()\r\n\tnc.status = orgStatus\r\n\tnc.mu.Unlock()\r\n\r\n\tif _, err := nc.Request(\"foo\", []byte(\"request\"), 500*time.Millisecond); err != nil {\r\n\t\tt.Fatalf(\"Error on request: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestGetRTT(t *testing.T) {\r\n\ts := RunServerOnPort(-1)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(s.ClientURL(), ReconnectWait(10*time.Millisecond), ReconnectJitter(0, 0))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect to server, got %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\trtt, err := nc.RTT()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Unexpected error getting RTT: %v\", err)\r\n\t}\r\n\tif rtt > time.Second {\r\n\t\tt.Fatalf(\"RTT value too large: %v\", rtt)\r\n\t}\r\n\t// We should not get a value when in any disconnected state.\r\n\ts.Shutdown()\r\n\ttime.Sleep(5 * time.Millisecond)\r\n\tif _, err = nc.RTT(); err != ErrDisconnected {\r\n\t\tt.Fatalf(\"Expected disconnected error getting RTT when disconnected, got %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestGetClientIP(t *testing.T) {\r\n\ts := RunServerOnPort(-1)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := Connect(s.ClientURL())\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to connect to server, got %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tip, err := nc.GetClientIP()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Got error looking up IP: %v\", err)\r\n\t}\r\n\tif !ip.IsLoopback() {\r\n\t\tt.Fatalf(\"Expected a loopback IP, got %v\", ip)\r\n\t}\r\n\tnc.Close()\r\n\tif _, err := nc.GetClientIP(); err != ErrConnectionClosed {\r\n\t\tt.Fatalf(\"Expected a connection closed error, got %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestNoPanicOnSrvPoolSizeChanging(t *testing.T) {\r\n\tlisteners := []net.Listener{}\r\n\tports := []int{}\r\n\r\n\tfor i := 0; i < 3; i++ {\r\n\t\tl, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not listen on an ephemeral port: %v\", err)\r\n\t\t}\r\n\t\tdefer l.Close()\r\n\t\ttl := l.(*net.TCPListener)\r\n\t\tports = append(ports, tl.Addr().(*net.TCPAddr).Port)\r\n\t\tlisteners = append(listeners, l)\r\n\t}\r\n\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(len(listeners))\r\n\r\n\tconnect := int32(0)\r\n\tsrv := func(l net.Listener) {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\tconn, err := l.Accept()\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tdefer conn.Close()\r\n\r\n\t\t\tvar info string\r\n\r\n\t\t\treject := atomic.AddInt32(&connect, 1) <= 2\r\n\t\t\tif reject {\r\n\t\t\t\t// Sends a list of 3 servers, where the second does not actually run.\r\n\t\t\t\t// This server is going to reject the connect (with auth error), so\r\n\t\t\t\t// client will move to 2nd, fail, then go to third...\r\n\t\t\t\tinfo = fmt.Sprintf(\"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"connect_urls\\\":[\\\"127.0.0.1:%d\\\",\\\"127.0.0.1:%d\\\",\\\"127.0.0.1:%d\\\"]}\\r\\n\",\r\n\t\t\t\t\tports[0], ports[1], ports[2])\r\n\t\t\t} else {\r\n\t\t\t\t// This third server will return the INFO with only the original server\r\n\t\t\t\t// and the third one, which will make the srvPool size shrink down to 2.\r\n\t\t\t\tinfo = fmt.Sprintf(\"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"connect_urls\\\":[\\\"127.0.0.1:%d\\\",\\\"127.0.0.1:%d\\\"]}\\r\\n\",\r\n\t\t\t\t\tports[0], ports[2])\r\n\t\t\t}\r\n\t\t\tconn.Write([]byte(info))\r\n\r\n\t\t\t// Read connect and ping commands sent from the client\r\n\t\t\tbr := bufio.NewReaderSize(conn, 10*1024)\r\n\t\t\tbr.ReadLine()\r\n\t\t\tbr.ReadLine()\r\n\r\n\t\t\tif reject {\r\n\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", AUTHORIZATION_ERR)))\r\n\t\t\t\tconn.Close()\r\n\t\t\t} else {\r\n\t\t\t\tconn.Write([]byte(pongProto))\r\n\t\t\t\tbr.ReadLine()\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tfor _, l := range listeners {\r\n\t\tgo srv(l)\r\n\t}\r\n\r\n\ttime.Sleep(250 * time.Millisecond)\r\n\r\n\tnc, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", ports[0]))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n\tfor _, l := range listeners {\r\n\t\tl.Close()\r\n\t}\r\n\twg.Wait()\r\n}\r\n\r\nfunc TestReconnectWaitJitter(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\trch := make(chan time.Time, 1)\r\n\tnc, err := Connect(s.ClientURL(),\r\n\t\tReconnectWait(100*time.Millisecond),\r\n\t\tReconnectJitter(500*time.Millisecond, 0),\r\n\t\tReconnectHandler(func(_ *Conn) {\r\n\t\t\trch <- time.Now()\r\n\t\t}),\r\n\t)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error during connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\ts.Shutdown()\r\n\tstart := time.Now()\r\n\t// Wait a bit so that the library tries a first time without waiting.\r\n\ttime.Sleep(50 * time.Millisecond)\r\n\ts = RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\tselect {\r\n\tcase end := <-rch:\r\n\t\tdur := end.Sub(start)\r\n\t\t// We should wait at least the reconnect wait + random up to 500ms.\r\n\t\t// Account for a bit of variation since we rely on the reconnect\r\n\t\t// handler which is not invoked in place.\r\n\t\tif dur < 90*time.Millisecond || dur > 800*time.Millisecond {\r\n\t\t\tt.Fatalf(\"Wrong wait: %v\", dur)\r\n\t\t}\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatalf(\"Should have reconnected\")\r\n\t}\r\n\tnc.Close()\r\n\r\n\t// Use a long reconnect wait\r\n\tnc, err = Connect(s.ClientURL(), ReconnectWait(10*time.Minute))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error during connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Cause a disconnect\r\n\ts.Shutdown()\r\n\t// Wait a bit for the reconnect loop to go into wait mode.\r\n\ttime.Sleep(50 * time.Millisecond)\r\n\ts = RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\t// Now close and expect the reconnect go routine to return..\r\n\tnc.Close()\r\n\t// Wait a bit to give a chance for the go routine to exit.\r\n\ttime.Sleep(50 * time.Millisecond)\r\n\tbuf := make([]byte, 100000)\r\n\tn := runtime.Stack(buf, true)\r\n\tif strings.Contains(string(buf[:n]), \"doReconnect\") {\r\n\t\tt.Fatalf(\"doReconnect go routine still running:\\n%s\", buf[:n])\r\n\t}\r\n}\r\n\r\nfunc TestCustomReconnectDelay(t *testing.T) {\r\n\ts := RunServerOnPort(TEST_PORT)\r\n\tdefer s.Shutdown()\r\n\r\n\texpectedAttempt := 1\r\n\terrCh := make(chan error, 1)\r\n\tcCh := make(chan bool, 1)\r\n\tnc, err := Connect(s.ClientURL(),\r\n\t\tCustomReconnectDelay(func(n int) time.Duration {\r\n\t\t\tvar err error\r\n\t\t\tvar delay time.Duration\r\n\t\t\tif n != expectedAttempt {\r\n\t\t\t\terr = fmt.Errorf(\"Expected attempt to be %v, got %v\", expectedAttempt, n)\r\n\t\t\t} else {\r\n\t\t\t\texpectedAttempt++\r\n\t\t\t\tif n <= 4 {\r\n\t\t\t\t\tdelay = 100 * time.Millisecond\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif err != nil {\r\n\t\t\t\tselect {\r\n\t\t\t\tcase errCh <- err:\r\n\t\t\t\tdefault:\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\treturn delay\r\n\t\t}),\r\n\t\tMaxReconnects(4),\r\n\t\tClosedHandler(func(_ *Conn) {\r\n\t\t\tcCh <- true\r\n\t\t}),\r\n\t)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error during connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Cause disconnect\r\n\ts.Shutdown()\r\n\r\n\t// We should be trying to reconnect 4 times\r\n\tstart := time.Now()\r\n\r\n\t// Wait on error or completion of test.\r\n\tselect {\r\n\tcase e := <-errCh:\r\n\t\tif e != nil {\r\n\t\t\tt.Fatal(e.Error())\r\n\t\t}\r\n\tcase <-cCh:\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatalf(\"No CB invoked\")\r\n\t}\r\n\tif dur := time.Since(start); dur >= 500*time.Millisecond {\r\n\t\tt.Fatalf(\"Waited too long on each reconnect: %v\", dur)\r\n\t}\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/client/nats/nats_test.go b/client/nats/nats_test.go
--- a/client/nats/nats_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/client/nats/nats_test.go	(date 1665399050020)
@@ -38,7 +38,7 @@
 
 	"github.com/kubemq-io/broker/server/gnatsd/server"
 	natsserver "github.com/kubemq-io/broker/server/gnatsd/test"
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/test/service_latency_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage test\r\n\r\nimport (\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"math/rand\"\r\n\t\"os\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/kubemq-io/broker/client/nats\"\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/server\"\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\n// Used to setup superclusters for tests.\r\ntype supercluster struct {\r\n\tclusters []*cluster\r\n}\r\n\r\nfunc (sc *supercluster) shutdown() {\r\n\tfor _, c := range sc.clusters {\r\n\t\tshutdownCluster(c)\r\n\t}\r\n}\r\n\r\nconst digits = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\r\nconst base = 36\r\nconst cnlen = 8\r\n\r\nfunc randClusterName() string {\r\n\tvar name []byte\r\n\trn := rand.Int63()\r\n\tfor i := 0; i < cnlen; i++ {\r\n\t\tname = append(name, digits[rn%base])\r\n\t\trn /= base\r\n\t}\r\n\treturn string(name[:cnlen])\r\n}\r\n\r\nfunc createSuperCluster(t *testing.T, numServersPer, numClusters int) *supercluster {\r\n\tclusters := []*cluster{}\r\n\r\n\tfor i := 0; i < numClusters; i++ {\r\n\t\t// Pick cluster name and setup default accounts.\r\n\t\tc := createClusterEx(t, true, randClusterName(), numServersPer, clusters...)\r\n\t\tclusters = append(clusters, c)\r\n\t}\r\n\treturn &supercluster{clusters}\r\n}\r\n\r\nfunc (sc *supercluster) setupLatencyTracking(t *testing.T, p int) {\r\n\tt.Helper()\r\n\tfor _, c := range sc.clusters {\r\n\t\tfor _, s := range c.servers {\r\n\t\t\tfoo, err := s.LookupAccount(\"FOO\")\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error looking up account 'FOO': %v\", err)\r\n\t\t\t}\r\n\t\t\tbar, err := s.LookupAccount(\"BAR\")\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error looking up account 'BAR': %v\", err)\r\n\t\t\t}\r\n\t\t\tif err := foo.AddServiceExport(\"ngs.usage.*\", nil); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error adding service export to 'FOO': %v\", err)\r\n\t\t\t}\r\n\t\t\tif err := foo.TrackServiceExportWithSampling(\"ngs.usage.*\", \"results\", p); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error adding latency tracking to 'FOO': %v\", err)\r\n\t\t\t}\r\n\t\t\tif err := bar.AddServiceImport(foo, \"ngs.usage\", \"ngs.usage.bar\"); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error adding latency tracking to 'FOO': %v\", err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc (sc *supercluster) removeLatencyTracking(t *testing.T) {\r\n\tt.Helper()\r\n\tfor _, c := range sc.clusters {\r\n\t\tfor _, s := range c.servers {\r\n\t\t\tfoo, err := s.LookupAccount(\"FOO\")\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error looking up account 'FOO': %v\", err)\r\n\t\t\t}\r\n\t\t\tfoo.UnTrackServiceExport(\"ngs.usage.*\")\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc (sc *supercluster) totalSubs() int {\r\n\ttotalSubs := 0\r\n\tfor _, c := range sc.clusters {\r\n\t\ttotalSubs += c.totalSubs()\r\n\t}\r\n\treturn totalSubs\r\n}\r\n\r\nfunc clientConnectWithName(t *testing.T, opts *server.Options, user, appname string) *nats.Conn {\r\n\tt.Helper()\r\n\turl := fmt.Sprintf(\"nats://%s:pass@%s:%d\", user, opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, nats.Name(appname))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\treturn nc\r\n}\r\n\r\nfunc clientConnect(t *testing.T, opts *server.Options, user string) *nats.Conn {\r\n\tt.Helper()\r\n\treturn clientConnectWithName(t, opts, user, \"\")\r\n}\r\n\r\nfunc checkServiceLatency(t *testing.T, sl server.ServiceLatency, start time.Time, serviceTime time.Duration) {\r\n\tt.Helper()\r\n\r\n\tserviceTime = serviceTime.Round(time.Millisecond)\r\n\r\n\tstartDelta := sl.RequestStart.Sub(start)\r\n\tif startDelta > 5*time.Millisecond {\r\n\t\tt.Fatalf(\"Bad start delta %v\", startDelta)\r\n\t}\r\n\tif sl.ServiceLatency < time.Duration(float64(serviceTime)*0.8) {\r\n\t\tt.Fatalf(\"Bad service latency: %v (%v)\", sl.ServiceLatency, serviceTime)\r\n\t}\r\n\tif sl.TotalLatency < sl.ServiceLatency {\r\n\t\tt.Fatalf(\"Bad total latency: %v (%v)\", sl.TotalLatency, sl.ServiceLatency)\r\n\t}\r\n\r\n\t// We should have NATS latency here that is non-zero with real clients.\r\n\tif sl.NATSLatency.Requestor == 0 {\r\n\t\tt.Fatalf(\"Expected non-zero NATS Requestor latency\")\r\n\t}\r\n\tif sl.NATSLatency.Responder == 0 {\r\n\t\tt.Fatalf(\"Expected non-zero NATS Requestor latency\")\r\n\t}\r\n\r\n\t// Make sure they add up\r\n\tgot := sl.TotalLatency\r\n\texpected := sl.ServiceLatency + sl.NATSLatency.TotalTime()\r\n\tif got != expected {\r\n\t\tt.Fatalf(\"Numbers do not add up: %+v,\\ngot: %v\\nexpected: %v\", sl, got, expected)\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencySingleServerConnect(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\t// Now we can setup and test, do single node only first.\r\n\t// This is the service provider.\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tserviceTime := 25 * time.Millisecond\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\ttime.Sleep(serviceTime)\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, _ := nc.SubscribeSync(\"results\")\r\n\r\n\t// Requestor\r\n\tnc2 := clientConnect(t, sc.clusters[0].opts[0], \"bar\")\r\n\tdefer nc2.Close()\r\n\r\n\t// Send the request.\r\n\tstart := time.Now()\r\n\t_, err := nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\r\n\tvar sl server.ServiceLatency\r\n\trmsg, _ := rsub.NextMsg(time.Second)\r\n\tjson.Unmarshal(rmsg.Data, &sl)\r\n\r\n\tcheckServiceLatency(t, sl, start, serviceTime)\r\n}\r\n\r\n// If a client has a longer RTT that the effective RTT for NATS + responder\r\n// the requestor RTT will be marked as 0. This can happen quite often with\r\n// utility programs that are far away from a cluster like NGS but the service\r\n// response time has a shorter RTT.\r\nfunc TestServiceLatencyClientRTTSlowerVsServiceRTT(t *testing.T) {\r\n\tsc := createSuperCluster(t, 2, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener. Instant response.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\ttime.Sleep(time.Millisecond)\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, _ := nc.SubscribeSync(\"results\")\r\n\r\n\tnc.Flush()\r\n\r\n\t// Requestor and processing\r\n\trequestAndCheck := func(sopts *server.Options) {\r\n\t\trtt := 10 * time.Millisecond\r\n\t\tsp, opts := newSlowProxy(rtt, sopts)\r\n\t\tdefer sp.Stop()\r\n\r\n\t\tnc2 := clientConnect(t, opts, \"bar\")\r\n\t\tdefer nc2.Close()\r\n\r\n\t\tstart := time.Now()\r\n\t\tnc2.Flush()\r\n\t\tif d := time.Since(start); d < rtt {\r\n\t\t\tt.Fatalf(\"Expected an rtt of at least %v, got %v\", rtt, d)\r\n\t\t}\r\n\r\n\t\t// Send the request.\r\n\t\tstart = time.Now()\r\n\t\t_, err := nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected a response\")\r\n\t\t}\r\n\r\n\t\tvar sl server.ServiceLatency\r\n\t\trmsg, _ := rsub.NextMsg(time.Second)\r\n\t\tjson.Unmarshal(rmsg.Data, &sl)\r\n\r\n\t\t// We want to test here that when the client requestor RTT is larger then the response time\r\n\t\t// we still deliver a requestor value > 0.\r\n\t\t// Now check that it is close to rtt.\r\n\t\tif sl.NATSLatency.Requestor < rtt {\r\n\t\t\tt.Fatalf(\"Expected requestor latency to be > %v, got %v\", rtt, sl.NATSLatency.Requestor)\r\n\t\t}\r\n\t\tif sl.TotalLatency < rtt {\r\n\t\t\tt.Fatalf(\"Expected total latency to be > %v, got %v\", rtt, sl.TotalLatency)\r\n\t\t}\r\n\t}\r\n\r\n\t// Check same server.\r\n\trequestAndCheck(sc.clusters[0].opts[0])\r\n\t// Check from remote server across GW.\r\n\trequestAndCheck(sc.clusters[1].opts[1])\r\n\t// Same cluster but different server\r\n\trequestAndCheck(sc.clusters[0].opts[1])\r\n}\r\n\r\nfunc connRTT(nc *nats.Conn) time.Duration {\r\n\t// Do 5x to flatten\r\n\ttotal := time.Duration(0)\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tstart := time.Now()\r\n\t\tnc.Flush()\r\n\t\ttotal += time.Since(start)\r\n\t}\r\n\treturn total / 5\r\n}\r\n\r\nfunc TestServiceLatencyRemoteConnect(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\t// Now we can setup and test, do single node only first.\r\n\t// This is the service provider.\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tserviceTime := 25 * time.Millisecond\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\ttime.Sleep(serviceTime)\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, _ := nc.SubscribeSync(\"results\")\r\n\r\n\t// Same Cluster Requestor\r\n\tnc2 := clientConnect(t, sc.clusters[0].opts[2], \"bar\")\r\n\tdefer nc2.Close()\r\n\r\n\t// Send the request.\r\n\tstart := time.Now()\r\n\t_, err := nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\r\n\tvar sl server.ServiceLatency\r\n\trmsg, err := rsub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error getting latency measurement: %v\", err)\r\n\t}\r\n\tjson.Unmarshal(rmsg.Data, &sl)\r\n\tcheckServiceLatency(t, sl, start, serviceTime)\r\n\r\n\t// Lastly here, we need to make sure we are properly tracking the extra hops.\r\n\t// We will make sure that NATS latency is close to what we see from the outside in terms of RTT.\r\n\tif crtt := connRTT(nc) + connRTT(nc2); sl.NATSLatency.TotalTime() < crtt {\r\n\t\tt.Fatalf(\"Not tracking second measurement for NATS latency across servers: %v vs %v\", sl.NATSLatency.TotalTime(), crtt)\r\n\t}\r\n\r\n\t// Gateway Requestor\r\n\tnc2 = clientConnect(t, sc.clusters[1].opts[1], \"bar\")\r\n\tdefer nc2.Close()\r\n\r\n\t// Send the request.\r\n\tstart = time.Now()\r\n\t_, err = nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\r\n\trmsg, err = rsub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error getting latency measurement: %v\", err)\r\n\t}\r\n\tjson.Unmarshal(rmsg.Data, &sl)\r\n\tcheckServiceLatency(t, sl, start, serviceTime)\r\n\r\n\t// Lastly here, we need to make sure we are properly tracking the extra hops.\r\n\t// We will make sure that NATS latency is close to what we see from the outside in terms of RTT.\r\n\tif crtt := connRTT(nc) + connRTT(nc2); sl.NATSLatency.TotalTime() < crtt {\r\n\t\tt.Fatalf(\"Not tracking second measurement for NATS latency across servers: %v vs %v\", sl.NATSLatency.TotalTime(), crtt)\r\n\t}\r\n\r\n\t// Now turn off and make sure we no longer receive updates.\r\n\tsc.removeLatencyTracking(t)\r\n\t_, err = nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\r\n\t_, err = rsub.NextMsg(100 * time.Millisecond)\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Did not expect to receive a latency metric\")\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencySampling(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 50)\r\n\r\n\t// Now we can setup and test, do single node only first.\r\n\t// This is the service provider.\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\treceived := int32(0)\r\n\r\n\tnc.Subscribe(\"results\", func(msg *nats.Msg) {\r\n\t\tatomic.AddInt32(&received, 1)\r\n\t})\r\n\r\n\t// Same Cluster Requestor\r\n\tnc2 := clientConnect(t, sc.clusters[0].opts[2], \"bar\")\r\n\tdefer nc2.Close()\r\n\r\n\ttoSend := 1000\r\n\tfor i := 0; i < toSend; i++ {\r\n\t\tnc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\t}\r\n\t// Wait for results to flow in.\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\r\n\tmid := toSend / 2\r\n\tdelta := toSend / 10 // 10%\r\n\tgot := int(atomic.LoadInt32(&received))\r\n\r\n\tif got > mid+delta || got < mid-delta {\r\n\t\tt.Fatalf(\"Sampling number incorrect: %d vs %d\", mid, got)\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencyNoSubsLeak(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 3)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\tnc := clientConnectWithName(t, sc.clusters[0].opts[1], \"foo\", \"dlc22\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\tnc.Flush()\r\n\t// Propagation of sub through super cluster.\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\r\n\tstartSubs := sc.totalSubs()\r\n\r\n\tfooAcc, _ := sc.clusters[1].servers[1].LookupAccount(\"FOO\")\r\n\tstartNumSis := fooAcc.NumServiceImports()\r\n\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tnc := clientConnect(t, sc.clusters[1].opts[1], \"bar\")\r\n\t\tif _, err := nc.Request(\"ngs.usage\", []byte(\"1h\"), time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Error on request: %v\", err)\r\n\t\t}\r\n\t\tnc.Close()\r\n\t}\r\n\r\n\t// We are adding 2 here for the wildcard response subject for service replies.\r\n\t// we only have one but it will show in two places.\r\n\tstartSubs += 2\r\n\r\n\tcheckFor(t, time.Second, 50*time.Millisecond, func() error {\r\n\t\tif numSubs := sc.totalSubs(); numSubs != startSubs {\r\n\t\t\treturn fmt.Errorf(\"Leaked %d subs\", numSubs-startSubs)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Now also check to make sure the service imports created for the request go away as well.\r\n\tcheckFor(t, time.Second, 50*time.Millisecond, func() error {\r\n\t\tif numSis := fooAcc.NumServiceImports(); numSis != startNumSis {\r\n\t\t\treturn fmt.Errorf(\"Leaked %d service imports\", numSis-startNumSis)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestServiceLatencyWithName(t *testing.T) {\r\n\tsc := createSuperCluster(t, 1, 1)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\topts := sc.clusters[0].opts[0]\r\n\r\n\tnc := clientConnectWithName(t, opts, \"foo\", \"dlc22\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, _ := nc.SubscribeSync(\"results\")\r\n\r\n\tnc2 := clientConnect(t, opts, \"bar\")\r\n\tdefer nc2.Close()\r\n\tnc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\r\n\tvar sl server.ServiceLatency\r\n\trmsg, _ := rsub.NextMsg(time.Second)\r\n\tjson.Unmarshal(rmsg.Data, &sl)\r\n\r\n\t// Make sure we have AppName set.\r\n\tif sl.AppName != \"dlc22\" {\r\n\t\tt.Fatalf(\"Expected to have AppName set correctly, %q vs %q\", \"dlc22\", sl.AppName)\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencyWithNameMultiServer(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\tnc := clientConnectWithName(t, sc.clusters[0].opts[1], \"foo\", \"dlc22\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, _ := nc.SubscribeSync(\"results\")\r\n\tnc.Flush()\r\n\r\n\tnc2 := clientConnect(t, sc.clusters[1].opts[1], \"bar\")\r\n\tdefer nc2.Close()\r\n\tnc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\r\n\tvar sl server.ServiceLatency\r\n\trmsg, _ := rsub.NextMsg(time.Second)\r\n\tjson.Unmarshal(rmsg.Data, &sl)\r\n\r\n\t// Make sure we have AppName set.\r\n\tif sl.AppName != \"dlc22\" {\r\n\t\tt.Fatalf(\"Expected to have AppName set correctly, %q vs %q\", \"dlc22\", sl.AppName)\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencyWithQueueSubscribersAndNames(t *testing.T) {\r\n\tnumServers := 3\r\n\tnumClusters := 3\r\n\tsc := createSuperCluster(t, numServers, numClusters)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\tselectServer := func() *server.Options {\r\n\t\tsi, ci := rand.Int63n(int64(numServers)), rand.Int63n(int64(numServers))\r\n\t\treturn sc.clusters[ci].opts[si]\r\n\t}\r\n\r\n\tsname := func(i int) string {\r\n\t\treturn fmt.Sprintf(\"SERVICE-%d\", i+1)\r\n\t}\r\n\r\n\tnumResponders := 5\r\n\r\n\t// Create 10 queue subscribers for the service. Randomly select the server.\r\n\tfor i := 0; i < numResponders; i++ {\r\n\t\tnc := clientConnectWithName(t, selectServer(), \"foo\", sname(i))\r\n\t\tdefer nc.Close()\r\n\t\tnc.QueueSubscribe(\"ngs.usage.*\", \"SERVICE\", func(msg *nats.Msg) {\r\n\t\t\ttime.Sleep(time.Duration(rand.Int63n(10)) * time.Millisecond)\r\n\t\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t\t})\r\n\t\tnc.Flush()\r\n\t}\r\n\r\n\tdoRequest := func() {\r\n\t\tnc := clientConnect(t, selectServer(), \"bar\")\r\n\t\tdefer nc.Close()\r\n\t\tif _, err := nc.Request(\"ngs.usage\", []byte(\"1h\"), time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Failed to get request response: %v\", err)\r\n\t\t}\r\n\t\tnc.Close()\r\n\t}\r\n\r\n\t// To collect the metrics\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\tresults := make(map[string]time.Duration)\r\n\tvar rlock sync.Mutex\r\n\tch := make(chan (bool))\r\n\treceived := int32(0)\r\n\ttoSend := int32(100)\r\n\r\n\t// Capture the results.\r\n\tnc.Subscribe(\"results\", func(msg *nats.Msg) {\r\n\t\tvar sl server.ServiceLatency\r\n\t\tjson.Unmarshal(msg.Data, &sl)\r\n\t\trlock.Lock()\r\n\t\tresults[sl.AppName] += sl.ServiceLatency\r\n\t\trlock.Unlock()\r\n\t\tif r := atomic.AddInt32(&received, 1); r >= toSend {\r\n\t\t\tch <- true\r\n\t\t}\r\n\t})\r\n\tnc.Flush()\r\n\r\n\t// Send 100 requests from random locations.\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tdoRequest()\r\n\t}\r\n\r\n\t// Wait on all results.\r\n\t<-ch\r\n\r\n\trlock.Lock()\r\n\tdefer rlock.Unlock()\r\n\r\n\t// Make sure each total is generally over 10ms\r\n\tthresh := 10 * time.Millisecond\r\n\tfor i := 0; i < numResponders; i++ {\r\n\t\tif rl := results[sname(i)]; rl < thresh {\r\n\t\t\tt.Fatalf(\"Total for %q is less then threshold: %v vs %v\", sname(i), thresh, rl)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc createAccountWithJWT(t *testing.T) (string, nkeys.KeyPair, *jwt.AccountClaims) {\r\n\tt.Helper()\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\treturn jwt, akp, nac\r\n}\r\n\r\nfunc TestServiceLatencyWithJWT(t *testing.T) {\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create three accounts, system, service and normal account.\r\n\tsysJWT, sysKP, _ := createAccountWithJWT(t)\r\n\tsysPub, _ := sysKP.PublicKey()\r\n\r\n\t_, svcKP, svcAcc := createAccountWithJWT(t)\r\n\tsvcPub, _ := svcKP.PublicKey()\r\n\r\n\t// Add in the service export with latency tracking here.\r\n\tserviceExport := &jwt.Export{Subject: \"req.*\", Type: jwt.Service}\r\n\tsvcAcc.Exports.Add(serviceExport)\r\n\tsvcJWT, err := svcAcc.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\t_, accKP, accAcc := createAccountWithJWT(t)\r\n\taccPub, _ := accKP.PublicKey()\r\n\r\n\t// Add in the import.\r\n\tserviceImport := &jwt.Import{Account: svcPub, Subject: \"request\", To: \"req.echo\", Type: jwt.Service}\r\n\taccAcc.Imports.Add(serviceImport)\r\n\taccJWT, err := accAcc.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tcf := `\r\n\tlisten: 127.0.0.1:-1\r\n\tcluster {\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tauthorization {\r\n\t\t\ttimeout: 2.2\r\n\t\t} %s\r\n\t}\r\n\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tsystem_account = \"%s\"\r\n\r\n\tresolver = MEMORY\r\n\tresolver_preload = {\r\n\t\t%s : \"%s\"\r\n\t\t%s : \"%s\"\r\n\t\t%s : \"%s\"\r\n\t}\r\n\t`\r\n\tcontents := strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, svcPub, svcJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\tconf := createConfFile(t, []byte(contents))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create a new server and route to main one.\r\n\trouteStr := fmt.Sprintf(\"\\n\\t\\troutes = [nats-route://%s:%d]\", opts.Cluster.Host, opts.Cluster.Port)\r\n\tcontents2 := strings.Replace(fmt.Sprintf(cf, routeStr, sysPub, sysPub, sysJWT, svcPub, svcJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\r\n\tconf2 := createConfFile(t, []byte(contents2))\r\n\tdefer os.Remove(conf2)\r\n\r\n\ts2, opts2 := RunServerWithConfig(conf2)\r\n\tdefer s2.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, s2)\r\n\r\n\t// Create service provider.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, svcKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tserviceTime := 25 * time.Millisecond\r\n\tnc.Subscribe(\"req.echo\", func(msg *nats.Msg) {\r\n\t\ttime.Sleep(serviceTime)\r\n\t\tmsg.Respond(msg.Data)\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, _ := nc.SubscribeSync(\"results\")\r\n\r\n\t// Create second client and send request from this one.\r\n\turl2 := fmt.Sprintf(\"nats://%s:%d/\", opts2.Host, opts2.Port)\r\n\tnc2, err := nats.Connect(url2, createUserCreds(t, s2, accKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Send the request.\r\n\t_, err = nc2.Request(\"request\", []byte(\"hello\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\r\n\t// We should not receive latency at this time.\r\n\t_, err = rsub.NextMsg(100 * time.Millisecond)\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Did not expect to receive a latency metric\")\r\n\t}\r\n\r\n\t// Now turn it on..\r\n\tupdateAccount := func() {\r\n\t\tt.Helper()\r\n\t\tfor _, s := range []*server.Server{s, s2} {\r\n\t\t\tsvcAccount, err := s.LookupAccount(svcPub)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Could not lookup service account from server %+v\", s)\r\n\t\t\t}\r\n\t\t\ts.UpdateAccountClaims(svcAccount, svcAcc)\r\n\t\t}\r\n\t}\r\n\tserviceExport.Latency = &jwt.ServiceLatency{Sampling: 100, Results: \"results\"}\r\n\tupdateAccount()\r\n\r\n\t// Send the request.\r\n\tstart := time.Now()\r\n\t_, err = nc2.Request(\"request\", []byte(\"hello\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\r\n\tvar sl server.ServiceLatency\r\n\trmsg, err := rsub.NextMsg(time.Second)\r\n\tif err != nil || rmsg == nil {\r\n\t\tt.Fatalf(\"Did not receive a latency metric\")\r\n\t}\r\n\tjson.Unmarshal(rmsg.Data, &sl)\r\n\tcheckServiceLatency(t, sl, start, serviceTime)\r\n\r\n\t// Now we will remove tracking. Do this by simulating a JWT update.\r\n\tserviceExport.Latency = nil\r\n\tupdateAccount()\r\n\r\n\t// Now we should not get any tracking data.\r\n\t_, err = nc2.Request(\"request\", []byte(\"hello\"), time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n\t_, err = rsub.NextMsg(100 * time.Millisecond)\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Did not expect to receive a latency metric\")\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencyAdjustNegativeLatencyValues(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import\r\n\t// that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\t// Now we can setup and test, do single node only first.\r\n\t// This is the service provider.\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, err := nc.SubscribeSync(\"results\")\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tnc.Flush()\r\n\r\n\t// Requestor\r\n\tnc2 := clientConnect(t, sc.clusters[0].opts[0], \"bar\")\r\n\tdefer nc2.Close()\r\n\r\n\t// Send the request.\r\n\ttotalSamples := 50\r\n\tfor i := 0; i < totalSamples; i++ {\r\n\t\tif _, err := nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Expected a response\")\r\n\t\t}\r\n\t}\r\n\r\n\tvar sl server.ServiceLatency\r\n\tfor i := 0; i < totalSamples; i++ {\r\n\t\trmsg, err := rsub.NextMsg(time.Second)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to receive latency metric: %d, %s\", i, err)\r\n\t\t}\r\n\t\tif err := json.Unmarshal(rmsg.Data, &sl); err != nil {\r\n\t\t\tt.Errorf(\"Unexpected error processing latency metric: %s\", err)\r\n\t\t}\r\n\t\tif sl.ServiceLatency < 0 {\r\n\t\t\tt.Fatalf(\"Unexpected negative latency value: %v\", sl)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestServiceLatencyRemoteConnectAdjustNegativeValues(t *testing.T) {\r\n\tsc := createSuperCluster(t, 3, 2)\r\n\tdefer sc.shutdown()\r\n\r\n\t// Now add in new service export to FOO and have bar import that with tracking enabled.\r\n\tsc.setupLatencyTracking(t, 100)\r\n\r\n\t// Now we can setup and test, do single node only first.\r\n\t// This is the service provider.\r\n\tnc := clientConnect(t, sc.clusters[0].opts[0], \"foo\")\r\n\tdefer nc.Close()\r\n\r\n\t// The service listener.\r\n\tnc.Subscribe(\"ngs.usage.*\", func(msg *nats.Msg) {\r\n\t\t// time.Sleep(serviceTime)\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\r\n\t// Listen for metrics\r\n\trsub, err := nc.SubscribeSync(\"results\")\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tnc.Flush()\r\n\r\n\t// Same Cluster Requestor\r\n\tnc2 := clientConnect(t, sc.clusters[0].opts[2], \"bar\")\r\n\tdefer nc2.Close()\r\n\r\n\t// Gateway Requestor\r\n\tnc3 := clientConnect(t, sc.clusters[1].opts[1], \"bar\")\r\n\tdefer nc3.Close()\r\n\r\n\t// Send a few initial requests to ensure interest is propagated\r\n\t// both for cluster and gateway requestors.\r\n\tcheckFor(t, 3*time.Second, time.Second, func() error {\r\n\t\t_, err1 := nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\t\t_, err2 := nc3.Request(\"ngs.usage\", []byte(\"1h\"), time.Second)\r\n\r\n\t\tif err1 != nil || err2 != nil {\r\n\t\t\treturn fmt.Errorf(\"Timed out waiting for super cluster to be ready\")\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Send the request.\r\n\ttotalSamples := 20\r\n\tfor i := 0; i < totalSamples; i++ {\r\n\t\tif _, err := nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Expected a response\")\r\n\t\t}\r\n\t}\r\n\r\n\tfor i := 0; i < totalSamples; i++ {\r\n\t\tif _, err := nc3.Request(\"ngs.usage\", []byte(\"1h\"), time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Expected a response\")\r\n\t\t}\r\n\t}\r\n\r\n\tvar sl server.ServiceLatency\r\n\tfor i := 0; i < totalSamples*2; i++ {\r\n\t\trmsg, err := rsub.NextMsg(time.Second)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to receive latency metric: %d, %s\", i, err)\r\n\t\t}\r\n\t\tif err = json.Unmarshal(rmsg.Data, &sl); err != nil {\r\n\t\t\tt.Errorf(\"Unexpected error processing latency metric: %s\", err)\r\n\t\t}\r\n\t\tif sl.ServiceLatency < 0 {\r\n\t\t\tt.Fatalf(\"Unexpected negative service latency value: %v\", sl)\r\n\t\t}\r\n\t\tif sl.NATSLatency.System < 0 {\r\n\t\t\tt.Fatalf(\"Unexpected negative system latency value: %v\", sl)\r\n\t\t}\r\n\t}\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/test/service_latency_test.go b/server/gnatsd/test/service_latency_test.go
--- a/server/gnatsd/test/service_latency_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/test/service_latency_test.go	(date 1665399049732)
@@ -26,7 +26,7 @@
 
 	"github.com/kubemq-io/broker/client/nats"
 	"github.com/kubemq-io/broker/server/gnatsd/server"
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/server/monitor_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2013-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"math/rand\"\r\n\t\"net\"\r\n\t\"net/http\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"reflect\"\r\n\t\"runtime\"\r\n\t\"sort\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\t\"unicode\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nats.go\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nconst CLIENT_PORT = -1\r\nconst MONITOR_PORT = -1\r\nconst CLUSTER_PORT = -1\r\n\r\nfunc DefaultMonitorOptions() *Options {\r\n\treturn &Options{\r\n\t\tHost:         \"127.0.0.1\",\r\n\t\tPort:         CLIENT_PORT,\r\n\t\tHTTPHost:     \"127.0.0.1\",\r\n\t\tHTTPPort:     MONITOR_PORT,\r\n\t\tHTTPBasePath: \"/\",\r\n\t\tServerName:   \"monitor_server\",\r\n\t\tNoLog:        true,\r\n\t\tNoSigs:       true,\r\n\t}\r\n}\r\n\r\nfunc runMonitorServer() *Server {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\treturn RunServer(opts)\r\n}\r\n\r\nfunc runMonitorServerWithAccounts() *Server {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\taA := NewAccount(\"A\")\r\n\taB := NewAccount(\"B\")\r\n\topts.Accounts = append(opts.Accounts, aA, aB)\r\n\topts.Users = append(opts.Users,\r\n\t\t&User{Username: \"a\", Password: \"a\", Account: aA},\r\n\t\t&User{Username: \"b\", Password: \"b\", Account: aB})\r\n\treturn RunServer(opts)\r\n}\r\n\r\nfunc runMonitorServerNoHTTPPort() *Server {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.HTTPPort = 0\r\n\treturn RunServer(opts)\r\n}\r\n\r\nfunc resetPreviousHTTPConnections() {\r\n\thttp.DefaultTransport.(*http.Transport).CloseIdleConnections()\r\n}\r\n\r\nfunc TestMyUptime(t *testing.T) {\r\n\t// Make sure we print this stuff right.\r\n\tvar d time.Duration\r\n\tvar s string\r\n\r\n\td = 22 * time.Second\r\n\ts = myUptime(d)\r\n\tif s != \"22s\" {\r\n\t\tt.Fatalf(\"Expected `22s`, go ``%s`\", s)\r\n\t}\r\n\td = 4*time.Minute + d\r\n\ts = myUptime(d)\r\n\tif s != \"4m22s\" {\r\n\t\tt.Fatalf(\"Expected `4m22s`, go ``%s`\", s)\r\n\t}\r\n\td = 4*time.Hour + d\r\n\ts = myUptime(d)\r\n\tif s != \"4h4m22s\" {\r\n\t\tt.Fatalf(\"Expected `4h4m22s`, go ``%s`\", s)\r\n\t}\r\n\td = 32*24*time.Hour + d\r\n\ts = myUptime(d)\r\n\tif s != \"32d4h4m22s\" {\r\n\t\tt.Fatalf(\"Expected `32d4h4m22s`, go ``%s`\", s)\r\n\t}\r\n\td = 22*365*24*time.Hour + d\r\n\ts = myUptime(d)\r\n\tif s != \"22y32d4h4m22s\" {\r\n\t\tt.Fatalf(\"Expected `22y32d4h4m22s`, go ``%s`\", s)\r\n\t}\r\n}\r\n\r\n// Make sure that we do not run the http server for monitoring unless asked.\r\nfunc TestNoMonitorPort(t *testing.T) {\r\n\ts := runMonitorServerNoHTTPPort()\r\n\tdefer s.Shutdown()\r\n\r\n\t// this test might be meaningless now that we're testing with random ports?\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", 11245)\r\n\tif resp, err := http.Get(url + \"varz\"); err == nil {\r\n\t\tt.Fatalf(\"Expected error: Got %+v\\n\", resp)\r\n\t}\r\n\tif resp, err := http.Get(url + \"healthz\"); err == nil {\r\n\t\tt.Fatalf(\"Expected error: Got %+v\\n\", resp)\r\n\t}\r\n\tif resp, err := http.Get(url + \"connz\"); err == nil {\r\n\t\tt.Fatalf(\"Expected error: Got %+v\\n\", resp)\r\n\t}\r\n}\r\n\r\nvar (\r\n\tappJSONContent = \"application/json\"\r\n\tappJSContent   = \"application/javascript\"\r\n\ttextPlain      = \"text/plain; charset=utf-8\"\r\n\ttextHTML       = \"text/html; charset=utf-8\"\r\n)\r\n\r\nfunc readBodyEx(t *testing.T, url string, status int, content string) []byte {\r\n\tresp, err := http.Get(url)\r\n\tif err != nil {\r\n\t\tstackFatalf(t, \"Expected no error: Got %v\\n\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\tif resp.StatusCode != status {\r\n\t\tstackFatalf(t, \"Expected a %d response, got %d\\n\", status, resp.StatusCode)\r\n\t}\r\n\tct := resp.Header.Get(\"Content-Type\")\r\n\tif ct != content {\r\n\t\tstackFatalf(t, \"Expected %s content-type, got %s\\n\", content, ct)\r\n\t}\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tstackFatalf(t, \"Got an error reading the body: %v\\n\", err)\r\n\t}\r\n\treturn body\r\n}\r\n\r\nfunc TestHTTPBasePath(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.HTTPBasePath = \"/nats\"\r\n\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/nats\", s.MonitorAddr().Port)\r\n\treadBodyEx(t, url, http.StatusOK, textHTML)\r\n}\r\n\r\nfunc readBody(t *testing.T, url string) []byte {\r\n\treturn readBodyEx(t, url, http.StatusOK, appJSONContent)\r\n}\r\n\r\nfunc pollVarz(t *testing.T, s *Server, mode int, url string, opts *VarzOptions) *Varz {\r\n\tt.Helper()\r\n\tif mode == 0 {\r\n\t\tv := &Varz{}\r\n\t\tbody := readBody(t, url)\r\n\t\tif err := json.Unmarshal(body, v); err != nil {\r\n\t\t\tt.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\treturn v\r\n\t}\r\n\tv, err := s.Varz(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on Varz: %v\", err)\r\n\t}\r\n\treturn v\r\n}\r\n\r\nfunc TestHandleVarz(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tv := pollVarz(t, s, mode, url+\"varz\", nil)\r\n\r\n\t\t// Do some sanity checks on values\r\n\t\tif time.Since(v.Start) > 10*time.Second {\r\n\t\t\tt.Fatal(\"Expected start time to be within 10 seconds.\")\r\n\t\t}\r\n\t}\r\n\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tv := pollVarz(t, s, mode, url+\"varz\", nil)\r\n\r\n\t\tif v.Connections != 1 {\r\n\t\t\tt.Fatalf(\"Expected Connections of 1, got %v\\n\", v.Connections)\r\n\t\t}\r\n\t\tif v.TotalConnections < 1 {\r\n\t\t\tt.Fatalf(\"Expected Total Connections of at least 1, got %v\\n\", v.TotalConnections)\r\n\t\t}\r\n\t\tif v.InMsgs != 1 {\r\n\t\t\tt.Fatalf(\"Expected InMsgs of 1, got %v\\n\", v.InMsgs)\r\n\t\t}\r\n\t\tif v.OutMsgs != 1 {\r\n\t\t\tt.Fatalf(\"Expected OutMsgs of 1, got %v\\n\", v.OutMsgs)\r\n\t\t}\r\n\t\tif v.InBytes != 5 {\r\n\t\t\tt.Fatalf(\"Expected InBytes of 5, got %v\\n\", v.InBytes)\r\n\t\t}\r\n\t\tif v.OutBytes != 5 {\r\n\t\t\tt.Fatalf(\"Expected OutBytes of 5, got %v\\n\", v.OutBytes)\r\n\t\t}\r\n\t\tif v.Subscriptions != 0 {\r\n\t\t\tt.Fatalf(\"Expected Subscriptions of 0, got %v\\n\", v.Subscriptions)\r\n\t\t}\r\n\t\tif v.Name != \"monitor_server\" {\r\n\t\t\tt.Fatal(\"Expected ServerName to be 'monitor_server'\")\r\n\t\t}\r\n\t}\r\n\r\n\t// Test JSONP\r\n\treadBodyEx(t, url+\"varz?callback=callback\", http.StatusOK, appJSContent)\r\n}\r\n\r\nfunc pollConz(t *testing.T, s *Server, mode int, url string, opts *ConnzOptions) *Connz {\r\n\tt.Helper()\r\n\tif mode == 0 {\r\n\t\tbody := readBody(t, url)\r\n\t\tc := &Connz{}\r\n\t\tif err := json.Unmarshal(body, &c); err != nil {\r\n\t\t\tt.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\treturn c\r\n\t}\r\n\tc, err := s.Connz(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on Connz(): %v\", err)\r\n\t}\r\n\treturn c\r\n}\r\n\r\nfunc TestConnz(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\ttestConnz := func(mode int) {\r\n\t\tc := pollConz(t, s, mode, url+\"connz\", nil)\r\n\r\n\t\t// Test contents..\r\n\t\tif c.NumConns != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 connections, got %d\\n\", c.NumConns)\r\n\t\t}\r\n\t\tif c.Total != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 live connections, got %d\\n\", c.Total)\r\n\t\t}\r\n\t\tif c.Conns == nil || len(c.Conns) != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 connections in array, got %p\\n\", c.Conns)\r\n\t\t}\r\n\r\n\t\t// Test with connections.\r\n\t\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer nc.Close()\r\n\r\n\t\ttime.Sleep(50 * time.Millisecond)\r\n\r\n\t\tc = pollConz(t, s, mode, url+\"connz\", nil)\r\n\r\n\t\tif c.NumConns != 1 {\r\n\t\t\tt.Fatalf(\"Expected 1 connection, got %d\\n\", c.NumConns)\r\n\t\t}\r\n\t\tif c.Total != 1 {\r\n\t\t\tt.Fatalf(\"Expected 1 live connection, got %d\\n\", c.Total)\r\n\t\t}\r\n\t\tif c.Conns == nil || len(c.Conns) != 1 {\r\n\t\t\tt.Fatalf(\"Expected 1 connection in array, got %d\\n\", len(c.Conns))\r\n\t\t}\r\n\r\n\t\tif c.Limit != DefaultConnListSize {\r\n\t\t\tt.Fatalf(\"Expected limit of %d, got %v\\n\", DefaultConnListSize, c.Limit)\r\n\t\t}\r\n\r\n\t\tif c.Offset != 0 {\r\n\t\t\tt.Fatalf(\"Expected offset of 0, got %v\\n\", c.Offset)\r\n\t\t}\r\n\r\n\t\t// Test inside details of each connection\r\n\t\tci := c.Conns[0]\r\n\r\n\t\tif ci.Cid == 0 {\r\n\t\t\tt.Fatalf(\"Expected non-zero cid, got %v\\n\", ci.Cid)\r\n\t\t}\r\n\t\tif ci.IP != \"127.0.0.1\" {\r\n\t\t\tt.Fatalf(\"Expected \\\"127.0.0.1\\\" for IP, got %v\\n\", ci.IP)\r\n\t\t}\r\n\t\tif ci.Port == 0 {\r\n\t\t\tt.Fatalf(\"Expected non-zero port, got %v\\n\", ci.Port)\r\n\t\t}\r\n\t\tif ci.NumSubs != 0 {\r\n\t\t\tt.Fatalf(\"Expected num_subs of 0, got %v\\n\", ci.NumSubs)\r\n\t\t}\r\n\t\tif len(ci.Subs) != 0 {\r\n\t\t\tt.Fatalf(\"Expected subs of 0, got %v\\n\", ci.Subs)\r\n\t\t}\r\n\t\tif len(ci.SubsDetail) != 0 {\r\n\t\t\tt.Fatalf(\"Expected subsdetail of 0, got %v\\n\", ci.SubsDetail)\r\n\t\t}\r\n\t\tif ci.InMsgs != 1 {\r\n\t\t\tt.Fatalf(\"Expected InMsgs of 1, got %v\\n\", ci.InMsgs)\r\n\t\t}\r\n\t\tif ci.OutMsgs != 1 {\r\n\t\t\tt.Fatalf(\"Expected OutMsgs of 1, got %v\\n\", ci.OutMsgs)\r\n\t\t}\r\n\t\tif ci.InBytes != 5 {\r\n\t\t\tt.Fatalf(\"Expected InBytes of 1, got %v\\n\", ci.InBytes)\r\n\t\t}\r\n\t\tif ci.OutBytes != 5 {\r\n\t\t\tt.Fatalf(\"Expected OutBytes of 1, got %v\\n\", ci.OutBytes)\r\n\t\t}\r\n\t\tif ci.Start.IsZero() {\r\n\t\t\tt.Fatal(\"Expected Start to be valid\\n\")\r\n\t\t}\r\n\t\tif ci.Uptime == \"\" {\r\n\t\t\tt.Fatal(\"Expected Uptime to be valid\\n\")\r\n\t\t}\r\n\t\tif ci.LastActivity.IsZero() {\r\n\t\t\tt.Fatal(\"Expected LastActivity to be valid\\n\")\r\n\t\t}\r\n\t\tif ci.LastActivity.UnixNano() < ci.Start.UnixNano() {\r\n\t\t\tt.Fatalf(\"Expected LastActivity [%v] to be > Start [%v]\\n\", ci.LastActivity, ci.Start)\r\n\t\t}\r\n\t\tif ci.Idle == \"\" {\r\n\t\t\tt.Fatal(\"Expected Idle to be valid\\n\")\r\n\t\t}\r\n\t\t// This is a change, we now expect them to be set for connections when the\r\n\t\t// client sends a connect.\r\n\t\tif ci.RTT == \"\" {\r\n\t\t\tt.Fatal(\"Expected RTT to be set for new connection\\n\")\r\n\t\t}\r\n\t}\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\ttestConnz(mode)\r\n\t\tcheckClientsCount(t, s, 0)\r\n\t}\r\n\r\n\t// Test JSONP\r\n\treadBodyEx(t, url+\"connz?callback=callback\", http.StatusOK, appJSContent)\r\n}\r\n\r\nfunc TestConnzBadParams(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/connz?\", s.MonitorAddr().Port)\r\n\treadBodyEx(t, url+\"auth=xxx\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, url+\"subs=xxx\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, url+\"offset=xxx\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, url+\"limit=xxx\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, url+\"state=xxx\", http.StatusBadRequest, textPlain)\r\n}\r\n\r\nfunc TestConnzWithSubs(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tnc.Subscribe(\"hello.foo\", func(m *nats.Msg) {})\r\n\tensureServerActivityRecorded(t, nc)\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?subs=1\", &ConnzOptions{Subscriptions: true})\r\n\t\t// Test inside details of each connection\r\n\t\tci := c.Conns[0]\r\n\t\tif len(ci.Subs) != 1 || ci.Subs[0] != \"hello.foo\" {\r\n\t\t\tt.Fatalf(\"Expected subs of 1, got %v\\n\", ci.Subs)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzWithSubsDetail(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tnc.Subscribe(\"hello.foo\", func(m *nats.Msg) {})\r\n\tensureServerActivityRecorded(t, nc)\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?subs=detail\", &ConnzOptions{SubscriptionsDetail: true})\r\n\t\t// Test inside details of each connection\r\n\t\tci := c.Conns[0]\r\n\t\tif len(ci.SubsDetail) != 1 || ci.SubsDetail[0].Subject != \"hello.foo\" {\r\n\t\t\tt.Fatalf(\"Expected subsdetail of 1, got %v\\n\", ci.Subs)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestClosedConnzWithSubsDetail(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\r\n\tnc.Subscribe(\"hello.foo\", func(m *nats.Msg) {})\r\n\tensureServerActivityRecorded(t, nc)\r\n\tnc.Close()\r\n\r\n\ts.mu.Lock()\r\n\tfor len(s.clients) != 0 {\r\n\t\ts.mu.Unlock()\r\n\t\t<-time.After(100 * time.Millisecond)\r\n\t\ts.mu.Lock()\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?state=closed&subs=detail\", &ConnzOptions{State: ConnClosed,\r\n\t\t\tSubscriptionsDetail: true})\r\n\t\t// Test inside details of each connection\r\n\t\tci := c.Conns[0]\r\n\t\tif len(ci.SubsDetail) != 1 || ci.SubsDetail[0].Subject != \"hello.foo\" {\r\n\t\t\tt.Fatalf(\"Expected subsdetail of 1, got %v\\n\", ci.Subs)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzWithCID(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\t// The one we will request\r\n\tcid := 5\r\n\ttotal := 10\r\n\r\n\t// Create 10\r\n\tfor i := 1; i <= total; i++ {\r\n\t\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer nc.Close()\r\n\t\tif i == cid {\r\n\t\t\tnc.Subscribe(\"hello.foo\", func(m *nats.Msg) {})\r\n\t\t\tnc.Subscribe(\"hello.bar\", func(m *nats.Msg) {})\r\n\t\t\tensureServerActivityRecorded(t, nc)\r\n\t\t}\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/connz?cid=%d\", s.MonitorAddr().Port, cid)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url, &ConnzOptions{CID: uint64(cid)})\r\n\t\t// Test inside details of each connection\r\n\t\tif len(c.Conns) != 1 {\r\n\t\t\tt.Fatalf(\"Expected only one connection, but got %d\\n\", len(c.Conns))\r\n\t\t}\r\n\t\tif c.NumConns != 1 {\r\n\t\t\tt.Fatalf(\"Expected NumConns to be 1, but got %d\\n\", c.NumConns)\r\n\t\t}\r\n\t\tci := c.Conns[0]\r\n\t\tif ci.Cid != uint64(cid) {\r\n\t\t\tt.Fatalf(\"Expected to receive connection %v, but received %v\\n\", cid, ci.Cid)\r\n\t\t}\r\n\t\tif ci.NumSubs != 2 {\r\n\t\t\tt.Fatalf(\"Expected to receive connection with %d subs, but received %d\\n\", 2, ci.NumSubs)\r\n\t\t}\r\n\t\t// Now test a miss\r\n\t\tbadUrl := fmt.Sprintf(\"http://127.0.0.1:%d/connz?cid=%d\", s.MonitorAddr().Port, 100)\r\n\t\tc = pollConz(t, s, mode, badUrl, &ConnzOptions{CID: uint64(100)})\r\n\t\tif len(c.Conns) != 0 {\r\n\t\t\tt.Fatalf(\"Expected no connections, got %d\\n\", len(c.Conns))\r\n\t\t}\r\n\t\tif c.NumConns != 0 {\r\n\t\t\tt.Fatalf(\"Expected NumConns of 0, got %d\\n\", c.NumConns)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Helper to map to connection name\r\nfunc createConnMap(t *testing.T, cz *Connz) map[string]*ConnInfo {\r\n\tcm := make(map[string]*ConnInfo)\r\n\tfor _, c := range cz.Conns {\r\n\t\tcm[c.Name] = c\r\n\t}\r\n\treturn cm\r\n}\r\n\r\nfunc getFooAndBar(t *testing.T, cm map[string]*ConnInfo) (*ConnInfo, *ConnInfo) {\r\n\treturn cm[\"foo\"], cm[\"bar\"]\r\n}\r\n\r\nfunc ensureServerActivityRecorded(t *testing.T, nc *nats.Conn) {\r\n\tnc.Flush()\r\n\terr := nc.Flush()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error flushing: %v\\n\", err)\r\n\t}\r\n}\r\n\r\nfunc TestConnzRTT(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\ttestRTT := func(mode int) {\r\n\t\t// Test with connections.\r\n\t\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer nc.Close()\r\n\r\n\t\tc := pollConz(t, s, mode, url+\"connz\", nil)\r\n\r\n\t\tif c.NumConns != 1 {\r\n\t\t\tt.Fatalf(\"Expected 1 connection, got %d\\n\", c.NumConns)\r\n\t\t}\r\n\r\n\t\t// Send a server side PING to record RTT\r\n\t\ts.mu.Lock()\r\n\t\tci := c.Conns[0]\r\n\t\tsc := s.clients[ci.Cid]\r\n\t\tif sc == nil {\r\n\t\t\tt.Fatalf(\"Error looking up client %v\\n\", ci.Cid)\r\n\t\t}\r\n\t\ts.mu.Unlock()\r\n\t\tsc.mu.Lock()\r\n\t\tsc.sendPing()\r\n\t\tsc.mu.Unlock()\r\n\r\n\t\t// Wait for client to respond with PONG\r\n\t\ttime.Sleep(20 * time.Millisecond)\r\n\r\n\t\t// Repoll for updated information.\r\n\t\tc = pollConz(t, s, mode, url+\"connz\", nil)\r\n\t\tci = c.Conns[0]\r\n\r\n\t\trtt, err := time.ParseDuration(ci.RTT)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not parse RTT properly, %v (ci.RTT=%v)\", err, ci.RTT)\r\n\t\t}\r\n\t\tif rtt <= 0 {\r\n\t\t\tt.Fatal(\"Expected RTT to be valid and non-zero\\n\")\r\n\t\t}\r\n\t\tif (runtime.GOOS == \"windows\" && rtt > 20*time.Millisecond) ||\r\n\t\t\trtt > 20*time.Millisecond || rtt < 100*time.Nanosecond {\r\n\t\t\tt.Fatalf(\"Invalid RTT of %s\\n\", ci.RTT)\r\n\t\t}\r\n\t}\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\ttestRTT(mode)\r\n\t\tcheckClientsCount(t, s, 0)\r\n\t}\r\n}\r\n\r\nfunc TestConnzLastActivity(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\turl += \"connz?subs=1\"\r\n\topts := &ConnzOptions{Subscriptions: true}\r\n\r\n\tvar sleepTime time.Duration\r\n\tif runtime.GOOS == \"windows\" {\r\n\t\tsleepTime = 10 * time.Millisecond\r\n\t}\r\n\r\n\ttestActivity := func(mode int) {\r\n\t\tncFoo := createClientConnWithName(t, \"foo\", s)\r\n\t\tdefer ncFoo.Close()\r\n\r\n\t\tncBar := createClientConnWithName(t, \"bar\", s)\r\n\t\tdefer ncBar.Close()\r\n\r\n\t\t// Test inside details of each connection\r\n\t\tciFoo, ciBar := getFooAndBar(t, createConnMap(t, pollConz(t, s, mode, url, opts)))\r\n\r\n\t\t// Test that LastActivity is non-zero\r\n\t\tif ciFoo.LastActivity.IsZero() {\r\n\t\t\tt.Fatalf(\"Expected LastActivity for connection '%s'to be valid\\n\", ciFoo.Name)\r\n\t\t}\r\n\t\tif ciBar.LastActivity.IsZero() {\r\n\t\t\tt.Fatalf(\"Expected LastActivity for connection '%s'to be valid\\n\", ciBar.Name)\r\n\t\t}\r\n\t\t// Foo should be older than Bar\r\n\t\tif ciFoo.LastActivity.After(ciBar.LastActivity) {\r\n\t\t\tt.Fatal(\"Expected connection 'foo' to be older than 'bar'\\n\")\r\n\t\t}\r\n\r\n\t\tfooLA := ciFoo.LastActivity\r\n\t\tbarLA := ciBar.LastActivity\r\n\r\n\t\tensureServerActivityRecorded(t, ncFoo)\r\n\t\tensureServerActivityRecorded(t, ncBar)\r\n\r\n\t\ttime.Sleep(sleepTime)\r\n\r\n\t\t// Sub should trigger update.\r\n\t\tsub, _ := ncFoo.Subscribe(\"hello.world\", func(m *nats.Msg) {})\r\n\t\tensureServerActivityRecorded(t, ncFoo)\r\n\r\n\t\tciFoo, _ = getFooAndBar(t, createConnMap(t, pollConz(t, s, mode, url, opts)))\r\n\t\tnextLA := ciFoo.LastActivity\r\n\t\tif fooLA.Equal(nextLA) {\r\n\t\t\tt.Fatalf(\"Subscribe should have triggered update to LastActivity %+v\\n\", ciFoo)\r\n\t\t}\r\n\t\tfooLA = nextLA\r\n\r\n\t\ttime.Sleep(sleepTime)\r\n\r\n\t\t// Publish and Message Delivery should trigger as well. So both connections\r\n\t\t// should have updates.\r\n\t\tncBar.Publish(\"hello.world\", []byte(\"Hello\"))\r\n\r\n\t\tensureServerActivityRecorded(t, ncFoo)\r\n\t\tensureServerActivityRecorded(t, ncBar)\r\n\r\n\t\tciFoo, ciBar = getFooAndBar(t, createConnMap(t, pollConz(t, s, mode, url, opts)))\r\n\t\tnextLA = ciBar.LastActivity\r\n\t\tif barLA.Equal(nextLA) {\r\n\t\t\tt.Fatalf(\"Publish should have triggered update to LastActivity\\n\")\r\n\t\t}\r\n\t\tbarLA = nextLA\r\n\r\n\t\t// Message delivery on ncFoo should have triggered as well.\r\n\t\tnextLA = ciFoo.LastActivity\r\n\t\tif fooLA.Equal(nextLA) {\r\n\t\t\tt.Fatalf(\"Message delivery should have triggered update to LastActivity\\n\")\r\n\t\t}\r\n\t\tfooLA = nextLA\r\n\r\n\t\ttime.Sleep(sleepTime)\r\n\r\n\t\t// Unsub should trigger as well\r\n\t\tsub.Unsubscribe()\r\n\t\tensureServerActivityRecorded(t, ncFoo)\r\n\r\n\t\tciFoo, _ = getFooAndBar(t, createConnMap(t, pollConz(t, s, mode, url, opts)))\r\n\t\tnextLA = ciFoo.LastActivity\r\n\t\tif fooLA.Equal(nextLA) {\r\n\t\t\tt.Fatalf(\"Message delivery should have triggered update to LastActivity\\n\")\r\n\t\t}\r\n\t}\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\ttestActivity(mode)\r\n\t}\r\n}\r\n\r\nfunc TestConnzWithOffsetAndLimit(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?offset=1&limit=1\", &ConnzOptions{Offset: 1, Limit: 1})\r\n\t\tif c.Conns == nil || len(c.Conns) != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 connections in array, got %p\\n\", c.Conns)\r\n\t\t}\r\n\r\n\t\t// Test that when given negative values, 0 or default is used\r\n\t\tc = pollConz(t, s, mode, url+\"connz?offset=-1&limit=-1\", &ConnzOptions{Offset: -11, Limit: -11})\r\n\t\tif c.Conns == nil || len(c.Conns) != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 connections in array, got %p\\n\", c.Conns)\r\n\t\t}\r\n\t\tif c.Offset != 0 {\r\n\t\t\tt.Fatalf(\"Expected offset to be 0, and limit to be %v, got %v and %v\",\r\n\t\t\t\tDefaultConnListSize, c.Offset, c.Limit)\r\n\t\t}\r\n\t}\r\n\r\n\tcl1 := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer cl1.Close()\r\n\r\n\tcl2 := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer cl2.Close()\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?offset=1&limit=1\", &ConnzOptions{Offset: 1, Limit: 1})\r\n\t\tif c.Limit != 1 {\r\n\t\t\tt.Fatalf(\"Expected limit of 1, got %v\\n\", c.Limit)\r\n\t\t}\r\n\r\n\t\tif c.Offset != 1 {\r\n\t\t\tt.Fatalf(\"Expected offset of 1, got %v\\n\", c.Offset)\r\n\t\t}\r\n\r\n\t\tif len(c.Conns) != 1 {\r\n\t\t\tt.Fatalf(\"Expected conns of 1, got %v\\n\", len(c.Conns))\r\n\t\t}\r\n\r\n\t\tif c.NumConns != 1 {\r\n\t\t\tt.Fatalf(\"Expected NumConns to be 1, got %v\\n\", c.NumConns)\r\n\t\t}\r\n\r\n\t\tif c.Total != 2 {\r\n\t\t\tt.Fatalf(\"Expected Total to be at least 2, got %v\", c.Total)\r\n\t\t}\r\n\r\n\t\tc = pollConz(t, s, mode, url+\"connz?offset=2&limit=1\", &ConnzOptions{Offset: 2, Limit: 1})\r\n\t\tif c.Limit != 1 {\r\n\t\t\tt.Fatalf(\"Expected limit of 1, got %v\\n\", c.Limit)\r\n\t\t}\r\n\r\n\t\tif c.Offset != 2 {\r\n\t\t\tt.Fatalf(\"Expected offset of 2, got %v\\n\", c.Offset)\r\n\t\t}\r\n\r\n\t\tif len(c.Conns) != 0 {\r\n\t\t\tt.Fatalf(\"Expected conns of 0, got %v\\n\", len(c.Conns))\r\n\t\t}\r\n\r\n\t\tif c.NumConns != 0 {\r\n\t\t\tt.Fatalf(\"Expected NumConns to be 0, got %v\\n\", c.NumConns)\r\n\t\t}\r\n\r\n\t\tif c.Total != 2 {\r\n\t\t\tt.Fatalf(\"Expected Total to be 2, got %v\", c.Total)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzDefaultSorted(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tclients := make([]*nats.Conn, 4)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz\", nil)\r\n\t\tif c.Conns[0].Cid > c.Conns[1].Cid ||\r\n\t\t\tc.Conns[1].Cid > c.Conns[2].Cid ||\r\n\t\t\tc.Conns[2].Cid > c.Conns[3].Cid {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in ascending order by cid, got %v < %v\\n\", c.Conns[0].Cid, c.Conns[3].Cid)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByCid(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tclients := make([]*nats.Conn, 4)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=cid\", &ConnzOptions{Sort: ByCid})\r\n\t\tif c.Conns[0].Cid > c.Conns[1].Cid ||\r\n\t\t\tc.Conns[1].Cid > c.Conns[2].Cid ||\r\n\t\t\tc.Conns[2].Cid > c.Conns[3].Cid {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in ascending order by cid, got [%v, %v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].Cid, c.Conns[1].Cid, c.Conns[2].Cid, c.Conns[3].Cid)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByStart(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tclients := make([]*nats.Conn, 4)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=start\", &ConnzOptions{Sort: ByStart})\r\n\t\tif c.Conns[0].Start.After(c.Conns[1].Start) ||\r\n\t\t\tc.Conns[1].Start.After(c.Conns[2].Start) ||\r\n\t\t\tc.Conns[2].Start.After(c.Conns[3].Start) {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in ascending order by startime, got [%v, %v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].Start, c.Conns[1].Start, c.Conns[2].Start, c.Conns[3].Start)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByBytesAndMsgs(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create a connection and make it send more messages than others\r\n\tfirstClient := createClientConnSubscribeAndPublish(t, s)\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tfirstClient.Publish(\"foo\", []byte(\"Hello World\"))\r\n\t}\r\n\tdefer firstClient.Close()\r\n\tfirstClient.Flush()\r\n\r\n\tclients := make([]*nats.Conn, 3)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=bytes_to\", &ConnzOptions{Sort: ByOutBytes})\r\n\t\tif c.Conns[0].OutBytes < c.Conns[1].OutBytes ||\r\n\t\t\tc.Conns[0].OutBytes < c.Conns[2].OutBytes ||\r\n\t\t\tc.Conns[0].OutBytes < c.Conns[3].OutBytes {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by bytes to, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].OutBytes, c.Conns[1].OutBytes, c.Conns[2].OutBytes, c.Conns[3].OutBytes)\r\n\t\t}\r\n\r\n\t\tc = pollConz(t, s, mode, url+\"connz?sort=msgs_to\", &ConnzOptions{Sort: ByOutMsgs})\r\n\t\tif c.Conns[0].OutMsgs < c.Conns[1].OutMsgs ||\r\n\t\t\tc.Conns[0].OutMsgs < c.Conns[2].OutMsgs ||\r\n\t\t\tc.Conns[0].OutMsgs < c.Conns[3].OutMsgs {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by msgs from, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].OutMsgs, c.Conns[1].OutMsgs, c.Conns[2].OutMsgs, c.Conns[3].OutMsgs)\r\n\t\t}\r\n\r\n\t\tc = pollConz(t, s, mode, url+\"connz?sort=bytes_from\", &ConnzOptions{Sort: ByInBytes})\r\n\t\tif c.Conns[0].InBytes < c.Conns[1].InBytes ||\r\n\t\t\tc.Conns[0].InBytes < c.Conns[2].InBytes ||\r\n\t\t\tc.Conns[0].InBytes < c.Conns[3].InBytes {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by bytes from, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].InBytes, c.Conns[1].InBytes, c.Conns[2].InBytes, c.Conns[3].InBytes)\r\n\t\t}\r\n\r\n\t\tc = pollConz(t, s, mode, url+\"connz?sort=msgs_from\", &ConnzOptions{Sort: ByInMsgs})\r\n\t\tif c.Conns[0].InMsgs < c.Conns[1].InMsgs ||\r\n\t\t\tc.Conns[0].InMsgs < c.Conns[2].InMsgs ||\r\n\t\t\tc.Conns[0].InMsgs < c.Conns[3].InMsgs {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by msgs from, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].InMsgs, c.Conns[1].InMsgs, c.Conns[2].InMsgs, c.Conns[3].InMsgs)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByPending(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tfirstClient := createClientConnSubscribeAndPublish(t, s)\r\n\tfirstClient.Subscribe(\"hello.world\", func(m *nats.Msg) {})\r\n\tclients := make([]*nats.Conn, 3)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\tdefer firstClient.Close()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=pending\", &ConnzOptions{Sort: ByPending})\r\n\t\tif c.Conns[0].Pending < c.Conns[1].Pending ||\r\n\t\t\tc.Conns[0].Pending < c.Conns[2].Pending ||\r\n\t\t\tc.Conns[0].Pending < c.Conns[3].Pending {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by number of pending, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].Pending, c.Conns[1].Pending, c.Conns[2].Pending, c.Conns[3].Pending)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedBySubs(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tfirstClient := createClientConnSubscribeAndPublish(t, s)\r\n\tfirstClient.Subscribe(\"hello.world\", func(m *nats.Msg) {})\r\n\tdefer firstClient.Close()\r\n\r\n\tclients := make([]*nats.Conn, 3)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=subs\", &ConnzOptions{Sort: BySubs})\r\n\t\tif c.Conns[0].NumSubs < c.Conns[1].NumSubs ||\r\n\t\t\tc.Conns[0].NumSubs < c.Conns[2].NumSubs ||\r\n\t\t\tc.Conns[0].NumSubs < c.Conns[3].NumSubs {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by number of subs, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].NumSubs, c.Conns[1].NumSubs, c.Conns[2].NumSubs, c.Conns[3].NumSubs)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByLast(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tfirstClient := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer firstClient.Close()\r\n\tfirstClient.Subscribe(\"hello.world\", func(m *nats.Msg) {})\r\n\tfirstClient.Flush()\r\n\r\n\tclients := make([]*nats.Conn, 3)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t\tclients[i].Flush()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=last\", &ConnzOptions{Sort: ByLast})\r\n\t\tif c.Conns[0].LastActivity.UnixNano() < c.Conns[1].LastActivity.UnixNano() ||\r\n\t\t\tc.Conns[1].LastActivity.UnixNano() < c.Conns[2].LastActivity.UnixNano() ||\r\n\t\t\tc.Conns[2].LastActivity.UnixNano() < c.Conns[3].LastActivity.UnixNano() {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by lastActivity, got %v < one of [%v, %v, %v]\\n\",\r\n\t\t\t\tc.Conns[0].LastActivity, c.Conns[1].LastActivity, c.Conns[2].LastActivity, c.Conns[3].LastActivity)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByUptime(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tclient := createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer client.Close()\r\n\t\t// Since we check times (now-start) does not have to be big.\r\n\t\ttime.Sleep(50 * time.Millisecond)\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=uptime\", &ConnzOptions{Sort: ByUptime})\r\n\t\tnow := time.Now()\r\n\t\tups := make([]int, 4)\r\n\t\tfor i := 0; i < 4; i++ {\r\n\t\t\tups[i] = int(now.Sub(c.Conns[i].Start))\r\n\t\t}\r\n\t\tif !sort.IntsAreSorted(ups) {\r\n\t\t\td := make([]time.Duration, 4)\r\n\t\t\tfor i := 0; i < 4; i++ {\r\n\t\t\t\td[i] = time.Duration(ups[i])\r\n\t\t\t}\r\n\t\t\tt.Fatalf(\"Expected conns sorted in ascending order by uptime (now-Start), got %+v\\n\", d)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByUptimeClosedConn(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tfor i := time.Duration(1); i <= 4; i++ {\r\n\t\tc := createClientConnSubscribeAndPublish(t, s)\r\n\r\n\t\t// Grab client and asjust start time such that\r\n\t\tclient := s.getClient(uint64(i))\r\n\t\tif client == nil {\r\n\t\t\tt.Fatalf(\"Could nopt retrieve client for %d\\n\", i)\r\n\t\t}\r\n\t\tclient.mu.Lock()\r\n\t\tclient.start = client.start.Add(-10 * (4 - i) * time.Second)\r\n\t\tclient.mu.Unlock()\r\n\r\n\t\tc.Close()\r\n\t}\r\n\r\n\tcheckClosedConns(t, s, 4, time.Second)\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?state=closed&sort=uptime\", &ConnzOptions{State: ConnClosed, Sort: ByUptime})\r\n\t\tups := make([]int, 4)\r\n\t\tfor i := 0; i < 4; i++ {\r\n\t\t\tups[i] = int(c.Conns[i].Stop.Sub(c.Conns[i].Start))\r\n\t\t}\r\n\t\tif !sort.IntsAreSorted(ups) {\r\n\t\t\td := make([]time.Duration, 4)\r\n\t\t\tfor i := 0; i < 4; i++ {\r\n\t\t\t\td[i] = time.Duration(ups[i])\r\n\t\t\t}\r\n\t\t\tt.Fatalf(\"Expected conns sorted in ascending order by uptime, got %+v\\n\", d)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByStopOnOpen(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\t// 4 clients\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not create client: %v\\n\", err)\r\n\t\t}\r\n\t\tdefer c.Close()\r\n\t}\r\n\r\n\tc, err := s.Connz(&ConnzOptions{Sort: ByStop})\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expected err to be non-nil, got %+v\\n\", c)\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByStopTimeClosedConn(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\t// 4 clients\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not create client: %v\\n\", err)\r\n\t\t}\r\n\t\tc.Close()\r\n\t}\r\n\tcheckClosedConns(t, s, 4, time.Second)\r\n\r\n\t//Now adjust the Stop times for these with some random values.\r\n\ts.mu.Lock()\r\n\tnow := time.Now()\r\n\tccs := s.closed.closedClients()\r\n\tfor _, cc := range ccs {\r\n\t\tnewStop := now.Add(time.Duration(rand.Int()%120) * -time.Minute)\r\n\t\tcc.Stop = &newStop\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\turl = fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?state=closed&sort=stop\", &ConnzOptions{State: ConnClosed, Sort: ByStop})\r\n\t\tups := make([]int, 4)\r\n\t\tnowU := time.Now().UnixNano()\r\n\t\tfor i := 0; i < 4; i++ {\r\n\t\t\tups[i] = int(nowU - c.Conns[i].Stop.UnixNano())\r\n\t\t}\r\n\t\tif !sort.IntsAreSorted(ups) {\r\n\t\t\td := make([]time.Duration, 4)\r\n\t\t\tfor i := 0; i < 4; i++ {\r\n\t\t\t\td[i] = time.Duration(ups[i])\r\n\t\t\t}\r\n\t\t\tt.Fatalf(\"Expected conns sorted in ascending order by stop time, got %+v\\n\", d)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByReason(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\t// 20 clients\r\n\tfor i := 0; i < 20; i++ {\r\n\t\tc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not create client: %v\\n\", err)\r\n\t\t}\r\n\t\tc.Close()\r\n\t}\r\n\tcheckClosedConns(t, s, 20, time.Second)\r\n\r\n\t//Now adjust the Reasons for these with some random values.\r\n\ts.mu.Lock()\r\n\tccs := s.closed.closedClients()\r\n\tmax := int(ServerShutdown)\r\n\tfor _, cc := range ccs {\r\n\t\tcc.Reason = ClosedState(rand.Int() % max).String()\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\turl = fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz?state=closed&sort=reason\", &ConnzOptions{State: ConnClosed, Sort: ByReason})\r\n\t\trs := make([]string, 20)\r\n\t\tfor i := 0; i < 20; i++ {\r\n\t\t\trs[i] = c.Conns[i].Reason\r\n\t\t}\r\n\t\tif !sort.StringsAreSorted(rs) {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in order by stop reason, got %#v\\n\", rs)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByReasonOnOpen(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\t// 4 clients\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not create client: %v\\n\", err)\r\n\t\t}\r\n\t\tdefer c.Close()\r\n\t}\r\n\r\n\tc, err := s.Connz(&ConnzOptions{Sort: ByReason})\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expected err to be non-nil, got %+v\\n\", c)\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortedByIdle(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\ttestIdle := func(mode int) {\r\n\t\tfirstClient := createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer firstClient.Close()\r\n\t\tfirstClient.Subscribe(\"client.1\", func(m *nats.Msg) {})\r\n\t\tfirstClient.Flush()\r\n\r\n\t\tsecondClient := createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer secondClient.Close()\r\n\r\n\t\t// Make it such that the second client started 10 secs ago. 10 is important since bug\r\n\t\t// was strcmp, e.g. 1s vs 11s\r\n\t\tvar cid uint64\r\n\t\tswitch mode {\r\n\t\tcase 0:\r\n\t\t\tcid = uint64(2)\r\n\t\tcase 1:\r\n\t\t\tcid = uint64(4)\r\n\t\t}\r\n\t\tclient := s.getClient(cid)\r\n\t\tif client == nil {\r\n\t\t\tt.Fatalf(\"Error looking up client %v\\n\", 2)\r\n\t\t}\r\n\r\n\t\t// We want to make sure that we set start/last after the server has finished\r\n\t\t// updating this client's last activity. Doing another Flush() now (even though\r\n\t\t// one is done in createClientConnSubscribeAndPublish) ensures that server has\r\n\t\t// finished updating the client's last activity, since for that last flush there\r\n\t\t// should be no new message/sub/unsub activity.\r\n\t\tsecondClient.Flush()\r\n\r\n\t\tclient.mu.Lock()\r\n\t\tclient.start = client.start.Add(-10 * time.Second)\r\n\t\tclient.last = client.start\r\n\t\tclient.mu.Unlock()\r\n\r\n\t\t// The Idle granularity is a whole second\r\n\t\ttime.Sleep(time.Second)\r\n\t\tfirstClient.Publish(\"client.1\", []byte(\"new message\"))\r\n\r\n\t\tc := pollConz(t, s, mode, url+\"connz?sort=idle\", &ConnzOptions{Sort: ByIdle})\r\n\t\t// Make sure we are returned 2 connections...\r\n\t\tif len(c.Conns) != 2 {\r\n\t\t\tt.Fatalf(\"Expected to get two connections, got %v\", len(c.Conns))\r\n\t\t}\r\n\r\n\t\t// And that the Idle time is valid (even if equal to \"0s\")\r\n\t\tif c.Conns[0].Idle == \"\" || c.Conns[1].Idle == \"\" {\r\n\t\t\tt.Fatal(\"Expected Idle value to be valid\")\r\n\t\t}\r\n\r\n\t\tidle1, err := time.ParseDuration(c.Conns[0].Idle)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Unable to parse duration %v, err=%v\", c.Conns[0].Idle, err)\r\n\t\t}\r\n\t\tidle2, err := time.ParseDuration(c.Conns[1].Idle)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Unable to parse duration %v, err=%v\", c.Conns[0].Idle, err)\r\n\t\t}\r\n\r\n\t\tif idle2 < idle1 {\r\n\t\t\tt.Fatalf(\"Expected conns sorted in descending order by Idle, got %v < %v\\n\",\r\n\t\t\t\tidle2, idle1)\r\n\t\t}\r\n\t}\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\ttestIdle(mode)\r\n\t}\r\n}\r\n\r\nfunc TestConnzSortBadRequest(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tfirstClient := createClientConnSubscribeAndPublish(t, s)\r\n\tfirstClient.Subscribe(\"hello.world\", func(m *nats.Msg) {})\r\n\tclients := make([]*nats.Conn, 3)\r\n\tfor i := range clients {\r\n\t\tclients[i] = createClientConnSubscribeAndPublish(t, s)\r\n\t\tdefer clients[i].Close()\r\n\t}\r\n\tdefer firstClient.Close()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\treadBodyEx(t, url+\"connz?sort=foo\", http.StatusBadRequest, textPlain)\r\n\r\n\tif _, err := s.Connz(&ConnzOptions{Sort: \"foo\"}); err == nil {\r\n\t\tt.Fatal(\"Expected error, got none\")\r\n\t}\r\n}\r\n\r\nfunc pollRoutez(t *testing.T, s *Server, mode int, url string, opts *RoutezOptions) *Routez {\r\n\tt.Helper()\r\n\tif mode == 0 {\r\n\t\trz := &Routez{}\r\n\t\tbody := readBody(t, url)\r\n\t\tif err := json.Unmarshal(body, rz); err != nil {\r\n\t\t\tt.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\treturn rz\r\n\t}\r\n\trz, err := s.Routez(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on Routez: %v\", err)\r\n\t}\r\n\treturn rz\r\n}\r\n\r\nfunc TestConnzWithRoutes(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.Cluster.Host = \"127.0.0.1\"\r\n\topts.Cluster.Port = CLUSTER_PORT\r\n\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\topts = &Options{\r\n\t\tHost: \"127.0.0.1\",\r\n\t\tPort: -1,\r\n\t\tCluster: ClusterOpts{\r\n\t\t\tHost: \"127.0.0.1\",\r\n\t\t\tPort: -1,\r\n\t\t},\r\n\t\tNoLog:  true,\r\n\t\tNoSigs: true,\r\n\t}\r\n\trouteURL, _ := url.Parse(fmt.Sprintf(\"nats-route://127.0.0.1:%d\", s.ClusterAddr().Port))\r\n\topts.Routes = []*url.URL{routeURL}\r\n\r\n\tsc := RunServer(opts)\r\n\tdefer sc.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, sc)\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tc := pollConz(t, s, mode, url+\"connz\", nil)\r\n\t\t// Test contents..\r\n\t\t// Make sure routes don't show up under connz, but do under routez\r\n\t\tif c.NumConns != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 connections, got %d\\n\", c.NumConns)\r\n\t\t}\r\n\t\tif c.Conns == nil || len(c.Conns) != 0 {\r\n\t\t\tt.Fatalf(\"Expected 0 connections in array, got %p\\n\", c.Conns)\r\n\t\t}\r\n\t}\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, sc)\r\n\tdefer nc.Close()\r\n\r\n\tnc.Subscribe(\"hello.bar\", func(m *nats.Msg) {})\r\n\tnc.Flush()\r\n\tcheckExpectedSubs(t, 1, s, sc)\r\n\r\n\t// Now check routez\r\n\turls := []string{\"routez\", \"routez?subs=1\", \"routez?subs=detail\"}\r\n\tfor subs, urlSuffix := range urls {\r\n\t\tfor mode := 0; mode < 2; mode++ {\r\n\t\t\trz := pollRoutez(t, s, mode, url+urlSuffix, &RoutezOptions{Subscriptions: subs == 1, SubscriptionsDetail: subs == 2})\r\n\r\n\t\t\tif rz.NumRoutes != 1 {\r\n\t\t\t\tt.Fatalf(\"Expected 1 route, got %d\\n\", rz.NumRoutes)\r\n\t\t\t}\r\n\r\n\t\t\tif len(rz.Routes) != 1 {\r\n\t\t\t\tt.Fatalf(\"Expected route array of 1, got %v\\n\", len(rz.Routes))\r\n\t\t\t}\r\n\r\n\t\t\troute := rz.Routes[0]\r\n\r\n\t\t\tif route.DidSolicit {\r\n\t\t\t\tt.Fatalf(\"Expected unsolicited route, got %v\\n\", route.DidSolicit)\r\n\t\t\t}\r\n\r\n\t\t\t// Don't ask for subs, so there should not be any\r\n\t\t\tif subs == 0 {\r\n\t\t\t\tif len(route.Subs) != 0 {\r\n\t\t\t\t\tt.Fatalf(\"There should not be subs, got %v\", len(route.Subs))\r\n\t\t\t\t}\r\n\t\t\t} else if subs == 1 {\r\n\t\t\t\tif len(route.Subs) != 1 && len(route.SubsDetail) != 0 {\r\n\t\t\t\t\tt.Fatalf(\"There should be 1 sub, got %v\", len(route.Subs))\r\n\t\t\t\t}\r\n\t\t\t} else if subs == 2 {\r\n\t\t\t\tif len(route.SubsDetail) != 1 && len(route.Subs) != 0 {\r\n\t\t\t\t\tt.Fatalf(\"There should be 1 sub, got %v\", len(route.SubsDetail))\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Test JSONP\r\n\treadBodyEx(t, url+\"routez?callback=callback\", http.StatusOK, appJSContent)\r\n}\r\n\r\nfunc TestRoutezWithBadParams(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/routez?\", s.MonitorAddr().Port)\r\n\treadBodyEx(t, url+\"subs=xxx\", http.StatusBadRequest, textPlain)\r\n}\r\n\r\nfunc pollSubsz(t *testing.T, s *Server, mode int, url string, opts *SubszOptions) *Subsz {\r\n\tt.Helper()\r\n\tif mode == 0 {\r\n\t\tbody := readBody(t, url)\r\n\t\tsz := &Subsz{}\r\n\t\tif err := json.Unmarshal(body, sz); err != nil {\r\n\t\t\tt.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\treturn sz\r\n\t}\r\n\tsz, err := s.Subsz(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on Subsz: %v\", err)\r\n\t}\r\n\treturn sz\r\n}\r\n\r\nfunc TestSubsz(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tsl := pollSubsz(t, s, mode, url+\"subsz\", nil)\r\n\t\tif sl.NumSubs != 0 {\r\n\t\t\tt.Fatalf(\"Expected NumSubs of 0, got %d\\n\", sl.NumSubs)\r\n\t\t}\r\n\t\tif sl.NumInserts != 1 {\r\n\t\t\tt.Fatalf(\"Expected NumInserts of 1, got %d\\n\", sl.NumInserts)\r\n\t\t}\r\n\t\tif sl.NumMatches != 1 {\r\n\t\t\tt.Fatalf(\"Expected NumMatches of 1, got %d\\n\", sl.NumMatches)\r\n\t\t}\r\n\t}\r\n\r\n\t// Test JSONP\r\n\treadBodyEx(t, url+\"subsz?callback=callback\", http.StatusOK, appJSContent)\r\n}\r\n\r\nfunc TestSubszDetails(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tnc.Subscribe(\"foo.*\", func(m *nats.Msg) {})\r\n\tnc.Subscribe(\"foo.bar\", func(m *nats.Msg) {})\r\n\tnc.Subscribe(\"foo.foo\", func(m *nats.Msg) {})\r\n\r\n\tnc.Publish(\"foo.bar\", []byte(\"Hello\"))\r\n\tnc.Publish(\"foo.baz\", []byte(\"Hello\"))\r\n\tnc.Publish(\"foo.foo\", []byte(\"Hello\"))\r\n\r\n\tnc.Flush()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tsl := pollSubsz(t, s, mode, url+\"subsz?subs=1\", &SubszOptions{Subscriptions: true})\r\n\t\tif sl.NumSubs != 3 {\r\n\t\t\tt.Fatalf(\"Expected NumSubs of 3, got %d\\n\", sl.NumSubs)\r\n\t\t}\r\n\t\tif sl.Total != 3 {\r\n\t\t\tt.Fatalf(\"Expected Total of 3, got %d\\n\", sl.Total)\r\n\t\t}\r\n\t\tif len(sl.Subs) != 3 {\r\n\t\t\tt.Fatalf(\"Expected subscription details for 3 subs, got %d\\n\", len(sl.Subs))\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestSubszWithOffsetAndLimit(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tfor i := 0; i < 200; i++ {\r\n\t\tnc.Subscribe(fmt.Sprintf(\"foo.%d\", i), func(m *nats.Msg) {})\r\n\t}\r\n\tnc.Flush()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tsl := pollSubsz(t, s, mode, url+\"subsz?subs=1&offset=10&limit=100\", &SubszOptions{Subscriptions: true, Offset: 10, Limit: 100})\r\n\t\tif sl.NumSubs != 200 {\r\n\t\t\tt.Fatalf(\"Expected NumSubs of 200, got %d\\n\", sl.NumSubs)\r\n\t\t}\r\n\t\tif sl.Total != 100 {\r\n\t\t\tt.Fatalf(\"Expected Total of 100, got %d\\n\", sl.Total)\r\n\t\t}\r\n\t\tif sl.Offset != 10 {\r\n\t\t\tt.Fatalf(\"Expected Offset of 10, got %d\\n\", sl.Offset)\r\n\t\t}\r\n\t\tif sl.Limit != 100 {\r\n\t\t\tt.Fatalf(\"Expected Total of 100, got %d\\n\", sl.Limit)\r\n\t\t}\r\n\t\tif len(sl.Subs) != 100 {\r\n\t\t\tt.Fatalf(\"Expected subscription details for 100 subs, got %d\\n\", len(sl.Subs))\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestSubszTestPubSubject(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tnc.Subscribe(\"foo.*\", func(m *nats.Msg) {})\r\n\tnc.Subscribe(\"foo.bar\", func(m *nats.Msg) {})\r\n\tnc.Subscribe(\"foo.foo\", func(m *nats.Msg) {})\r\n\tnc.Flush()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tsl := pollSubsz(t, s, mode, url+\"subsz?subs=1&test=foo.foo\", &SubszOptions{Subscriptions: true, Test: \"foo.foo\"})\r\n\t\tif sl.Total != 2 {\r\n\t\t\tt.Fatalf(\"Expected Total of 2 match, got %d\\n\", sl.Total)\r\n\t\t}\r\n\t\tif len(sl.Subs) != 2 {\r\n\t\t\tt.Fatalf(\"Expected subscription details for 2 matching subs, got %d\\n\", len(sl.Subs))\r\n\t\t}\r\n\t\tsl = pollSubsz(t, s, mode, url+\"subsz?subs=1&test=foo\", &SubszOptions{Subscriptions: true, Test: \"foo\"})\r\n\t\tif len(sl.Subs) != 0 {\r\n\t\t\tt.Fatalf(\"Expected no matching subs, got %d\\n\", len(sl.Subs))\r\n\t\t}\r\n\t}\r\n\t// Make sure we get an error with invalid test subject.\r\n\ttestUrl := url + \"subsz?subs=1&\"\r\n\treadBodyEx(t, testUrl+\"test=*\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, testUrl+\"test=foo.*\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, testUrl+\"test=foo.>\", http.StatusBadRequest, textPlain)\r\n\treadBodyEx(t, testUrl+\"test=foo..bar\", http.StatusBadRequest, textPlain)\r\n}\r\n\r\nfunc TestSubszMultiAccount(t *testing.T) {\r\n\ts := runMonitorServerWithAccounts()\r\n\tdefer s.Shutdown()\r\n\r\n\tncA := createClientConnWithUserSubscribeAndPublish(t, s, \"a\", \"a\")\r\n\tdefer ncA.Close()\r\n\r\n\tncA.Subscribe(\"foo.*\", func(m *nats.Msg) {})\r\n\tncA.Subscribe(\"foo.bar\", func(m *nats.Msg) {})\r\n\tncA.Subscribe(\"foo.foo\", func(m *nats.Msg) {})\r\n\r\n\tncA.Publish(\"foo.bar\", []byte(\"Hello\"))\r\n\tncA.Publish(\"foo.baz\", []byte(\"Hello\"))\r\n\tncA.Publish(\"foo.foo\", []byte(\"Hello\"))\r\n\r\n\tncA.Flush()\r\n\r\n\tncB := createClientConnWithUserSubscribeAndPublish(t, s, \"b\", \"b\")\r\n\tdefer ncB.Close()\r\n\r\n\tncB.Subscribe(\"foo.*\", func(m *nats.Msg) {})\r\n\tncB.Subscribe(\"foo.bar\", func(m *nats.Msg) {})\r\n\tncB.Subscribe(\"foo.foo\", func(m *nats.Msg) {})\r\n\r\n\tncB.Publish(\"foo.bar\", []byte(\"Hello\"))\r\n\tncB.Publish(\"foo.baz\", []byte(\"Hello\"))\r\n\tncB.Publish(\"foo.foo\", []byte(\"Hello\"))\r\n\r\n\tncB.Flush()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tsl := pollSubsz(t, s, mode, url+\"subsz?subs=1\", &SubszOptions{Subscriptions: true})\r\n\t\tif sl.NumSubs != 6 {\r\n\t\t\tt.Fatalf(\"Expected NumSubs of 6, got %d\\n\", sl.NumSubs)\r\n\t\t}\r\n\t\tif sl.Total != 6 {\r\n\t\t\tt.Fatalf(\"Expected Total of 6, got %d\\n\", sl.Total)\r\n\t\t}\r\n\t\tif len(sl.Subs) != 6 {\r\n\t\t\tt.Fatalf(\"Expected subscription details for 6 subs, got %d\\n\", len(sl.Subs))\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestSubszMultiAccountWithOffsetAndLimit(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tncA := createClientConnWithUserSubscribeAndPublish(t, s, \"a\", \"a\")\r\n\tdefer ncA.Close()\r\n\r\n\tfor i := 0; i < 200; i++ {\r\n\t\tncA.Subscribe(fmt.Sprintf(\"foo.%d\", i), func(m *nats.Msg) {})\r\n\t}\r\n\tncA.Flush()\r\n\r\n\tncB := createClientConnWithUserSubscribeAndPublish(t, s, \"b\", \"b\")\r\n\tdefer ncB.Close()\r\n\r\n\tfor i := 0; i < 200; i++ {\r\n\t\tncB.Subscribe(fmt.Sprintf(\"foo.%d\", i), func(m *nats.Msg) {})\r\n\t}\r\n\tncB.Flush()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tsl := pollSubsz(t, s, mode, url+\"subsz?subs=1&offset=10&limit=100\", &SubszOptions{Subscriptions: true, Offset: 10, Limit: 100})\r\n\t\tif sl.NumSubs != 400 {\r\n\t\t\tt.Fatalf(\"Expected NumSubs of 200, got %d\\n\", sl.NumSubs)\r\n\t\t}\r\n\t\tif sl.Total != 100 {\r\n\t\t\tt.Fatalf(\"Expected Total of 100, got %d\\n\", sl.Total)\r\n\t\t}\r\n\t\tif sl.Offset != 10 {\r\n\t\t\tt.Fatalf(\"Expected Offset of 10, got %d\\n\", sl.Offset)\r\n\t\t}\r\n\t\tif sl.Limit != 100 {\r\n\t\t\tt.Fatalf(\"Expected Total of 100, got %d\\n\", sl.Limit)\r\n\t\t}\r\n\t\tif len(sl.Subs) != 100 {\r\n\t\t\tt.Fatalf(\"Expected subscription details for 100 subs, got %d\\n\", len(sl.Subs))\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Tests handle root\r\nfunc TestHandleRoot(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\tdefer nc.Close()\r\n\r\n\tresp, err := http.Get(fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected no error: Got %v\\n\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tt.Fatalf(\"Expected a %d response, got %d\\n\", http.StatusOK, resp.StatusCode)\r\n\t}\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected no error reading body: Got %v\\n\", err)\r\n\t}\r\n\tfor _, b := range body {\r\n\t\tif b > unicode.MaxASCII {\r\n\t\t\tt.Fatalf(\"Expected body to contain only ASCII characters, but got %v\\n\", b)\r\n\t\t}\r\n\t}\r\n\r\n\tct := resp.Header.Get(\"Content-Type\")\r\n\tif !strings.Contains(ct, \"text/html\") {\r\n\t\tt.Fatalf(\"Expected text/html response, got %s\\n\", ct)\r\n\t}\r\n}\r\n\r\nfunc TestConnzWithNamedClient(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tclientName := \"test-client\"\r\n\tnc := createClientConnWithName(t, clientName, s)\r\n\tdefer nc.Close()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\t// Confirm server is exposing client name in monitoring endpoint.\r\n\t\tc := pollConz(t, s, mode, url+\"connz\", nil)\r\n\t\tgot := len(c.Conns)\r\n\t\texpected := 1\r\n\t\tif got != expected {\r\n\t\t\tt.Fatalf(\"Expected %d connection in array, got %d\\n\", expected, got)\r\n\t\t}\r\n\r\n\t\tconn := c.Conns[0]\r\n\t\tif conn.Name != clientName {\r\n\t\t\tt.Fatalf(\"Expected client to have name %q. got %q\", clientName, conn.Name)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzWithStateForClosedConns(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnumEach := 10\r\n\t// Create 10 closed, and 10 to leave open.\r\n\tfor i := 0; i < numEach; i++ {\r\n\t\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\t\tnc.Subscribe(\"hello.closed.conns\", func(m *nats.Msg) {})\r\n\t\tnc.Close()\r\n\t\tnc = createClientConnSubscribeAndPublish(t, s)\r\n\t\tnc.Subscribe(\"hello.open.conns\", func(m *nats.Msg) {})\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tcheckFor(t, 2*time.Second, 10*time.Millisecond, func() error {\r\n\t\t\t// Look at all open\r\n\t\t\tc := pollConz(t, s, mode, url+\"connz?state=open\", &ConnzOptions{State: ConnOpen})\r\n\t\t\tif lc := len(c.Conns); lc != numEach {\r\n\t\t\t\treturn fmt.Errorf(\"Expected %d connections in array, got %d\", numEach, lc)\r\n\t\t\t}\r\n\t\t\t// Look at all closed\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?state=closed\", &ConnzOptions{State: ConnClosed})\r\n\t\t\tif lc := len(c.Conns); lc != numEach {\r\n\t\t\t\treturn fmt.Errorf(\"Expected %d connections in array, got %d\", numEach, lc)\r\n\t\t\t}\r\n\t\t\t// Look at all\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?state=ALL\", &ConnzOptions{State: ConnAll})\r\n\t\t\tif lc := len(c.Conns); lc != numEach*2 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected %d connections in array, got %d\", 2*numEach, lc)\r\n\t\t\t}\r\n\t\t\t// Look at CID #1, which is in closed.\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=1&state=open\", &ConnzOptions{CID: 1, State: ConnOpen})\r\n\t\t\tif lc := len(c.Conns); lc != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected no connections in open array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=1&state=closed\", &ConnzOptions{CID: 1, State: ConnClosed})\r\n\t\t\tif lc := len(c.Conns); lc != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected a connection in closed array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=1&state=ALL\", &ConnzOptions{CID: 1, State: ConnAll})\r\n\t\t\tif lc := len(c.Conns); lc != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected a connection in closed array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=1&state=closed&subs=true\",\r\n\t\t\t\t&ConnzOptions{CID: 1, State: ConnClosed, Subscriptions: true})\r\n\t\t\tif lc := len(c.Conns); lc != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected a connection in closed array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\tci := c.Conns[0]\r\n\t\t\tif ci.NumSubs != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected NumSubs to be 1, got %d\", ci.NumSubs)\r\n\t\t\t}\r\n\t\t\tif len(ci.Subs) != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected len(ci.Subs) to be 1 also, got %d\", len(ci.Subs))\r\n\t\t\t}\r\n\t\t\t// Now ask for same thing without subs and make sure they are not returned.\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=1&state=closed&subs=false\",\r\n\t\t\t\t&ConnzOptions{CID: 1, State: ConnClosed, Subscriptions: false})\r\n\t\t\tif lc := len(c.Conns); lc != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected a connection in closed array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\tci = c.Conns[0]\r\n\t\t\tif ci.NumSubs != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected NumSubs to be 1, got %d\", ci.NumSubs)\r\n\t\t\t}\r\n\t\t\tif len(ci.Subs) != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected len(ci.Subs) to be 0 since subs=false, got %d\", len(ci.Subs))\r\n\t\t\t}\r\n\r\n\t\t\t// CID #2 is in open\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=2&state=open\", &ConnzOptions{CID: 2, State: ConnOpen})\r\n\t\t\tif lc := len(c.Conns); lc != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected a connection in open array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\tc = pollConz(t, s, mode, url+\"connz?cid=2&state=closed\", &ConnzOptions{CID: 2, State: ConnClosed})\r\n\t\t\tif lc := len(c.Conns); lc != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"Expected no connections in closed array, got %d\", lc)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n}\r\n\r\n// Make sure options for ConnInfo like subs=1, authuser, etc do not cause a race.\r\nfunc TestConnzClosedConnsRace(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create 100 closed connections.\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tnc := createClientConnSubscribeAndPublish(t, s)\r\n\t\tnc.Close()\r\n\t}\r\n\r\n\turlWithoutSubs := fmt.Sprintf(\"http://127.0.0.1:%d/connz?state=closed\", s.MonitorAddr().Port)\r\n\turlWithSubs := urlWithoutSubs + \"&subs=true\"\r\n\r\n\tcheckClosedConns(t, s, 100, 2*time.Second)\r\n\r\n\twg := &sync.WaitGroup{}\r\n\r\n\tfn := func(url string) {\r\n\t\tdeadline := time.Now().Add(1 * time.Second)\r\n\t\tfor time.Now().Before(deadline) {\r\n\t\t\tc := pollConz(t, s, 0, url, nil)\r\n\t\t\tif len(c.Conns) != 100 {\r\n\t\t\t\tt.Errorf(\"Incorrect Results: %+v\\n\", c)\r\n\t\t\t}\r\n\t\t}\r\n\t\twg.Done()\r\n\t}\r\n\r\n\twg.Add(2)\r\n\tgo fn(urlWithSubs)\r\n\tgo fn(urlWithoutSubs)\r\n\twg.Wait()\r\n}\r\n\r\n// Make sure a bad client that is disconnected right away has proper values.\r\nfunc TestConnzClosedConnsBadClient(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\r\n\trc, err := net.Dial(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on dial: %v\", err)\r\n\t}\r\n\trc.Close()\r\n\r\n\tcheckClosedConns(t, s, 1, 2*time.Second)\r\n\r\n\tc := pollConz(t, s, 1, \"\", &ConnzOptions{State: ConnClosed})\r\n\tif len(c.Conns) != 1 {\r\n\t\tt.Errorf(\"Incorrect Results: %+v\\n\", c)\r\n\t}\r\n\tci := c.Conns[0]\r\n\r\n\tuptime := ci.Stop.Sub(ci.Start)\r\n\tidle, err := time.ParseDuration(ci.Idle)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not parse Idle: %v\\n\", err)\r\n\t}\r\n\tif idle > uptime {\r\n\t\tt.Fatalf(\"Idle can't be larger then uptime, %v vs %v\\n\", idle, uptime)\r\n\t}\r\n\tif ci.LastActivity.IsZero() {\r\n\t\tt.Fatalf(\"LastActivity should not be Zero\\n\")\r\n\t}\r\n}\r\n\r\n// Make sure a bad client that tries to connect plain to TLS has proper values.\r\nfunc TestConnzClosedConnsBadTLSClient(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\r\n\ttc := &TLSConfigOpts{}\r\n\ttc.CertFile = \"configs/certs/server.pem\"\r\n\ttc.KeyFile = \"configs/certs/key.pem\"\r\n\r\n\tvar err error\r\n\topts := DefaultMonitorOptions()\r\n\topts.TLSTimeout = 1.5 // 1.5 seconds\r\n\topts.TLSConfig, err = GenTLSConfig(tc)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating TSL config: %v\", err)\r\n\t}\r\n\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\topts = s.getOpts()\r\n\r\n\trc, err := net.Dial(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on dial: %v\", err)\r\n\t}\r\n\trc.Write([]byte(\"CONNECT {}\\r\\n\"))\r\n\trc.Close()\r\n\r\n\tcheckClosedConns(t, s, 1, 2*time.Second)\r\n\r\n\tc := pollConz(t, s, 1, \"\", &ConnzOptions{State: ConnClosed})\r\n\tif len(c.Conns) != 1 {\r\n\t\tt.Errorf(\"Incorrect Results: %+v\\n\", c)\r\n\t}\r\n\tci := c.Conns[0]\r\n\r\n\tuptime := ci.Stop.Sub(ci.Start)\r\n\tidle, err := time.ParseDuration(ci.Idle)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not parse Idle: %v\\n\", err)\r\n\t}\r\n\tif idle > uptime {\r\n\t\tt.Fatalf(\"Idle can't be larger then uptime, %v vs %v\\n\", idle, uptime)\r\n\t}\r\n\tif ci.LastActivity.IsZero() {\r\n\t\tt.Fatalf(\"LastActivity should not be Zero\\n\")\r\n\t}\r\n}\r\n\r\n// Create a connection to test ConnInfo\r\nfunc createClientConnWithUserSubscribeAndPublish(t *testing.T, s *Server, user, pwd string) *nats.Conn {\r\n\tnatsURL := \"\"\r\n\tif user == \"\" {\r\n\t\tnatsURL = fmt.Sprintf(\"nats://127.0.0.1:%d\", s.Addr().(*net.TCPAddr).Port)\r\n\t} else {\r\n\t\tnatsURL = fmt.Sprintf(\"nats://%s:%s@127.0.0.1:%d\", user, pwd, s.Addr().(*net.TCPAddr).Port)\r\n\t}\r\n\tclient := nats.DefaultOptions\r\n\tclient.Servers = []string{natsURL}\r\n\tnc, err := client.Connect()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v to: %s\\n\", err, natsURL)\r\n\t}\r\n\r\n\tch := make(chan bool)\r\n\tinbox := nats.NewInbox()\r\n\tsub, err := nc.Subscribe(inbox, func(m *nats.Msg) { ch <- true })\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing to `%s`: %v\\n\", inbox, err)\r\n\t}\r\n\tnc.Publish(inbox, []byte(\"Hello\"))\r\n\t// Wait for message\r\n\t<-ch\r\n\tsub.Unsubscribe()\r\n\tclose(ch)\r\n\tnc.Flush()\r\n\treturn nc\r\n}\r\n\r\nfunc createClientConnSubscribeAndPublish(t *testing.T, s *Server) *nats.Conn {\r\n\treturn createClientConnWithUserSubscribeAndPublish(t, s, \"\", \"\")\r\n}\r\n\r\nfunc createClientConnWithName(t *testing.T, name string, s *Server) *nats.Conn {\r\n\tnatsURI := fmt.Sprintf(\"nats://127.0.0.1:%d\", s.Addr().(*net.TCPAddr).Port)\r\n\r\n\tclient := nats.DefaultOptions\r\n\tclient.Servers = []string{natsURI}\r\n\tclient.Name = name\r\n\tnc, err := client.Connect()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\treturn nc\r\n}\r\n\r\nfunc TestStacksz(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\tbody := readBody(t, url+\"stacksz\")\r\n\t// Check content\r\n\tstr := string(body)\r\n\tif !strings.Contains(str, \"HandleStacksz\") {\r\n\t\tt.Fatalf(\"Result does not seem to contain server's stacks:\\n%v\", str)\r\n\t}\r\n}\r\n\r\nfunc TestConcurrentMonitoring(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\t// Get some endpoints. Make sure we have at least varz,\r\n\t// and the more the merrier.\r\n\tendpoints := []string{\"varz\", \"varz\", \"varz\", \"connz\", \"connz\", \"subsz\", \"subsz\", \"routez\", \"routez\"}\r\n\twg := &sync.WaitGroup{}\r\n\twg.Add(len(endpoints))\r\n\tech := make(chan string, len(endpoints))\r\n\r\n\tfor _, e := range endpoints {\r\n\t\tgo func(endpoint string) {\r\n\t\t\tdefer wg.Done()\r\n\t\t\tfor i := 0; i < 50; i++ {\r\n\t\t\t\tresp, err := http.Get(url + endpoint)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tech <- fmt.Sprintf(\"Expected no error: Got %v\\n\", err)\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\tdefer resp.Body.Close()\r\n\t\t\t\tif resp.StatusCode != http.StatusOK {\r\n\t\t\t\t\tech <- fmt.Sprintf(\"Expected a %v response, got %d\\n\", http.StatusOK, resp.StatusCode)\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\tct := resp.Header.Get(\"Content-Type\")\r\n\t\t\t\tif ct != \"application/json\" {\r\n\t\t\t\t\tech <- fmt.Sprintf(\"Expected application/json content-type, got %s\\n\", ct)\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\tif _, err := ioutil.ReadAll(resp.Body); err != nil {\r\n\t\t\t\t\tech <- fmt.Sprintf(\"Got an error reading the body: %v\\n\", err)\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\tresp.Body.Close()\r\n\t\t\t}\r\n\t\t}(e)\r\n\t}\r\n\twg.Wait()\r\n\t// Check for any errors\r\n\tselect {\r\n\tcase err := <-ech:\r\n\t\tt.Fatal(err)\r\n\tdefault:\r\n\t}\r\n}\r\n\r\nfunc TestMonitorHandler(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\thandler := s.HTTPHandler()\r\n\tif handler == nil {\r\n\t\tt.Fatal(\"HTTP Handler should be set\")\r\n\t}\r\n\ts.Shutdown()\r\n\thandler = s.HTTPHandler()\r\n\tif handler != nil {\r\n\t\tt.Fatal(\"HTTP Handler should be nil\")\r\n\t}\r\n}\r\n\r\nfunc TestMonitorRoutezRace(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\tsrvAOpts := DefaultMonitorOptions()\r\n\tsrvAOpts.Cluster.Port = -1\r\n\tsrvA := RunServer(srvAOpts)\r\n\tdefer srvA.Shutdown()\r\n\r\n\tsrvBOpts := nextServerOpts(srvAOpts)\r\n\tsrvBOpts.Routes = RoutesFromStr(fmt.Sprintf(\"nats://127.0.0.1:%d\", srvA.ClusterAddr().Port))\r\n\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/\", srvA.MonitorAddr().Port)\r\n\tdoneCh := make(chan struct{})\r\n\tgo func() {\r\n\t\tdefer func() {\r\n\t\t\tdoneCh <- struct{}{}\r\n\t\t}()\r\n\t\tfor i := 0; i < 10; i++ {\r\n\t\t\ttime.Sleep(10 * time.Millisecond)\r\n\t\t\t// Reset ports\r\n\t\t\tsrvBOpts.Port = -1\r\n\t\t\tsrvBOpts.Cluster.Port = -1\r\n\t\t\tsrvB := RunServer(srvBOpts)\r\n\t\t\ttime.Sleep(20 * time.Millisecond)\r\n\t\t\tsrvB.Shutdown()\r\n\t\t}\r\n\t}()\r\n\tdone := false\r\n\tfor !done {\r\n\t\tif resp, err := http.Get(url + \"routez\"); err != nil {\r\n\t\t\ttime.Sleep(10 * time.Millisecond)\r\n\t\t} else {\r\n\t\t\tresp.Body.Close()\r\n\t\t}\r\n\t\tselect {\r\n\t\tcase <-doneCh:\r\n\t\t\tdone = true\r\n\t\tdefault:\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestConnzTLSInHandshake(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\r\n\ttc := &TLSConfigOpts{}\r\n\ttc.CertFile = \"configs/certs/server.pem\"\r\n\ttc.KeyFile = \"configs/certs/key.pem\"\r\n\r\n\tvar err error\r\n\topts := DefaultMonitorOptions()\r\n\topts.TLSTimeout = 1.5 // 1.5 seconds\r\n\topts.TLSConfig, err = GenTLSConfig(tc)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating TSL config: %v\", err)\r\n\t}\r\n\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create bare TCP connection to delay client TLS handshake\r\n\tc, err := net.Dial(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on dial: %v\", err)\r\n\t}\r\n\tdefer c.Close()\r\n\r\n\t// Wait for the connection to be registered\r\n\tcheckClientsCount(t, s, 1)\r\n\r\n\tstart := time.Now()\r\n\tendpoint := fmt.Sprintf(\"http://%s:%d/connz\", opts.HTTPHost, s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tconnz := pollConz(t, s, mode, endpoint, nil)\r\n\t\tduration := time.Since(start)\r\n\t\tif duration >= 1500*time.Millisecond {\r\n\t\t\tt.Fatalf(\"Looks like connz blocked on handshake, took %v\", duration)\r\n\t\t}\r\n\t\tif len(connz.Conns) != 1 {\r\n\t\t\tt.Fatalf(\"Expected 1 conn, got %v\", len(connz.Conns))\r\n\t\t}\r\n\t\tconn := connz.Conns[0]\r\n\t\t// TLS fields should be not set\r\n\t\tif conn.TLSVersion != \"\" || conn.TLSCipher != \"\" {\r\n\t\t\tt.Fatalf(\"Expected TLS fields to not be set, got version:%v cipher:%v\", conn.TLSVersion, conn.TLSCipher)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestServerIDs(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tmurl := fmt.Sprintf(\"http://127.0.0.1:%d/\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tv := pollVarz(t, s, mode, murl+\"varz\", nil)\r\n\t\tif v.ID == \"\" {\r\n\t\t\tt.Fatal(\"Varz ID is empty\")\r\n\t\t}\r\n\t\tc := pollConz(t, s, mode, murl+\"connz\", nil)\r\n\t\tif c.ID == \"\" {\r\n\t\t\tt.Fatal(\"Connz ID is empty\")\r\n\t\t}\r\n\t\tr := pollRoutez(t, s, mode, murl+\"routez\", nil)\r\n\t\tif r.ID == \"\" {\r\n\t\t\tt.Fatal(\"Routez ID is empty\")\r\n\t\t}\r\n\t\tif v.ID != c.ID || v.ID != r.ID {\r\n\t\t\tt.Fatalf(\"Varz ID [%s] is not equal to Connz ID [%s] or Routez ID [%s]\", v.ID, c.ID, r.ID)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestHttpStatsNoUpdatedWhenUsingServerFuncs(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tfor i := 0; i < 10; i++ {\r\n\t\ts.Varz(nil)\r\n\t\ts.Connz(nil)\r\n\t\ts.Routez(nil)\r\n\t\ts.Subsz(nil)\r\n\t}\r\n\r\n\tv, _ := s.Varz(nil)\r\n\tendpoints := []string{VarzPath, ConnzPath, RoutezPath, SubszPath}\r\n\tfor _, e := range endpoints {\r\n\t\tstats := v.HTTPReqStats[e]\r\n\t\tif stats != 0 {\r\n\t\t\tt.Fatalf(\"Expected HTTPReqStats for %q to be 0, got %v\", e, stats)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestClusterEmptyWhenNotDefined(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tbody := readBody(t, fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port))\r\n\tvar v map[string]interface{}\r\n\tif err := json.Unmarshal(body, &v); err != nil {\r\n\t\tstackFatalf(t, \"Got an error unmarshalling the body: %v\\n\", err)\r\n\t}\r\n\t// Cluster can empty, or be defined but that needs to be empty.\r\n\tc, ok := v[\"cluster\"]\r\n\tif !ok {\r\n\t\treturn\r\n\t}\r\n\tif len(c.(map[string]interface{})) != 0 {\r\n\t\tt.Fatalf(\"Expected an empty cluster definition, instead got %+v\\n\", c)\r\n\t}\r\n}\r\n\r\nfunc TestRoutezPermissions(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.Cluster.Host = \"127.0.0.1\"\r\n\topts.Cluster.Port = -1\r\n\topts.Cluster.Permissions = &RoutePermissions{\r\n\t\tImport: &SubjectPermission{\r\n\t\t\tAllow: []string{\"foo\"},\r\n\t\t},\r\n\t\tExport: &SubjectPermission{\r\n\t\t\tAllow: []string{\"*\"},\r\n\t\t\tDeny:  []string{\"foo\", \"nats\"},\r\n\t\t},\r\n\t}\r\n\r\n\ts1 := RunServer(opts)\r\n\tdefer s1.Shutdown()\r\n\r\n\topts = DefaultMonitorOptions()\r\n\topts.Cluster.Host = \"127.0.0.1\"\r\n\topts.Cluster.Port = -1\r\n\trouteURL, _ := url.Parse(fmt.Sprintf(\"nats-route://127.0.0.1:%d\", s1.ClusterAddr().Port))\r\n\topts.Routes = []*url.URL{routeURL}\r\n\topts.HTTPPort = -1\r\n\r\n\ts2 := RunServer(opts)\r\n\tdefer s2.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s1, s2)\r\n\r\n\turls := []string{\r\n\t\tfmt.Sprintf(\"http://127.0.0.1:%d/routez\", s1.MonitorAddr().Port),\r\n\t\tfmt.Sprintf(\"http://127.0.0.1:%d/routez\", s2.MonitorAddr().Port),\r\n\t}\r\n\tservers := []*Server{s1, s2}\r\n\r\n\tfor i, url := range urls {\r\n\t\tfor mode := 0; mode < 2; mode++ {\r\n\t\t\trz := pollRoutez(t, servers[i], mode, url, nil)\r\n\t\t\t// For server 1, we expect to see imports and exports\r\n\t\t\tif i == 0 {\r\n\t\t\t\tif rz.Import == nil || rz.Import.Allow == nil ||\r\n\t\t\t\t\tlen(rz.Import.Allow) != 1 || rz.Import.Allow[0] != \"foo\" ||\r\n\t\t\t\t\trz.Import.Deny != nil {\r\n\t\t\t\t\tt.Fatalf(\"Unexpected Import %v\", rz.Import)\r\n\t\t\t\t}\r\n\t\t\t\tif rz.Export == nil || rz.Export.Allow == nil || rz.Export.Deny == nil ||\r\n\t\t\t\t\tlen(rz.Export.Allow) != 1 || rz.Export.Allow[0] != \"*\" ||\r\n\t\t\t\t\tlen(rz.Export.Deny) != 2 || rz.Export.Deny[0] != \"foo\" || rz.Export.Deny[1] != \"nats\" {\r\n\t\t\t\t\tt.Fatalf(\"Unexpected Export %v\", rz.Export)\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\t// We expect to see NO imports and exports for server B by default.\r\n\t\t\t\tif rz.Import != nil {\r\n\t\t\t\t\tt.Fatal(\"Routez body should NOT contain \\\"import\\\" information.\")\r\n\t\t\t\t}\r\n\t\t\t\tif rz.Export != nil {\r\n\t\t\t\t\tt.Fatal(\"Routez body should NOT contain \\\"export\\\" information.\")\r\n\t\t\t\t}\r\n\t\t\t\t// We do expect to see them show up for the information we have on Server A though.\r\n\t\t\t\tif len(rz.Routes) != 1 {\r\n\t\t\t\t\tt.Fatalf(\"Expected route array of 1, got %v\\n\", len(rz.Routes))\r\n\t\t\t\t}\r\n\t\t\t\troute := rz.Routes[0]\r\n\t\t\t\tif route.Import == nil || route.Import.Allow == nil ||\r\n\t\t\t\t\tlen(route.Import.Allow) != 1 || route.Import.Allow[0] != \"foo\" ||\r\n\t\t\t\t\troute.Import.Deny != nil {\r\n\t\t\t\t\tt.Fatalf(\"Unexpected Import %v\", route.Import)\r\n\t\t\t\t}\r\n\t\t\t\tif route.Export == nil || route.Export.Allow == nil || route.Export.Deny == nil ||\r\n\t\t\t\t\tlen(route.Export.Allow) != 1 || route.Export.Allow[0] != \"*\" ||\r\n\t\t\t\t\tlen(route.Export.Deny) != 2 || route.Export.Deny[0] != \"foo\" || route.Export.Deny[1] != \"nats\" {\r\n\t\t\t\t\tt.Fatalf(\"Unexpected Export %v\", route.Export)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Benchmark our Connz generation. Don't use HTTP here, just measure server endpoint.\r\nfunc Benchmark_Connz(b *testing.B) {\r\n\truntime.MemProfileRate = 0\r\n\r\n\ts := runMonitorServerNoHTTPPort()\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\t// Create 250 connections with 100 subs each.\r\n\tfor i := 0; i < 250; i++ {\r\n\t\tnc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tb.Fatalf(\"Error on connection[%d] to %s: %v\", i, url, err)\r\n\t\t}\r\n\t\tfor x := 0; x < 100; x++ {\r\n\t\t\tsubj := fmt.Sprintf(\"foo.%d\", x)\r\n\t\t\tnc.Subscribe(subj, func(m *nats.Msg) {})\r\n\t\t}\r\n\t\tnc.Flush()\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\tb.ResetTimer()\r\n\truntime.MemProfileRate = 1\r\n\r\n\tcopts := &ConnzOptions{Subscriptions: false}\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\t_, err := s.Connz(copts)\r\n\t\tif err != nil {\r\n\t\t\tb.Fatalf(\"Error on Connz(): %v\", err)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc Benchmark_Varz(b *testing.B) {\r\n\truntime.MemProfileRate = 0\r\n\r\n\ts := runMonitorServerNoHTTPPort()\r\n\tdefer s.Shutdown()\r\n\r\n\tb.ResetTimer()\r\n\truntime.MemProfileRate = 1\r\n\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\t_, err := s.Varz(nil)\r\n\t\tif err != nil {\r\n\t\t\tb.Fatalf(\"Error on Connz(): %v\", err)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc Benchmark_VarzHttp(b *testing.B) {\r\n\truntime.MemProfileRate = 0\r\n\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tmurl := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port)\r\n\r\n\tb.ResetTimer()\r\n\truntime.MemProfileRate = 1\r\n\r\n\tfor i := 0; i < b.N; i++ {\r\n\t\tv := &Varz{}\r\n\t\tresp, err := http.Get(murl)\r\n\t\tif err != nil {\r\n\t\t\tb.Fatalf(\"Expected no error: Got %v\\n\", err)\r\n\t\t}\r\n\t\tdefer resp.Body.Close()\r\n\t\tbody, err := ioutil.ReadAll(resp.Body)\r\n\t\tif err != nil {\r\n\t\t\tb.Fatalf(\"Got an error reading the body: %v\\n\", err)\r\n\t\t}\r\n\t\tif err := json.Unmarshal(body, v); err != nil {\r\n\t\t\tb.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\tresp.Body.Close()\r\n\t}\r\n}\r\n\r\nfunc TestVarzRaces(t *testing.T) {\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tmurl := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port)\r\n\tdone := make(chan struct{})\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(1)\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\tfor i := 0; i < 2; i++ {\r\n\t\t\t\tv := pollVarz(t, s, i, murl, nil)\r\n\t\t\t\t// Check the field that we are setting in main thread\r\n\t\t\t\t// to ensure that we have a copy and there is no\r\n\t\t\t\t// race with fields set in s.info and s.opts\r\n\t\t\t\tif v.ID == \"abc\" || v.MaxConn == -1 {\r\n\t\t\t\t\t// We will not get there. Need to have something\r\n\t\t\t\t\t// otherwise staticcheck will report empty branch\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\r\n\t\t\t\tselect {\r\n\t\t\t\tcase <-done:\r\n\t\t\t\t\treturn\r\n\t\t\t\tdefault:\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\tfor i := 0; i < 1000; i++ {\r\n\t\t// Simulate a change in server's info and options\r\n\t\t// by changing something.\r\n\t\ts.mu.Lock()\r\n\t\ts.info.ID = fmt.Sprintf(\"serverid_%d\", i)\r\n\t\ts.opts.MaxConn = 100 + i\r\n\t\ts.mu.Unlock()\r\n\t\ttime.Sleep(time.Nanosecond)\r\n\t}\r\n\tclose(done)\r\n\twg.Wait()\r\n\r\n\t// Now check that there is no race doing parallel polling\r\n\twg.Add(3)\r\n\tdone = make(chan struct{})\r\n\tpoll := func() {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\tfor mode := 0; mode < 2; mode++ {\r\n\t\t\t\tpollVarz(t, s, mode, murl, nil)\r\n\t\t\t}\r\n\t\t\tselect {\r\n\t\t\tcase <-done:\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tfor i := 0; i < 3; i++ {\r\n\t\tgo poll()\r\n\t}\r\n\ttime.Sleep(500 * time.Millisecond)\r\n\tclose(done)\r\n\twg.Wait()\r\n}\r\n\r\nfunc testMonitorStructPresent(t *testing.T, tag string) {\r\n\tt.Helper()\r\n\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tvarzURL := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port)\r\n\tbody := readBody(t, varzURL)\r\n\tif !bytes.Contains(body, []byte(`\"`+tag+`\": {}`)) {\r\n\t\tt.Fatalf(\"%s should be present and empty, got %s\", tag, body)\r\n\t}\r\n}\r\n\r\nfunc TestMonitorCluster(t *testing.T) {\r\n\ttestMonitorStructPresent(t, \"cluster\")\r\n\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.Cluster.Port = -1\r\n\topts.Cluster.AuthTimeout = 1\r\n\topts.Routes = RoutesFromStr(\"nats://127.0.0.1:1234\")\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\texpected := ClusterOptsVarz{\r\n\t\topts.Cluster.Host,\r\n\t\topts.Cluster.Port,\r\n\t\topts.Cluster.AuthTimeout,\r\n\t\t[]string{\"127.0.0.1:1234\"},\r\n\t}\r\n\r\n\tvarzURL := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tcheck := func(t *testing.T, v *Varz) {\r\n\t\t\tt.Helper()\r\n\t\t\tif !reflect.DeepEqual(v.Cluster, expected) {\r\n\t\t\t\tt.Fatalf(\"mode=%v - expected %+v, got %+v\", mode, expected, v.Cluster)\r\n\t\t\t}\r\n\t\t}\r\n\t\tv := pollVarz(t, s, mode, varzURL, nil)\r\n\t\tcheck(t, v)\r\n\r\n\t\t// Having this here to make sure that if fields are added in ClusterOptsVarz,\r\n\t\t// we make sure to update this test (compiler will report an error if we don't)\r\n\t\t_ = ClusterOptsVarz{\"\", 0, 0, nil}\r\n\r\n\t\t// Alter the fields to make sure that we have a proper deep copy\r\n\t\t// of what may be stored in the server. Anything we change here\r\n\t\t// should not affect the next returned value.\r\n\t\tv.Cluster.Host = \"wrong\"\r\n\t\tv.Cluster.Port = 0\r\n\t\tv.Cluster.AuthTimeout = 0\r\n\t\tv.Cluster.URLs = []string{\"wrong\"}\r\n\t\tv = pollVarz(t, s, mode, varzURL, nil)\r\n\t\tcheck(t, v)\r\n\t}\r\n}\r\n\r\nfunc TestMonitorClusterURLs(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\r\n\to2 := DefaultOptions()\r\n\to2.Cluster.Host = \"127.0.0.1\"\r\n\ts2 := RunServer(o2)\r\n\tdefer s2.Shutdown()\r\n\r\n\ts2ClusterHostPort := fmt.Sprintf(\"127.0.0.1:%d\", s2.ClusterAddr().Port)\r\n\r\n\ttemplate := `\r\n\t\tport: -1\r\n\t\thttp: -1\r\n\t\tcluster: {\r\n\t\t\tport: -1\r\n\t\t\troutes [\r\n\t\t\t\t%s\r\n\t\t\t\t%s\r\n\t\t\t]\r\n\t\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(template, \"nats://\"+s2ClusterHostPort, \"\")))\r\n\tdefer os.Remove(conf)\r\n\ts1, _ := RunServerWithConfig(conf)\r\n\tdefer s1.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s1, s2)\r\n\r\n\t// Check /varz cluster{} to see the URLs from s1 to s2\r\n\tvarzURL := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s1.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tv := pollVarz(t, s1, mode, varzURL, nil)\r\n\t\tif n := len(v.Cluster.URLs); n != 1 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected 1 URL, got %v\", mode, n)\r\n\t\t}\r\n\t\tif v.Cluster.URLs[0] != s2ClusterHostPort {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected url %q, got %q\", mode, s2ClusterHostPort, v.Cluster.URLs[0])\r\n\t\t}\r\n\t}\r\n\r\n\totherClusterHostPort := \"127.0.0.1:1234\"\r\n\t// Now update the config and add a route\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(template, \"nats://\"+s2ClusterHostPort, \"nats://\"+otherClusterHostPort)))\r\n\r\n\tif err := s1.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error on reload: %v\", err)\r\n\t}\r\n\r\n\t// Verify cluster still ok\r\n\tcheckClusterFormed(t, s1, s2)\r\n\r\n\t// Now verify that s1 reports in /varz the new URL\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor mode := 0; mode < 2; mode++ {\r\n\t\t\tv := pollVarz(t, s1, mode, varzURL, nil)\r\n\t\t\tif n := len(v.Cluster.URLs); n != 2 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected 2 URL, got %v\", mode, n)\r\n\t\t\t}\r\n\t\t\tgotS2 := false\r\n\t\t\tgotOther := false\r\n\t\t\tfor _, u := range v.Cluster.URLs {\r\n\t\t\t\tif u == s2ClusterHostPort {\r\n\t\t\t\t\tgotS2 = true\r\n\t\t\t\t} else if u == otherClusterHostPort {\r\n\t\t\t\t\tgotOther = true\r\n\t\t\t\t} else {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Incorrect url: %q\", mode, u)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif !gotS2 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Did not get cluster URL for s2\", mode)\r\n\t\t\t}\r\n\t\t\tif !gotOther {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Did not get the new cluster URL\", mode)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Remove all routes from config\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(template, \"\", \"\")))\r\n\r\n\tif err := s1.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error on reload: %v\", err)\r\n\t}\r\n\r\n\t// Now verify that s1 reports no ULRs in /varz\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor mode := 0; mode < 2; mode++ {\r\n\t\t\tv := pollVarz(t, s1, mode, varzURL, nil)\r\n\t\t\tif n := len(v.Cluster.URLs); n != 0 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected 0 URL, got %v\", mode, n)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestMonitorGateway(t *testing.T) {\r\n\ttestMonitorStructPresent(t, \"gateway\")\r\n\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.Gateway.Name = \"A\"\r\n\topts.Gateway.Port = -1\r\n\topts.Gateway.AuthTimeout = 1\r\n\topts.Gateway.TLSTimeout = 1\r\n\topts.Gateway.Advertise = \"127.0.0.1\"\r\n\topts.Gateway.ConnectRetries = 1\r\n\topts.Gateway.RejectUnknown = false\r\n\tu1, _ := url.Parse(\"nats://ivan:pwd@localhost:1234\")\r\n\tu2, _ := url.Parse(\"nats://localhost:1235\")\r\n\topts.Gateway.Gateways = []*RemoteGatewayOpts{\r\n\t\t&RemoteGatewayOpts{\r\n\t\t\tName:       \"B\",\r\n\t\t\tTLSTimeout: 1,\r\n\t\t\tURLs: []*url.URL{\r\n\t\t\t\tu1,\r\n\t\t\t\tu2,\r\n\t\t\t},\r\n\t\t},\r\n\t}\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\texpected := GatewayOptsVarz{\r\n\t\t\"A\",\r\n\t\topts.Gateway.Host,\r\n\t\topts.Gateway.Port,\r\n\t\topts.Gateway.AuthTimeout,\r\n\t\topts.Gateway.TLSTimeout,\r\n\t\topts.Gateway.Advertise,\r\n\t\topts.Gateway.ConnectRetries,\r\n\t\t[]RemoteGatewayOptsVarz{{\"B\", 1, nil}},\r\n\t\topts.Gateway.RejectUnknown,\r\n\t}\r\n\t// Since URLs array is not guaranteed to be always the same order,\r\n\t// we don't add it in the expected GatewayOptsVarz, instead we\r\n\t// maintain here.\r\n\texpectedURLs := []string{\"localhost:1234\", \"localhost:1235\"}\r\n\r\n\tvarzURL := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port)\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tcheck := func(t *testing.T, v *Varz) {\r\n\t\t\tt.Helper()\r\n\t\t\tvar urls []string\r\n\t\t\tif len(v.Gateway.Gateways) == 1 {\r\n\t\t\t\turls = v.Gateway.Gateways[0].URLs\r\n\t\t\t\tv.Gateway.Gateways[0].URLs = nil\r\n\t\t\t}\r\n\t\t\tif !reflect.DeepEqual(v.Gateway, expected) {\r\n\t\t\t\tt.Fatalf(\"mode=%v - expected %+v, got %+v\", mode, expected, v.Gateway)\r\n\t\t\t}\r\n\t\t\t// Now compare urls\r\n\t\t\tfor _, u := range expectedURLs {\r\n\t\t\t\tok := false\r\n\t\t\t\tfor _, u2 := range urls {\r\n\t\t\t\t\tif u == u2 {\r\n\t\t\t\t\t\tok = true\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tif !ok {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - expected urls to be %v, got %v\", mode, expected.Gateways[0].URLs, urls)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tv := pollVarz(t, s, mode, varzURL, nil)\r\n\t\tcheck(t, v)\r\n\r\n\t\t// Having this here to make sure that if fields are added in GatewayOptsVarz,\r\n\t\t// we make sure to update this test (compiler will report an error if we don't)\r\n\t\t_ = GatewayOptsVarz{\"\", \"\", 0, 0, 0, \"\", 0, []RemoteGatewayOptsVarz{{\"\", 0, nil}}, false}\r\n\r\n\t\t// Alter the fields to make sure that we have a proper deep copy\r\n\t\t// of what may be stored in the server. Anything we change here\r\n\t\t// should not affect the next returned value.\r\n\t\tv.Gateway.Name = \"wrong\"\r\n\t\tv.Gateway.Host = \"wrong\"\r\n\t\tv.Gateway.Port = 0\r\n\t\tv.Gateway.AuthTimeout = 1234.5\r\n\t\tv.Gateway.TLSTimeout = 1234.5\r\n\t\tv.Gateway.Advertise = \"wrong\"\r\n\t\tv.Gateway.ConnectRetries = 1234\r\n\t\tv.Gateway.Gateways[0].Name = \"wrong\"\r\n\t\tv.Gateway.Gateways[0].TLSTimeout = 1234.5\r\n\t\tv.Gateway.Gateways[0].URLs = []string{\"wrong\"}\r\n\t\tv.Gateway.RejectUnknown = true\r\n\t\tv = pollVarz(t, s, mode, varzURL, nil)\r\n\t\tcheck(t, v)\r\n\t}\r\n}\r\n\r\nfunc TestMonitorGatewayURLsUpdated(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\r\n\tob1 := testDefaultOptionsForGateway(\"B\")\r\n\tsb1 := runGatewayServer(ob1)\r\n\tdefer sb1.Shutdown()\r\n\r\n\t// Start a1 that has a single URL to sb1.\r\n\toa := testGatewayOptionsFromToWithServers(t, \"A\", \"B\", sb1)\r\n\toa.HTTPHost = \"127.0.0.1\"\r\n\toa.HTTPPort = MONITOR_PORT\r\n\tsa := runGatewayServer(oa)\r\n\tdefer sa.Shutdown()\r\n\r\n\twaitForOutboundGateways(t, sa, 1, 2*time.Second)\r\n\r\n\tvarzURL := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", sa.MonitorAddr().Port)\r\n\t// Check the /varz gateway's URLs\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tv := pollVarz(t, sa, mode, varzURL, nil)\r\n\t\tif n := len(v.Gateway.Gateways); n != 1 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected 1 remote gateway, got %v\", mode, n)\r\n\t\t}\r\n\t\tgw := v.Gateway.Gateways[0]\r\n\t\tif n := len(gw.URLs); n != 1 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected 1 url, got %v\", mode, n)\r\n\t\t}\r\n\t\texpected := oa.Gateway.Gateways[0].URLs[0].Host\r\n\t\tif u := gw.URLs[0]; u != expected {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected URL %q, got %q\", mode, expected, u)\r\n\t\t}\r\n\t}\r\n\r\n\t// Now start sb2 that clusters with sb1. sa should add to its list of URLs\r\n\t// sb2 gateway's connect URL.\r\n\tob2 := testDefaultOptionsForGateway(\"B\")\r\n\tob2.Routes = RoutesFromStr(fmt.Sprintf(\"nats://127.0.0.1:%d\", sb1.ClusterAddr().Port))\r\n\tsb2 := runGatewayServer(ob2)\r\n\tdefer sb2.Shutdown()\r\n\r\n\t// Wait for sb1 and sb2 to connect\r\n\tcheckClusterFormed(t, sb1, sb2)\r\n\t// sb2 should be made aware of gateway A and connect to sa\r\n\twaitForInboundGateways(t, sa, 2, 2*time.Second)\r\n\t// Now check that URLs in /varz get updated\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor mode := 0; mode < 2; mode++ {\r\n\t\t\tv := pollVarz(t, sa, mode, varzURL, nil)\r\n\t\t\tif n := len(v.Gateway.Gateways); n != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected 1 remote gateway, got %v\", mode, n)\r\n\t\t\t}\r\n\t\t\tgw := v.Gateway.Gateways[0]\r\n\t\t\tif n := len(gw.URLs); n != 2 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected 2 urls, got %v\", mode, n)\r\n\t\t\t}\r\n\r\n\t\t\tgotSB1 := false\r\n\t\t\tgotSB2 := false\r\n\t\t\tfor _, u := range gw.URLs {\r\n\t\t\t\tif u == fmt.Sprintf(\"127.0.0.1:%d\", sb1.GatewayAddr().Port) {\r\n\t\t\t\t\tgotSB1 = true\r\n\t\t\t\t} else if u == fmt.Sprintf(\"127.0.0.1:%d\", sb2.GatewayAddr().Port) {\r\n\t\t\t\t\tgotSB2 = true\r\n\t\t\t\t} else {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Incorrect URL to gateway B: %v\", mode, u)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif !gotSB1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Did not get URL to sb1\", mode)\r\n\t\t\t}\r\n\t\t\tif !gotSB2 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Did not get URL to sb2\", mode)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestMonitorLeafNode(t *testing.T) {\r\n\ttestMonitorStructPresent(t, \"leaf\")\r\n\r\n\tresetPreviousHTTPConnections()\r\n\topts := DefaultMonitorOptions()\r\n\topts.LeafNode.Port = -1\r\n\topts.LeafNode.AuthTimeout = 1\r\n\topts.LeafNode.TLSTimeout = 1\r\n\topts.Accounts = []*Account{NewAccount(\"acc\")}\r\n\tu, _ := url.Parse(\"nats://ivan:pwd@localhost:1234\")\r\n\topts.LeafNode.Remotes = []*RemoteLeafOpts{\r\n\t\t&RemoteLeafOpts{\r\n\t\t\tLocalAccount: \"acc\",\r\n\t\t\tURLs:         []*url.URL{u},\r\n\t\t\tTLSTimeout:   1,\r\n\t\t},\r\n\t}\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\texpected := LeafNodeOptsVarz{\r\n\t\topts.LeafNode.Host,\r\n\t\topts.LeafNode.Port,\r\n\t\topts.LeafNode.AuthTimeout,\r\n\t\topts.LeafNode.TLSTimeout,\r\n\t\t[]RemoteLeafOptsVarz{\r\n\t\t\t{\r\n\t\t\t\t\"acc\", 1, []string{\"localhost:1234\"},\r\n\t\t\t},\r\n\t\t},\r\n\t}\r\n\r\n\tvarzURL := fmt.Sprintf(\"http://127.0.0.1:%d/varz\", s.MonitorAddr().Port)\r\n\r\n\tfor mode := 0; mode < 2; mode++ {\r\n\t\tcheck := func(t *testing.T, v *Varz) {\r\n\t\t\tt.Helper()\r\n\t\t\tif !reflect.DeepEqual(v.LeafNode, expected) {\r\n\t\t\t\tt.Fatalf(\"mode=%v - expected %+v, got %+v\", mode, expected, v.LeafNode)\r\n\t\t\t}\r\n\t\t}\r\n\t\tv := pollVarz(t, s, mode, varzURL, nil)\r\n\t\tcheck(t, v)\r\n\r\n\t\t// Having this here to make sure that if fields are added in ClusterOptsVarz,\r\n\t\t// we make sure to update this test (compiler will report an error if we don't)\r\n\t\t_ = LeafNodeOptsVarz{\"\", 0, 0, 0, []RemoteLeafOptsVarz{{\"\", 0, nil}}}\r\n\r\n\t\t// Alter the fields to make sure that we have a proper deep copy\r\n\t\t// of what may be stored in the server. Anything we change here\r\n\t\t// should not affect the next returned value.\r\n\t\tv.LeafNode.Host = \"wrong\"\r\n\t\tv.LeafNode.Port = 0\r\n\t\tv.LeafNode.AuthTimeout = 1234.5\r\n\t\tv.LeafNode.TLSTimeout = 1234.5\r\n\t\tv.LeafNode.Remotes[0].LocalAccount = \"wrong\"\r\n\t\tv.LeafNode.Remotes[0].URLs = append(v.LeafNode.Remotes[0].URLs, \"wrong\")\r\n\t\tv.LeafNode.Remotes[0].TLSTimeout = 1234.5\r\n\t\tv = pollVarz(t, s, mode, varzURL, nil)\r\n\t\tcheck(t, v)\r\n\t}\r\n}\r\n\r\nfunc pollGatewayz(t *testing.T, s *Server, mode int, url string, opts *GatewayzOptions) *Gatewayz {\r\n\tt.Helper()\r\n\tif mode == 0 {\r\n\t\tg := &Gatewayz{}\r\n\t\tbody := readBody(t, url)\r\n\t\tif err := json.Unmarshal(body, g); err != nil {\r\n\t\t\tt.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\treturn g\r\n\t}\r\n\tg, err := s.Gatewayz(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on Gatewayz: %v\", err)\r\n\t}\r\n\treturn g\r\n}\r\n\r\nfunc TestMonitorGatewayz(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\r\n\t// First check that without gateway configured\r\n\ts := runMonitorServer()\r\n\tdefer s.Shutdown()\r\n\turl := fmt.Sprintf(\"http://127.0.0.1:%d/gatewayz\", s.MonitorAddr().Port)\r\n\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\tg := pollGatewayz(t, s, pollMode, url, nil)\r\n\t\t// Expect Name and port to be empty\r\n\t\tif g.Name != _EMPTY_ || g.Port != 0 {\r\n\t\t\tt.Fatalf(\"Expected no gateway, got %+v\", g)\r\n\t\t}\r\n\t}\r\n\ts.Shutdown()\r\n\r\n\tob1 := testDefaultOptionsForGateway(\"B\")\r\n\tsb1 := runGatewayServer(ob1)\r\n\tdefer sb1.Shutdown()\r\n\r\n\t// Start a1 that has a single URL to sb1.\r\n\toa := testGatewayOptionsFromToWithServers(t, \"A\", \"B\", sb1)\r\n\toa.HTTPHost = \"127.0.0.1\"\r\n\toa.HTTPPort = MONITOR_PORT\r\n\tsa := runGatewayServer(oa)\r\n\tdefer sa.Shutdown()\r\n\r\n\twaitForOutboundGateways(t, sa, 1, 2*time.Second)\r\n\twaitForInboundGateways(t, sa, 1, 2*time.Second)\r\n\r\n\twaitForOutboundGateways(t, sb1, 1, 2*time.Second)\r\n\twaitForInboundGateways(t, sb1, 1, 2*time.Second)\r\n\r\n\tgatewayzURL := fmt.Sprintf(\"http://127.0.0.1:%d/gatewayz\", sa.MonitorAddr().Port)\r\n\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\tg := pollGatewayz(t, sa, pollMode, gatewayzURL, nil)\r\n\t\tif g.Host != oa.Gateway.Host {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected host to be %q, got %q\", pollMode, oa.Gateway.Host, g.Host)\r\n\t\t}\r\n\t\tif g.Port != oa.Gateway.Port {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected port to be %v, got %v\", pollMode, oa.Gateway.Port, g.Port)\r\n\t\t}\r\n\t\tif n := len(g.OutboundGateways); n != 1 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected outbound to 1 gateway, got %v\", pollMode, n)\r\n\t\t}\r\n\t\tif n := len(g.InboundGateways); n != 1 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected inbound from 1 gateway, got %v\", pollMode, n)\r\n\t\t}\r\n\t\tog := g.OutboundGateways[\"B\"]\r\n\t\tif og == nil {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected to find outbound connection to B, got none\", pollMode)\r\n\t\t}\r\n\t\tif !og.IsConfigured {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected gw connection to be configured, was not\", pollMode)\r\n\t\t}\r\n\t\tif og.Connection == nil {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected outbound connection to B to be set, wat not\", pollMode)\r\n\t\t}\r\n\t\tif og.Connection.Name != sb1.ID() {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected outbound connection to B to have name %q, got %q\", pollMode, sb1.ID(), og.Connection.Name)\r\n\t\t}\r\n\t\tif n := len(og.Accounts); n != 0 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected no account, got %v\", pollMode, n)\r\n\t\t}\r\n\t\tig := g.InboundGateways[\"B\"]\r\n\t\tif ig == nil {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected to find inbound connection from B, got none\", pollMode)\r\n\t\t}\r\n\t\tif n := len(ig); n != 1 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected 1 inbound connection, got %v\", pollMode, n)\r\n\t\t}\r\n\t\tigc := ig[0]\r\n\t\tif igc.Connection == nil {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected inbound connection to B to be set, wat not\", pollMode)\r\n\t\t}\r\n\t\tif igc.Connection.Name != sb1.ID() {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected inbound connection to B to have name %q, got %q\", pollMode, sb1.ID(), igc.Connection.Name)\r\n\t\t}\r\n\t}\r\n\r\n\t// Now start sb2 that clusters with sb1. sa should add to its list of URLs\r\n\t// sb2 gateway's connect URL.\r\n\tob2 := testDefaultOptionsForGateway(\"B\")\r\n\tob2.Routes = RoutesFromStr(fmt.Sprintf(\"nats://127.0.0.1:%d\", sb1.ClusterAddr().Port))\r\n\tsb2 := runGatewayServer(ob2)\r\n\tdefer sb2.Shutdown()\r\n\r\n\t// Wait for sb1 and sb2 to connect\r\n\tcheckClusterFormed(t, sb1, sb2)\r\n\t// sb2 should be made aware of gateway A and connect to sa\r\n\twaitForInboundGateways(t, sa, 2, 2*time.Second)\r\n\t// Now check that URLs in /varz get updated\r\n\tcheckGatewayB := func(t *testing.T, url string, opts *GatewayzOptions) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\t\tg := pollGatewayz(t, sa, pollMode, url, opts)\r\n\t\t\t\tif n := len(g.OutboundGateways); n != 1 {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Expected outbound to 1 gateway, got %v\", pollMode, n)\r\n\t\t\t\t}\r\n\t\t\t\t// The InboundGateways is a map with key the gateway names,\r\n\t\t\t\t// then value is array of connections. So should be 1 here.\r\n\t\t\t\tif n := len(g.InboundGateways); n != 1 {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Expected inbound from 1 gateway, got %v\", pollMode, n)\r\n\t\t\t\t}\r\n\t\t\t\tig := g.InboundGateways[\"B\"]\r\n\t\t\t\tif ig == nil {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Expected to find inbound connection from B, got none\", pollMode)\r\n\t\t\t\t}\r\n\t\t\t\tif n := len(ig); n != 2 {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Expected 2 inbound connections from gateway B, got %v\", pollMode, n)\r\n\t\t\t\t}\r\n\t\t\t\tgotSB1 := false\r\n\t\t\t\tgotSB2 := false\r\n\t\t\t\tfor _, rg := range ig {\r\n\t\t\t\t\tif rg.Connection != nil {\r\n\t\t\t\t\t\tif rg.Connection.Name == sb1.ID() {\r\n\t\t\t\t\t\t\tgotSB1 = true\r\n\t\t\t\t\t\t} else if rg.Connection.Name == sb2.ID() {\r\n\t\t\t\t\t\t\tgotSB2 = true\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tif !gotSB1 {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Missing inbound connection from sb1\", pollMode)\r\n\t\t\t\t}\r\n\t\t\t\tif !gotSB2 {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Missing inbound connection from sb2\", pollMode)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\tcheckGatewayB(t, gatewayzURL, nil)\r\n\r\n\t// Start a new cluser C that connects to B. A should see it as\r\n\t// a non-configured gateway.\r\n\toc := testGatewayOptionsFromToWithServers(t, \"C\", \"B\", sb1)\r\n\tsc := runGatewayServer(oc)\r\n\tdefer sc.Shutdown()\r\n\r\n\t// All servers should have 2 outbound connections (one for each other cluster)\r\n\twaitForOutboundGateways(t, sa, 2, 2*time.Second)\r\n\twaitForOutboundGateways(t, sb1, 2, 2*time.Second)\r\n\twaitForOutboundGateways(t, sb2, 2, 2*time.Second)\r\n\twaitForOutboundGateways(t, sc, 2, 2*time.Second)\r\n\r\n\t// Server sa should have 3 inbounds now\r\n\twaitForInboundGateways(t, sa, 3, 2*time.Second)\r\n\r\n\t// Check gatewayz again to see that we have C now.\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\tg := pollGatewayz(t, sa, pollMode, gatewayzURL, nil)\r\n\t\t\tif n := len(g.OutboundGateways); n != 2 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected outbound to 2 gateways, got %v\", pollMode, n)\r\n\t\t\t}\r\n\t\t\t// The InboundGateways is a map with key the gateway names,\r\n\t\t\t// then value is array of connections. So should be 2 here.\r\n\t\t\tif n := len(g.InboundGateways); n != 2 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected inbound from 2 gateways, got %v\", pollMode, n)\r\n\t\t\t}\r\n\t\t\tog := g.OutboundGateways[\"C\"]\r\n\t\t\tif og == nil {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected to find outbound connection to C, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif og.IsConfigured {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected IsConfigured for gateway C to be false, was true\", pollMode)\r\n\t\t\t}\r\n\t\t\tif og.Connection == nil {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected connection to C, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif og.Connection.Name != sc.ID() {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected outbound connection to C to have name %q, got %q\", pollMode, sc.ID(), og.Connection.Name)\r\n\t\t\t}\r\n\t\t\tig := g.InboundGateways[\"C\"]\r\n\t\t\tif ig == nil {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected to find inbound connection from C, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif n := len(ig); n != 1 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected 1 inbound connections from gateway C, got %v\", pollMode, n)\r\n\t\t\t}\r\n\t\t\tigc := ig[0]\r\n\t\t\tif igc.Connection == nil {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected connection to C, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif igc.Connection.Name != sc.ID() {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected outbound connection to C to have name %q, got %q\", pollMode, sc.ID(), og.Connection.Name)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Select only 1 gateway by passing the name to option/url\r\n\topts := &GatewayzOptions{Name: \"B\"}\r\n\tcheckGatewayB(t, gatewayzURL+\"?gw_name=B\", opts)\r\n\r\n\t// Stop gateway C and check that we have only B, with and without filter.\r\n\tsc.Shutdown()\r\n\tcheckGatewayB(t, gatewayzURL+\"?gw_name=B\", opts)\r\n\tcheckGatewayB(t, gatewayzURL, nil)\r\n}\r\n\r\nfunc TestMonitorGatewayzAccounts(t *testing.T) {\r\n\tresetPreviousHTTPConnections()\r\n\r\n\t// Create bunch of Accounts\r\n\ttotalAccounts := 15\r\n\taccounts := \"\"\r\n\tfor i := 0; i < totalAccounts; i++ {\r\n\t\tacc := fmt.Sprintf(\"\tacc_%d: { users=[{user:user_%d, password: pwd}] }\\n\", i, i)\r\n\t\taccounts += acc\r\n\t}\r\n\r\n\tbConf := createConfFile(t, []byte(fmt.Sprintf(`\r\n\t\taccounts {\r\n\t\t\t%s\r\n\t\t}\r\n\t\tport: -1\r\n\t\thttp: -1\r\n\t\tgateway: {\r\n\t\t\tname: \"B\"\r\n\t\t\tport: -1\r\n\t\t}\r\n\t`, accounts)))\r\n\tdefer os.Remove(bConf)\r\n\r\n\tsb, ob := RunServerWithConfig(bConf)\r\n\tdefer sb.Shutdown()\r\n\tsb.SetLogger(&DummyLogger{}, true, true)\r\n\r\n\t// Start a1 that has a single URL to sb1.\r\n\taConf := createConfFile(t, []byte(fmt.Sprintf(`\r\n\t\taccounts {\r\n\t\t\t%s\r\n\t\t}\r\n\t\tport: -1\r\n\t\thttp: -1\r\n\t\tgateway: {\r\n\t\t\tname: \"A\"\r\n\t\t\tport: -1\r\n\t\t\tgateways [\r\n\t\t\t\t{\r\n\t\t\t\t\tname: \"B\"\r\n\t\t\t\t\turl: \"nats://127.0.0.1:%d\"\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t`, accounts, sb.GatewayAddr().Port)))\r\n\tdefer os.Remove(aConf)\r\n\r\n\tsa, oa := RunServerWithConfig(aConf)\r\n\tdefer sa.Shutdown()\r\n\tsa.SetLogger(&DummyLogger{}, true, true)\r\n\r\n\twaitForOutboundGateways(t, sa, 1, 2*time.Second)\r\n\twaitForInboundGateways(t, sa, 1, 2*time.Second)\r\n\twaitForOutboundGateways(t, sb, 1, 2*time.Second)\r\n\twaitForInboundGateways(t, sb, 1, 2*time.Second)\r\n\r\n\t// Create clients for each account on A and publish a message\r\n\t// so that list of accounts appear in gatewayz\r\n\tproduceMsgsFromA := func(t *testing.T) {\r\n\t\tt.Helper()\r\n\t\tfor i := 0; i < totalAccounts; i++ {\r\n\t\t\tnc, err := nats.Connect(fmt.Sprintf(\"nats://user_%d:pwd@%s:%d\", i, oa.Host, oa.Port))\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t\t}\r\n\t\t\tnc.Publish(\"foo\", []byte(\"hello\"))\r\n\t\t\tnc.Flush()\r\n\t\t\tnc.Close()\r\n\t\t}\r\n\t}\r\n\tproduceMsgsFromA(t)\r\n\r\n\t// Wait for A- for all accounts\r\n\tgwc := sa.getOutboundGatewayConnection(\"B\")\r\n\tfor i := 0; i < totalAccounts; i++ {\r\n\t\tcheckForAccountNoInterest(t, gwc, fmt.Sprintf(\"acc_%d\", i), true, 2*time.Second)\r\n\t}\r\n\r\n\t// Check accounts...\r\n\tgatewayzURL := fmt.Sprintf(\"http://127.0.0.1:%d/gatewayz\", sa.MonitorAddr().Port)\r\n\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t// First, without asking for it, they should not be present.\r\n\t\tg := pollGatewayz(t, sa, pollMode, gatewayzURL, nil)\r\n\t\tog := g.OutboundGateways[\"B\"]\r\n\t\tif og == nil {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected outbound gateway to B, got none\", pollMode)\r\n\t\t}\r\n\t\tif n := len(og.Accounts); n != 0 {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected accounts list to not be present by default, got %v\", pollMode, n)\r\n\t\t}\r\n\t\t// Now ask for the accounts\r\n\t\tg = pollGatewayz(t, sa, pollMode, gatewayzURL+\"?accs=1\", &GatewayzOptions{Accounts: true})\r\n\t\tog = g.OutboundGateways[\"B\"]\r\n\t\tif og == nil {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected outbound gateway to B, got none\", pollMode)\r\n\t\t}\r\n\t\tif n := len(og.Accounts); n != totalAccounts {\r\n\t\t\tt.Fatalf(\"mode=%v - Expected to get all %d accounts, got %v\", pollMode, totalAccounts, n)\r\n\t\t}\r\n\t\t// Now account details\r\n\t\tfor _, acc := range og.Accounts {\r\n\t\t\tif acc.InterestMode != Optimistic.String() {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected optimistic mode, got %q\", pollMode, acc.InterestMode)\r\n\t\t\t}\r\n\t\t\t// Since there is no interest at all on B, the publish\r\n\t\t\t// will have resulted in total account no interest, so\r\n\t\t\t// the number of no interest (subject wise) should be 0\r\n\t\t\tif acc.NoInterestCount != 0 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected 0 no-interest, got %v\", pollMode, acc.NoInterestCount)\r\n\t\t\t}\r\n\t\t\tif acc.NumQueueSubscriptions != 0 || acc.TotalSubscriptions != 0 {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected total subs to be 0, got %v - and num queue subs to be 0, got %v\",\r\n\t\t\t\t\tpollMode, acc.TotalSubscriptions, acc.NumQueueSubscriptions)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Check inbound on B\r\n\tgwURLServerB := fmt.Sprintf(\"http://127.0.0.1:%d/gatewayz\", sb.MonitorAddr().Port)\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\t// First, without asking for it, they should not be present.\r\n\t\t\tg := pollGatewayz(t, sb, pollMode, gwURLServerB, nil)\r\n\t\t\tigs := g.InboundGateways[\"A\"]\r\n\t\t\tif igs == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected inbound gateway to A, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif len(igs) != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected single inbound, got %v\", pollMode, len(igs))\r\n\t\t\t}\r\n\t\t\tig := igs[0]\r\n\t\t\tif n := len(ig.Accounts); n != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected no account, got %v\", pollMode, n)\r\n\t\t\t}\r\n\t\t\t// Check that list of accounts\r\n\t\t\tg = pollGatewayz(t, sb, pollMode, gwURLServerB+\"?accs=1\", &GatewayzOptions{Accounts: true})\r\n\t\t\tigs = g.InboundGateways[\"A\"]\r\n\t\t\tif igs == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected inbound gateway to A, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif len(igs) != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected single inbound, got %v\", pollMode, len(igs))\r\n\t\t\t}\r\n\t\t\tig = igs[0]\r\n\t\t\tif ig.Connection == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected inbound connection from A to be set, wat not\", pollMode)\r\n\t\t\t}\r\n\t\t\tif ig.Connection.Name != sa.ID() {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected inbound connection from A to have name %q, got %q\", pollMode, sa.ID(), ig.Connection.Name)\r\n\t\t\t}\r\n\t\t\tif n := len(ig.Accounts); n != totalAccounts {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected to get all %d accounts, got %v\", pollMode, totalAccounts, n)\r\n\t\t\t}\r\n\t\t\t// Now account details\r\n\t\t\tfor _, acc := range ig.Accounts {\r\n\t\t\t\tif acc.InterestMode != Optimistic.String() {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected optimistic mode, got %q\", pollMode, acc.InterestMode)\r\n\t\t\t\t}\r\n\t\t\t\t// Since there is no interest at all on B, the publish\r\n\t\t\t\t// will have resulted in total account no interest, so\r\n\t\t\t\t// the number of no interest (subject wise) should be 0\r\n\t\t\t\tif acc.NoInterestCount != 0 {\r\n\t\t\t\t\tt.Fatalf(\"mode=%v - Expected 0 no-interest, got %v\", pollMode, acc.NoInterestCount)\r\n\t\t\t\t}\r\n\t\t\t\t// For inbound gateway, NumQueueSubscriptions and TotalSubscriptions\r\n\t\t\t\t// are not relevant.\r\n\t\t\t\tif acc.NumQueueSubscriptions != 0 || acc.TotalSubscriptions != 0 {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - For inbound connection, expected num queue subs and total subs to be 0, got %v and %v\",\r\n\t\t\t\t\t\tpollMode, acc.TotalSubscriptions, acc.NumQueueSubscriptions)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Now create subscriptions on B to prevent A- and check on subject no interest\r\n\tfor i := 0; i < totalAccounts; i++ {\r\n\t\tnc, err := nats.Connect(fmt.Sprintf(\"nats://user_%d:pwd@%s:%d\", i, ob.Host, ob.Port))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t\t// Create a queue sub so it shows up in gatewayz\r\n\t\tnc.QueueSubscribeSync(\"bar\", \"queue\")\r\n\t\t// Create plain subscriptions on baz.0, baz.1 and baz.2.\r\n\t\t// Create to for each subject. Since gateways will send\r\n\t\t// only once per subject, the number of subs should be 3, not 6.\r\n\t\tfor j := 0; j < 3; j++ {\r\n\t\t\tsubj := fmt.Sprintf(\"baz.%d\", j)\r\n\t\t\tnc.SubscribeSync(subj)\r\n\t\t\tnc.SubscribeSync(subj)\r\n\t\t}\r\n\t\tnc.Flush()\r\n\t}\r\n\r\n\tfor i := 0; i < totalAccounts; i++ {\r\n\t\taccName := fmt.Sprintf(\"acc_%d\", i)\r\n\t\tcheckForRegisteredQSubInterest(t, sa, \"B\", accName, \"bar\", 1, 2*time.Second)\r\n\t}\r\n\r\n\t// Resend msgs from A on foo, on all accounts. There will be no interest on this subject.\r\n\tproduceMsgsFromA(t)\r\n\r\n\tfor i := 0; i < totalAccounts; i++ {\r\n\t\taccName := fmt.Sprintf(\"acc_%d\", i)\r\n\t\tcheckForSubjectNoInterest(t, gwc, accName, \"foo\", true, 2*time.Second)\r\n\t\t// Verify that we still have the queue interest registered\r\n\t\tcheckForRegisteredQSubInterest(t, sa, \"B\", accName, \"bar\", 1, 2*time.Second)\r\n\t}\r\n\r\n\t// Check accounts...\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\tg := pollGatewayz(t, sa, pollMode, gatewayzURL+\"?accs=1\", &GatewayzOptions{Accounts: true})\r\n\t\t\tog := g.OutboundGateways[\"B\"]\r\n\t\t\tif og == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected outbound gateway to B, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif n := len(og.Accounts); n != totalAccounts {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected to get all %d accounts, got %v\", pollMode, totalAccounts, n)\r\n\t\t\t}\r\n\t\t\t// Now account details\r\n\t\t\tfor _, acc := range og.Accounts {\r\n\t\t\t\tif acc.InterestMode != Optimistic.String() {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected optimistic mode, got %q\", pollMode, acc.InterestMode)\r\n\t\t\t\t}\r\n\t\t\t\tif acc.NoInterestCount != 1 {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected 1 no-interest, got %v\", pollMode, acc.NoInterestCount)\r\n\t\t\t\t}\r\n\t\t\t\tif acc.NumQueueSubscriptions != 1 || acc.TotalSubscriptions != 1 {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected total subs to be 1, got %v - and num queue subs to be 1, got %v\",\r\n\t\t\t\t\t\tpollMode, acc.TotalSubscriptions, acc.NumQueueSubscriptions)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Check inbound on server B\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\t// Ask for accounts list\r\n\t\t\tg := pollGatewayz(t, sb, pollMode, gwURLServerB+\"?accs=1\", &GatewayzOptions{Accounts: true})\r\n\t\t\tigs := g.InboundGateways[\"A\"]\r\n\t\t\tif igs == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected inbound gateway to A, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif len(igs) != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected single inbound, got %v\", pollMode, len(igs))\r\n\t\t\t}\r\n\t\t\tig := igs[0]\r\n\t\t\tif ig.Connection == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected inbound connection from A to be set, wat not\", pollMode)\r\n\t\t\t}\r\n\t\t\tif ig.Connection.Name != sa.ID() {\r\n\t\t\t\tt.Fatalf(\"mode=%v - Expected inbound connection from A to have name %q, got %q\", pollMode, sa.ID(), ig.Connection.Name)\r\n\t\t\t}\r\n\t\t\tif n := len(ig.Accounts); n != totalAccounts {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected to get all %d accounts, got %v\", pollMode, totalAccounts, n)\r\n\t\t\t}\r\n\t\t\t// Now account details\r\n\t\t\tfor _, acc := range ig.Accounts {\r\n\t\t\t\tif acc.InterestMode != Optimistic.String() {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected optimistic mode, got %q\", pollMode, acc.InterestMode)\r\n\t\t\t\t}\r\n\t\t\t\tif acc.NoInterestCount != 1 {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected 1 no-interest, got %v\", pollMode, acc.NoInterestCount)\r\n\t\t\t\t}\r\n\t\t\t\t// For inbound gateway, NumQueueSubscriptions and TotalSubscriptions\r\n\t\t\t\t// are not relevant.\r\n\t\t\t\tif acc.NumQueueSubscriptions != 0 || acc.TotalSubscriptions != 0 {\r\n\t\t\t\t\treturn fmt.Errorf(\"mode=%v - For inbound connection, expected num queue subs and total subs to be 0, got %v and %v\",\r\n\t\t\t\t\t\tpollMode, acc.TotalSubscriptions, acc.NumQueueSubscriptions)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Make one of the account to switch to interest only\r\n\tnc, err := nats.Connect(fmt.Sprintf(\"nats://user_1:pwd@%s:%d\", oa.Host, oa.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tfor i := 0; i < 1100; i++ {\r\n\t\tnc.Publish(fmt.Sprintf(\"foo.%d\", i), []byte(\"hello\"))\r\n\t}\r\n\tnc.Flush()\r\n\tnc.Close()\r\n\r\n\t// Check that we can select single account\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\tg := pollGatewayz(t, sa, pollMode, gatewayzURL+\"?gw_name=B&acc_name=acc_1\", &GatewayzOptions{Name: \"B\", AccountName: \"acc_1\"})\r\n\t\t\tog := g.OutboundGateways[\"B\"]\r\n\t\t\tif og == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected outbound gateway to B, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif n := len(og.Accounts); n != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected to get 1 account, got %v\", pollMode, n)\r\n\t\t\t}\r\n\t\t\t// Now account details\r\n\t\t\tacc := og.Accounts[0]\r\n\t\t\tif acc.InterestMode != InterestOnly.String() {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected interest-only mode, got %q\", pollMode, acc.InterestMode)\r\n\t\t\t}\r\n\t\t\t// Since we switched, this should be set to 0\r\n\t\t\tif acc.NoInterestCount != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected 0 no-interest, got %v\", pollMode, acc.NoInterestCount)\r\n\t\t\t}\r\n\t\t\t// We have created 3 subs on that account on B, and 1 queue sub.\r\n\t\t\t// So total should be 4 and 1 for queue sub.\r\n\t\t\tif acc.NumQueueSubscriptions != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected num queue subs to be 1, got %v\",\r\n\t\t\t\t\tpollMode, acc.NumQueueSubscriptions)\r\n\t\t\t}\r\n\t\t\tif acc.TotalSubscriptions != 4 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected total subs to be 4, got %v\",\r\n\t\t\t\t\tpollMode, acc.TotalSubscriptions)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Check inbound on B now...\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\tg := pollGatewayz(t, sb, pollMode, gwURLServerB+\"?gw_name=A&acc_name=acc_1\", &GatewayzOptions{Name: \"A\", AccountName: \"acc_1\"})\r\n\t\t\tigs := g.InboundGateways[\"A\"]\r\n\t\t\tif igs == nil {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected inbound gateway from A, got none\", pollMode)\r\n\t\t\t}\r\n\t\t\tif len(igs) != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected single inbound, got %v\", pollMode, len(igs))\r\n\t\t\t}\r\n\t\t\tig := igs[0]\r\n\t\t\tif n := len(ig.Accounts); n != 1 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected to get 1 account, got %v\", pollMode, n)\r\n\t\t\t}\r\n\t\t\t// Now account details\r\n\t\t\tacc := ig.Accounts[0]\r\n\t\t\tif acc.InterestMode != InterestOnly.String() {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected interest-only mode, got %q\", pollMode, acc.InterestMode)\r\n\t\t\t}\r\n\t\t\tif acc.InterestMode != InterestOnly.String() {\r\n\t\t\t\treturn fmt.Errorf(\"Should be in %q mode, got %q\", InterestOnly.String(), acc.InterestMode)\r\n\t\t\t}\r\n\t\t\t// Since we switched, this should be set to 0\r\n\t\t\tif acc.NoInterestCount != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - Expected 0 no-interest, got %v\", pollMode, acc.NoInterestCount)\r\n\t\t\t}\r\n\t\t\t// Again, for inbound, these should be always 0.\r\n\t\t\tif acc.NumQueueSubscriptions != 0 || acc.TotalSubscriptions != 0 {\r\n\t\t\t\treturn fmt.Errorf(\"mode=%v - For inbound connection, expected num queue subs and total subs to be 0, got %v and %v\",\r\n\t\t\t\t\tpollMode, acc.TotalSubscriptions, acc.NumQueueSubscriptions)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestMonitorRouteRTT(t *testing.T) {\r\n\t// Do not change default PingInterval and expect RTT to still be reported\r\n\r\n\tob := DefaultOptions()\r\n\tsb := RunServer(ob)\r\n\tdefer sb.Shutdown()\r\n\r\n\toa := DefaultOptions()\r\n\toa.Routes = RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", ob.Cluster.Host, ob.Cluster.Port))\r\n\tsa := RunServer(oa)\r\n\tdefer sa.Shutdown()\r\n\r\n\tcheckClusterFormed(t, sa, sb)\r\n\r\n\tcheckRouteInfo := func(t *testing.T, s *Server) {\r\n\t\tt.Helper()\r\n\t\troutezURL := fmt.Sprintf(\"http://127.0.0.1:%d/routez\", s.MonitorAddr().Port)\r\n\t\tfor pollMode := 0; pollMode < 2; pollMode++ {\r\n\t\t\tcheckFor(t, 2*firstPingInterval, 15*time.Millisecond, func() error {\r\n\t\t\t\trz := pollRoutez(t, s, pollMode, routezURL, nil)\r\n\t\t\t\tif len(rz.Routes) != 1 {\r\n\t\t\t\t\treturn fmt.Errorf(\"Expected 1 route, got %v\", len(rz.Routes))\r\n\t\t\t\t}\r\n\t\t\t\tri := rz.Routes[0]\r\n\t\t\t\tif ri.RTT == _EMPTY_ {\r\n\t\t\t\t\treturn fmt.Errorf(\"Route's RTT not reported\")\r\n\t\t\t\t}\r\n\t\t\t\treturn nil\r\n\t\t\t})\r\n\t\t}\r\n\t}\r\n\tcheckRouteInfo(t, sa)\r\n\tcheckRouteInfo(t, sb)\r\n}\r\n\r\nfunc pollLeafz(t *testing.T, s *Server, mode int, url string, opts *LeafzOptions) *Leafz {\r\n\tt.Helper()\r\n\tif mode == 0 {\r\n\t\tl := &Leafz{}\r\n\t\tbody := readBody(t, url)\r\n\t\tif err := json.Unmarshal(body, l); err != nil {\r\n\t\t\tt.Fatalf(\"Got an error unmarshalling the body: %v\\n\", err)\r\n\t\t}\r\n\t\treturn l\r\n\t}\r\n\tl, err := s.Leafz(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on Leafz: %v\", err)\r\n\t}\r\n\treturn l\r\n}\r\n\r\nfunc TestMonitorLeafz(t *testing.T) {\r\n\tcontent := `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\thttp: \"127.0.0.1:-1\"\r\n\toperator = \"../test/configs/nkeys/op.jwt\"\r\n\tresolver = MEMORY\r\n\tping_interval = 1\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\tdefer os.Remove(conf)\r\n\tsb, ob := RunServerWithConfig(conf)\r\n\tdefer sb.Shutdown()\r\n\r\n\tcreateAcc := func(t *testing.T) (*Account, string) {\r\n\t\tt.Helper()\r\n\t\tacc, akp := createAccount(sb)\r\n\t\tkp, _ := nkeys.CreateUser()\r\n\t\tpub, _ := kp.PublicKey()\r\n\t\tnuc := jwt.NewUserClaims(pub)\r\n\t\tujwt, err := nuc.Encode(akp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t\t}\r\n\t\tseed, _ := kp.Seed()\r\n\t\tcreds := genCredsFile(t, ujwt, seed)\r\n\t\treturn acc, creds\r\n\t}\r\n\tacc1, mycreds1 := createAcc(t)\r\n\tdefer os.Remove(mycreds1)\r\n\tacc2, mycreds2 := createAcc(t)\r\n\tdefer os.Remove(mycreds2)\r\n\r\n\tcontent = `\r\n\t\tport: -1\r\n\t\thttp: \"127.0.0.1:-1\"\r\n\t\tping_interval = 1\r\n\t\taccounts {\r\n\t\t\t%s {\r\n\t\t\t\tusers [\r\n\t\t\t\t\t{user: user1, password: pwd}\r\n\t\t\t\t]\r\n\t\t\t}\r\n\t\t\t%s {\r\n\t\t\t\tusers [\r\n\t\t\t\t\t{user: user2, password: pwd}\r\n\t\t\t\t]\r\n\t\t\t}\r\n\t\t}\r\n\t\tleafnodes {\r\n\t\t\tremotes = [\r\n\t\t\t\t{\r\n\t\t\t\t\taccount: \"%s\"\r\n\t\t\t\t\turl: nats-leaf://127.0.0.1:%d\r\n\t\t\t\t\tcredentials: '%s'\r\n\t\t\t\t}\r\n\t\t\t\t{\r\n\t\t\t\t\taccount: \"%s\"\r\n\t\t\t\t\turl: nats-leaf://127.0.0.1:%d\r\n\t\t\t\t\tcredentials: '%s'\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t\t`\r\n\tconfig := fmt.Sprintf(content,\r\n\t\tacc1.Name, acc2.Name,\r\n\t\tacc1.Name, ob.LeafNode.Port, mycreds1,\r\n\t\tacc2.Name, ob.LeafNode.Port, mycreds2)\r\n\tconf = createConfFile(t, []byte(config))\r\n\tdefer os.Remove(conf)\r\n\tsa, oa := RunServerWithConfig(conf)\r\n\tdefer sa.Shutdown()\r\n\r\n\tcheckFor(t, time.Second, 15*time.Millisecond, func() error {\r\n\t\tif n := sa.NumLeafNodes(); n != 2 {\r\n\t\t\treturn fmt.Errorf(\"Expected 2 leaf connections, got %v\", n)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Wait for initial RTT to be computed\r\n\ttime.Sleep(firstPingInterval + 500*time.Millisecond)\r\n\r\n\tch := make(chan bool, 1)\r\n\tnc1B := natsConnect(t, fmt.Sprintf(\"nats://127.0.0.1:%d\", ob.Port), nats.UserCredentials(mycreds1))\r\n\tdefer nc1B.Close()\r\n\tnatsSub(t, nc1B, \"foo\", func(_ *nats.Msg) { ch <- true })\r\n\tnatsSub(t, nc1B, \"bar\", func(_ *nats.Msg) {})\r\n\tnatsFlush(t, nc1B)\r\n\r\n\tnc2B := natsConnect(t, fmt.Sprintf(\"nats://127.0.0.1:%d\", ob.Port), nats.UserCredentials(mycreds2))\r\n\tdefer nc2B.Close()\r\n\tnatsSub(t, nc2B, \"bar\", func(_ *nats.Msg) { ch <- true })\r\n\tnatsSub(t, nc2B, \"foo\", func(_ *nats.Msg) {})\r\n\tnatsFlush(t, nc2B)\r\n\r\n\tnc1A := natsConnect(t, fmt.Sprintf(\"nats://user1:pwd@127.0.0.1:%d\", oa.Port))\r\n\tdefer nc1A.Close()\r\n\tnatsPub(t, nc1A, \"foo\", []byte(\"hello\"))\r\n\tnatsFlush(t, nc1A)\r\n\r\n\twaitCh(t, ch, \"Did not get the message\")\r\n\r\n\tnc2A := natsConnect(t, fmt.Sprintf(\"nats://user2:pwd@127.0.0.1:%d\", oa.Port))\r\n\tdefer nc2A.Close()\r\n\tnatsPub(t, nc2A, \"bar\", []byte(\"hello\"))\r\n\tnatsPub(t, nc2A, \"bar\", []byte(\"hello\"))\r\n\tnatsFlush(t, nc2A)\r\n\r\n\twaitCh(t, ch, \"Did not get the message\")\r\n\twaitCh(t, ch, \"Did not get the message\")\r\n\r\n\t// Let's poll server A\r\n\tpollURL := fmt.Sprintf(\"http://127.0.0.1:%d/leafz?subs=1\", sa.MonitorAddr().Port)\r\n\tfor pollMode := 1; pollMode < 2; pollMode++ {\r\n\t\tl := pollLeafz(t, sa, pollMode, pollURL, &LeafzOptions{Subscriptions: true})\r\n\t\tif l.ID != sa.ID() {\r\n\t\t\tt.Fatalf(\"Expected ID to be %q, got %q\", sa.ID(), l.ID)\r\n\t\t}\r\n\t\tif l.Now.IsZero() {\r\n\t\t\tt.Fatalf(\"Expected Now to be set, was not\")\r\n\t\t}\r\n\t\tif l.NumLeafs != 2 {\r\n\t\t\tt.Fatalf(\"Expected NumLeafs to be 2, got %v\", l.NumLeafs)\r\n\t\t}\r\n\t\tif len(l.Leafs) != 2 {\r\n\t\t\tt.Fatalf(\"Expected array to be len 2, got %v\", len(l.Leafs))\r\n\t\t}\r\n\t\tfor _, ln := range l.Leafs {\r\n\t\t\tif ln.Account == acc1.Name {\r\n\t\t\t\tif ln.OutMsgs != 1 || ln.OutBytes == 0 || ln.InMsgs != 0 || ln.InBytes != 0 {\r\n\t\t\t\t\tt.Fatalf(\"Expected 1 OutMsgs/Bytes and 0 InMsgs/Bytes, got %+v\", ln)\r\n\t\t\t\t}\r\n\t\t\t} else if ln.Account == acc2.Name {\r\n\t\t\t\tif ln.OutMsgs != 2 || ln.OutBytes == 0 || ln.InMsgs != 0 || ln.InBytes != 0 {\r\n\t\t\t\t\tt.Fatalf(\"Expected 2 OutMsgs/Bytes and 0 InMsgs/Bytes, got %+v\", ln)\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\tt.Fatalf(\"Expected account to be %q or %q, got %q\", acc1.Name, acc2.Name, ln.Account)\r\n\t\t\t}\r\n\t\t\tif ln.RTT == \"\" {\r\n\t\t\t\tt.Fatalf(\"RTT not tracked?\")\r\n\t\t\t}\r\n\t\t\tif ln.NumSubs != 3 {\r\n\t\t\t\tt.Fatalf(\"Expected 3 subs, got %v\", ln.NumSubs)\r\n\t\t\t}\r\n\t\t\tif len(ln.Subs) != 3 {\r\n\t\t\t\tt.Fatalf(\"Expected subs to be returned, got %v\", len(ln.Subs))\r\n\t\t\t}\r\n\t\t\tvar foundFoo bool\r\n\t\t\tvar foundBar bool\r\n\t\t\tfor _, sub := range ln.Subs {\r\n\t\t\t\tif sub == \"foo\" {\r\n\t\t\t\t\tfoundFoo = true\r\n\t\t\t\t} else if sub == \"bar\" {\r\n\t\t\t\t\tfoundBar = true\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif !foundFoo {\r\n\t\t\t\tt.Fatal(\"Did not find subject foo\")\r\n\t\t\t}\r\n\t\t\tif !foundBar {\r\n\t\t\t\tt.Fatal(\"Did not find subject bar\")\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t// Make sure that if we don't ask for subs, we don't get them\r\n\tpollURL = fmt.Sprintf(\"http://127.0.0.1:%d/leafz\", sa.MonitorAddr().Port)\r\n\tfor pollMode := 1; pollMode < 2; pollMode++ {\r\n\t\tl := pollLeafz(t, sa, pollMode, pollURL, nil)\r\n\t\tfor _, ln := range l.Leafs {\r\n\t\t\tif ln.NumSubs != 3 {\r\n\t\t\t\tt.Fatalf(\"Number of subs should be 3, got %v\", ln.NumSubs)\r\n\t\t\t}\r\n\t\t\tif len(ln.Subs) != 0 {\r\n\t\t\t\tt.Fatalf(\"Subs should not have been returned, got %v\", ln.Subs)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Now polling server B.\r\n\tpollURL = fmt.Sprintf(\"http://127.0.0.1:%d/leafz?subs=1\", sb.MonitorAddr().Port)\r\n\tfor pollMode := 1; pollMode < 2; pollMode++ {\r\n\t\tl := pollLeafz(t, sb, pollMode, pollURL, &LeafzOptions{Subscriptions: true})\r\n\t\tif l.ID != sb.ID() {\r\n\t\t\tt.Fatalf(\"Expected ID to be %q, got %q\", sb.ID(), l.ID)\r\n\t\t}\r\n\t\tif l.Now.IsZero() {\r\n\t\t\tt.Fatalf(\"Expected Now to be set, was not\")\r\n\t\t}\r\n\t\tif l.NumLeafs != 2 {\r\n\t\t\tt.Fatalf(\"Expected NumLeafs to be 1, got %v\", l.NumLeafs)\r\n\t\t}\r\n\t\tif len(l.Leafs) != 2 {\r\n\t\t\tt.Fatalf(\"Expected array to be len 2, got %v\", len(l.Leafs))\r\n\t\t}\r\n\t\tfor _, ln := range l.Leafs {\r\n\t\t\tif ln.Account == acc1.Name {\r\n\t\t\t\tif ln.OutMsgs != 0 || ln.OutBytes != 0 || ln.InMsgs != 1 || ln.InBytes == 0 {\r\n\t\t\t\t\tt.Fatalf(\"Expected 1 InMsgs/Bytes and 0 OutMsgs/Bytes, got %+v\", ln)\r\n\t\t\t\t}\r\n\t\t\t} else if ln.Account == acc2.Name {\r\n\t\t\t\tif ln.OutMsgs != 0 || ln.OutBytes != 0 || ln.InMsgs != 2 || ln.InBytes == 0 {\r\n\t\t\t\t\tt.Fatalf(\"Expected 2 InMsgs/Bytes and 0 OutMsgs/Bytes, got %+v\", ln)\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\tt.Fatalf(\"Expected account to be %q or %q, got %q\", acc1.Name, acc2.Name, ln.Account)\r\n\t\t\t}\r\n\t\t\tif ln.RTT == \"\" {\r\n\t\t\t\tt.Fatalf(\"RTT not tracked?\")\r\n\t\t\t}\r\n\t\t\t// LDS should be only one.\r\n\t\t\tif ln.NumSubs != 1 || len(ln.Subs) != 1 {\r\n\t\t\t\tt.Fatalf(\"Expected 1 sub, got %v (%v)\", ln.NumSubs, ln.Subs)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/monitor_test.go b/server/gnatsd/server/monitor_test.go
--- a/server/gnatsd/server/monitor_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/monitor_test.go	(date 1665399049707)
@@ -32,7 +32,7 @@
 	"time"
 	"unicode"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nats.go"
 	"github.com/nats-io/nkeys"
 )
Index: server/gnatsd/server/opts.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2012-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"crypto/tls\"\r\n\t\"crypto/x509\"\r\n\t\"errors\"\r\n\t\"flag\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"path\"\r\n\t\"path/filepath\"\r\n\t\"regexp\"\r\n\t\"runtime\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n\t\"sync/atomic\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/conf\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nvar allowUnknownTopLevelField = int32(0)\r\n\r\n// NoErrOnUnknownFields can be used to change the behavior the processing\r\n// of a configuration file. By default, an error is reported if unknown\r\n// fields are found. If `noError` is set to true, no error will be reported\r\n// if top-level unknown fields are found.\r\nfunc NoErrOnUnknownFields(noError bool) {\r\n\tvar val int32\r\n\tif noError {\r\n\t\tval = int32(1)\r\n\t}\r\n\tatomic.StoreInt32(&allowUnknownTopLevelField, val)\r\n}\r\n\r\n// ClusterOpts are options for clusters.\r\n// NOTE: This structure is no longer used for monitoring endpoints\r\n// and json tags are deprecated and may be removed in the future.\r\ntype ClusterOpts struct {\r\n\tHost           string            `json:\"addr,omitempty\"`\r\n\tPort           int               `json:\"cluster_port,omitempty\"`\r\n\tUsername       string            `json:\"-\"`\r\n\tPassword       string            `json:\"-\"`\r\n\tAuthTimeout    float64           `json:\"auth_timeout,omitempty\"`\r\n\tPermissions    *RoutePermissions `json:\"-\"`\r\n\tTLSTimeout     float64           `json:\"-\"`\r\n\tTLSConfig      *tls.Config       `json:\"-\"`\r\n\tTLSMap         bool              `json:\"-\"`\r\n\tListenStr      string            `json:\"-\"`\r\n\tAdvertise      string            `json:\"-\"`\r\n\tNoAdvertise    bool              `json:\"-\"`\r\n\tConnectRetries int               `json:\"-\"`\r\n}\r\n\r\n// GatewayOpts are options for gateways.\r\n// NOTE: This structure is no longer used for monitoring endpoints\r\n// and json tags are deprecated and may be removed in the future.\r\ntype GatewayOpts struct {\r\n\tName           string               `json:\"name\"`\r\n\tHost           string               `json:\"addr,omitempty\"`\r\n\tPort           int                  `json:\"port,omitempty\"`\r\n\tUsername       string               `json:\"-\"`\r\n\tPassword       string               `json:\"-\"`\r\n\tAuthTimeout    float64              `json:\"auth_timeout,omitempty\"`\r\n\tTLSConfig      *tls.Config          `json:\"-\"`\r\n\tTLSTimeout     float64              `json:\"tls_timeout,omitempty\"`\r\n\tTLSMap         bool                 `json:\"-\"`\r\n\tAdvertise      string               `json:\"advertise,omitempty\"`\r\n\tConnectRetries int                  `json:\"connect_retries,omitempty\"`\r\n\tGateways       []*RemoteGatewayOpts `json:\"gateways,omitempty\"`\r\n\tRejectUnknown  bool                 `json:\"reject_unknown,omitempty\"`\r\n\r\n\t// Not exported, for tests.\r\n\tresolver         netResolver\r\n\tsendQSubsBufSize int\r\n}\r\n\r\n// RemoteGatewayOpts are options for connecting to a remote gateway\r\n// NOTE: This structure is no longer used for monitoring endpoints\r\n// and json tags are deprecated and may be removed in the future.\r\ntype RemoteGatewayOpts struct {\r\n\tName       string      `json:\"name\"`\r\n\tTLSConfig  *tls.Config `json:\"-\"`\r\n\tTLSTimeout float64     `json:\"tls_timeout,omitempty\"`\r\n\tURLs       []*url.URL  `json:\"urls,omitempty\"`\r\n}\r\n\r\n// LeafNodeOpts are options for a given server to accept leaf node connections and/or connect to a remote cluster.\r\ntype LeafNodeOpts struct {\r\n\tHost              string        `json:\"addr,omitempty\"`\r\n\tPort              int           `json:\"port,omitempty\"`\r\n\tUsername          string        `json:\"-\"`\r\n\tPassword          string        `json:\"-\"`\r\n\tAccount           string        `json:\"-\"`\r\n\tUsers             []*User       `json:\"-\"`\r\n\tAuthTimeout       float64       `json:\"auth_timeout,omitempty\"`\r\n\tTLSConfig         *tls.Config   `json:\"-\"`\r\n\tTLSTimeout        float64       `json:\"tls_timeout,omitempty\"`\r\n\tTLSMap            bool          `json:\"-\"`\r\n\tAdvertise         string        `json:\"-\"`\r\n\tNoAdvertise       bool          `json:\"-\"`\r\n\tReconnectInterval time.Duration `json:\"-\"`\r\n\r\n\t// For solicited connections to other clusters/superclusters.\r\n\tRemotes []*RemoteLeafOpts `json:\"remotes,omitempty\"`\r\n\r\n\t// Not exported, for tests.\r\n\tresolver    netResolver\r\n\tdialTimeout time.Duration\r\n\tconnDelay   time.Duration\r\n}\r\n\r\n// RemoteLeafOpts are options for connecting to a remote server as a leaf node.\r\ntype RemoteLeafOpts struct {\r\n\tLocalAccount string      `json:\"local_account,omitempty\"`\r\n\tURLs         []*url.URL  `json:\"urls,omitempty\"`\r\n\tCredentials  string      `json:\"-\"`\r\n\tTLS          bool        `json:\"-\"`\r\n\tTLSConfig    *tls.Config `json:\"-\"`\r\n\tTLSTimeout   float64     `json:\"tls_timeout,omitempty\"`\r\n\tHub          bool        `json:\"hub,omitempty\"`\r\n\tDenyImports  []string    `json:\"-\"`\r\n\tDenyExports  []string    `json:\"-\"`\r\n}\r\n\r\n// Options block for nats-server.\r\n// NOTE: This structure is no longer used for monitoring endpoints\r\n// and json tags are deprecated and may be removed in the future.\r\ntype Options struct {\r\n\tConfigFile            string        `json:\"-\"`\r\n\tServerName            string        `json:\"server_name\"`\r\n\tHost                  string        `json:\"addr\"`\r\n\tPort                  int           `json:\"port\"`\r\n\tClientAdvertise       string        `json:\"-\"`\r\n\tTrace                 bool          `json:\"-\"`\r\n\tDebug                 bool          `json:\"-\"`\r\n\tTraceVerbose          bool          `json:\"-\"`\r\n\tNoLog                 bool          `json:\"-\"`\r\n\tNoSigs                bool          `json:\"-\"`\r\n\tNoSublistCache        bool          `json:\"-\"`\r\n\tDisableShortFirstPing bool          `json:\"-\"`\r\n\tLogtime               bool          `json:\"-\"`\r\n\tMaxConn               int           `json:\"max_connections\"`\r\n\tMaxSubs               int           `json:\"max_subscriptions,omitempty\"`\r\n\tNkeys                 []*NkeyUser   `json:\"-\"`\r\n\tUsers                 []*User       `json:\"-\"`\r\n\tAccounts              []*Account    `json:\"-\"`\r\n\tNoAuthUser            string        `json:\"-\"`\r\n\tSystemAccount         string        `json:\"-\"`\r\n\tAllowNewAccounts      bool          `json:\"-\"`\r\n\tUsername              string        `json:\"-\"`\r\n\tPassword              string        `json:\"-\"`\r\n\tAuthorization         string        `json:\"-\"`\r\n\tPingInterval          time.Duration `json:\"ping_interval\"`\r\n\tMaxPingsOut           int           `json:\"ping_max\"`\r\n\tHTTPHost              string        `json:\"http_host\"`\r\n\tHTTPPort              int           `json:\"http_port\"`\r\n\tHTTPBasePath          string        `json:\"http_base_path\"`\r\n\tHTTPSPort             int           `json:\"https_port\"`\r\n\tAuthTimeout           float64       `json:\"auth_timeout\"`\r\n\tMaxControlLine        int32         `json:\"max_control_line\"`\r\n\tMaxPayload            int32         `json:\"max_payload\"`\r\n\tMaxPending            int64         `json:\"max_pending\"`\r\n\tCluster               ClusterOpts   `json:\"cluster,omitempty\"`\r\n\tGateway               GatewayOpts   `json:\"gateway,omitempty\"`\r\n\tLeafNode              LeafNodeOpts  `json:\"leaf,omitempty\"`\r\n\tProfPort              int           `json:\"-\"`\r\n\tPidFile               string        `json:\"-\"`\r\n\tPortsFileDir          string        `json:\"-\"`\r\n\tLogFile               string        `json:\"-\"`\r\n\tLogSizeLimit          int64         `json:\"-\"`\r\n\tSyslog                bool          `json:\"-\"`\r\n\tRemoteSyslog          string        `json:\"-\"`\r\n\tRoutes                []*url.URL    `json:\"-\"`\r\n\tRoutesStr             string        `json:\"-\"`\r\n\tTLSTimeout            float64       `json:\"tls_timeout\"`\r\n\tTLS                   bool          `json:\"-\"`\r\n\tTLSVerify             bool          `json:\"-\"`\r\n\tTLSMap                bool          `json:\"-\"`\r\n\tTLSCert               string        `json:\"-\"`\r\n\tTLSKey                string        `json:\"-\"`\r\n\tTLSCaCert             string        `json:\"-\"`\r\n\tTLSConfig             *tls.Config   `json:\"-\"`\r\n\tWriteDeadline         time.Duration `json:\"-\"`\r\n\tMaxClosedClients      int           `json:\"-\"`\r\n\tLameDuckDuration      time.Duration `json:\"-\"`\r\n\t// MaxTracedMsgLen is the maximum printable length for traced messages.\r\n\tMaxTracedMsgLen int `json:\"-\"`\r\n\r\n\t// Operating a trusted NATS server\r\n\tTrustedKeys              []string              `json:\"-\"`\r\n\tTrustedOperators         []*jwt.OperatorClaims `json:\"-\"`\r\n\tAccountResolver          AccountResolver       `json:\"-\"`\r\n\tAccountResolverTLSConfig *tls.Config           `json:\"-\"`\r\n\tresolverPreloads         map[string]string\r\n\r\n\tCustomClientAuthentication Authentication `json:\"-\"`\r\n\tCustomRouterAuthentication Authentication `json:\"-\"`\r\n\r\n\t// CheckConfig configuration file syntax test was successful and exit.\r\n\tCheckConfig bool `json:\"-\"`\r\n\r\n\t// ConnectErrorReports specifies the number of failed attempts\r\n\t// at which point server should report the failure of an initial\r\n\t// connection to a route, gateway or leaf node.\r\n\t// See DEFAULT_CONNECT_ERROR_REPORTS for default value.\r\n\tConnectErrorReports int\r\n\r\n\t// ReconnectErrorReports is similar to ConnectErrorReports except\r\n\t// that this applies to reconnect events.\r\n\tReconnectErrorReports int\r\n\r\n\t// private fields, used to know if bool options are explicitly\r\n\t// defined in config and/or command line params.\r\n\tinConfig  map[string]bool\r\n\tinCmdLine map[string]bool\r\n\r\n\t// private fields, used for testing\r\n\tgatewaysSolicitDelay time.Duration\r\n\trouteProto           int\r\n}\r\n\r\ntype netResolver interface {\r\n\tLookupHost(ctx context.Context, host string) ([]string, error)\r\n}\r\n\r\n// Clone performs a deep copy of the Options struct, returning a new clone\r\n// with all values copied.\r\nfunc (o *Options) Clone() *Options {\r\n\tif o == nil {\r\n\t\treturn nil\r\n\t}\r\n\tclone := &Options{}\r\n\t*clone = *o\r\n\tif o.Users != nil {\r\n\t\tclone.Users = make([]*User, len(o.Users))\r\n\t\tfor i, user := range o.Users {\r\n\t\t\tclone.Users[i] = user.clone()\r\n\t\t}\r\n\t}\r\n\tif o.Nkeys != nil {\r\n\t\tclone.Nkeys = make([]*NkeyUser, len(o.Nkeys))\r\n\t\tfor i, nkey := range o.Nkeys {\r\n\t\t\tclone.Nkeys[i] = nkey.clone()\r\n\t\t}\r\n\t}\r\n\r\n\tif o.Routes != nil {\r\n\t\tclone.Routes = deepCopyURLs(o.Routes)\r\n\t}\r\n\tif o.TLSConfig != nil {\r\n\t\tclone.TLSConfig = o.TLSConfig.Clone()\r\n\t}\r\n\tif o.Cluster.TLSConfig != nil {\r\n\t\tclone.Cluster.TLSConfig = o.Cluster.TLSConfig.Clone()\r\n\t}\r\n\tif o.Gateway.TLSConfig != nil {\r\n\t\tclone.Gateway.TLSConfig = o.Gateway.TLSConfig.Clone()\r\n\t}\r\n\tif len(o.Gateway.Gateways) > 0 {\r\n\t\tclone.Gateway.Gateways = make([]*RemoteGatewayOpts, len(o.Gateway.Gateways))\r\n\t\tfor i, g := range o.Gateway.Gateways {\r\n\t\t\tclone.Gateway.Gateways[i] = g.clone()\r\n\t\t}\r\n\t}\r\n\t// FIXME(dlc) - clone leaf node stuff.\r\n\treturn clone\r\n}\r\n\r\nfunc deepCopyURLs(urls []*url.URL) []*url.URL {\r\n\tif urls == nil {\r\n\t\treturn nil\r\n\t}\r\n\tcurls := make([]*url.URL, len(urls))\r\n\tfor i, u := range urls {\r\n\t\tcu := &url.URL{}\r\n\t\t*cu = *u\r\n\t\tcurls[i] = cu\r\n\t}\r\n\treturn curls\r\n}\r\n\r\n// Configuration file authorization section.\r\ntype authorization struct {\r\n\t// Singles\r\n\tuser  string\r\n\tpass  string\r\n\ttoken string\r\n\tacc   string\r\n\t// Multiple Nkeys/Users\r\n\tnkeys              []*NkeyUser\r\n\tusers              []*User\r\n\ttimeout            float64\r\n\tdefaultPermissions *Permissions\r\n}\r\n\r\n// TLSConfigOpts holds the parsed tls config information,\r\n// used with flag parsing\r\ntype TLSConfigOpts struct {\r\n\tCertFile         string\r\n\tKeyFile          string\r\n\tCaFile           string\r\n\tVerify           bool\r\n\tInsecure         bool\r\n\tMap              bool\r\n\tTimeout          float64\r\n\tCiphers          []uint16\r\n\tCurvePreferences []tls.CurveID\r\n}\r\n\r\nvar tlsUsage = `\r\nTLS configuration is specified in the tls section of a configuration file:\r\n\r\ne.g.\r\n\r\n    tls {\r\n        cert_file:      \"./certs/server-cert.pem\"\r\n        key_file:       \"./certs/server-key.pem\"\r\n        ca_file:        \"./certs/ca.pem\"\r\n        verify:         true\r\n        verify_and_map: true\r\n\r\n        cipher_suites: [\r\n            \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\r\n            \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"\r\n        ]\r\n        curve_preferences: [\r\n            \"CurveP256\",\r\n            \"CurveP384\",\r\n            \"CurveP521\"\r\n        ]\r\n    }\r\n\r\nAvailable cipher suites include:\r\n`\r\n\r\n// ProcessConfigFile processes a configuration file.\r\n// FIXME(dlc): A bit hacky\r\nfunc ProcessConfigFile(configFile string) (*Options, error) {\r\n\topts := &Options{}\r\n\tif err := opts.ProcessConfigFile(configFile); err != nil {\r\n\t\t// If only warnings then continue and return the options.\r\n\t\tif cerr, ok := err.(*processConfigErr); ok && len(cerr.Errors()) == 0 {\r\n\t\t\treturn opts, nil\r\n\t\t}\r\n\r\n\t\treturn nil, err\r\n\t}\r\n\treturn opts, nil\r\n}\r\n\r\n// token is an item parsed from the configuration.\r\ntype token interface {\r\n\tValue() interface{}\r\n\tLine() int\r\n\tIsUsedVariable() bool\r\n\tSourceFile() string\r\n\tPosition() int\r\n}\r\n\r\n// unwrapValue can be used to get the token and value from an item\r\n// to be able to report the line number in case of an incorrect\r\n// configuration.\r\n// also stores the token in lastToken for use in convertPanicToError\r\nfunc unwrapValue(v interface{}, lastToken *token) (token, interface{}) {\r\n\tswitch tk := v.(type) {\r\n\tcase token:\r\n\t\tif lastToken != nil {\r\n\t\t\t*lastToken = tk\r\n\t\t}\r\n\t\treturn tk, tk.Value()\r\n\tdefault:\r\n\t\treturn nil, v\r\n\t}\r\n}\r\n\r\n// use in defer to recover from panic and turn it into an error associated with last token\r\nfunc convertPanicToErrorList(lastToken *token, errors *[]error) {\r\n\t// only recover if an error can be stored\r\n\tif errors == nil {\r\n\t\treturn\r\n\t} else if err := recover(); err == nil {\r\n\t\treturn\r\n\t} else if lastToken != nil && *lastToken != nil {\r\n\t\t*errors = append(*errors, &configErr{*lastToken, fmt.Sprint(err)})\r\n\t} else {\r\n\t\t*errors = append(*errors, fmt.Errorf(\"encountered panic without a token %v\", err))\r\n\t}\r\n}\r\n\r\n// use in defer to recover from panic and turn it into an error associated with last token\r\nfunc convertPanicToError(lastToken *token, e *error) {\r\n\t// only recover if an error can be stored\r\n\tif e == nil || *e != nil {\r\n\t\treturn\r\n\t} else if err := recover(); err == nil {\r\n\t\treturn\r\n\t} else if lastToken != nil && *lastToken != nil {\r\n\t\t*e = &configErr{*lastToken, fmt.Sprint(err)}\r\n\t} else {\r\n\t\t*e = fmt.Errorf(\"%v\", err)\r\n\t}\r\n}\r\n\r\n// configureSystemAccount configures a system account\r\n// if present in the configuration.\r\nfunc configureSystemAccount(o *Options, m map[string]interface{}) (retErr error) {\r\n\tvar lt token\r\n\tdefer convertPanicToError(&lt, &retErr)\r\n\tconfigure := func(v interface{}) error {\r\n\t\ttk, v := unwrapValue(v, &lt)\r\n\t\tsa, ok := v.(string)\r\n\t\tif !ok {\r\n\t\t\treturn &configErr{tk, \"system account name must be a string\"}\r\n\t\t}\r\n\t\to.SystemAccount = sa\r\n\t\treturn nil\r\n\t}\r\n\r\n\tif v, ok := m[\"system_account\"]; ok {\r\n\t\treturn configure(v)\r\n\t} else if v, ok := m[\"system\"]; ok {\r\n\t\treturn configure(v)\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// ProcessConfigFile updates the Options structure with options\r\n// present in the given configuration file.\r\n// This version is convenient if one wants to set some default\r\n// options and then override them with what is in the config file.\r\n// For instance, this version allows you to do something such as:\r\n//\r\n// opts := &Options{Debug: true}\r\n// opts.ProcessConfigFile(myConfigFile)\r\n//\r\n// If the config file contains \"debug: false\", after this call,\r\n// opts.Debug would really be false. It would be impossible to\r\n// achieve that with the non receiver ProcessConfigFile() version,\r\n// since one would not know after the call if \"debug\" was not present\r\n// or was present but set to false.\r\nfunc (o *Options) ProcessConfigFile(configFile string) error {\r\n\to.ConfigFile = configFile\r\n\tif configFile == \"\" {\r\n\t\treturn nil\r\n\t}\r\n\tm, err := conf.ParseFileWithChecks(configFile)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\t// Collect all errors and warnings and report them all together.\r\n\terrors := make([]error, 0)\r\n\twarnings := make([]error, 0)\r\n\r\n\t// First check whether a system account has been defined,\r\n\t// as that is a condition for other features to be enabled.\r\n\tif err := configureSystemAccount(o, m); err != nil {\r\n\t\terrors = append(errors, err)\r\n\t}\r\n\r\n\tfor k, v := range m {\r\n\t\to.processConfigFileLine(k, v, &errors, &warnings)\r\n\t}\r\n\r\n\tif len(errors) > 0 || len(warnings) > 0 {\r\n\t\treturn &processConfigErr{\r\n\t\t\terrors:   errors,\r\n\t\t\twarnings: warnings,\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc (o *Options) processConfigFileLine(k string, v interface{}, errors *[]error, warnings *[]error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tswitch strings.ToLower(k) {\r\n\tcase \"listen\":\r\n\t\thp, err := parseListen(v)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.Host = hp.host\r\n\t\to.Port = hp.port\r\n\tcase \"client_advertise\":\r\n\t\to.ClientAdvertise = v.(string)\r\n\tcase \"port\":\r\n\t\to.Port = int(v.(int64))\r\n\tcase \"server_name\":\r\n\t\to.ServerName = v.(string)\r\n\tcase \"host\", \"net\":\r\n\t\to.Host = v.(string)\r\n\tcase \"debug\":\r\n\t\to.Debug = v.(bool)\r\n\t\ttrackExplicitVal(o, &o.inConfig, \"Debug\", o.Debug)\r\n\tcase \"trace\":\r\n\t\to.Trace = v.(bool)\r\n\t\ttrackExplicitVal(o, &o.inConfig, \"Trace\", o.Trace)\r\n\tcase \"trace_verbose\":\r\n\t\to.TraceVerbose = v.(bool)\r\n\t\to.Trace = v.(bool)\r\n\t\ttrackExplicitVal(o, &o.inConfig, \"TraceVerbose\", o.TraceVerbose)\r\n\t\ttrackExplicitVal(o, &o.inConfig, \"Trace\", o.Trace)\r\n\tcase \"logtime\":\r\n\t\to.Logtime = v.(bool)\r\n\t\ttrackExplicitVal(o, &o.inConfig, \"Logtime\", o.Logtime)\r\n\tcase \"disable_sublist_cache\", \"no_sublist_cache\":\r\n\t\to.NoSublistCache = v.(bool)\r\n\tcase \"accounts\":\r\n\t\terr := parseAccounts(tk, o, errors, warnings)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\tcase \"authorization\":\r\n\t\tauth, err := parseAuthorization(tk, o, errors, warnings)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\to.Username = auth.user\r\n\t\to.Password = auth.pass\r\n\t\to.Authorization = auth.token\r\n\t\tif (auth.user != \"\" || auth.pass != \"\") && auth.token != \"\" {\r\n\t\t\terr := &configErr{tk, \"Cannot have a user/pass and token\"}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.AuthTimeout = auth.timeout\r\n\t\t// Check for multiple users defined\r\n\t\tif auth.users != nil {\r\n\t\t\tif auth.user != \"\" {\r\n\t\t\t\terr := &configErr{tk, \"Can not have a single user/pass and a users array\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tif auth.token != \"\" {\r\n\t\t\t\terr := &configErr{tk, \"Can not have a token and a users array\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\t// Users may have been added from Accounts parsing, so do an append here\r\n\t\t\to.Users = append(o.Users, auth.users...)\r\n\t\t}\r\n\r\n\t\t// Check for nkeys\r\n\t\tif auth.nkeys != nil {\r\n\t\t\t// NKeys may have been added from Accounts parsing, so do an append here\r\n\t\t\to.Nkeys = append(o.Nkeys, auth.nkeys...)\r\n\t\t}\r\n\tcase \"http\":\r\n\t\thp, err := parseListen(v)\r\n\t\tif err != nil {\r\n\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.HTTPHost = hp.host\r\n\t\to.HTTPPort = hp.port\r\n\tcase \"https\":\r\n\t\thp, err := parseListen(v)\r\n\t\tif err != nil {\r\n\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.HTTPHost = hp.host\r\n\t\to.HTTPSPort = hp.port\r\n\tcase \"http_port\", \"monitor_port\":\r\n\t\to.HTTPPort = int(v.(int64))\r\n\tcase \"https_port\":\r\n\t\to.HTTPSPort = int(v.(int64))\r\n\tcase \"http_base_path\":\r\n\t\to.HTTPBasePath = v.(string)\r\n\tcase \"cluster\":\r\n\t\terr := parseCluster(tk, o, errors, warnings)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\tcase \"gateway\":\r\n\t\tif err := parseGateway(tk, o, errors, warnings); err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\tcase \"leaf\", \"leafnodes\":\r\n\t\terr := parseLeafNodes(tk, o, errors, warnings)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\tcase \"logfile\", \"log_file\":\r\n\t\to.LogFile = v.(string)\r\n\tcase \"logfile_size_limit\", \"log_size_limit\":\r\n\t\to.LogSizeLimit = v.(int64)\r\n\tcase \"syslog\":\r\n\t\to.Syslog = v.(bool)\r\n\t\ttrackExplicitVal(o, &o.inConfig, \"Syslog\", o.Syslog)\r\n\tcase \"remote_syslog\":\r\n\t\to.RemoteSyslog = v.(string)\r\n\tcase \"pidfile\", \"pid_file\":\r\n\t\to.PidFile = v.(string)\r\n\tcase \"ports_file_dir\":\r\n\t\to.PortsFileDir = v.(string)\r\n\tcase \"prof_port\":\r\n\t\to.ProfPort = int(v.(int64))\r\n\tcase \"max_control_line\":\r\n\t\tif v.(int64) > 1<<31-1 {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"%s value is too big\", k)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.MaxControlLine = int32(v.(int64))\r\n\tcase \"max_payload\":\r\n\t\tif v.(int64) > 1<<31-1 {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"%s value is too big\", k)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.MaxPayload = int32(v.(int64))\r\n\tcase \"max_pending\":\r\n\t\to.MaxPending = v.(int64)\r\n\tcase \"max_connections\", \"max_conn\":\r\n\t\to.MaxConn = int(v.(int64))\r\n\tcase \"max_traced_msg_len\":\r\n\t\to.MaxTracedMsgLen = int(v.(int64))\r\n\tcase \"max_subscriptions\", \"max_subs\":\r\n\t\to.MaxSubs = int(v.(int64))\r\n\tcase \"ping_interval\":\r\n\t\to.PingInterval = parseDuration(\"ping_interval\", tk, v, errors, warnings)\r\n\tcase \"ping_max\":\r\n\t\to.MaxPingsOut = int(v.(int64))\r\n\tcase \"tls\":\r\n\t\ttc, err := parseTLS(tk)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif o.TLSConfig, err = GenTLSConfig(tc); err != nil {\r\n\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.TLSTimeout = tc.Timeout\r\n\t\to.TLSMap = tc.Map\r\n\tcase \"write_deadline\":\r\n\t\to.WriteDeadline = parseDuration(\"write_deadline\", tk, v, errors, warnings)\r\n\tcase \"lame_duck_duration\":\r\n\t\tdur, err := time.ParseDuration(v.(string))\r\n\t\tif err != nil {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing lame_duck_duration: %v\", err)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif dur < 30*time.Second {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"invalid lame_duck_duration of %v, minimum is 30 seconds\", dur)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.LameDuckDuration = dur\r\n\tcase \"operator\", \"operators\", \"roots\", \"root\", \"root_operators\", \"root_operator\":\r\n\t\topFiles := []string{}\r\n\t\tswitch v := v.(type) {\r\n\t\tcase string:\r\n\t\t\topFiles = append(opFiles, v)\r\n\t\tcase []string:\r\n\t\t\topFiles = append(opFiles, v...)\r\n\t\tdefault:\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operators: unsupported type %T\", v)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t}\r\n\t\t// Assume for now these are file names, but they can also be the JWT itself inline.\r\n\t\to.TrustedOperators = make([]*jwt.OperatorClaims, 0, len(opFiles))\r\n\t\tfor _, fname := range opFiles {\r\n\t\t\topc, err := ReadOperatorJWT(fname)\r\n\t\t\tif err != nil {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operator JWT: %v\", err)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\to.TrustedOperators = append(o.TrustedOperators, opc)\r\n\t\t}\r\n\t\t// In case \"resolver\" is defined as well, it takes precedence\r\n\t\tif o.AccountResolver == nil && len(o.TrustedOperators) == 1 {\r\n\t\t\tif accUrl, err := parseURL(o.TrustedOperators[0].AccountServerURL, \"account resolver\"); err == nil {\r\n\t\t\t\t// accommodate nsc which appends \"/accounts\" during nsc push\r\n\t\t\t\tsuffix := \"\"\r\n\t\t\t\tif accUrl.Path == \"/jwt/v1/\" || accUrl.Path == \"/jwt/v1\" {\r\n\t\t\t\t\tsuffix = \"/accounts\"\r\n\t\t\t\t}\r\n\t\t\t\to.AccountResolver, _ = NewURLAccResolver(accUrl.String() + suffix)\r\n\t\t\t}\r\n\t\t}\r\n\tcase \"resolver\", \"account_resolver\", \"accounts_resolver\":\r\n\t\t// \"resolver\" takes precedence over value obtained from \"operator\".\r\n\t\t// Clear so that parsing errors are not silently ignored.\r\n\t\to.AccountResolver = nil\r\n\t\tvar memResolverRe = regexp.MustCompile(`(MEM|MEMORY|mem|memory)\\s*`)\r\n\t\tvar resolverRe = regexp.MustCompile(`(?:URL|url){1}(?:\\({1}\\s*\"?([^\\s\"]*)\"?\\s*\\){1})?\\s*`)\r\n\t\tstr, ok := v.(string)\r\n\t\tif !ok {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing operator resolver, wrong type %T\", v)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif memResolverRe.MatchString(str) {\r\n\t\t\to.AccountResolver = &MemAccResolver{}\r\n\t\t} else {\r\n\t\t\titems := resolverRe.FindStringSubmatch(str)\r\n\t\t\tif len(items) == 2 {\r\n\t\t\t\turl := items[1]\r\n\t\t\t\t_, err := parseURL(url, \"account resolver\")\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\tif ur, err := NewURLAccResolver(url); err != nil {\r\n\t\t\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\treturn\r\n\t\t\t\t} else {\r\n\t\t\t\t\to.AccountResolver = ur\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tif o.AccountResolver == nil {\r\n\t\t\terr := &configErr{tk, \"error parsing account resolver, should be MEM or URL(\\\"url\\\")\"}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t}\r\n\tcase \"resolver_tls\":\r\n\t\ttc, err := parseTLS(tk)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif o.AccountResolverTLSConfig, err = GenTLSConfig(tc); err != nil {\r\n\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\tcase \"resolver_preload\":\r\n\t\tmp, ok := v.(map[string]interface{})\r\n\t\tif !ok {\r\n\t\t\terr := &configErr{tk, \"preload should be a map of account_public_key:account_jwt\"}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\to.resolverPreloads = make(map[string]string)\r\n\t\tfor key, val := range mp {\r\n\t\t\ttk, val = unwrapValue(val, &lt)\r\n\t\t\tif jwtstr, ok := val.(string); !ok {\r\n\t\t\t\terr := &configErr{tk, \"preload map value should be a string JWT\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t} else {\r\n\t\t\t\t// Make sure this is a valid account JWT, that is a config error.\r\n\t\t\t\t// We will warn of expirations, etc later.\r\n\t\t\t\tif _, err := jwt.DecodeAccountClaims(jwtstr); err != nil {\r\n\t\t\t\t\terr := &configErr{tk, \"invalid account JWT\"}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\to.resolverPreloads[key] = jwtstr\r\n\t\t\t}\r\n\t\t}\r\n\tcase \"no_auth_user\":\r\n\t\to.NoAuthUser = v.(string)\r\n\tcase \"system_account\", \"system\":\r\n\t\t// Already processed at the beginning so we just skip them\r\n\t\t// to not treat them as unknown values.\r\n\t\treturn\r\n\tcase \"trusted\", \"trusted_keys\":\r\n\t\tswitch v := v.(type) {\r\n\t\tcase string:\r\n\t\t\to.TrustedKeys = []string{v}\r\n\t\tcase []string:\r\n\t\t\to.TrustedKeys = v\r\n\t\tcase []interface{}:\r\n\t\t\tkeys := make([]string, 0, len(v))\r\n\t\t\tfor _, mv := range v {\r\n\t\t\t\ttk, mv = unwrapValue(mv, &lt)\r\n\t\t\t\tif key, ok := mv.(string); ok {\r\n\t\t\t\t\tkeys = append(keys, key)\r\n\t\t\t\t} else {\r\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing trusted: unsupported type in array %T\", mv)}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\to.TrustedKeys = keys\r\n\t\tdefault:\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing trusted: unsupported type %T\", v)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t}\r\n\t\t// Do a quick sanity check on keys\r\n\t\tfor _, key := range o.TrustedKeys {\r\n\t\t\tif !nkeys.IsValidPublicOperatorKey(key) {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"trust key %q required to be a valid public operator nkey\", key)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\tcase \"connect_error_reports\":\r\n\t\to.ConnectErrorReports = int(v.(int64))\r\n\tcase \"reconnect_error_reports\":\r\n\t\to.ReconnectErrorReports = int(v.(int64))\r\n\tdefault:\r\n\t\tif au := atomic.LoadInt32(&allowUnknownTopLevelField); au == 0 && !tk.IsUsedVariable() {\r\n\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\tfield: k,\r\n\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\ttoken: tk,\r\n\t\t\t\t},\r\n\t\t\t}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc parseDuration(field string, tk token, v interface{}, errors *[]error, warnings *[]error) time.Duration {\r\n\tif wd, ok := v.(string); ok {\r\n\t\tif dur, err := time.ParseDuration(wd); err != nil {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing %s: %v\", field, err)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\treturn 0\r\n\t\t} else {\r\n\t\t\treturn dur\r\n\t\t}\r\n\t} else {\r\n\t\t// Backward compatible with old type, assume this is the\r\n\t\t// number of seconds.\r\n\t\terr := &configWarningErr{\r\n\t\t\tfield: field,\r\n\t\t\tconfigErr: configErr{\r\n\t\t\t\ttoken:  tk,\r\n\t\t\t\treason: field + \" should be converted to a duration\",\r\n\t\t\t},\r\n\t\t}\r\n\t\t*warnings = append(*warnings, err)\r\n\t\treturn time.Duration(v.(int64)) * time.Second\r\n\t}\r\n}\r\n\r\nfunc trackExplicitVal(opts *Options, pm *map[string]bool, name string, val bool) {\r\n\tm := *pm\r\n\tif m == nil {\r\n\t\tm = make(map[string]bool)\r\n\t\t*pm = m\r\n\t}\r\n\tm[name] = val\r\n}\r\n\r\n// hostPort is simple struct to hold parsed listen/addr strings.\r\ntype hostPort struct {\r\n\thost string\r\n\tport int\r\n}\r\n\r\n// parseListen will parse listen option which is replacing host/net and port\r\nfunc parseListen(v interface{}) (*hostPort, error) {\r\n\thp := &hostPort{}\r\n\tswitch vv := v.(type) {\r\n\t// Only a port\r\n\tcase int64:\r\n\t\thp.port = int(vv)\r\n\tcase string:\r\n\t\thost, port, err := net.SplitHostPort(vv)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"could not parse address string %q\", vv)\r\n\t\t}\r\n\t\thp.port, err = strconv.Atoi(port)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"could not parse port %q\", port)\r\n\t\t}\r\n\t\thp.host = host\r\n\t}\r\n\treturn hp, nil\r\n}\r\n\r\n// parseCluster will parse the cluster config.\r\nfunc parseCluster(v interface{}, opts *Options, errors *[]error, warnings *[]error) error {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tcm, ok := v.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected map to define cluster, got %T\", v)}\r\n\t}\r\n\r\n\tfor mk, mv := range cm {\r\n\t\t// Again, unwrap token value if line check is required.\r\n\t\ttk, mv = unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"listen\":\r\n\t\t\thp, err := parseListen(mv)\r\n\t\t\tif err != nil {\r\n\t\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.Cluster.Host = hp.host\r\n\t\t\topts.Cluster.Port = hp.port\r\n\t\tcase \"port\":\r\n\t\t\topts.Cluster.Port = int(mv.(int64))\r\n\t\tcase \"host\", \"net\":\r\n\t\t\topts.Cluster.Host = mv.(string)\r\n\t\tcase \"authorization\":\r\n\t\t\tauth, err := parseAuthorization(tk, opts, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif auth.users != nil {\r\n\t\t\t\terr := &configErr{tk, \"Cluster authorization does not allow multiple users\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.Cluster.Username = auth.user\r\n\t\t\topts.Cluster.Password = auth.pass\r\n\t\t\topts.Cluster.AuthTimeout = auth.timeout\r\n\r\n\t\t\tif auth.defaultPermissions != nil {\r\n\t\t\t\terr := &configWarningErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken:  tk,\r\n\t\t\t\t\t\treason: `setting \"permissions\" within cluster authorization block is deprecated`,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*warnings = append(*warnings, err)\r\n\r\n\t\t\t\t// Do not set permissions if they were specified in top-level cluster block.\r\n\t\t\t\tif opts.Cluster.Permissions == nil {\r\n\t\t\t\t\tsetClusterPermissions(&opts.Cluster, auth.defaultPermissions)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\tcase \"routes\":\r\n\t\t\tra := mv.([]interface{})\r\n\t\t\troutes, errs := parseURLs(ra, \"route\")\r\n\t\t\tif errs != nil {\r\n\t\t\t\t*errors = append(*errors, errs...)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.Routes = routes\r\n\t\tcase \"tls\":\r\n\t\t\tconfig, tlsopts, err := getTLSConfig(tk)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.Cluster.TLSConfig = config\r\n\t\t\topts.Cluster.TLSTimeout = tlsopts.Timeout\r\n\t\t\topts.Cluster.TLSMap = tlsopts.Map\r\n\t\tcase \"cluster_advertise\", \"advertise\":\r\n\t\t\topts.Cluster.Advertise = mv.(string)\r\n\t\tcase \"no_advertise\":\r\n\t\t\topts.Cluster.NoAdvertise = mv.(bool)\r\n\t\t\ttrackExplicitVal(opts, &opts.inConfig, \"Cluster.NoAdvertise\", opts.Cluster.NoAdvertise)\r\n\t\tcase \"connect_retries\":\r\n\t\t\topts.Cluster.ConnectRetries = int(mv.(int64))\r\n\t\tcase \"permissions\":\r\n\t\t\tperms, err := parseUserPermissions(mv, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\t// Dynamic response permissions do not make sense here.\r\n\t\t\tif perms.Response != nil {\r\n\t\t\t\terr := &configErr{tk, \"Cluster permissions do not support dynamic responses\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\t// This will possibly override permissions that were define in auth block\r\n\t\t\tsetClusterPermissions(&opts.Cluster, perms)\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc parseURLs(a []interface{}, typ string) (urls []*url.URL, errors []error) {\r\n\turls = make([]*url.URL, 0, len(a))\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, &errors)\r\n\r\n\tfor _, u := range a {\r\n\t\ttk, u := unwrapValue(u, &lt)\r\n\t\tsURL := u.(string)\r\n\t\turl, err := parseURL(sURL, typ)\r\n\t\tif err != nil {\r\n\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\terrors = append(errors, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\turls = append(urls, url)\r\n\t}\r\n\treturn urls, errors\r\n}\r\n\r\nfunc parseURL(u string, typ string) (*url.URL, error) {\r\n\turlStr := strings.TrimSpace(u)\r\n\turl, err := url.Parse(urlStr)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"error parsing %s url [%q]\", typ, urlStr)\r\n\t}\r\n\treturn url, nil\r\n}\r\n\r\nfunc parseGateway(v interface{}, o *Options, errors *[]error, warnings *[]error) error {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tgm, ok := v.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected gateway to be a map, got %T\", v)}\r\n\t}\r\n\tfor mk, mv := range gm {\r\n\t\t// Again, unwrap token value if line check is required.\r\n\t\ttk, mv = unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"name\":\r\n\t\t\to.Gateway.Name = mv.(string)\r\n\t\tcase \"listen\":\r\n\t\t\thp, err := parseListen(mv)\r\n\t\t\tif err != nil {\r\n\t\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\to.Gateway.Host = hp.host\r\n\t\t\to.Gateway.Port = hp.port\r\n\t\tcase \"port\":\r\n\t\t\to.Gateway.Port = int(mv.(int64))\r\n\t\tcase \"host\", \"net\":\r\n\t\t\to.Gateway.Host = mv.(string)\r\n\t\tcase \"authorization\":\r\n\t\t\tauth, err := parseAuthorization(tk, o, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif auth.users != nil {\r\n\t\t\t\t*errors = append(*errors, &configErr{tk, \"Gateway authorization does not allow multiple users\"})\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\to.Gateway.Username = auth.user\r\n\t\t\to.Gateway.Password = auth.pass\r\n\t\t\to.Gateway.AuthTimeout = auth.timeout\r\n\t\tcase \"tls\":\r\n\t\t\tconfig, tlsopts, err := getTLSConfig(tk)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\to.Gateway.TLSConfig = config\r\n\t\t\to.Gateway.TLSTimeout = tlsopts.Timeout\r\n\t\t\to.Gateway.TLSMap = tlsopts.Map\r\n\t\tcase \"advertise\":\r\n\t\t\to.Gateway.Advertise = mv.(string)\r\n\t\tcase \"connect_retries\":\r\n\t\t\to.Gateway.ConnectRetries = int(mv.(int64))\r\n\t\tcase \"gateways\":\r\n\t\t\tgateways, err := parseGateways(mv, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn err\r\n\t\t\t}\r\n\t\t\to.Gateway.Gateways = gateways\r\n\t\tcase \"reject_unknown\":\r\n\t\t\to.Gateway.RejectUnknown = mv.(bool)\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// parseLeafNodes will parse the leaf node config.\r\nfunc parseLeafNodes(v interface{}, opts *Options, errors *[]error, warnings *[]error) error {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tcm, ok := v.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn &configErr{tk, fmt.Sprintf(\"Expected map to define a leafnode, got %T\", v)}\r\n\t}\r\n\r\n\tfor mk, mv := range cm {\r\n\t\t// Again, unwrap token value if line check is required.\r\n\t\ttk, mv = unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"listen\":\r\n\t\t\thp, err := parseListen(mv)\r\n\t\t\tif err != nil {\r\n\t\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.LeafNode.Host = hp.host\r\n\t\t\topts.LeafNode.Port = hp.port\r\n\t\tcase \"port\":\r\n\t\t\topts.LeafNode.Port = int(mv.(int64))\r\n\t\tcase \"host\", \"net\":\r\n\t\t\topts.LeafNode.Host = mv.(string)\r\n\t\tcase \"authorization\":\r\n\t\t\tauth, err := parseLeafAuthorization(tk, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.LeafNode.Username = auth.user\r\n\t\t\topts.LeafNode.Password = auth.pass\r\n\t\t\topts.LeafNode.AuthTimeout = auth.timeout\r\n\t\t\topts.LeafNode.Account = auth.acc\r\n\t\t\topts.LeafNode.Users = auth.users\r\n\t\t\t// Validate user info config for leafnode authorization\r\n\t\t\tif err := validateLeafNodeAuthOptions(opts); err != nil {\r\n\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\tcase \"remotes\":\r\n\t\t\t// Parse the remote options here.\r\n\t\t\tremotes, err := parseRemoteLeafNodes(mv, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.LeafNode.Remotes = remotes\r\n\t\tcase \"reconnect\", \"reconnect_delay\", \"reconnect_interval\":\r\n\t\t\topts.LeafNode.ReconnectInterval = time.Duration(int(mv.(int64))) * time.Second\r\n\t\tcase \"tls\":\r\n\t\t\ttc, err := parseTLS(tk)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif opts.LeafNode.TLSConfig, err = GenTLSConfig(tc); err != nil {\r\n\t\t\t\terr := &configErr{tk, err.Error()}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.LeafNode.TLSTimeout = tc.Timeout\r\n\t\tcase \"leafnode_advertise\", \"advertise\":\r\n\t\t\topts.LeafNode.Advertise = mv.(string)\r\n\t\tcase \"no_advertise\":\r\n\t\t\topts.LeafNode.NoAdvertise = mv.(bool)\r\n\t\t\ttrackExplicitVal(opts, &opts.inConfig, \"LeafNode.NoAdvertise\", opts.LeafNode.NoAdvertise)\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// This is the authorization parser adapter for the leafnode's\r\n// authorization config.\r\nfunc parseLeafAuthorization(v interface{}, errors *[]error, warnings *[]error) (*authorization, error) {\r\n\tvar (\r\n\t\tam   map[string]interface{}\r\n\t\ttk   token\r\n\t\tlt   token\r\n\t\tauth = &authorization{}\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\t_, v = unwrapValue(v, &lt)\r\n\tam = v.(map[string]interface{})\r\n\tfor mk, mv := range am {\r\n\t\ttk, mv = unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"user\", \"username\":\r\n\t\t\tauth.user = mv.(string)\r\n\t\tcase \"pass\", \"password\":\r\n\t\t\tauth.pass = mv.(string)\r\n\t\tcase \"timeout\":\r\n\t\t\tat := float64(1)\r\n\t\t\tswitch mv := mv.(type) {\r\n\t\t\tcase int64:\r\n\t\t\t\tat = float64(mv)\r\n\t\t\tcase float64:\r\n\t\t\t\tat = mv\r\n\t\t\t}\r\n\t\t\tauth.timeout = at\r\n\t\tcase \"users\":\r\n\t\t\tusers, err := parseLeafUsers(tk, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tauth.users = users\r\n\t\tcase \"account\":\r\n\t\t\tauth.acc = mv.(string)\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t\tcontinue\r\n\t\t}\r\n\t}\r\n\treturn auth, nil\r\n}\r\n\r\n// This is a trimmed down version of parseUsers that is adapted\r\n// for the users possibly defined in the authorization{} section\r\n// of leafnodes {}.\r\nfunc parseLeafUsers(mv interface{}, errors *[]error, warnings *[]error) ([]*User, error) {\r\n\tvar (\r\n\t\ttk    token\r\n\t\tlt    token\r\n\t\tusers = []*User{}\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, mv = unwrapValue(mv, &lt)\r\n\t// Make sure we have an array\r\n\tuv, ok := mv.([]interface{})\r\n\tif !ok {\r\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected users field to be an array, got %v\", mv)}\r\n\t}\r\n\tfor _, u := range uv {\r\n\t\ttk, u = unwrapValue(u, &lt)\r\n\t\t// Check its a map/struct\r\n\t\tum, ok := u.(map[string]interface{})\r\n\t\tif !ok {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected user entry to be a map/struct, got %v\", u)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tuser := &User{}\r\n\t\tfor k, v := range um {\r\n\t\t\ttk, v = unwrapValue(v, &lt)\r\n\t\t\tswitch strings.ToLower(k) {\r\n\t\t\tcase \"user\", \"username\":\r\n\t\t\t\tuser.Username = v.(string)\r\n\t\t\tcase \"pass\", \"password\":\r\n\t\t\t\tuser.Password = v.(string)\r\n\t\t\tcase \"account\":\r\n\t\t\t\t// We really want to save just the account name here, but\r\n\t\t\t\t// the User object is *Account. So we create an account object\r\n\t\t\t\t// but it won't be registered anywhere. The server will just\r\n\t\t\t\t// use opts.LeafNode.Users[].Account.Name. Alternatively\r\n\t\t\t\t// we need to create internal objects to store u/p and account\r\n\t\t\t\t// name and have a server structure to hold that.\r\n\t\t\t\tuser.Account = NewAccount(v.(string))\r\n\t\t\tdefault:\r\n\t\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\t\tfield: k,\r\n\t\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tusers = append(users, user)\r\n\t}\r\n\treturn users, nil\r\n}\r\n\r\nfunc parseRemoteLeafNodes(v interface{}, errors *[]error, warnings *[]error) ([]*RemoteLeafOpts, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tra, ok := v.([]interface{})\r\n\tif !ok {\r\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected remotes field to be an array, got %T\", v)}\r\n\t}\r\n\tremotes := make([]*RemoteLeafOpts, 0, len(ra))\r\n\tfor _, r := range ra {\r\n\t\ttk, r = unwrapValue(r, &lt)\r\n\t\t// Check its a map/struct\r\n\t\trm, ok := r.(map[string]interface{})\r\n\t\tif !ok {\r\n\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"Expected remote leafnode entry to be a map/struct, got %v\", r)})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tremote := &RemoteLeafOpts{}\r\n\t\tfor k, v := range rm {\r\n\t\t\ttk, v = unwrapValue(v, &lt)\r\n\t\t\tswitch strings.ToLower(k) {\r\n\t\t\tcase \"url\", \"urls\":\r\n\t\t\t\tswitch v := v.(type) {\r\n\t\t\t\tcase []interface{}, []string:\r\n\t\t\t\t\turls, errs := parseURLs(v.([]interface{}), \"leafnode\")\r\n\t\t\t\t\tif errs != nil {\r\n\t\t\t\t\t\t*errors = append(*errors, errs...)\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\tremote.URLs = urls\r\n\t\t\t\tcase string:\r\n\t\t\t\t\turl, err := parseURL(v, \"leafnode\")\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\tremote.URLs = append(remote.URLs, url)\r\n\t\t\t\t}\r\n\t\t\tcase \"account\", \"local\":\r\n\t\t\t\tremote.LocalAccount = v.(string)\r\n\t\t\tcase \"creds\", \"credentials\":\r\n\t\t\t\tp, err := expandPath(v.(string))\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tremote.Credentials = p\r\n\t\t\tcase \"tls\":\r\n\t\t\t\ttc, err := parseTLS(tk)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tif remote.TLSConfig, err = GenTLSConfig(tc); err != nil {\r\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\t// If ca_file is defined, GenTLSConfig() sets TLSConfig.ClientCAs.\r\n\t\t\t\t// Set RootCAs since this tls.Config is used when soliciting\r\n\t\t\t\t// a connection (therefore behaves as a client).\r\n\t\t\t\tremote.TLSConfig.RootCAs = remote.TLSConfig.ClientCAs\r\n\t\t\t\tif tc.Timeout > 0 {\r\n\t\t\t\t\tremote.TLSTimeout = tc.Timeout\r\n\t\t\t\t} else {\r\n\t\t\t\t\tremote.TLSTimeout = float64(DEFAULT_LEAF_TLS_TIMEOUT)\r\n\t\t\t\t}\r\n\t\t\tcase \"hub\":\r\n\t\t\t\tremote.Hub = v.(bool)\r\n\t\t\tcase \"deny_imports\", \"deny_import\":\r\n\t\t\t\tsubjects, err := parseSubjects(tk, errors, warnings)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tremote.DenyImports = subjects\r\n\t\t\tcase \"deny_exports\", \"deny_export\":\r\n\t\t\t\tsubjects, err := parseSubjects(tk, errors, warnings)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tremote.DenyExports = subjects\r\n\t\t\tdefault:\r\n\t\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\t\tfield: k,\r\n\t\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tremotes = append(remotes, remote)\r\n\t}\r\n\treturn remotes, nil\r\n}\r\n\r\n// Parse TLS and returns a TLSConfig and TLSTimeout.\r\n// Used by cluster and gateway parsing.\r\nfunc getTLSConfig(tk token) (*tls.Config, *TLSConfigOpts, error) {\r\n\ttc, err := parseTLS(tk)\r\n\tif err != nil {\r\n\t\treturn nil, nil, err\r\n\t}\r\n\tconfig, err := GenTLSConfig(tc)\r\n\tif err != nil {\r\n\t\terr := &configErr{tk, err.Error()}\r\n\t\treturn nil, nil, err\r\n\t}\r\n\t// For clusters/gateways, we will force strict verification. We also act\r\n\t// as both client and server, so will mirror the rootCA to the\r\n\t// clientCA pool.\r\n\tconfig.ClientAuth = tls.RequireAndVerifyClientCert\r\n\tconfig.RootCAs = config.ClientCAs\r\n\treturn config, tc, nil\r\n}\r\n\r\nfunc parseGateways(v interface{}, errors *[]error, warnings *[]error) ([]*RemoteGatewayOpts, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\t// Make sure we have an array\r\n\tga, ok := v.([]interface{})\r\n\tif !ok {\r\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected gateways field to be an array, got %T\", v)}\r\n\t}\r\n\tgateways := []*RemoteGatewayOpts{}\r\n\tfor _, g := range ga {\r\n\t\ttk, g = unwrapValue(g, &lt)\r\n\t\t// Check its a map/struct\r\n\t\tgm, ok := g.(map[string]interface{})\r\n\t\tif !ok {\r\n\t\t\t*errors = append(*errors, &configErr{tk, fmt.Sprintf(\"Expected gateway entry to be a map/struct, got %v\", g)})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tgateway := &RemoteGatewayOpts{}\r\n\t\tfor k, v := range gm {\r\n\t\t\ttk, v = unwrapValue(v, &lt)\r\n\t\t\tswitch strings.ToLower(k) {\r\n\t\t\tcase \"name\":\r\n\t\t\t\tgateway.Name = v.(string)\r\n\t\t\tcase \"tls\":\r\n\t\t\t\ttls, tlsopts, err := getTLSConfig(tk)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tgateway.TLSConfig = tls\r\n\t\t\t\tgateway.TLSTimeout = tlsopts.Timeout\r\n\t\t\tcase \"url\":\r\n\t\t\t\turl, err := parseURL(v.(string), \"gateway\")\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, &configErr{tk, err.Error()})\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tgateway.URLs = append(gateway.URLs, url)\r\n\t\t\tcase \"urls\":\r\n\t\t\t\turls, errs := parseURLs(v.([]interface{}), \"gateway\")\r\n\t\t\t\tif errs != nil {\r\n\t\t\t\t\t*errors = append(*errors, errs...)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tgateway.URLs = urls\r\n\t\t\tdefault:\r\n\t\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\t\tfield: k,\r\n\t\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tgateways = append(gateways, gateway)\r\n\t}\r\n\treturn gateways, nil\r\n}\r\n\r\n// Sets cluster's permissions based on given pub/sub permissions,\r\n// doing the appropriate translation.\r\nfunc setClusterPermissions(opts *ClusterOpts, perms *Permissions) {\r\n\t// Import is whether or not we will send a SUB for interest to the other side.\r\n\t// Export is whether or not we will accept a SUB from the remote for a given subject.\r\n\t// Both only effect interest registration.\r\n\t// The parsing sets Import into Publish and Export into Subscribe, convert\r\n\t// accordingly.\r\n\topts.Permissions = &RoutePermissions{\r\n\t\tImport: perms.Publish,\r\n\t\tExport: perms.Subscribe,\r\n\t}\r\n}\r\n\r\n// Temp structures to hold account import and export defintions since they need\r\n// to be processed after being parsed.\r\ntype export struct {\r\n\tacc  *Account\r\n\tsub  string\r\n\taccs []string\r\n\trt   ServiceRespType\r\n\tlat  *serviceLatency\r\n}\r\n\r\ntype importStream struct {\r\n\tacc *Account\r\n\tan  string\r\n\tsub string\r\n\tpre string\r\n}\r\n\r\ntype importService struct {\r\n\tacc *Account\r\n\tan  string\r\n\tsub string\r\n\tto  string\r\n}\r\n\r\n// Checks if an account name is reserved.\r\nfunc isReservedAccount(name string) bool {\r\n\treturn name == globalAccountName\r\n}\r\n\r\n// parseAccounts will parse the different accounts syntax.\r\nfunc parseAccounts(v interface{}, opts *Options, errors *[]error, warnings *[]error) error {\r\n\tvar (\r\n\t\timportStreams  []*importStream\r\n\t\timportServices []*importService\r\n\t\texportStreams  []*export\r\n\t\texportServices []*export\r\n\t\tlt             token\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tswitch vv := v.(type) {\r\n\t// Simple array of account names.\r\n\tcase []interface{}, []string:\r\n\t\tm := make(map[string]struct{}, len(v.([]interface{})))\r\n\t\tfor _, n := range v.([]interface{}) {\r\n\t\t\ttk, name := unwrapValue(n, &lt)\r\n\t\t\tns := name.(string)\r\n\t\t\t// Check for reserved names.\r\n\t\t\tif isReservedAccount(ns) {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"%q is a Reserved Account\", ns)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif _, ok := m[ns]; ok {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Duplicate Account Entry: %s\", ns)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\topts.Accounts = append(opts.Accounts, NewAccount(ns))\r\n\t\t\tm[ns] = struct{}{}\r\n\t\t}\r\n\t// More common map entry\r\n\tcase map[string]interface{}:\r\n\t\t// Track users across accounts, must be unique across\r\n\t\t// accounts and nkeys vs users.\r\n\t\tuorn := make(map[string]struct{})\r\n\t\tfor aname, mv := range vv {\r\n\t\t\ttk, amv := unwrapValue(mv, &lt)\r\n\r\n\t\t\t// Skip referenced config vars within the account block.\r\n\t\t\tif tk.IsUsedVariable() {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\t// These should be maps.\r\n\t\t\tmv, ok := amv.(map[string]interface{})\r\n\t\t\tif !ok {\r\n\t\t\t\terr := &configErr{tk, \"Expected map entries for accounts\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif isReservedAccount(aname) {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"%q is a Reserved Account\", aname)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tacc := NewAccount(aname)\r\n\t\t\topts.Accounts = append(opts.Accounts, acc)\r\n\r\n\t\t\tfor k, v := range mv {\r\n\t\t\t\ttk, mv := unwrapValue(v, &lt)\r\n\t\t\t\tswitch strings.ToLower(k) {\r\n\t\t\t\tcase \"nkey\":\r\n\t\t\t\t\tnk, ok := mv.(string)\r\n\t\t\t\t\tif !ok || !nkeys.IsValidPublicAccountKey(nk) {\r\n\t\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Not a valid public nkey for an account: %q\", mv)}\r\n\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\tacc.Nkey = nk\r\n\t\t\t\tcase \"imports\":\r\n\t\t\t\t\tstreams, services, err := parseAccountImports(tk, acc, errors, warnings)\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\timportStreams = append(importStreams, streams...)\r\n\t\t\t\t\timportServices = append(importServices, services...)\r\n\t\t\t\tcase \"exports\":\r\n\t\t\t\t\tstreams, services, err := parseAccountExports(tk, acc, errors, warnings)\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\texportStreams = append(exportStreams, streams...)\r\n\t\t\t\t\texportServices = append(exportServices, services...)\r\n\t\t\t\tcase \"users\":\r\n\t\t\t\t\tnkeys, users, err := parseUsers(mv, opts, errors, warnings)\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\tfor _, u := range users {\r\n\t\t\t\t\t\tif _, ok := uorn[u.Username]; ok {\r\n\t\t\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Duplicate user %q detected\", u.Username)}\r\n\t\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\tuorn[u.Username] = struct{}{}\r\n\t\t\t\t\t\tu.Account = acc\r\n\t\t\t\t\t}\r\n\t\t\t\t\topts.Users = append(opts.Users, users...)\r\n\r\n\t\t\t\t\tfor _, u := range nkeys {\r\n\t\t\t\t\t\tif _, ok := uorn[u.Nkey]; ok {\r\n\t\t\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Duplicate nkey %q detected\", u.Nkey)}\r\n\t\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\tuorn[u.Nkey] = struct{}{}\r\n\t\t\t\t\t\tu.Account = acc\r\n\t\t\t\t\t}\r\n\t\t\t\t\topts.Nkeys = append(opts.Nkeys, nkeys...)\r\n\t\t\t\tdefault:\r\n\t\t\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\t\t\tfield: k,\r\n\t\t\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tlt = tk\r\n\t// Bail already if there are previous errors.\r\n\tif len(*errors) > 0 {\r\n\t\treturn nil\r\n\t}\r\n\r\n\t// Parse Imports and Exports here after all accounts defined.\r\n\t// Do exports first since they need to be defined for imports to succeed\r\n\t// since we do permissions checks.\r\n\r\n\t// Create a lookup map for accounts lookups.\r\n\tam := make(map[string]*Account, len(opts.Accounts))\r\n\tfor _, a := range opts.Accounts {\r\n\t\tam[a.Name] = a\r\n\t}\r\n\t// Do stream exports\r\n\tfor _, stream := range exportStreams {\r\n\t\t// Make array of accounts if applicable.\r\n\t\tvar accounts []*Account\r\n\t\tfor _, an := range stream.accs {\r\n\t\t\tta := am[an]\r\n\t\t\tif ta == nil {\r\n\t\t\t\tmsg := fmt.Sprintf(\"%q account not defined for stream export\", an)\r\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\taccounts = append(accounts, ta)\r\n\t\t}\r\n\t\tif err := stream.acc.AddStreamExport(stream.sub, accounts); err != nil {\r\n\t\t\tmsg := fmt.Sprintf(\"Error adding stream export %q: %v\", stream.sub, err)\r\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t}\r\n\tfor _, service := range exportServices {\r\n\t\t// Make array of accounts if applicable.\r\n\t\tvar accounts []*Account\r\n\t\tfor _, an := range service.accs {\r\n\t\t\tta := am[an]\r\n\t\t\tif ta == nil {\r\n\t\t\t\tmsg := fmt.Sprintf(\"%q account not defined for service export\", an)\r\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\taccounts = append(accounts, ta)\r\n\t\t}\r\n\t\tif err := service.acc.AddServiceExportWithResponse(service.sub, service.rt, accounts); err != nil {\r\n\t\t\tmsg := fmt.Sprintf(\"Error adding service export %q: %v\", service.sub, err)\r\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tif service.lat != nil {\r\n\t\t\tif opts.SystemAccount == \"\" {\r\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding service latency sampling for %q: %v\", service.sub, ErrNoSysAccount.Error())\r\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tif err := service.acc.TrackServiceExportWithSampling(service.sub, service.lat.subject, int(service.lat.sampling)); err != nil {\r\n\t\t\t\tmsg := fmt.Sprintf(\"Error adding service latency sampling for %q on subject %q: %v\", service.sub, service.lat.subject, err)\r\n\t\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tfor _, stream := range importStreams {\r\n\t\tta := am[stream.an]\r\n\t\tif ta == nil {\r\n\t\t\tmsg := fmt.Sprintf(\"%q account not defined for stream import\", stream.an)\r\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif err := stream.acc.AddStreamImport(ta, stream.sub, stream.pre); err != nil {\r\n\t\t\tmsg := fmt.Sprintf(\"Error adding stream import %q: %v\", stream.sub, err)\r\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t}\r\n\tfor _, service := range importServices {\r\n\t\tta := am[service.an]\r\n\t\tif ta == nil {\r\n\t\t\tmsg := fmt.Sprintf(\"%q account not defined for service import\", service.an)\r\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif service.to == \"\" {\r\n\t\t\tservice.to = service.sub\r\n\t\t}\r\n\t\tif err := service.acc.AddServiceImport(ta, service.to, service.sub); err != nil {\r\n\t\t\tmsg := fmt.Sprintf(\"Error adding service import %q: %v\", service.sub, err)\r\n\t\t\t*errors = append(*errors, &configErr{tk, msg})\r\n\t\t\tcontinue\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// Parse the account exports\r\nfunc parseAccountExports(v interface{}, acc *Account, errors, warnings *[]error) ([]*export, []*export, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\t// This should be an array of objects/maps.\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tims, ok := v.([]interface{})\r\n\tif !ok {\r\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Exports should be an array, got %T\", v)}\r\n\t}\r\n\r\n\tvar services []*export\r\n\tvar streams []*export\r\n\r\n\tfor _, v := range ims {\r\n\t\t// Should have stream or service\r\n\t\tstream, service, err := parseExportStreamOrService(v, errors, warnings)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif service != nil {\r\n\t\t\tservice.acc = acc\r\n\t\t\tservices = append(services, service)\r\n\t\t}\r\n\t\tif stream != nil {\r\n\t\t\tstream.acc = acc\r\n\t\t\tstreams = append(streams, stream)\r\n\t\t}\r\n\t}\r\n\treturn streams, services, nil\r\n}\r\n\r\n// Parse the account imports\r\nfunc parseAccountImports(v interface{}, acc *Account, errors, warnings *[]error) ([]*importStream, []*importService, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\t// This should be an array of objects/maps.\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tims, ok := v.([]interface{})\r\n\tif !ok {\r\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Imports should be an array, got %T\", v)}\r\n\t}\r\n\r\n\tvar services []*importService\r\n\tvar streams []*importStream\r\n\tsvcSubjects := map[string]*importService{}\r\n\r\n\tfor _, v := range ims {\r\n\t\t// Should have stream or service\r\n\t\tstream, service, err := parseImportStreamOrService(v, errors, warnings)\r\n\t\tif err != nil {\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif service != nil {\r\n\t\t\tif dup := svcSubjects[service.to]; dup != nil {\r\n\t\t\t\ttk, _ := unwrapValue(v, &lt)\r\n\t\t\t\terr := &configErr{tk,\r\n\t\t\t\t\tfmt.Sprintf(\"Duplicate service import subject %q, previously used in import for account %q, subject %q\",\r\n\t\t\t\t\t\tservice.to, dup.an, dup.sub)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tsvcSubjects[service.to] = service\r\n\t\t\tservice.acc = acc\r\n\t\t\tservices = append(services, service)\r\n\t\t}\r\n\t\tif stream != nil {\r\n\t\t\tstream.acc = acc\r\n\t\t\tstreams = append(streams, stream)\r\n\t\t}\r\n\t}\r\n\treturn streams, services, nil\r\n}\r\n\r\n// Helper to parse an embedded account description for imported services or streams.\r\nfunc parseAccount(v map[string]interface{}, errors, warnings *[]error) (string, string, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\tvar accountName, subject string\r\n\tfor mk, mv := range v {\r\n\t\ttk, mv := unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"account\":\r\n\t\t\taccountName = mv.(string)\r\n\t\tcase \"subject\":\r\n\t\t\tsubject = mv.(string)\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn accountName, subject, nil\r\n}\r\n\r\n// Parse an export stream or service.\r\n// e.g.\r\n//   {stream: \"public.>\"} # No accounts means public.\r\n//   {stream: \"synadia.private.>\", accounts: [cncf, natsio]}\r\n//   {service: \"pub.request\"} # No accounts means public.\r\n//   {service: \"pub.special.request\", accounts: [nats.io]}\r\nfunc parseExportStreamOrService(v interface{}, errors, warnings *[]error) (*export, *export, error) {\r\n\tvar (\r\n\t\tcurStream  *export\r\n\t\tcurService *export\r\n\t\taccounts   []string\r\n\t\trt         ServiceRespType\r\n\t\trtSeen     bool\r\n\t\trtToken    token\r\n\t\tlat        *serviceLatency\r\n\t\tlatToken   token\r\n\t\tlt         token\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tvv, ok := v.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Export Items should be a map with type entry, got %T\", v)}\r\n\t}\r\n\tfor mk, mv := range vv {\r\n\t\ttk, mv := unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"stream\":\r\n\t\t\tif curService != nil {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Detected stream %q but already saw a service\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif rtToken != nil {\r\n\t\t\t\terr := &configErr{rtToken, \"Detected response directive on non-service\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif latToken != nil {\r\n\t\t\t\terr := &configErr{latToken, \"Detected latency directive on non-service\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tmvs, ok := mv.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected stream name to be string, got %T\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tcurStream = &export{sub: mvs}\r\n\t\t\tif accounts != nil {\r\n\t\t\t\tcurStream.accs = accounts\r\n\t\t\t}\r\n\t\tcase \"response\", \"response_type\":\r\n\t\t\trtSeen = true\r\n\t\t\trtToken = tk\r\n\t\t\tmvs, ok := mv.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected response type to be string, got %T\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tswitch strings.ToLower(mvs) {\r\n\t\t\tcase \"single\", \"singleton\":\r\n\t\t\t\trt = Singleton\r\n\t\t\tcase \"stream\":\r\n\t\t\t\trt = Stream\r\n\t\t\tcase \"chunk\", \"chunked\":\r\n\t\t\t\trt = Chunked\r\n\t\t\tdefault:\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown response type: %q\", mvs)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif curService != nil {\r\n\t\t\t\tcurService.rt = rt\r\n\t\t\t}\r\n\t\t\tif curStream != nil {\r\n\t\t\t\terr := &configErr{tk, \"Detected response directive on non-service\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\tcase \"service\":\r\n\t\t\tif curStream != nil {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Detected service %q but already saw a stream\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tmvs, ok := mv.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected service name to be string, got %T\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tcurService = &export{sub: mvs}\r\n\t\t\tif accounts != nil {\r\n\t\t\t\tcurService.accs = accounts\r\n\t\t\t}\r\n\t\t\tif rtSeen {\r\n\t\t\t\tcurService.rt = rt\r\n\t\t\t}\r\n\t\t\tif lat != nil {\r\n\t\t\t\tcurService.lat = lat\r\n\t\t\t}\r\n\t\tcase \"accounts\":\r\n\t\t\tfor _, iv := range mv.([]interface{}) {\r\n\t\t\t\t_, mv := unwrapValue(iv, &lt)\r\n\t\t\t\taccounts = append(accounts, mv.(string))\r\n\t\t\t}\r\n\t\t\tif curStream != nil {\r\n\t\t\t\tcurStream.accs = accounts\r\n\t\t\t} else if curService != nil {\r\n\t\t\t\tcurService.accs = accounts\r\n\t\t\t}\r\n\t\tcase \"latency\":\r\n\t\t\tlatToken = tk\r\n\t\t\tvar err error\r\n\t\t\tlat, err = parseServiceLatency(tk, mv)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif curStream != nil {\r\n\t\t\t\terr = &configErr{tk, \"Detected latency directive on non-service\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif curService != nil {\r\n\t\t\t\tcurService.lat = lat\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn curStream, curService, nil\r\n}\r\n\r\n// parseServiceLatency returns a latency config block.\r\nfunc parseServiceLatency(root token, v interface{}) (l *serviceLatency, retErr error) {\r\n\tvar lt token\r\n\tdefer convertPanicToError(&lt, &retErr)\r\n\r\n\tif subject, ok := v.(string); ok {\r\n\t\treturn &serviceLatency{\r\n\t\t\tsubject:  subject,\r\n\t\t\tsampling: DEFAULT_SERVICE_LATENCY_SAMPLING,\r\n\t\t}, nil\r\n\t}\r\n\r\n\tlatency, ok := v.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn nil, &configErr{token: root,\r\n\t\t\treason: fmt.Sprintf(\"Expected latency entry to be a map/struct or string, got %T\", v)}\r\n\t}\r\n\r\n\tsl := serviceLatency{\r\n\t\tsampling: DEFAULT_SERVICE_LATENCY_SAMPLING,\r\n\t}\r\n\r\n\t// Read sampling value.\r\n\tif v, ok := latency[\"sampling\"]; ok {\r\n\t\ttk, v := unwrapValue(v, &lt)\r\n\r\n\t\tvar sample int64\r\n\t\tswitch vv := v.(type) {\r\n\t\tcase int64:\r\n\t\t\t// Sample is an int, like 50.\r\n\t\t\tsample = vv\r\n\t\tcase string:\r\n\t\t\t// Sample is a string, like \"50%\".\r\n\t\t\ts := strings.TrimSuffix(vv, \"%\")\r\n\t\t\tn, err := strconv.Atoi(s)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn nil, &configErr{token: tk,\r\n\t\t\t\t\treason: fmt.Sprintf(\"Failed to parse latency sample: %v\", err)}\r\n\t\t\t}\r\n\t\t\tsample = int64(n)\r\n\t\tdefault:\r\n\t\t\treturn nil, &configErr{token: tk,\r\n\t\t\t\treason: fmt.Sprintf(\"Expected latency sample to be a string or map/struct, got %T\", v)}\r\n\t\t}\r\n\t\tif sample < 1 || sample > 100 {\r\n\t\t\treturn nil, &configErr{token: tk,\r\n\t\t\t\treason: ErrBadSampling.Error()}\r\n\t\t}\r\n\r\n\t\tsl.sampling = int8(sample)\r\n\t}\r\n\r\n\t// Read subject value.\r\n\tv, ok = latency[\"subject\"]\r\n\tif !ok {\r\n\t\treturn nil, &configErr{token: root,\r\n\t\t\treason: \"Latency subject required, but missing\"}\r\n\t}\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\tsubject, ok := v.(string)\r\n\tif !ok {\r\n\t\treturn nil, &configErr{token: tk,\r\n\t\t\treason: fmt.Sprintf(\"Expected latency subject to be a string, got %T\", subject)}\r\n\t}\r\n\tsl.subject = subject\r\n\r\n\treturn &sl, nil\r\n}\r\n\r\n// Parse an import stream or service.\r\n// e.g.\r\n//   {stream: {account: \"synadia\", subject:\"public.synadia\"}, prefix: \"imports.synadia\"}\r\n//   {stream: {account: \"synadia\", subject:\"synadia.private.*\"}}\r\n//   {service: {account: \"synadia\", subject: \"pub.special.request\"}, to: \"synadia.request\"}\r\nfunc parseImportStreamOrService(v interface{}, errors, warnings *[]error) (*importStream, *importService, error) {\r\n\tvar (\r\n\t\tcurStream  *importStream\r\n\t\tcurService *importService\r\n\t\tpre, to    string\r\n\t\tlt         token\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, mv := unwrapValue(v, &lt)\r\n\tvv, ok := mv.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Import Items should be a map with type entry, got %T\", mv)}\r\n\t}\r\n\tfor mk, mv := range vv {\r\n\t\ttk, mv := unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"stream\":\r\n\t\t\tif curService != nil {\r\n\t\t\t\terr := &configErr{tk, \"Detected stream but already saw a service\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tac, ok := mv.(map[string]interface{})\r\n\t\t\tif !ok {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Stream entry should be an account map, got %T\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\t// Make sure this is a map with account and subject\r\n\t\t\taccountName, subject, err := parseAccount(ac, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif accountName == \"\" || subject == \"\" {\r\n\t\t\t\terr := &configErr{tk, \"Expect an account name and a subject\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tcurStream = &importStream{an: accountName, sub: subject}\r\n\t\t\tif pre != \"\" {\r\n\t\t\t\tcurStream.pre = pre\r\n\t\t\t}\r\n\t\tcase \"service\":\r\n\t\t\tif curStream != nil {\r\n\t\t\t\terr := &configErr{tk, \"Detected service but already saw a stream\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tac, ok := mv.(map[string]interface{})\r\n\t\t\tif !ok {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Service entry should be an account map, got %T\", mv)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\t// Make sure this is a map with account and subject\r\n\t\t\taccountName, subject, err := parseAccount(ac, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif accountName == \"\" || subject == \"\" {\r\n\t\t\t\terr := &configErr{tk, \"Expect an account name and a subject\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tcurService = &importService{an: accountName, sub: subject}\r\n\t\t\tif to != \"\" {\r\n\t\t\t\tcurService.to = to\r\n\t\t\t}\r\n\t\tcase \"prefix\":\r\n\t\t\tpre = mv.(string)\r\n\t\t\tif curStream != nil {\r\n\t\t\t\tcurStream.pre = pre\r\n\t\t\t}\r\n\t\tcase \"to\":\r\n\t\t\tto = mv.(string)\r\n\t\t\tif curService != nil {\r\n\t\t\t\tcurService.to = to\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t}\r\n\treturn curStream, curService, nil\r\n}\r\n\r\n// Helper function to parse Authorization configs.\r\nfunc parseAuthorization(v interface{}, opts *Options, errors *[]error, warnings *[]error) (*authorization, error) {\r\n\tvar (\r\n\t\tam   map[string]interface{}\r\n\t\ttk   token\r\n\t\tlt   token\r\n\t\tauth = &authorization{}\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\t_, v = unwrapValue(v, &lt)\r\n\tam = v.(map[string]interface{})\r\n\tfor mk, mv := range am {\r\n\t\ttk, mv = unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"user\", \"username\":\r\n\t\t\tauth.user = mv.(string)\r\n\t\tcase \"pass\", \"password\":\r\n\t\t\tauth.pass = mv.(string)\r\n\t\tcase \"token\":\r\n\t\t\tauth.token = mv.(string)\r\n\t\tcase \"timeout\":\r\n\t\t\tat := float64(1)\r\n\t\t\tswitch mv := mv.(type) {\r\n\t\t\tcase int64:\r\n\t\t\t\tat = float64(mv)\r\n\t\t\tcase float64:\r\n\t\t\t\tat = mv\r\n\t\t\t}\r\n\t\t\tauth.timeout = at\r\n\t\tcase \"users\":\r\n\t\t\tnkeys, users, err := parseUsers(tk, opts, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tauth.users = users\r\n\t\t\tauth.nkeys = nkeys\r\n\t\tcase \"default_permission\", \"default_permissions\", \"permissions\":\r\n\t\t\tpermissions, err := parseUserPermissions(tk, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tauth.defaultPermissions = permissions\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\tfield: mk,\r\n\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t},\r\n\t\t\t\t}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// Now check for permission defaults with multiple users, etc.\r\n\t\tif auth.defaultPermissions != nil {\r\n\t\t\tif auth.users != nil {\r\n\t\t\t\tfor _, user := range auth.users {\r\n\t\t\t\t\tif user.Permissions == nil {\r\n\t\t\t\t\t\tuser.Permissions = auth.defaultPermissions\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif auth.nkeys != nil {\r\n\t\t\t\tfor _, user := range auth.nkeys {\r\n\t\t\t\t\tif user.Permissions == nil {\r\n\t\t\t\t\t\tuser.Permissions = auth.defaultPermissions\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn auth, nil\r\n}\r\n\r\n// Helper function to parse multiple users array with optional permissions.\r\nfunc parseUsers(mv interface{}, opts *Options, errors *[]error, warnings *[]error) ([]*NkeyUser, []*User, error) {\r\n\tvar (\r\n\t\ttk    token\r\n\t\tlt    token\r\n\t\tkeys  []*NkeyUser\r\n\t\tusers = []*User{}\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\ttk, mv = unwrapValue(mv, &lt)\r\n\r\n\t// Make sure we have an array\r\n\tuv, ok := mv.([]interface{})\r\n\tif !ok {\r\n\t\treturn nil, nil, &configErr{tk, fmt.Sprintf(\"Expected users field to be an array, got %v\", mv)}\r\n\t}\r\n\tfor _, u := range uv {\r\n\t\ttk, u = unwrapValue(u, &lt)\r\n\r\n\t\t// Check its a map/struct\r\n\t\tum, ok := u.(map[string]interface{})\r\n\t\tif !ok {\r\n\t\t\terr := &configErr{tk, fmt.Sprintf(\"Expected user entry to be a map/struct, got %v\", u)}\r\n\t\t\t*errors = append(*errors, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tvar (\r\n\t\t\tuser  = &User{}\r\n\t\t\tnkey  = &NkeyUser{}\r\n\t\t\tperms *Permissions\r\n\t\t\terr   error\r\n\t\t)\r\n\t\tfor k, v := range um {\r\n\t\t\t// Also needs to unwrap first\r\n\t\t\ttk, v = unwrapValue(v, &lt)\r\n\r\n\t\t\tswitch strings.ToLower(k) {\r\n\t\t\tcase \"nkey\":\r\n\t\t\t\tnkey.Nkey = v.(string)\r\n\t\t\tcase \"user\", \"username\":\r\n\t\t\t\tuser.Username = v.(string)\r\n\t\t\tcase \"pass\", \"password\":\r\n\t\t\t\tuser.Password = v.(string)\r\n\t\t\tcase \"permission\", \"permissions\", \"authorization\":\r\n\t\t\t\tperms, err = parseUserPermissions(tk, errors, warnings)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\tdefault:\r\n\t\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\t\terr := &unknownConfigFieldErr{\r\n\t\t\t\t\t\tfield: k,\r\n\t\t\t\t\t\tconfigErr: configErr{\r\n\t\t\t\t\t\t\ttoken: tk,\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\t// Place perms if we have them.\r\n\t\tif perms != nil {\r\n\t\t\t// nkey takes precedent.\r\n\t\t\tif nkey.Nkey != \"\" {\r\n\t\t\t\tnkey.Permissions = perms\r\n\t\t\t} else {\r\n\t\t\t\tuser.Permissions = perms\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\t// Check to make sure we have at least an nkey or username <password> defined.\r\n\t\tif nkey.Nkey == \"\" && user.Username == \"\" {\r\n\t\t\treturn nil, nil, &configErr{tk, \"User entry requires a user\"}\r\n\t\t} else if nkey.Nkey != \"\" {\r\n\t\t\t// Make sure the nkey a proper public nkey for a user..\r\n\t\t\tif !nkeys.IsValidPublicUserKey(nkey.Nkey) {\r\n\t\t\t\treturn nil, nil, &configErr{tk, \"Not a valid public nkey for a user\"}\r\n\t\t\t}\r\n\t\t\t// If we have user or password defined here that is an error.\r\n\t\t\tif user.Username != \"\" || user.Password != \"\" {\r\n\t\t\t\treturn nil, nil, &configErr{tk, \"Nkey users do not take usernames or passwords\"}\r\n\t\t\t}\r\n\t\t\tkeys = append(keys, nkey)\r\n\t\t} else {\r\n\t\t\tusers = append(users, user)\r\n\t\t}\r\n\t}\r\n\treturn keys, users, nil\r\n}\r\n\r\n// Helper function to parse user/account permissions\r\nfunc parseUserPermissions(mv interface{}, errors, warnings *[]error) (*Permissions, error) {\r\n\tvar (\r\n\t\ttk token\r\n\t\tlt token\r\n\t\tp  = &Permissions{}\r\n\t)\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, mv = unwrapValue(mv, &lt)\r\n\tpm, ok := mv.(map[string]interface{})\r\n\tif !ok {\r\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected permissions to be a map/struct, got %+v\", mv)}\r\n\t}\r\n\tfor k, v := range pm {\r\n\t\ttk, mv = unwrapValue(v, &lt)\r\n\r\n\t\tswitch strings.ToLower(k) {\r\n\t\t// For routes:\r\n\t\t// Import is Publish\r\n\t\t// Export is Subscribe\r\n\t\tcase \"pub\", \"publish\", \"import\":\r\n\t\t\tperms, err := parseVariablePermissions(mv, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tp.Publish = perms\r\n\t\tcase \"sub\", \"subscribe\", \"export\":\r\n\t\t\tperms, err := parseVariablePermissions(mv, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tp.Subscribe = perms\r\n\t\tcase \"publish_allow_responses\", \"allow_responses\":\r\n\t\t\trp := &ResponsePermission{\r\n\t\t\t\tMaxMsgs: DEFAULT_ALLOW_RESPONSE_MAX_MSGS,\r\n\t\t\t\tExpires: DEFAULT_ALLOW_RESPONSE_EXPIRATION,\r\n\t\t\t}\r\n\t\t\t// Try boolean first\r\n\t\t\tresponses, ok := mv.(bool)\r\n\t\t\tif ok {\r\n\t\t\t\tif responses {\r\n\t\t\t\t\tp.Response = rp\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\tp.Response = parseAllowResponses(v, errors, warnings)\r\n\t\t\t}\r\n\t\t\tif p.Response != nil {\r\n\t\t\t\tif p.Publish == nil {\r\n\t\t\t\t\tp.Publish = &SubjectPermission{}\r\n\t\t\t\t}\r\n\t\t\t\tif p.Publish.Allow == nil {\r\n\t\t\t\t\t// We turn off the blanket allow statement.\r\n\t\t\t\t\tp.Publish.Allow = []string{}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing permissions\", k)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn p, nil\r\n}\r\n\r\n// Top level parser for authorization configurations.\r\nfunc parseVariablePermissions(v interface{}, errors, warnings *[]error) (*SubjectPermission, error) {\r\n\tswitch vv := v.(type) {\r\n\tcase map[string]interface{}:\r\n\t\t// New style with allow and/or deny properties.\r\n\t\treturn parseSubjectPermission(vv, errors, warnings)\r\n\tdefault:\r\n\t\t// Old style\r\n\t\treturn parseOldPermissionStyle(v, errors, warnings)\r\n\t}\r\n}\r\n\r\n// Helper function to parse subject singletons and/or arrays\r\nfunc parseSubjects(v interface{}, errors, warnings *[]error) ([]string, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\r\n\tvar subjects []string\r\n\tswitch vv := v.(type) {\r\n\tcase string:\r\n\t\tsubjects = append(subjects, vv)\r\n\tcase []string:\r\n\t\tsubjects = vv\r\n\tcase []interface{}:\r\n\t\tfor _, i := range vv {\r\n\t\t\ttk, i := unwrapValue(i, &lt)\r\n\r\n\t\t\tsubject, ok := i.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"Subject in permissions array cannot be cast to string\"}\r\n\t\t\t}\r\n\t\t\tsubjects = append(subjects, subject)\r\n\t\t}\r\n\tdefault:\r\n\t\treturn nil, &configErr{tk, fmt.Sprintf(\"Expected subject permissions to be a subject, or array of subjects, got %T\", v)}\r\n\t}\r\n\tif err := checkSubjectArray(subjects); err != nil {\r\n\t\treturn nil, &configErr{tk, err.Error()}\r\n\t}\r\n\treturn subjects, nil\r\n}\r\n\r\n// Helper function to parse a ResponsePermission.\r\nfunc parseAllowResponses(v interface{}, errors, warnings *[]error) *ResponsePermission {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\ttk, v := unwrapValue(v, &lt)\r\n\t// Check if this is a map.\r\n\tpm, ok := v.(map[string]interface{})\r\n\tif !ok {\r\n\t\terr := &configErr{tk, \"error parsing response permissions, expected a boolean or a map\"}\r\n\t\t*errors = append(*errors, err)\r\n\t\treturn nil\r\n\t}\r\n\r\n\trp := &ResponsePermission{\r\n\t\tMaxMsgs: DEFAULT_ALLOW_RESPONSE_MAX_MSGS,\r\n\t\tExpires: DEFAULT_ALLOW_RESPONSE_EXPIRATION,\r\n\t}\r\n\r\n\tfor k, v := range pm {\r\n\t\ttk, v = unwrapValue(v, &lt)\r\n\t\tswitch strings.ToLower(k) {\r\n\t\tcase \"max\", \"max_msgs\", \"max_messages\", \"max_responses\":\r\n\t\t\tmax := int(v.(int64))\r\n\t\t\t// Negative values are accepted (mean infinite), and 0\r\n\t\t\t// means default value (set above).\r\n\t\t\tif max != 0 {\r\n\t\t\t\trp.MaxMsgs = max\r\n\t\t\t}\r\n\t\tcase \"expires\", \"expiration\", \"ttl\":\r\n\t\t\twd, ok := v.(string)\r\n\t\t\tif ok {\r\n\t\t\t\tttl, err := time.ParseDuration(wd)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"error parsing expires: %v\", err)}\r\n\t\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\t\treturn nil\r\n\t\t\t\t}\r\n\t\t\t\t// Negative values are accepted (mean infinite), and 0\r\n\t\t\t\t// means default value (set above).\r\n\t\t\t\tif ttl != 0 {\r\n\t\t\t\t\trp.Expires = ttl\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\terr := &configErr{tk, \"error parsing expires, not a duration string\"}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field %q parsing permissions\", k)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn rp\r\n}\r\n\r\n// Helper function to parse old style authorization configs.\r\nfunc parseOldPermissionStyle(v interface{}, errors, warnings *[]error) (*SubjectPermission, error) {\r\n\tsubjects, err := parseSubjects(v, errors, warnings)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\treturn &SubjectPermission{Allow: subjects}, nil\r\n}\r\n\r\n// Helper function to parse new style authorization into a SubjectPermission with Allow and Deny.\r\nfunc parseSubjectPermission(v interface{}, errors, warnings *[]error) (*SubjectPermission, error) {\r\n\tvar lt token\r\n\tdefer convertPanicToErrorList(&lt, errors)\r\n\r\n\tm := v.(map[string]interface{})\r\n\tif len(m) == 0 {\r\n\t\treturn nil, nil\r\n\t}\r\n\tp := &SubjectPermission{}\r\n\tfor k, v := range m {\r\n\t\ttk, _ := unwrapValue(v, &lt)\r\n\t\tswitch strings.ToLower(k) {\r\n\t\tcase \"allow\":\r\n\t\t\tsubjects, err := parseSubjects(tk, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tp.Allow = subjects\r\n\t\tcase \"deny\":\r\n\t\t\tsubjects, err := parseSubjects(tk, errors, warnings)\r\n\t\t\tif err != nil {\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tp.Deny = subjects\r\n\t\tdefault:\r\n\t\t\tif !tk.IsUsedVariable() {\r\n\t\t\t\terr := &configErr{tk, fmt.Sprintf(\"Unknown field name %q parsing subject permissions, only 'allow' or 'deny' are permitted\", k)}\r\n\t\t\t\t*errors = append(*errors, err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn p, nil\r\n}\r\n\r\n// Helper function to validate subjects, etc for account permissioning.\r\nfunc checkSubjectArray(sa []string) error {\r\n\tfor _, s := range sa {\r\n\t\tif !IsValidSubject(s) {\r\n\t\t\treturn fmt.Errorf(\"subject %q is not a valid subject\", s)\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// PrintTLSHelpAndDie prints TLS usage and exits.\r\nfunc PrintTLSHelpAndDie() {\r\n\tfmt.Printf(\"%s\", tlsUsage)\r\n\tfor k := range cipherMap {\r\n\t\tfmt.Printf(\"    %s\\n\", k)\r\n\t}\r\n\tfmt.Printf(\"\\nAvailable curve preferences include:\\n\")\r\n\tfor k := range curvePreferenceMap {\r\n\t\tfmt.Printf(\"    %s\\n\", k)\r\n\t}\r\n\tos.Exit(0)\r\n}\r\n\r\nfunc parseCipher(cipherName string) (uint16, error) {\r\n\tcipher, exists := cipherMap[cipherName]\r\n\tif !exists {\r\n\t\treturn 0, fmt.Errorf(\"unrecognized cipher %s\", cipherName)\r\n\t}\r\n\r\n\treturn cipher, nil\r\n}\r\n\r\nfunc parseCurvePreferences(curveName string) (tls.CurveID, error) {\r\n\tcurve, exists := curvePreferenceMap[curveName]\r\n\tif !exists {\r\n\t\treturn 0, fmt.Errorf(\"unrecognized curve preference %s\", curveName)\r\n\t}\r\n\treturn curve, nil\r\n}\r\n\r\n// Helper function to parse TLS configs.\r\nfunc parseTLS(v interface{}) (t *TLSConfigOpts, retErr error) {\r\n\tvar (\r\n\t\ttlsm map[string]interface{}\r\n\t\ttc   = TLSConfigOpts{}\r\n\t\tlt   token\r\n\t)\r\n\tdefer convertPanicToError(&lt, &retErr)\r\n\r\n\t_, v = unwrapValue(v, &lt)\r\n\ttlsm = v.(map[string]interface{})\r\n\tfor mk, mv := range tlsm {\r\n\t\ttk, mv := unwrapValue(mv, &lt)\r\n\t\tswitch strings.ToLower(mk) {\r\n\t\tcase \"cert_file\":\r\n\t\t\tcertFile, ok := mv.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'cert_file' to be filename\"}\r\n\t\t\t}\r\n\t\t\ttc.CertFile = certFile\r\n\t\tcase \"key_file\":\r\n\t\t\tkeyFile, ok := mv.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'key_file' to be filename\"}\r\n\t\t\t}\r\n\t\t\ttc.KeyFile = keyFile\r\n\t\tcase \"ca_file\":\r\n\t\t\tcaFile, ok := mv.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'ca_file' to be filename\"}\r\n\t\t\t}\r\n\t\t\ttc.CaFile = caFile\r\n\t\tcase \"insecure\":\r\n\t\t\tinsecure, ok := mv.(bool)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'insecure' to be a boolean\"}\r\n\t\t\t}\r\n\t\t\ttc.Insecure = insecure\r\n\t\tcase \"verify\":\r\n\t\t\tverify, ok := mv.(bool)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'verify' to be a boolean\"}\r\n\t\t\t}\r\n\t\t\ttc.Verify = verify\r\n\t\tcase \"verify_and_map\":\r\n\t\t\tverify, ok := mv.(bool)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, expected 'verify_and_map' to be a boolean\"}\r\n\t\t\t}\r\n\t\t\ttc.Verify = verify\r\n\t\t\ttc.Map = verify\r\n\t\tcase \"cipher_suites\":\r\n\t\t\tra := mv.([]interface{})\r\n\t\t\tif len(ra) == 0 {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, 'cipher_suites' cannot be empty\"}\r\n\t\t\t}\r\n\t\t\ttc.Ciphers = make([]uint16, 0, len(ra))\r\n\t\t\tfor _, r := range ra {\r\n\t\t\t\ttk, r := unwrapValue(r, &lt)\r\n\t\t\t\tcipher, err := parseCipher(r.(string))\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\treturn nil, &configErr{tk, err.Error()}\r\n\t\t\t\t}\r\n\t\t\t\ttc.Ciphers = append(tc.Ciphers, cipher)\r\n\t\t\t}\r\n\t\tcase \"curve_preferences\":\r\n\t\t\tra := mv.([]interface{})\r\n\t\t\tif len(ra) == 0 {\r\n\t\t\t\treturn nil, &configErr{tk, \"error parsing tls config, 'curve_preferences' cannot be empty\"}\r\n\t\t\t}\r\n\t\t\ttc.CurvePreferences = make([]tls.CurveID, 0, len(ra))\r\n\t\t\tfor _, r := range ra {\r\n\t\t\t\ttk, r := unwrapValue(r, &lt)\r\n\t\t\t\tcps, err := parseCurvePreferences(r.(string))\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\treturn nil, &configErr{tk, err.Error()}\r\n\t\t\t\t}\r\n\t\t\t\ttc.CurvePreferences = append(tc.CurvePreferences, cps)\r\n\t\t\t}\r\n\t\tcase \"timeout\":\r\n\t\t\tat := float64(0)\r\n\t\t\tswitch mv := mv.(type) {\r\n\t\t\tcase int64:\r\n\t\t\t\tat = float64(mv)\r\n\t\t\tcase float64:\r\n\t\t\t\tat = mv\r\n\t\t\t}\r\n\t\t\ttc.Timeout = at\r\n\t\tdefault:\r\n\t\t\treturn nil, &configErr{tk, fmt.Sprintf(\"error parsing tls config, unknown field [%q]\", mk)}\r\n\t\t}\r\n\t}\r\n\r\n\t// If cipher suites were not specified then use the defaults\r\n\tif tc.Ciphers == nil {\r\n\t\ttc.Ciphers = defaultCipherSuites()\r\n\t}\r\n\r\n\t// If curve preferences were not specified, then use the defaults\r\n\tif tc.CurvePreferences == nil {\r\n\t\ttc.CurvePreferences = defaultCurvePreferences()\r\n\t}\r\n\r\n\treturn &tc, nil\r\n}\r\n\r\n// GenTLSConfig loads TLS related configuration parameters.\r\nfunc GenTLSConfig(tc *TLSConfigOpts) (*tls.Config, error) {\r\n\t// Create the tls.Config from our options before including the certs.\r\n\t// It will determine the cipher suites that we prefer.\r\n\t// FIXME(dlc) change if ARM based.\r\n\tconfig := tls.Config{\r\n\t\tMinVersion:               tls.VersionTLS12,\r\n\t\tCipherSuites:             tc.Ciphers,\r\n\t\tPreferServerCipherSuites: true,\r\n\t\tCurvePreferences:         tc.CurvePreferences,\r\n\t\tInsecureSkipVerify:       tc.Insecure,\r\n\t}\r\n\r\n\tswitch {\r\n\tcase tc.CertFile != \"\" && tc.KeyFile == \"\":\r\n\t\treturn nil, fmt.Errorf(\"missing 'key_file' in TLS configuration\")\r\n\tcase tc.CertFile == \"\" && tc.KeyFile != \"\":\r\n\t\treturn nil, fmt.Errorf(\"missing 'cert_file' in TLS configuration\")\r\n\tcase tc.CertFile != \"\" && tc.KeyFile != \"\":\r\n\t\t// Now load in cert and private key\r\n\t\tcert, err := tls.LoadX509KeyPair(tc.CertFile, tc.KeyFile)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"error parsing X509 certificate/key pair: %v\", err)\r\n\t\t}\r\n\t\tcert.Leaf, err = x509.ParseCertificate(cert.Certificate[0])\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"error parsing certificate: %v\", err)\r\n\t\t}\r\n\t\tconfig.Certificates = []tls.Certificate{cert}\r\n\t}\r\n\r\n\t// Require client certificates as needed\r\n\tif tc.Verify {\r\n\t\tconfig.ClientAuth = tls.RequireAndVerifyClientCert\r\n\t}\r\n\t// Add in CAs if applicable.\r\n\tif tc.CaFile != \"\" {\r\n\t\trootPEM, err := ioutil.ReadFile(tc.CaFile)\r\n\t\tif err != nil || rootPEM == nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tpool := x509.NewCertPool()\r\n\t\tok := pool.AppendCertsFromPEM(rootPEM)\r\n\t\tif !ok {\r\n\t\t\treturn nil, fmt.Errorf(\"failed to parse root ca certificate\")\r\n\t\t}\r\n\t\tconfig.ClientCAs = pool\r\n\t}\r\n\r\n\treturn &config, nil\r\n}\r\n\r\n// MergeOptions will merge two options giving preference to the flagOpts\r\n// if the item is present.\r\nfunc MergeOptions(fileOpts, flagOpts *Options) *Options {\r\n\tif fileOpts == nil {\r\n\t\treturn flagOpts\r\n\t}\r\n\tif flagOpts == nil {\r\n\t\treturn fileOpts\r\n\t}\r\n\t// Merge the two, flagOpts override\r\n\topts := *fileOpts\r\n\r\n\tif flagOpts.Port != 0 {\r\n\t\topts.Port = flagOpts.Port\r\n\t}\r\n\tif flagOpts.Host != \"\" {\r\n\t\topts.Host = flagOpts.Host\r\n\t}\r\n\tif flagOpts.ClientAdvertise != \"\" {\r\n\t\topts.ClientAdvertise = flagOpts.ClientAdvertise\r\n\t}\r\n\tif flagOpts.Username != \"\" {\r\n\t\topts.Username = flagOpts.Username\r\n\t}\r\n\tif flagOpts.Password != \"\" {\r\n\t\topts.Password = flagOpts.Password\r\n\t}\r\n\tif flagOpts.Authorization != \"\" {\r\n\t\topts.Authorization = flagOpts.Authorization\r\n\t}\r\n\tif flagOpts.HTTPPort != 0 {\r\n\t\topts.HTTPPort = flagOpts.HTTPPort\r\n\t}\r\n\tif flagOpts.HTTPBasePath != \"\" {\r\n\t\topts.HTTPBasePath = flagOpts.HTTPBasePath\r\n\t}\r\n\tif flagOpts.Debug {\r\n\t\topts.Debug = true\r\n\t}\r\n\tif flagOpts.Trace {\r\n\t\topts.Trace = true\r\n\t}\r\n\tif flagOpts.Logtime {\r\n\t\topts.Logtime = true\r\n\t}\r\n\tif flagOpts.LogFile != \"\" {\r\n\t\topts.LogFile = flagOpts.LogFile\r\n\t}\r\n\tif flagOpts.PidFile != \"\" {\r\n\t\topts.PidFile = flagOpts.PidFile\r\n\t}\r\n\tif flagOpts.PortsFileDir != \"\" {\r\n\t\topts.PortsFileDir = flagOpts.PortsFileDir\r\n\t}\r\n\tif flagOpts.ProfPort != 0 {\r\n\t\topts.ProfPort = flagOpts.ProfPort\r\n\t}\r\n\tif flagOpts.Cluster.ListenStr != \"\" {\r\n\t\topts.Cluster.ListenStr = flagOpts.Cluster.ListenStr\r\n\t}\r\n\tif flagOpts.Cluster.NoAdvertise {\r\n\t\topts.Cluster.NoAdvertise = true\r\n\t}\r\n\tif flagOpts.Cluster.ConnectRetries != 0 {\r\n\t\topts.Cluster.ConnectRetries = flagOpts.Cluster.ConnectRetries\r\n\t}\r\n\tif flagOpts.Cluster.Advertise != \"\" {\r\n\t\topts.Cluster.Advertise = flagOpts.Cluster.Advertise\r\n\t}\r\n\tif flagOpts.RoutesStr != \"\" {\r\n\t\tmergeRoutes(&opts, flagOpts)\r\n\t}\r\n\treturn &opts\r\n}\r\n\r\n// RoutesFromStr parses route URLs from a string\r\nfunc RoutesFromStr(routesStr string) []*url.URL {\r\n\troutes := strings.Split(routesStr, \",\")\r\n\tif len(routes) == 0 {\r\n\t\treturn nil\r\n\t}\r\n\trouteUrls := []*url.URL{}\r\n\tfor _, r := range routes {\r\n\t\tr = strings.TrimSpace(r)\r\n\t\tu, _ := url.Parse(r)\r\n\t\trouteUrls = append(routeUrls, u)\r\n\t}\r\n\treturn routeUrls\r\n}\r\n\r\n// This will merge the flag routes and override anything that was present.\r\nfunc mergeRoutes(opts, flagOpts *Options) {\r\n\trouteUrls := RoutesFromStr(flagOpts.RoutesStr)\r\n\tif routeUrls == nil {\r\n\t\treturn\r\n\t}\r\n\topts.Routes = routeUrls\r\n\topts.RoutesStr = flagOpts.RoutesStr\r\n}\r\n\r\n// RemoveSelfReference removes this server from an array of routes\r\nfunc RemoveSelfReference(clusterPort int, routes []*url.URL) ([]*url.URL, error) {\r\n\tvar cleanRoutes []*url.URL\r\n\tcport := strconv.Itoa(clusterPort)\r\n\r\n\tselfIPs, err := getInterfaceIPs()\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tfor _, r := range routes {\r\n\t\thost, port, err := net.SplitHostPort(r.Host)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\r\n\t\tipList, err := getURLIP(host)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tif cport == port && isIPInList(selfIPs, ipList) {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tcleanRoutes = append(cleanRoutes, r)\r\n\t}\r\n\r\n\treturn cleanRoutes, nil\r\n}\r\n\r\nfunc isIPInList(list1 []net.IP, list2 []net.IP) bool {\r\n\tfor _, ip1 := range list1 {\r\n\t\tfor _, ip2 := range list2 {\r\n\t\t\tif ip1.Equal(ip2) {\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\nfunc getURLIP(ipStr string) ([]net.IP, error) {\r\n\tipList := []net.IP{}\r\n\r\n\tip := net.ParseIP(ipStr)\r\n\tif ip != nil {\r\n\t\tipList = append(ipList, ip)\r\n\t\treturn ipList, nil\r\n\t}\r\n\r\n\thostAddr, err := net.LookupHost(ipStr)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"Error looking up host with route hostname: %v\", err)\r\n\t}\r\n\tfor _, addr := range hostAddr {\r\n\t\tip = net.ParseIP(addr)\r\n\t\tif ip != nil {\r\n\t\t\tipList = append(ipList, ip)\r\n\t\t}\r\n\t}\r\n\treturn ipList, nil\r\n}\r\n\r\nfunc getInterfaceIPs() ([]net.IP, error) {\r\n\tvar localIPs []net.IP\r\n\r\n\tinterfaceAddr, err := net.InterfaceAddrs()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"Error getting self referencing address: %v\", err)\r\n\t}\r\n\r\n\tfor i := 0; i < len(interfaceAddr); i++ {\r\n\t\tinterfaceIP, _, _ := net.ParseCIDR(interfaceAddr[i].String())\r\n\t\tif net.ParseIP(interfaceIP.String()) != nil {\r\n\t\t\tlocalIPs = append(localIPs, interfaceIP)\r\n\t\t} else {\r\n\t\t\treturn nil, fmt.Errorf(\"Error parsing self referencing address: %v\", err)\r\n\t\t}\r\n\t}\r\n\treturn localIPs, nil\r\n}\r\n\r\nfunc setBaselineOptions(opts *Options) {\r\n\t// Setup non-standard Go defaults\r\n\tif opts.Host == \"\" {\r\n\t\topts.Host = DEFAULT_HOST\r\n\t}\r\n\tif opts.HTTPHost == \"\" {\r\n\t\t// Default to same bind from server if left undefined\r\n\t\topts.HTTPHost = opts.Host\r\n\t}\r\n\tif opts.Port == 0 {\r\n\t\topts.Port = DEFAULT_PORT\r\n\t} else if opts.Port == RANDOM_PORT {\r\n\t\t// Choose randomly inside of net.Listen\r\n\t\topts.Port = 0\r\n\t}\r\n\tif opts.MaxConn == 0 {\r\n\t\topts.MaxConn = DEFAULT_MAX_CONNECTIONS\r\n\t}\r\n\tif opts.PingInterval == 0 {\r\n\t\topts.PingInterval = DEFAULT_PING_INTERVAL\r\n\t}\r\n\tif opts.MaxPingsOut == 0 {\r\n\t\topts.MaxPingsOut = DEFAULT_PING_MAX_OUT\r\n\t}\r\n\tif opts.TLSTimeout == 0 {\r\n\t\topts.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\r\n\t}\r\n\tif opts.AuthTimeout == 0 {\r\n\t\topts.AuthTimeout = float64(AUTH_TIMEOUT) / float64(time.Second)\r\n\t}\r\n\tif opts.Cluster.Port != 0 {\r\n\t\tif opts.Cluster.Host == \"\" {\r\n\t\t\topts.Cluster.Host = DEFAULT_HOST\r\n\t\t}\r\n\t\tif opts.Cluster.TLSTimeout == 0 {\r\n\t\t\topts.Cluster.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\r\n\t\t}\r\n\t\tif opts.Cluster.AuthTimeout == 0 {\r\n\t\t\topts.Cluster.AuthTimeout = float64(AUTH_TIMEOUT) / float64(time.Second)\r\n\t\t}\r\n\t}\r\n\tif opts.LeafNode.Port != 0 {\r\n\t\tif opts.LeafNode.Host == \"\" {\r\n\t\t\topts.LeafNode.Host = DEFAULT_HOST\r\n\t\t}\r\n\t\tif opts.LeafNode.TLSTimeout == 0 {\r\n\t\t\topts.LeafNode.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\r\n\t\t}\r\n\t\tif opts.LeafNode.AuthTimeout == 0 {\r\n\t\t\topts.LeafNode.AuthTimeout = float64(AUTH_TIMEOUT) / float64(time.Second)\r\n\t\t}\r\n\t}\r\n\t// Set baseline connect port for remotes.\r\n\tfor _, r := range opts.LeafNode.Remotes {\r\n\t\tif r != nil {\r\n\t\t\tfor _, u := range r.URLs {\r\n\t\t\t\tif u.Port() == \"\" {\r\n\t\t\t\t\tu.Host = net.JoinHostPort(u.Host, strconv.Itoa(DEFAULT_LEAFNODE_PORT))\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Set this regardless of opts.LeafNode.Port\r\n\tif opts.LeafNode.ReconnectInterval == 0 {\r\n\t\topts.LeafNode.ReconnectInterval = DEFAULT_LEAF_NODE_RECONNECT\r\n\t}\r\n\r\n\tif opts.MaxControlLine == 0 {\r\n\t\topts.MaxControlLine = MAX_CONTROL_LINE_SIZE\r\n\t}\r\n\tif opts.MaxPayload == 0 {\r\n\t\topts.MaxPayload = MAX_PAYLOAD_SIZE\r\n\t}\r\n\tif opts.MaxPending == 0 {\r\n\t\topts.MaxPending = MAX_PENDING_SIZE\r\n\t}\r\n\tif opts.WriteDeadline == time.Duration(0) {\r\n\t\topts.WriteDeadline = DEFAULT_FLUSH_DEADLINE\r\n\t}\r\n\tif opts.MaxClosedClients == 0 {\r\n\t\topts.MaxClosedClients = DEFAULT_MAX_CLOSED_CLIENTS\r\n\t}\r\n\tif opts.LameDuckDuration == 0 {\r\n\t\topts.LameDuckDuration = DEFAULT_LAME_DUCK_DURATION\r\n\t}\r\n\tif opts.Gateway.Port != 0 {\r\n\t\tif opts.Gateway.Host == \"\" {\r\n\t\t\topts.Gateway.Host = DEFAULT_HOST\r\n\t\t}\r\n\t\tif opts.Gateway.TLSTimeout == 0 {\r\n\t\t\topts.Gateway.TLSTimeout = float64(TLS_TIMEOUT) / float64(time.Second)\r\n\t\t}\r\n\t\tif opts.Gateway.AuthTimeout == 0 {\r\n\t\t\topts.Gateway.AuthTimeout = float64(AUTH_TIMEOUT) / float64(time.Second)\r\n\t\t}\r\n\t}\r\n\tif opts.ConnectErrorReports == 0 {\r\n\t\topts.ConnectErrorReports = DEFAULT_CONNECT_ERROR_REPORTS\r\n\t}\r\n\tif opts.ReconnectErrorReports == 0 {\r\n\t\topts.ReconnectErrorReports = DEFAULT_RECONNECT_ERROR_REPORTS\r\n\t}\r\n}\r\n\r\n// ConfigureOptions accepts a flag set and augment it with NATS Server\r\n// specific flags. On success, an options structure is returned configured\r\n// based on the selected flags and/or configuration file.\r\n// The command line options take precedence to the ones in the configuration file.\r\nfunc ConfigureOptions(fs *flag.FlagSet, args []string, printVersion, printHelp, printTLSHelp func()) (*Options, error) {\r\n\topts := &Options{}\r\n\tvar (\r\n\t\tshowVersion            bool\r\n\t\tshowHelp               bool\r\n\t\tshowTLSHelp            bool\r\n\t\tsignal                 string\r\n\t\tconfigFile             string\r\n\t\tdbgAndTrace            bool\r\n\t\ttrcAndVerboseTrc       bool\r\n\t\tdbgAndTrcAndVerboseTrc bool\r\n\t\terr                    error\r\n\t)\r\n\r\n\tfs.BoolVar(&showHelp, \"h\", false, \"Show this message.\")\r\n\tfs.BoolVar(&showHelp, \"help\", false, \"Show this message.\")\r\n\tfs.IntVar(&opts.Port, \"port\", 0, \"Port to listen on.\")\r\n\tfs.IntVar(&opts.Port, \"p\", 0, \"Port to listen on.\")\r\n\tfs.StringVar(&opts.Host, \"addr\", \"\", \"Network host to listen on.\")\r\n\tfs.StringVar(&opts.Host, \"a\", \"\", \"Network host to listen on.\")\r\n\tfs.StringVar(&opts.Host, \"net\", \"\", \"Network host to listen on.\")\r\n\tfs.StringVar(&opts.ClientAdvertise, \"client_advertise\", \"\", \"Client URL to advertise to other servers.\")\r\n\tfs.BoolVar(&opts.Debug, \"D\", false, \"Enable Debug logging.\")\r\n\tfs.BoolVar(&opts.Debug, \"debug\", false, \"Enable Debug logging.\")\r\n\tfs.BoolVar(&opts.Trace, \"V\", false, \"Enable Trace logging.\")\r\n\tfs.BoolVar(&trcAndVerboseTrc, \"VV\", false, \"Enable Verbose Trace logging. (Traces system account as well)\")\r\n\tfs.BoolVar(&opts.Trace, \"trace\", false, \"Enable Trace logging.\")\r\n\tfs.BoolVar(&dbgAndTrace, \"DV\", false, \"Enable Debug and Trace logging.\")\r\n\tfs.BoolVar(&dbgAndTrcAndVerboseTrc, \"DVV\", false, \"Enable Debug and Verbose Trace logging. (Traces system account as well)\")\r\n\tfs.BoolVar(&opts.Logtime, \"T\", true, \"Timestamp log entries.\")\r\n\tfs.BoolVar(&opts.Logtime, \"logtime\", true, \"Timestamp log entries.\")\r\n\tfs.StringVar(&opts.Username, \"user\", \"\", \"Username required for connection.\")\r\n\tfs.StringVar(&opts.Password, \"pass\", \"\", \"Password required for connection.\")\r\n\tfs.StringVar(&opts.Authorization, \"auth\", \"\", \"Authorization token required for connection.\")\r\n\tfs.IntVar(&opts.HTTPPort, \"m\", 0, \"HTTP Port for /varz, /connz endpoints.\")\r\n\tfs.IntVar(&opts.HTTPPort, \"http_port\", 0, \"HTTP Port for /varz, /connz endpoints.\")\r\n\tfs.IntVar(&opts.HTTPSPort, \"ms\", 0, \"HTTPS Port for /varz, /connz endpoints.\")\r\n\tfs.IntVar(&opts.HTTPSPort, \"https_port\", 0, \"HTTPS Port for /varz, /connz endpoints.\")\r\n\tfs.StringVar(&configFile, \"c\", \"\", \"Configuration file.\")\r\n\tfs.StringVar(&configFile, \"config\", \"\", \"Configuration file.\")\r\n\tfs.BoolVar(&opts.CheckConfig, \"t\", false, \"Check configuration and exit.\")\r\n\tfs.StringVar(&signal, \"sl\", \"\", \"Send signal to nats-server process (stop, quit, reopen, reload)\")\r\n\tfs.StringVar(&signal, \"signal\", \"\", \"Send signal to nats-server process (stop, quit, reopen, reload)\")\r\n\tfs.StringVar(&opts.PidFile, \"P\", \"\", \"File to store process pid.\")\r\n\tfs.StringVar(&opts.PidFile, \"pid\", \"\", \"File to store process pid.\")\r\n\tfs.StringVar(&opts.PortsFileDir, \"ports_file_dir\", \"\", \"Creates a ports file in the specified directory (<executable_name>_<pid>.ports)\")\r\n\tfs.StringVar(&opts.LogFile, \"l\", \"\", \"File to store logging output.\")\r\n\tfs.StringVar(&opts.LogFile, \"log\", \"\", \"File to store logging output.\")\r\n\tfs.Int64Var(&opts.LogSizeLimit, \"log_size_limit\", 0, \"Logfile size limit being auto-rotated\")\r\n\tfs.BoolVar(&opts.Syslog, \"s\", false, \"Enable syslog as log method.\")\r\n\tfs.BoolVar(&opts.Syslog, \"syslog\", false, \"Enable syslog as log method..\")\r\n\tfs.StringVar(&opts.RemoteSyslog, \"r\", \"\", \"Syslog server addr (udp://127.0.0.1:514).\")\r\n\tfs.StringVar(&opts.RemoteSyslog, \"remote_syslog\", \"\", \"Syslog server addr (udp://127.0.0.1:514).\")\r\n\tfs.BoolVar(&showVersion, \"version\", false, \"Print version information.\")\r\n\tfs.BoolVar(&showVersion, \"v\", false, \"Print version information.\")\r\n\tfs.IntVar(&opts.ProfPort, \"profile\", 0, \"Profiling HTTP port\")\r\n\tfs.StringVar(&opts.RoutesStr, \"routes\", \"\", \"Routes to actively solicit a connection.\")\r\n\tfs.StringVar(&opts.Cluster.ListenStr, \"cluster\", \"\", \"Cluster url from which members can solicit routes.\")\r\n\tfs.StringVar(&opts.Cluster.ListenStr, \"cluster_listen\", \"\", \"Cluster url from which members can solicit routes.\")\r\n\tfs.StringVar(&opts.Cluster.Advertise, \"cluster_advertise\", \"\", \"Cluster URL to advertise to other servers.\")\r\n\tfs.BoolVar(&opts.Cluster.NoAdvertise, \"no_advertise\", false, \"Advertise known cluster IPs to clients.\")\r\n\tfs.IntVar(&opts.Cluster.ConnectRetries, \"connect_retries\", 0, \"For implicit routes, number of connect retries\")\r\n\tfs.BoolVar(&showTLSHelp, \"help_tls\", false, \"TLS help.\")\r\n\tfs.BoolVar(&opts.TLS, \"tls\", false, \"Enable TLS.\")\r\n\tfs.BoolVar(&opts.TLSVerify, \"tlsverify\", false, \"Enable TLS with client verification.\")\r\n\tfs.StringVar(&opts.TLSCert, \"tlscert\", \"\", \"Server certificate file.\")\r\n\tfs.StringVar(&opts.TLSKey, \"tlskey\", \"\", \"Private key for server certificate.\")\r\n\tfs.StringVar(&opts.TLSCaCert, \"tlscacert\", \"\", \"Client certificate CA for verification.\")\r\n\tfs.IntVar(&opts.MaxTracedMsgLen, \"max_traced_msg_len\", 0, \"Maximum printable length for traced messages. 0 for unlimited\")\r\n\r\n\t// The flags definition above set \"default\" values to some of the options.\r\n\t// Calling Parse() here will override the default options with any value\r\n\t// specified from the command line. This is ok. We will then update the\r\n\t// options with the content of the configuration file (if present), and then,\r\n\t// call Parse() again to override the default+config with command line values.\r\n\t// Calling Parse() before processing config file is necessary since configFile\r\n\t// itself is a command line argument, and also Parse() is required in order\r\n\t// to know if user wants simply to show \"help\" or \"version\", etc...\r\n\tif err := fs.Parse(args); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tif showVersion {\r\n\t\tprintVersion()\r\n\t\treturn nil, nil\r\n\t}\r\n\r\n\tif showHelp {\r\n\t\tprintHelp()\r\n\t\treturn nil, nil\r\n\t}\r\n\r\n\tif showTLSHelp {\r\n\t\tprintTLSHelp()\r\n\t\treturn nil, nil\r\n\t}\r\n\r\n\t// Process args looking for non-flag options,\r\n\t// 'version' and 'help' only for now\r\n\tshowVersion, showHelp, err = ProcessCommandLineArgs(fs)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t} else if showVersion {\r\n\t\tprintVersion()\r\n\t\treturn nil, nil\r\n\t} else if showHelp {\r\n\t\tprintHelp()\r\n\t\treturn nil, nil\r\n\t}\r\n\r\n\t// Snapshot flag options.\r\n\tFlagSnapshot = opts.Clone()\r\n\r\n\t// Keep track of the boolean flags that were explicitly set with their value.\r\n\tfs.Visit(func(f *flag.Flag) {\r\n\t\tswitch f.Name {\r\n\t\tcase \"DVV\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Debug\", dbgAndTrcAndVerboseTrc)\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Trace\", dbgAndTrcAndVerboseTrc)\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"TraceVerbose\", dbgAndTrcAndVerboseTrc)\r\n\t\tcase \"DV\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Debug\", dbgAndTrace)\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Trace\", dbgAndTrace)\r\n\t\tcase \"D\":\r\n\t\t\tfallthrough\r\n\t\tcase \"debug\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Debug\", FlagSnapshot.Debug)\r\n\t\tcase \"VV\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Trace\", trcAndVerboseTrc)\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"TraceVerbose\", trcAndVerboseTrc)\r\n\t\tcase \"V\":\r\n\t\t\tfallthrough\r\n\t\tcase \"trace\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Trace\", FlagSnapshot.Trace)\r\n\t\tcase \"T\":\r\n\t\t\tfallthrough\r\n\t\tcase \"logtime\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Logtime\", FlagSnapshot.Logtime)\r\n\t\tcase \"s\":\r\n\t\t\tfallthrough\r\n\t\tcase \"syslog\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Syslog\", FlagSnapshot.Syslog)\r\n\t\tcase \"no_advertise\":\r\n\t\t\ttrackExplicitVal(FlagSnapshot, &FlagSnapshot.inCmdLine, \"Cluster.NoAdvertise\", FlagSnapshot.Cluster.NoAdvertise)\r\n\t\t}\r\n\t})\r\n\r\n\t// Process signal control.\r\n\tif signal != \"\" {\r\n\t\tif err := processSignal(signal); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t}\r\n\r\n\t// Parse config if given\r\n\tif configFile != \"\" {\r\n\t\t// This will update the options with values from the config file.\r\n\t\terr := opts.ProcessConfigFile(configFile)\r\n\t\tif err != nil {\r\n\t\t\tif opts.CheckConfig {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\tif cerr, ok := err.(*processConfigErr); !ok || len(cerr.Errors()) != 0 {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\t// If we get here we only have warnings and can still continue\r\n\t\t\tfmt.Fprint(os.Stderr, err)\r\n\t\t} else if opts.CheckConfig {\r\n\t\t\t// Report configuration file syntax test was successful and exit.\r\n\t\t\treturn opts, nil\r\n\t\t}\r\n\r\n\t\t// Call this again to override config file options with options from command line.\r\n\t\t// Note: We don't need to check error here since if there was an error, it would\r\n\t\t// have been caught the first time this function was called (after setting up the\r\n\t\t// flags).\r\n\t\tfs.Parse(args)\r\n\t} else if opts.CheckConfig {\r\n\t\treturn nil, fmt.Errorf(\"must specify [-c, --config] option to check configuration file syntax\")\r\n\t}\r\n\r\n\t// Special handling of some flags\r\n\tvar (\r\n\t\tflagErr     error\r\n\t\ttlsDisabled bool\r\n\t\ttlsOverride bool\r\n\t)\r\n\tfs.Visit(func(f *flag.Flag) {\r\n\t\t// short-circuit if an error was encountered\r\n\t\tif flagErr != nil {\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif strings.HasPrefix(f.Name, \"tls\") {\r\n\t\t\tif f.Name == \"tls\" {\r\n\t\t\t\tif !opts.TLS {\r\n\t\t\t\t\t// User has specified \"-tls=false\", we need to disable TLS\r\n\t\t\t\t\topts.TLSConfig = nil\r\n\t\t\t\t\ttlsDisabled = true\r\n\t\t\t\t\ttlsOverride = false\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\ttlsOverride = true\r\n\t\t\t} else if !tlsDisabled {\r\n\t\t\t\ttlsOverride = true\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tswitch f.Name {\r\n\t\t\tcase \"VV\":\r\n\t\t\t\topts.Trace, opts.TraceVerbose = trcAndVerboseTrc, trcAndVerboseTrc\r\n\t\t\tcase \"DVV\":\r\n\t\t\t\topts.Trace, opts.Debug, opts.TraceVerbose = dbgAndTrcAndVerboseTrc, dbgAndTrcAndVerboseTrc, dbgAndTrcAndVerboseTrc\r\n\t\t\tcase \"DV\":\r\n\t\t\t\t// Check value to support -DV=false\r\n\t\t\t\topts.Trace, opts.Debug = dbgAndTrace, dbgAndTrace\r\n\t\t\tcase \"cluster\", \"cluster_listen\":\r\n\t\t\t\t// Override cluster config if explicitly set via flags.\r\n\t\t\t\tflagErr = overrideCluster(opts)\r\n\t\t\tcase \"routes\":\r\n\t\t\t\t// Keep in mind that the flag has updated opts.RoutesStr at this point.\r\n\t\t\t\tif opts.RoutesStr == \"\" {\r\n\t\t\t\t\t// Set routes array to nil since routes string is empty\r\n\t\t\t\t\topts.Routes = nil\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\trouteUrls := RoutesFromStr(opts.RoutesStr)\r\n\t\t\t\topts.Routes = routeUrls\r\n\t\t\t}\r\n\t\t}\r\n\t})\r\n\tif flagErr != nil {\r\n\t\treturn nil, flagErr\r\n\t}\r\n\r\n\t// This will be true if some of the `-tls` params have been set and\r\n\t// `-tls=false` has not been set.\r\n\tif tlsOverride {\r\n\t\tif err := overrideTLS(opts); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t}\r\n\r\n\t// If we don't have cluster defined in the configuration\r\n\t// file and no cluster listen string override, but we do\r\n\t// have a routes override, we need to report misconfiguration.\r\n\tif opts.RoutesStr != \"\" && opts.Cluster.ListenStr == \"\" && opts.Cluster.Host == \"\" && opts.Cluster.Port == 0 {\r\n\t\treturn nil, errors.New(\"solicited routes require cluster capabilities, e.g. --cluster\")\r\n\t}\r\n\r\n\treturn opts, nil\r\n}\r\n\r\nfunc normalizeBasePath(p string) string {\r\n\tif len(p) == 0 {\r\n\t\treturn \"/\"\r\n\t}\r\n\t// add leading slash\r\n\tif p[0] != '/' {\r\n\t\tp = \"/\" + p\r\n\t}\r\n\treturn path.Clean(p)\r\n}\r\n\r\n// overrideTLS is called when at least \"-tls=true\" has been set.\r\nfunc overrideTLS(opts *Options) error {\r\n\tif opts.TLSCert == \"\" {\r\n\t\treturn errors.New(\"TLS Server certificate must be present and valid\")\r\n\t}\r\n\tif opts.TLSKey == \"\" {\r\n\t\treturn errors.New(\"TLS Server private key must be present and valid\")\r\n\t}\r\n\r\n\ttc := TLSConfigOpts{}\r\n\ttc.CertFile = opts.TLSCert\r\n\ttc.KeyFile = opts.TLSKey\r\n\ttc.CaFile = opts.TLSCaCert\r\n\ttc.Verify = opts.TLSVerify\r\n\r\n\tvar err error\r\n\topts.TLSConfig, err = GenTLSConfig(&tc)\r\n\treturn err\r\n}\r\n\r\n// overrideCluster updates Options.Cluster if that flag \"cluster\" (or \"cluster_listen\")\r\n// has explicitly be set in the command line. If it is set to empty string, it will\r\n// clear the Cluster options.\r\nfunc overrideCluster(opts *Options) error {\r\n\tif opts.Cluster.ListenStr == \"\" {\r\n\t\t// This one is enough to disable clustering.\r\n\t\topts.Cluster.Port = 0\r\n\t\treturn nil\r\n\t}\r\n\t// -1 will fail url.Parse, so if we have -1, change it to\r\n\t// 0, and then after parse, replace the port with -1 so we get\r\n\t// automatic port allocation\r\n\twantsRandom := false\r\n\tif strings.HasSuffix(opts.Cluster.ListenStr, \":-1\") {\r\n\t\twantsRandom = true\r\n\t\tcls := fmt.Sprintf(\"%s:0\", opts.Cluster.ListenStr[0:len(opts.Cluster.ListenStr)-3])\r\n\t\topts.Cluster.ListenStr = cls\r\n\t}\r\n\tclusterURL, err := url.Parse(opts.Cluster.ListenStr)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\th, p, err := net.SplitHostPort(clusterURL.Host)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\tif wantsRandom {\r\n\t\tp = \"-1\"\r\n\t}\r\n\topts.Cluster.Host = h\r\n\t_, err = fmt.Sscan(p, &opts.Cluster.Port)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif clusterURL.User != nil {\r\n\t\tpass, hasPassword := clusterURL.User.Password()\r\n\t\tif !hasPassword {\r\n\t\t\treturn errors.New(\"expected cluster password to be set\")\r\n\t\t}\r\n\t\topts.Cluster.Password = pass\r\n\r\n\t\tuser := clusterURL.User.Username()\r\n\t\topts.Cluster.Username = user\r\n\t} else {\r\n\t\t// Since we override from flag and there is no user/pwd, make\r\n\t\t// sure we clear what we may have gotten from config file.\r\n\t\topts.Cluster.Username = \"\"\r\n\t\topts.Cluster.Password = \"\"\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc processSignal(signal string) error {\r\n\tvar (\r\n\t\tpid           string\r\n\t\tcommandAndPid = strings.Split(signal, \"=\")\r\n\t)\r\n\tif l := len(commandAndPid); l == 2 {\r\n\t\tpid = maybeReadPidFile(commandAndPid[1])\r\n\t} else if l > 2 {\r\n\t\treturn fmt.Errorf(\"invalid signal parameters: %v\", commandAndPid[2:])\r\n\t}\r\n\tif err := ProcessSignal(Command(commandAndPid[0]), pid); err != nil {\r\n\t\treturn err\r\n\t}\r\n\tos.Exit(0)\r\n\treturn nil\r\n}\r\n\r\n// maybeReadPidFile returns a PID or Windows service name obtained via the following method:\r\n// 1. Try to open a file with path \"pidStr\" (absolute or relative).\r\n// 2. If such a file exists and can be read, return its contents.\r\n// 3. Otherwise, return the original \"pidStr\" string.\r\nfunc maybeReadPidFile(pidStr string) string {\r\n\tif b, err := ioutil.ReadFile(pidStr); err == nil {\r\n\t\treturn string(b)\r\n\t}\r\n\treturn pidStr\r\n}\r\n\r\nfunc homeDir() (string, error) {\r\n\tif runtime.GOOS == \"windows\" {\r\n\t\thomeDrive, homePath := os.Getenv(\"HOMEDRIVE\"), os.Getenv(\"HOMEPATH\")\r\n\t\tuserProfile := os.Getenv(\"USERPROFILE\")\r\n\r\n\t\thome := filepath.Join(homeDrive, homePath)\r\n\t\tif homeDrive == \"\" || homePath == \"\" {\r\n\t\t\tif userProfile == \"\" {\r\n\t\t\t\treturn \"\", errors.New(\"nats: failed to get home dir, require %HOMEDRIVE% and %HOMEPATH% or %USERPROFILE%\")\r\n\t\t\t}\r\n\t\t\thome = userProfile\r\n\t\t}\r\n\r\n\t\treturn home, nil\r\n\t}\r\n\r\n\thome := os.Getenv(\"HOME\")\r\n\tif home == \"\" {\r\n\t\treturn \"\", errors.New(\"failed to get home dir, require $HOME\")\r\n\t}\r\n\treturn home, nil\r\n}\r\n\r\nfunc expandPath(p string) (string, error) {\r\n\tp = os.ExpandEnv(p)\r\n\r\n\tif !strings.HasPrefix(p, \"~\") {\r\n\t\treturn p, nil\r\n\t}\r\n\r\n\thome, err := homeDir()\r\n\tif err != nil {\r\n\t\treturn \"\", err\r\n\t}\r\n\r\n\treturn filepath.Join(home, p[1:]), nil\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/opts.go b/server/gnatsd/server/opts.go
--- a/server/gnatsd/server/opts.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/opts.go	(date 1665399058022)
@@ -33,8 +33,8 @@
 	"sync/atomic"
 	"time"
 
-	"github.com/nats-io/jwt"
 	"github.com/kubemq-io/broker/server/gnatsd/conf"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
@@ -1884,10 +1884,11 @@
 
 // Parse an export stream or service.
 // e.g.
-//   {stream: "public.>"} # No accounts means public.
-//   {stream: "synadia.private.>", accounts: [cncf, natsio]}
-//   {service: "pub.request"} # No accounts means public.
-//   {service: "pub.special.request", accounts: [nats.io]}
+//
+//	{stream: "public.>"} # No accounts means public.
+//	{stream: "synadia.private.>", accounts: [cncf, natsio]}
+//	{service: "pub.request"} # No accounts means public.
+//	{service: "pub.special.request", accounts: [nats.io]}
 func parseExportStreamOrService(v interface{}, errors, warnings *[]error) (*export, *export, error) {
 	var (
 		curStream  *export
@@ -2099,9 +2100,10 @@
 
 // Parse an import stream or service.
 // e.g.
-//   {stream: {account: "synadia", subject:"public.synadia"}, prefix: "imports.synadia"}
-//   {stream: {account: "synadia", subject:"synadia.private.*"}}
-//   {service: {account: "synadia", subject: "pub.special.request"}, to: "synadia.request"}
+//
+//	{stream: {account: "synadia", subject:"public.synadia"}, prefix: "imports.synadia"}
+//	{stream: {account: "synadia", subject:"synadia.private.*"}}
+//	{service: {account: "synadia", subject: "pub.special.request"}, to: "synadia.request"}
 func parseImportStreamOrService(v interface{}, errors, warnings *[]error) (*importStream, *importService, error) {
 	var (
 		curStream  *importStream
Index: server/gnatsd/server/jwt_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2018 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"encoding/base64\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"net/http\"\r\n\t\"net/http/httptest\"\r\n\t\"os\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nvar (\r\n\t// This matches ./configs/nkeys_jwts/test.seed\r\n\toSeed = []byte(\"SOAFYNORQLQFJYBYNUGC5D7SH2MXMUX5BFEWWGHN3EK4VGG5TPT5DZP7QU\")\r\n\t// This matches ./configs/nkeys/op.jwt\r\n\tojwt = \"eyJ0eXAiOiJqd3QiLCJhbGciOiJlZDI1NTE5In0.eyJhdWQiOiJURVNUUyIsImV4cCI6MTg1OTEyMTI3NSwianRpIjoiWE5MWjZYWVBIVE1ESlFSTlFPSFVPSlFHV0NVN01JNVc1SlhDWk5YQllVS0VRVzY3STI1USIsImlhdCI6MTU0Mzc2MTI3NSwiaXNzIjoiT0NBVDMzTVRWVTJWVU9JTUdOR1VOWEo2NkFIMlJMU0RBRjNNVUJDWUFZNVFNSUw2NU5RTTZYUUciLCJuYW1lIjoiU3luYWRpYSBDb21tdW5pY2F0aW9ucyBJbmMuIiwibmJmIjoxNTQzNzYxMjc1LCJzdWIiOiJPQ0FUMzNNVFZVMlZVT0lNR05HVU5YSjY2QUgyUkxTREFGM01VQkNZQVk1UU1JTDY1TlFNNlhRRyIsInR5cGUiOiJvcGVyYXRvciIsIm5hdHMiOnsic2lnbmluZ19rZXlzIjpbIk9EU0tSN01ZRlFaNU1NQUo2RlBNRUVUQ1RFM1JJSE9GTFRZUEpSTUFWVk40T0xWMllZQU1IQ0FDIiwiT0RTS0FDU1JCV1A1MzdEWkRSVko2NTdKT0lHT1BPUTZLRzdUNEhONk9LNEY2SUVDR1hEQUhOUDIiLCJPRFNLSTM2TFpCNDRPWTVJVkNSNlA1MkZaSlpZTVlXWlZXTlVEVExFWjVUSzJQTjNPRU1SVEFCUiJdfX0.hyfz6E39BMUh0GLzovFfk3wT4OfualftjdJ_eYkLfPvu5tZubYQ_Pn9oFYGCV_6yKy3KMGhWGUCyCdHaPhalBw\"\r\n\toKp  nkeys.KeyPair\r\n)\r\n\r\nfunc init() {\r\n\tvar err error\r\n\toKp, err = nkeys.FromSeed(oSeed)\r\n\tif err != nil {\r\n\t\tpanic(fmt.Sprintf(\"Parsing oSeed failed with: %v\", err))\r\n\t}\r\n}\r\n\r\nfunc chanRecv(t *testing.T, recvChan <-chan struct{}, limit time.Duration) {\r\n\tt.Helper()\r\n\tselect {\r\n\tcase <-recvChan:\r\n\tcase <-time.After(limit):\r\n\t\tt.Fatal(\"Should have received from channel\")\r\n\t}\r\n}\r\n\r\nfunc opTrustBasicSetup() *Server {\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\topts := defaultServerOptions\r\n\topts.TrustedKeys = []string{pub}\r\n\ts, c, _, _ := rawSetup(opts)\r\n\tc.close()\r\n\treturn s\r\n}\r\n\r\nfunc buildMemAccResolver(s *Server) {\r\n\tmr := &MemAccResolver{}\r\n\ts.SetAccountResolver(mr)\r\n}\r\n\r\nfunc addAccountToMemResolver(s *Server, pub, jwtclaim string) {\r\n\ts.AccountResolver().Store(pub, jwtclaim)\r\n}\r\n\r\nfunc createClient(t *testing.T, s *Server, akp nkeys.KeyPair) (*testAsyncClient, *bufio.Reader, string) {\r\n\treturn createClientWithIssuer(t, s, akp, \"\")\r\n}\r\n\r\nfunc createClientWithIssuer(t *testing.T, s *Server, akp nkeys.KeyPair, optIssuerAccount string) (*testAsyncClient, *bufio.Reader, string) {\r\n\tt.Helper()\r\n\tnkp, _ := nkeys.CreateUser()\r\n\tpub, _ := nkp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tif optIssuerAccount != \"\" {\r\n\t\tnuc.IssuerAccount = optIssuerAccount\r\n\t}\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tc, cr, l := newClientForServer(s)\r\n\r\n\t// Sign Nonce\r\n\tvar info nonceInfo\r\n\tjson.Unmarshal([]byte(l[5:]), &info)\r\n\tsigraw, _ := nkp.Sign([]byte(info.Nonce))\r\n\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\r\n\r\n\tcs := fmt.Sprintf(\"CONNECT {\\\"jwt\\\":%q,\\\"sig\\\":\\\"%s\\\"}\\r\\nPING\\r\\n\", ujwt, sig)\r\n\treturn c, cr, cs\r\n}\r\n\r\nfunc setupJWTTestWithClaims(t *testing.T, nac *jwt.AccountClaims, nuc *jwt.UserClaims, expected string) (*Server, nkeys.KeyPair, *testAsyncClient, *bufio.Reader) {\r\n\tt.Helper()\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tif nac == nil {\r\n\t\tnac = jwt.NewAccountClaims(apub)\r\n\t} else {\r\n\t\tnac.Subject = apub\r\n\t}\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tnkp, _ := nkeys.CreateUser()\r\n\tpub, _ := nkp.PublicKey()\r\n\tif nuc == nil {\r\n\t\tnuc = jwt.NewUserClaims(pub)\r\n\t} else {\r\n\t\tnuc.Subject = pub\r\n\t}\r\n\tjwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\r\n\ts := opTrustBasicSetup()\r\n\tbuildMemAccResolver(s)\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\r\n\tc, cr, l := newClientForServer(s)\r\n\r\n\t// Sign Nonce\r\n\tvar info nonceInfo\r\n\tjson.Unmarshal([]byte(l[5:]), &info)\r\n\tsigraw, _ := nkp.Sign([]byte(info.Nonce))\r\n\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\r\n\r\n\t// PING needed to flush the +OK/-ERR to us.\r\n\tcs := fmt.Sprintf(\"CONNECT {\\\"jwt\\\":%q,\\\"sig\\\":\\\"%s\\\",\\\"verbose\\\":true,\\\"pedantic\\\":true}\\r\\nPING\\r\\n\", jwt, sig)\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(1)\r\n\tgo func() {\r\n\t\tc.parse([]byte(cs))\r\n\t\twg.Done()\r\n\t}()\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, expected) {\r\n\t\tt.Fatalf(\"Expected %q, got %q\", expected, l)\r\n\t}\r\n\twg.Wait()\r\n\r\n\treturn s, akp, c, cr\r\n}\r\n\r\nfunc setupJWTTestWitAccountClaims(t *testing.T, nac *jwt.AccountClaims, expected string) (*Server, nkeys.KeyPair, *testAsyncClient, *bufio.Reader) {\r\n\tt.Helper()\r\n\treturn setupJWTTestWithClaims(t, nac, nil, expected)\r\n}\r\n\r\n// This is used in test to create account claims and pass it\r\n// to setupJWTTestWitAccountClaims.\r\nfunc newJWTTestAccountClaims() *jwt.AccountClaims {\r\n\t// We call NewAccountClaims() because it sets some defaults.\r\n\t// However, this call needs a subject, but the real subject will\r\n\t// be set in setupJWTTestWitAccountClaims(). Use some temporary one\r\n\t// here.\r\n\treturn jwt.NewAccountClaims(\"temp\")\r\n}\r\n\r\nfunc setupJWTTestWithUserClaims(t *testing.T, nuc *jwt.UserClaims, expected string) (*Server, *testAsyncClient, *bufio.Reader) {\r\n\tt.Helper()\r\n\ts, _, c, cr := setupJWTTestWithClaims(t, nil, nuc, expected)\r\n\treturn s, c, cr\r\n}\r\n\r\n// This is used in test to create user claims and pass it\r\n// to setupJWTTestWithUserClaims.\r\nfunc newJWTTestUserClaims() *jwt.UserClaims {\r\n\t// As of now, tests could simply do &jwt.UserClaims{}, but in\r\n\t// case some defaults are later added, we call NewUserClaims().\r\n\t// However, this call needs a subject, but the real subject will\r\n\t// be set in setupJWTTestWithUserClaims(). Use some temporary one\r\n\t// here.\r\n\treturn jwt.NewUserClaims(\"temp\")\r\n}\r\n\r\nfunc TestJWTUser(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\r\n\t// Check to make sure we would have an authTimer\r\n\tif !s.info.AuthRequired {\r\n\t\tt.Fatalf(\"Expect the server to require auth\")\r\n\t}\r\n\r\n\tc, cr, _ := newClientForServer(s)\r\n\tdefer c.close()\r\n\r\n\t// Don't send jwt field, should fail.\r\n\tc.parseAsync(\"CONNECT {\\\"verbose\\\":true,\\\"pedantic\\\":true}\\r\\nPING\\r\\n\")\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create an account that will be expired.\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tc, cr, cs := createClient(t, s, akp)\r\n\tdefer c.close()\r\n\r\n\t// PING needed to flush the +OK/-ERR to us.\r\n\t// This should fail too since no account resolver is defined.\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\r\n\t// Ok now let's walk through and make sure all is good.\r\n\t// We will set the account resolver by hand to a memory resolver.\r\n\tbuildMemAccResolver(s)\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\r\n\tc, cr, cs = createClient(t, s, akp)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected a PONG, got %q\", l)\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserBadTrusted(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\r\n\t// Check to make sure we would have an authTimer\r\n\tif !s.info.AuthRequired {\r\n\t\tt.Fatalf(\"Expect the server to require auth\")\r\n\t}\r\n\t// Now place bad trusted key\r\n\ts.mu.Lock()\r\n\ts.trustedKeys = []string{\"bad\"}\r\n\ts.mu.Unlock()\r\n\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create an account that will be expired.\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\r\n\tc, cr, cs := createClient(t, s, akp)\r\n\tdefer c.close()\r\n\tc.parseAsync(cs)\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n}\r\n\r\n// Test that if a user tries to connect with an expired user JWT we do the right thing.\r\nfunc TestJWTUserExpired(t *testing.T) {\r\n\tnuc := newJWTTestUserClaims()\r\n\tnuc.IssuedAt = time.Now().Add(-10 * time.Second).Unix()\r\n\tnuc.Expires = time.Now().Add(-2 * time.Second).Unix()\r\n\ts, c, _ := setupJWTTestWithUserClaims(t, nuc, \"-ERR \")\r\n\tc.close()\r\n\ts.Shutdown()\r\n}\r\n\r\nfunc TestJWTUserExpiresAfterConnect(t *testing.T) {\r\n\tnuc := newJWTTestUserClaims()\r\n\tnuc.IssuedAt = time.Now().Unix()\r\n\tnuc.Expires = time.Now().Add(time.Second).Unix()\r\n\ts, c, cr := setupJWTTestWithUserClaims(t, nuc, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected a PONG\")\r\n\t}\r\n\r\n\t// Now we should expire after 1 second or so.\r\n\ttime.Sleep(1250 * time.Millisecond)\r\n\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\tif !strings.Contains(l, \"Expired\") {\r\n\t\tt.Fatalf(\"Expected 'Expired' to be in the error\")\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserPermissionClaims(t *testing.T) {\r\n\tnuc := newJWTTestUserClaims()\r\n\tnuc.Permissions.Pub.Allow.Add(\"foo\")\r\n\tnuc.Permissions.Pub.Allow.Add(\"bar\")\r\n\tnuc.Permissions.Pub.Deny.Add(\"baz\")\r\n\tnuc.Permissions.Sub.Allow.Add(\"foo\")\r\n\tnuc.Permissions.Sub.Allow.Add(\"bar\")\r\n\tnuc.Permissions.Sub.Deny.Add(\"baz\")\r\n\r\n\ts, c, _ := setupJWTTestWithUserClaims(t, nuc, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\t// Now check client to make sure permissions transferred.\r\n\tc.mu.Lock()\r\n\tdefer c.mu.Unlock()\r\n\r\n\tif c.perms == nil {\r\n\t\tt.Fatalf(\"Expected client permissions to be set\")\r\n\t}\r\n\r\n\tif lpa := c.perms.pub.allow.Count(); lpa != 2 {\r\n\t\tt.Fatalf(\"Expected 2 publish allow subjects, got %d\", lpa)\r\n\t}\r\n\tif lpd := c.perms.pub.deny.Count(); lpd != 1 {\r\n\t\tt.Fatalf(\"Expected 1 publish deny subjects, got %d\", lpd)\r\n\t}\r\n\tif lsa := c.perms.sub.allow.Count(); lsa != 2 {\r\n\t\tt.Fatalf(\"Expected 2 subscribe allow subjects, got %d\", lsa)\r\n\t}\r\n\tif lsd := c.perms.sub.deny.Count(); lsd != 1 {\r\n\t\tt.Fatalf(\"Expected 1 subscribe deny subjects, got %d\", lsd)\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserResponsePermissionClaims(t *testing.T) {\r\n\tnuc := newJWTTestUserClaims()\r\n\tnuc.Permissions.Resp = &jwt.ResponsePermission{\r\n\t\tMaxMsgs: 22,\r\n\t\tExpires: 100 * time.Millisecond,\r\n\t}\r\n\ts, c, _ := setupJWTTestWithUserClaims(t, nuc, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\t// Now check client to make sure permissions transferred.\r\n\tc.mu.Lock()\r\n\tdefer c.mu.Unlock()\r\n\r\n\tif c.perms == nil {\r\n\t\tt.Fatalf(\"Expected client permissions to be set\")\r\n\t}\r\n\tif c.perms.pub.allow == nil {\r\n\t\tt.Fatalf(\"Expected client perms for pub allow to be non-nil\")\r\n\t}\r\n\tif lpa := c.perms.pub.allow.Count(); lpa != 0 {\r\n\t\tt.Fatalf(\"Expected 0 publish allow subjects, got %d\", lpa)\r\n\t}\r\n\tif c.perms.resp == nil {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions to be non-nil\")\r\n\t}\r\n\tif c.perms.resp.MaxMsgs != nuc.Permissions.Resp.MaxMsgs {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions MaxMsgs to be same as jwt: %d vs %d\",\r\n\t\t\tc.perms.resp.MaxMsgs, nuc.Permissions.Resp.MaxMsgs)\r\n\t}\r\n\tif c.perms.resp.Expires != nuc.Permissions.Resp.Expires {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions Expires to be same as jwt: %v vs %v\",\r\n\t\t\tc.perms.resp.Expires, nuc.Permissions.Resp.Expires)\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserResponsePermissionClaimsDefaultValues(t *testing.T) {\r\n\tnuc := newJWTTestUserClaims()\r\n\tnuc.Permissions.Resp = &jwt.ResponsePermission{}\r\n\ts, c, _ := setupJWTTestWithUserClaims(t, nuc, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\t// Now check client to make sure permissions transferred\r\n\t// and defaults are set.\r\n\tc.mu.Lock()\r\n\tdefer c.mu.Unlock()\r\n\r\n\tif c.perms == nil {\r\n\t\tt.Fatalf(\"Expected client permissions to be set\")\r\n\t}\r\n\tif c.perms.pub.allow == nil {\r\n\t\tt.Fatalf(\"Expected client perms for pub allow to be non-nil\")\r\n\t}\r\n\tif lpa := c.perms.pub.allow.Count(); lpa != 0 {\r\n\t\tt.Fatalf(\"Expected 0 publish allow subjects, got %d\", lpa)\r\n\t}\r\n\tif c.perms.resp == nil {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions to be non-nil\")\r\n\t}\r\n\tif c.perms.resp.MaxMsgs != DEFAULT_ALLOW_RESPONSE_MAX_MSGS {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions MaxMsgs to be default %v, got %v\",\r\n\t\t\tDEFAULT_ALLOW_RESPONSE_MAX_MSGS, c.perms.resp.MaxMsgs)\r\n\t}\r\n\tif c.perms.resp.Expires != DEFAULT_ALLOW_RESPONSE_EXPIRATION {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions Expires to be default %v, got %v\",\r\n\t\t\tDEFAULT_ALLOW_RESPONSE_EXPIRATION, c.perms.resp.Expires)\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserResponsePermissionClaimsNegativeValues(t *testing.T) {\r\n\tnuc := newJWTTestUserClaims()\r\n\tnuc.Permissions.Resp = &jwt.ResponsePermission{\r\n\t\tMaxMsgs: -1,\r\n\t\tExpires: -1 * time.Second,\r\n\t}\r\n\ts, c, _ := setupJWTTestWithUserClaims(t, nuc, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\t// Now check client to make sure permissions transferred\r\n\t// and negative values are transferred.\r\n\tc.mu.Lock()\r\n\tdefer c.mu.Unlock()\r\n\r\n\tif c.perms == nil {\r\n\t\tt.Fatalf(\"Expected client permissions to be set\")\r\n\t}\r\n\tif c.perms.pub.allow == nil {\r\n\t\tt.Fatalf(\"Expected client perms for pub allow to be non-nil\")\r\n\t}\r\n\tif lpa := c.perms.pub.allow.Count(); lpa != 0 {\r\n\t\tt.Fatalf(\"Expected 0 publish allow subjects, got %d\", lpa)\r\n\t}\r\n\tif c.perms.resp == nil {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions to be non-nil\")\r\n\t}\r\n\tif c.perms.resp.MaxMsgs != -1 {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions MaxMsgs to be %v, got %v\",\r\n\t\t\t-1, c.perms.resp.MaxMsgs)\r\n\t}\r\n\tif c.perms.resp.Expires != -1*time.Second {\r\n\t\tt.Fatalf(\"Expected client perms for response permissions Expires to be %v, got %v\",\r\n\t\t\t-1*time.Second, c.perms.resp.Expires)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountExpired(t *testing.T) {\r\n\tnac := newJWTTestAccountClaims()\r\n\tnac.IssuedAt = time.Now().Add(-10 * time.Second).Unix()\r\n\tnac.Expires = time.Now().Add(-2 * time.Second).Unix()\r\n\ts, _, c, _ := setupJWTTestWitAccountClaims(t, nac, \"-ERR \")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n}\r\n\r\nfunc TestJWTAccountExpiresAfterConnect(t *testing.T) {\r\n\tnac := newJWTTestAccountClaims()\r\n\tnow := time.Now()\r\n\tnac.IssuedAt = now.Add(-10 * time.Second).Unix()\r\n\tnac.Expires = now.Round(time.Second).Add(time.Second).Unix()\r\n\ts, akp, c, cr := setupJWTTestWitAccountClaims(t, nac, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\tapub, _ := akp.PublicKey()\r\n\tacc, err := s.LookupAccount(apub)\r\n\tif acc == nil || err != nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\tif l, _ := cr.ReadString('\\n'); !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected PONG, got %q\", l)\r\n\t}\r\n\r\n\t// Wait for the account to be expired.\r\n\tcheckFor(t, 3*time.Second, 100*time.Millisecond, func() error {\r\n\t\tif acc.IsExpired() {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\treturn fmt.Errorf(\"Account not expired yet\")\r\n\t})\r\n\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error, got %q\", l)\r\n\t}\r\n\tif !strings.Contains(l, \"Expired\") {\r\n\t\tt.Fatalf(\"Expected 'Expired' to be in the error\")\r\n\t}\r\n\r\n\t// Now make sure that accounts that have expired return an error.\r\n\tc, cr, cs := createClient(t, s, akp)\r\n\tdefer c.close()\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountRenew(t *testing.T) {\r\n\tnac := newJWTTestAccountClaims()\r\n\t// Create an account that has expired.\r\n\tnac.IssuedAt = time.Now().Add(-10 * time.Second).Unix()\r\n\tnac.Expires = time.Now().Add(-2 * time.Second).Unix()\r\n\t// Expect an error\r\n\ts, akp, c, _ := setupJWTTestWitAccountClaims(t, nac, \"-ERR \")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\tapub, _ := akp.PublicKey()\r\n\r\n\t// Now update with new expiration\r\n\tnac.IssuedAt = time.Now().Unix()\r\n\tnac.Expires = time.Now().Add(5 * time.Second).Unix()\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\t// Update the account\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\tacc, _ := s.LookupAccount(apub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\ts.updateAccountClaims(acc, nac)\r\n\r\n\t// Now make sure we can connect.\r\n\tc, cr, cs := createClient(t, s, akp)\r\n\tdefer c.close()\r\n\tc.parseAsync(cs)\r\n\tif l, _ := cr.ReadString('\\n'); !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected a PONG, got: %q\", l)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountRenewFromResolver(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tnac.IssuedAt = time.Now().Add(-10 * time.Second).Unix()\r\n\tnac.Expires = time.Now().Add(time.Second).Unix()\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\t// Force it to be loaded by the server and start the expiration timer.\r\n\tacc, _ := s.LookupAccount(apub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Could not retrieve account for %q\", apub)\r\n\t}\r\n\r\n\t// Create a new user\r\n\tc, cr, cs := createClient(t, s, akp)\r\n\tdefer c.close()\r\n\t// Wait for expiration.\r\n\ttime.Sleep(1250 * time.Millisecond)\r\n\r\n\tc.parseAsync(cs)\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\r\n\t// Now update with new expiration\r\n\tnac.IssuedAt = time.Now().Unix()\r\n\tnac.Expires = time.Now().Add(5 * time.Second).Unix()\r\n\tajwt, err = nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\t// Update the account\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\t// Make sure the too quick update suppression does not bite us.\r\n\tacc.mu.Lock()\r\n\tacc.updated = time.Now().Add(-1 * time.Hour)\r\n\tacc.mu.Unlock()\r\n\r\n\t// Do not update the account directly. The resolver should\r\n\t// happen automatically.\r\n\r\n\t// Now make sure we can connect.\r\n\tc, cr, cs = createClient(t, s, akp)\r\n\tdefer c.close()\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected a PONG, got: %q\", l)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountBasicImportExport(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\r\n\t// Now create Exports.\r\n\tstreamExport := &jwt.Export{Subject: \"foo\", Type: jwt.Stream}\r\n\tstreamExport2 := &jwt.Export{Subject: \"private\", Type: jwt.Stream, TokenReq: true}\r\n\tserviceExport := &jwt.Export{Subject: \"req.echo\", Type: jwt.Service, TokenReq: true}\r\n\tserviceExport2 := &jwt.Export{Subject: \"req.add\", Type: jwt.Service, TokenReq: true}\r\n\r\n\tfooAC.Exports.Add(streamExport, streamExport2, serviceExport, serviceExport2)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tacc, _ := s.LookupAccount(fooPub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\t// Check to make sure exports transferred over.\r\n\tif les := len(acc.exports.streams); les != 2 {\r\n\t\tt.Fatalf(\"Expected exports streams len of 2, got %d\", les)\r\n\t}\r\n\tif les := len(acc.exports.services); les != 2 {\r\n\t\tt.Fatalf(\"Expected exports services len of 2, got %d\", les)\r\n\t}\r\n\t_, ok := acc.exports.streams[\"foo\"]\r\n\tif !ok {\r\n\t\tt.Fatalf(\"Expected to map a stream export\")\r\n\t}\r\n\tse, ok := acc.exports.services[\"req.echo\"]\r\n\tif !ok || se == nil {\r\n\t\tt.Fatalf(\"Expected to map a service export\")\r\n\t}\r\n\tif !se.tokenReq {\r\n\t\tt.Fatalf(\"Expected the service export to require tokens\")\r\n\t}\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\r\n\tstreamImport := &jwt.Import{Account: fooPub, Subject: \"foo\", To: \"import.foo\", Type: jwt.Stream}\r\n\tserviceImport := &jwt.Import{Account: fooPub, Subject: \"req.echo\", Type: jwt.Service}\r\n\tbarAC.Imports.Add(streamImport, serviceImport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\tacc, _ = s.LookupAccount(barPub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\tif les := len(acc.imports.streams); les != 1 {\r\n\t\tt.Fatalf(\"Expected imports streams len of 1, got %d\", les)\r\n\t}\r\n\t// Our service import should have failed without a token.\r\n\tif les := len(acc.imports.services); les != 0 {\r\n\t\tt.Fatalf(\"Expected imports services len of 0, got %d\", les)\r\n\t}\r\n\r\n\t// Now add in a bad activation token.\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tserviceImport = &jwt.Import{Account: fooPub, Subject: \"req.echo\", Token: \"not a token\", Type: jwt.Service}\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err = barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\ts.updateAccountClaims(acc, barAC)\r\n\r\n\t// Our service import should have failed with a bad token.\r\n\tif les := len(acc.imports.services); les != 0 {\r\n\t\tt.Fatalf(\"Expected imports services len of 0, got %d\", les)\r\n\t}\r\n\r\n\t// Now make a correct one.\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tserviceImport = &jwt.Import{Account: fooPub, Subject: \"req.echo\", Type: jwt.Service}\r\n\r\n\tactivation := jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"req.echo\"\r\n\tactivation.ImportType = jwt.Service\r\n\tactJWT, err := activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\tserviceImport.Token = actJWT\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err = barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\t// Our service import should have succeeded.\r\n\tif les := len(acc.imports.services); les != 1 {\r\n\t\tt.Fatalf(\"Expected imports services len of 1, got %d\", les)\r\n\t}\r\n\r\n\t// Now test url\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tserviceImport = &jwt.Import{Account: fooPub, Subject: \"req.add\", Type: jwt.Service}\r\n\r\n\tactivation = jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"req.add\"\r\n\tactivation.ImportType = jwt.Service\r\n\tactJWT, err = activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tw.Write([]byte(actJWT))\r\n\t}))\r\n\tdefer ts.Close()\r\n\r\n\tserviceImport.Token = ts.URL\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err = barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\t// Our service import should have succeeded. Should be the only one since we reset.\r\n\tif les := len(acc.imports.services); les != 1 {\r\n\t\tt.Fatalf(\"Expected imports services len of 1, got %d\", les)\r\n\t}\r\n\r\n\t// Now streams\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tstreamImport = &jwt.Import{Account: fooPub, Subject: \"private\", To: \"import.private\", Type: jwt.Stream}\r\n\r\n\tbarAC.Imports.Add(streamImport)\r\n\tbarJWT, err = barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\t// Our stream import should have not succeeded.\r\n\tif les := len(acc.imports.streams); les != 0 {\r\n\t\tt.Fatalf(\"Expected imports services len of 0, got %d\", les)\r\n\t}\r\n\r\n\t// Now add in activation.\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tstreamImport = &jwt.Import{Account: fooPub, Subject: \"private\", To: \"import.private\", Type: jwt.Stream}\r\n\r\n\tactivation = jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"private\"\r\n\tactivation.ImportType = jwt.Stream\r\n\tactJWT, err = activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\tstreamImport.Token = actJWT\r\n\tbarAC.Imports.Add(streamImport)\r\n\tbarJWT, err = barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\t// Our stream import should have not succeeded.\r\n\tif les := len(acc.imports.streams); les != 1 {\r\n\t\tt.Fatalf(\"Expected imports services len of 1, got %d\", les)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountExportWithResponseType(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\r\n\t// Now create Exports.\r\n\tserviceStreamExport := &jwt.Export{Subject: \"test.stream\", Type: jwt.Service, ResponseType: jwt.ResponseTypeStream, TokenReq: false}\r\n\tserviceChunkExport := &jwt.Export{Subject: \"test.chunk\", Type: jwt.Service, ResponseType: jwt.ResponseTypeChunked, TokenReq: false}\r\n\tserviceSingletonExport := &jwt.Export{Subject: \"test.single\", Type: jwt.Service, ResponseType: jwt.ResponseTypeSingleton, TokenReq: true}\r\n\tserviceDefExport := &jwt.Export{Subject: \"test.def\", Type: jwt.Service, TokenReq: true}\r\n\tserviceOldExport := &jwt.Export{Subject: \"test.old\", Type: jwt.Service, TokenReq: false}\r\n\r\n\tfooAC.Exports.Add(serviceStreamExport, serviceSingletonExport, serviceChunkExport, serviceDefExport, serviceOldExport)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tfooAcc, _ := s.LookupAccount(fooPub)\r\n\tif fooAcc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\tservices := fooAcc.exports.services\r\n\r\n\tif len(services) != 5 {\r\n\t\tt.Fatalf(\"Expected 4 services\")\r\n\t}\r\n\r\n\tse, ok := services[\"test.stream\"]\r\n\tif !ok || se == nil {\r\n\t\tt.Fatalf(\"Expected to map a service export\")\r\n\t}\r\n\tif se.tokenReq {\r\n\t\tt.Fatalf(\"Expected the service export to not require tokens\")\r\n\t}\r\n\tif se.respType != Stream {\r\n\t\tt.Fatalf(\"Expected the service export to respond with a stream\")\r\n\t}\r\n\r\n\tse, ok = services[\"test.chunk\"]\r\n\tif !ok || se == nil {\r\n\t\tt.Fatalf(\"Expected to map a service export\")\r\n\t}\r\n\tif se.tokenReq {\r\n\t\tt.Fatalf(\"Expected the service export to not require tokens\")\r\n\t}\r\n\tif se.respType != Chunked {\r\n\t\tt.Fatalf(\"Expected the service export to respond with a stream\")\r\n\t}\r\n\r\n\tse, ok = services[\"test.def\"]\r\n\tif !ok || se == nil {\r\n\t\tt.Fatalf(\"Expected to map a service export\")\r\n\t}\r\n\tif !se.tokenReq {\r\n\t\tt.Fatalf(\"Expected the service export to not require tokens\")\r\n\t}\r\n\tif se.respType != Singleton {\r\n\t\tt.Fatalf(\"Expected the service export to respond with a stream\")\r\n\t}\r\n\r\n\tse, ok = services[\"test.single\"]\r\n\tif !ok || se == nil {\r\n\t\tt.Fatalf(\"Expected to map a service export\")\r\n\t}\r\n\tif !se.tokenReq {\r\n\t\tt.Fatalf(\"Expected the service export to not require tokens\")\r\n\t}\r\n\tif se.respType != Singleton {\r\n\t\tt.Fatalf(\"Expected the service export to respond with a stream\")\r\n\t}\r\n\r\n\tse, ok = services[\"test.old\"]\r\n\tif !ok || se != nil {\r\n\t\tt.Fatalf(\"Service with a singleton response and no tokens should be nil in the map\")\r\n\t}\r\n}\r\n\r\nfunc expectPong(t *testing.T, cr *bufio.Reader) {\r\n\tt.Helper()\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected a PONG, got %q\", l)\r\n\t}\r\n}\r\n\r\nfunc expectMsg(t *testing.T, cr *bufio.Reader, sub, payload string) {\r\n\tt.Helper()\r\n\tl, _ := cr.ReadString('\\n')\r\n\texpected := \"MSG \" + sub\r\n\tif !strings.HasPrefix(l, expected) {\r\n\t\tt.Fatalf(\"Expected %q, got %q\", expected, l)\r\n\t}\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif l != payload+\"\\r\\n\" {\r\n\t\tt.Fatalf(\"Expected %q, got %q\", payload, l)\r\n\t}\r\n\texpectPong(t, cr)\r\n}\r\n\r\nfunc TestJWTAccountImportExportUpdates(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tstreamExport := &jwt.Export{Subject: \"foo\", Type: jwt.Stream}\r\n\r\n\tfooAC.Exports.Add(streamExport)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\tstreamImport := &jwt.Import{Account: fooPub, Subject: \"foo\", To: \"import\", Type: jwt.Stream}\r\n\r\n\tbarAC.Imports.Add(streamImport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\t// Create a client.\r\n\tc, cr, cs := createClient(t, s, barKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\tc.parseAsync(\"SUB import.foo 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cr)\r\n\r\n\tcheckShadow := func(expected int) {\r\n\t\tt.Helper()\r\n\t\tc.mu.Lock()\r\n\t\tdefer c.mu.Unlock()\r\n\t\tsub := c.subs[\"1\"]\r\n\t\tif ls := len(sub.shadow); ls != expected {\r\n\t\t\tt.Fatalf(\"Expected shadows to be %d, got %d\", expected, ls)\r\n\t\t}\r\n\t}\r\n\r\n\t// We created a SUB on foo which should create a shadow subscription.\r\n\tcheckShadow(1)\r\n\r\n\t// Now update bar and remove the import which should make the shadow go away.\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tbarJWT, _ = barAC.Encode(okp)\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\tacc, _ := s.LookupAccount(barPub)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\r\n\tcheckShadow(0)\r\n\r\n\t// Now add it back and make sure the shadow comes back.\r\n\tstreamImport = &jwt.Import{Account: string(fooPub), Subject: \"foo\", To: \"import\", Type: jwt.Stream}\r\n\tbarAC.Imports.Add(streamImport)\r\n\tbarJWT, _ = barAC.Encode(okp)\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\r\n\tcheckShadow(1)\r\n\r\n\t// Now change export and make sure it goes away as well. So no exports anymore.\r\n\tfooAC = jwt.NewAccountClaims(fooPub)\r\n\tfooJWT, _ = fooAC.Encode(okp)\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\tacc, _ = s.LookupAccount(fooPub)\r\n\ts.updateAccountClaims(acc, fooAC)\r\n\tcheckShadow(0)\r\n\r\n\t// Now add it in but with permission required.\r\n\tstreamExport = &jwt.Export{Subject: \"foo\", Type: jwt.Stream, TokenReq: true}\r\n\tfooAC.Exports.Add(streamExport)\r\n\tfooJWT, _ = fooAC.Encode(okp)\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\ts.updateAccountClaims(acc, fooAC)\r\n\r\n\tcheckShadow(0)\r\n\r\n\t// Now put it back as normal.\r\n\tfooAC = jwt.NewAccountClaims(fooPub)\r\n\tstreamExport = &jwt.Export{Subject: \"foo\", Type: jwt.Stream}\r\n\tfooAC.Exports.Add(streamExport)\r\n\tfooJWT, _ = fooAC.Encode(okp)\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\ts.updateAccountClaims(acc, fooAC)\r\n\r\n\tcheckShadow(1)\r\n}\r\n\r\nfunc TestJWTAccountImportActivationExpires(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tstreamExport := &jwt.Export{Subject: \"foo\", Type: jwt.Stream, TokenReq: true}\r\n\tfooAC.Exports.Add(streamExport)\r\n\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\tacc, _ := s.LookupAccount(fooPub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\tstreamImport := &jwt.Import{Account: fooPub, Subject: \"foo\", To: \"import.\", Type: jwt.Stream}\r\n\r\n\tactivation := jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"foo\"\r\n\tactivation.ImportType = jwt.Stream\r\n\tnow := time.Now()\r\n\tactivation.IssuedAt = now.Add(-10 * time.Second).Unix()\r\n\t// These are second resolution. So round up before adding a second.\r\n\tactivation.Expires = now.Round(time.Second).Add(time.Second).Unix()\r\n\tactJWT, err := activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\tstreamImport.Token = actJWT\r\n\tbarAC.Imports.Add(streamImport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\tif acc, _ := s.LookupAccount(barPub); acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\t// Create a client.\r\n\tc, cr, cs := createClient(t, s, barKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\tc.parseAsync(\"SUB import.foo 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cr)\r\n\r\n\tcheckShadow := func(t *testing.T, expected int) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, 3*time.Second, 15*time.Millisecond, func() error {\r\n\t\t\tc.mu.Lock()\r\n\t\t\tdefer c.mu.Unlock()\r\n\t\t\tsub := c.subs[\"1\"]\r\n\t\t\tif ls := len(sub.shadow); ls != expected {\r\n\t\t\t\treturn fmt.Errorf(\"Expected shadows to be %d, got %d\", expected, ls)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\r\n\t// We created a SUB on foo which should create a shadow subscription.\r\n\tcheckShadow(t, 1)\r\n\r\n\ttime.Sleep(1250 * time.Millisecond)\r\n\r\n\t// Should have expired and been removed.\r\n\tcheckShadow(t, 0)\r\n}\r\n\r\nfunc TestJWTAccountLimitsSubs(t *testing.T) {\r\n\tfooAC := newJWTTestAccountClaims()\r\n\tfooAC.Limits.Subs = 10\r\n\ts, fooKP, c, _ := setupJWTTestWitAccountClaims(t, fooAC, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\r\n\t// Create a client.\r\n\tc, cr, cs := createClient(t, s, fooKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\t// Check to make sure we have the limit set.\r\n\t// Account first\r\n\tfooAcc, _ := s.LookupAccount(fooPub)\r\n\tfooAcc.mu.RLock()\r\n\tif fooAcc.msubs != 10 {\r\n\t\tfooAcc.mu.RUnlock()\r\n\t\tt.Fatalf(\"Expected account to have msubs of 10, got %d\", fooAcc.msubs)\r\n\t}\r\n\tfooAcc.mu.RUnlock()\r\n\t// Now test that the client has limits too.\r\n\tc.mu.Lock()\r\n\tif c.msubs != 10 {\r\n\t\tc.mu.Unlock()\r\n\t\tt.Fatalf(\"Expected client msubs to be 10, got %d\", c.msubs)\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\t// Now make sure its enforced.\r\n\t/// These should all work ok.\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tc.parseAsync(fmt.Sprintf(\"SUB foo %d\\r\\nPING\\r\\n\", i))\r\n\t\texpectPong(t, cr)\r\n\t}\r\n\r\n\t// This one should fail.\r\n\tc.parseAsync(\"SUB foo 22\\r\\n\")\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR\") {\r\n\t\tt.Fatalf(\"Expected an ERR, got: %v\", l)\r\n\t}\r\n\tif !strings.Contains(l, \"maximum subscriptions exceeded\") {\r\n\t\tt.Fatalf(\"Expected an ERR for max subscriptions exceeded, got: %v\", l)\r\n\t}\r\n\r\n\t// Now update the claims and expect if max is lower to be disconnected.\r\n\tfooAC.Limits.Subs = 5\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\ts.updateAccountClaims(fooAcc, fooAC)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR\") {\r\n\t\tt.Fatalf(\"Expected an ERR, got: %v\", l)\r\n\t}\r\n\tif !strings.Contains(l, \"maximum subscriptions exceeded\") {\r\n\t\tt.Fatalf(\"Expected an ERR for max subscriptions exceeded, got: %v\", l)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountLimitsSubsButServerOverrides(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\t// override with server setting of 2.\r\n\topts := s.getOpts()\r\n\topts.MaxSubs = 2\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tfooAC.Limits.Subs = 10\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\tfooAcc, _ := s.LookupAccount(fooPub)\r\n\tfooAcc.mu.RLock()\r\n\tif fooAcc.msubs != 10 {\r\n\t\tfooAcc.mu.RUnlock()\r\n\t\tt.Fatalf(\"Expected account to have msubs of 10, got %d\", fooAcc.msubs)\r\n\t}\r\n\tfooAcc.mu.RUnlock()\r\n\r\n\t// Create a client.\r\n\tc, cr, cs := createClient(t, s, fooKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\tc.parseAsync(\"SUB foo 1\\r\\nSUB bar 2\\r\\nSUB baz 3\\r\\nPING\\r\\n\")\r\n\tl, _ := cr.ReadString('\\n')\r\n\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\tif !strings.Contains(l, \"maximum subscriptions exceeded\") {\r\n\t\tt.Fatalf(\"Expected an ERR for max subscriptions exceeded, got: %v\", l)\r\n\t}\r\n\t// Read last PONG so does not hold up test.\r\n\tcr.ReadString('\\n')\r\n}\r\n\r\nfunc TestJWTAccountLimitsMaxPayload(t *testing.T) {\r\n\tfooAC := newJWTTestAccountClaims()\r\n\tfooAC.Limits.Payload = 8\r\n\ts, fooKP, c, _ := setupJWTTestWitAccountClaims(t, fooAC, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\r\n\t// Create a client.\r\n\tc, cr, cs := createClient(t, s, fooKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\t// Check to make sure we have the limit set.\r\n\t// Account first\r\n\tfooAcc, _ := s.LookupAccount(fooPub)\r\n\tfooAcc.mu.RLock()\r\n\tif fooAcc.mpay != 8 {\r\n\t\tfooAcc.mu.RUnlock()\r\n\t\tt.Fatalf(\"Expected account to have mpay of 8, got %d\", fooAcc.mpay)\r\n\t}\r\n\tfooAcc.mu.RUnlock()\r\n\t// Now test that the client has limits too.\r\n\tc.mu.Lock()\r\n\tif c.mpay != 8 {\r\n\t\tc.mu.Unlock()\r\n\t\tt.Fatalf(\"Expected client to have mpay of 10, got %d\", c.mpay)\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\tc.parseAsync(\"PUB foo 4\\r\\nXXXX\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cr)\r\n\r\n\tc.parseAsync(\"PUB foo 10\\r\\nXXXXXXXXXX\\r\\nPING\\r\\n\")\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\tif !strings.Contains(l, \"Maximum Payload\") {\r\n\t\tt.Fatalf(\"Expected an ERR for max payload violation, got: %v\", l)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountLimitsMaxPayloadButServerOverrides(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\t// override with server setting of 4.\r\n\topts := s.getOpts()\r\n\topts.MaxPayload = 4\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tfooAC.Limits.Payload = 8\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\t// Create a client.\r\n\tc, cr, cs := createClient(t, s, fooKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\tc.parseAsync(\"PUB foo 6\\r\\nXXXXXX\\r\\nPING\\r\\n\")\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\tif !strings.Contains(l, \"Maximum Payload\") {\r\n\t\tt.Fatalf(\"Expected an ERR for max payload violation, got: %v\", l)\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountLimitsMaxConns(t *testing.T) {\r\n\tfooAC := newJWTTestAccountClaims()\r\n\tfooAC.Limits.Conn = 8\r\n\ts, fooKP, c, _ := setupJWTTestWitAccountClaims(t, fooAC, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\tnewClient := func(expPre string) *testAsyncClient {\r\n\t\tt.Helper()\r\n\t\t// Create a client.\r\n\t\tc, cr, cs := createClient(t, s, fooKP)\r\n\t\tc.parseAsync(cs)\r\n\t\tl, _ := cr.ReadString('\\n')\r\n\t\tif !strings.HasPrefix(l, expPre) {\r\n\t\t\tt.Fatalf(\"Expected a response starting with %q, got %q\", expPre, l)\r\n\t\t}\r\n\t\treturn c\r\n\t}\r\n\r\n\t// A connection is created in setupJWTTestWitAccountClaims(), so limit\r\n\t// to 7 here (8 total).\r\n\tfor i := 0; i < 7; i++ {\r\n\t\tc := newClient(\"PONG\")\r\n\t\tdefer c.close()\r\n\t}\r\n\t// Now this one should fail.\r\n\tc = newClient(\"-ERR \")\r\n\tc.close()\r\n}\r\n\r\n// This will test that we can switch from a public export to a private\r\n// one and back with export claims to make sure the claim update mechanism\r\n// is working properly.\r\nfunc TestJWTAccountServiceImportAuthSwitch(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tserviceExport := &jwt.Export{Subject: \"ngs.usage.*\", Type: jwt.Service}\r\n\tfooAC.Exports.Add(serviceExport)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\tserviceImport := &jwt.Import{Account: fooPub, Subject: \"ngs.usage\", To: \"ngs.usage.DEREK\", Type: jwt.Service}\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\t// Create a client that will send the request\r\n\tca, cra, csa := createClient(t, s, barKP)\r\n\tdefer ca.close()\r\n\tca.parseAsync(csa)\r\n\texpectPong(t, cra)\r\n\r\n\t// Create the client that will respond to the requests.\r\n\tcb, crb, csb := createClient(t, s, fooKP)\r\n\tdefer cb.close()\r\n\tcb.parseAsync(csb)\r\n\texpectPong(t, crb)\r\n\r\n\t// Create Subscriber.\r\n\tcb.parseAsync(\"SUB ngs.usage.* 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, crb)\r\n\r\n\t// Send Request\r\n\tca.parseAsync(\"PUB ngs.usage 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should receive the request mapped into our account. PING needed to flush.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectMsg(t, crb, \"ngs.usage.DEREK\", \"hi\")\r\n\r\n\t// Now update to make the export private.\r\n\tfooACPrivate := jwt.NewAccountClaims(fooPub)\r\n\tserviceExport = &jwt.Export{Subject: \"ngs.usage.*\", Type: jwt.Service, TokenReq: true}\r\n\tfooACPrivate.Exports.Add(serviceExport)\r\n\tfooJWTPrivate, err := fooACPrivate.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWTPrivate)\r\n\tacc, _ := s.LookupAccount(fooPub)\r\n\ts.updateAccountClaims(acc, fooACPrivate)\r\n\r\n\t// Send Another Request\r\n\tca.parseAsync(\"PUB ngs.usage 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should not receive the request this time.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectPong(t, crb)\r\n\r\n\t// Now put it back again to public and make sure it works again.\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\ts.updateAccountClaims(acc, fooAC)\r\n\r\n\t// Send Request\r\n\tca.parseAsync(\"PUB ngs.usage 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should receive the request mapped into our account. PING needed to flush.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectMsg(t, crb, \"ngs.usage.DEREK\", \"hi\")\r\n}\r\n\r\nfunc TestJWTAccountServiceImportExpires(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tserviceExport := &jwt.Export{Subject: \"foo\", Type: jwt.Service}\r\n\r\n\tfooAC.Exports.Add(serviceExport)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\tserviceImport := &jwt.Import{Account: fooPub, Subject: \"foo\", Type: jwt.Service}\r\n\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\t// Create a client that will send the request\r\n\tca, cra, csa := createClient(t, s, barKP)\r\n\tdefer ca.close()\r\n\tca.parseAsync(csa)\r\n\texpectPong(t, cra)\r\n\r\n\t// Create the client that will respond to the requests.\r\n\tcb, crb, csb := createClient(t, s, fooKP)\r\n\tdefer cb.close()\r\n\tcb.parseAsync(csb)\r\n\texpectPong(t, crb)\r\n\r\n\t// Create Subscriber.\r\n\tcb.parseAsync(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, crb)\r\n\r\n\t// Send Request\r\n\tca.parseAsync(\"PUB foo 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should receive the request. PING needed to flush.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectMsg(t, crb, \"foo\", \"hi\")\r\n\r\n\t// Now update the exported service to require auth.\r\n\tfooAC = jwt.NewAccountClaims(fooPub)\r\n\tserviceExport = &jwt.Export{Subject: \"foo\", Type: jwt.Service, TokenReq: true}\r\n\r\n\tfooAC.Exports.Add(serviceExport)\r\n\tfooJWT, err = fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\tacc, _ := s.LookupAccount(fooPub)\r\n\ts.updateAccountClaims(acc, fooAC)\r\n\r\n\t// Send Another Request\r\n\tca.parseAsync(\"PUB foo 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should not receive the request this time.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectPong(t, crb)\r\n\r\n\t// Now get an activation token such that it will work, but will expire.\r\n\tbarAC = jwt.NewAccountClaims(barPub)\r\n\tserviceImport = &jwt.Import{Account: fooPub, Subject: \"foo\", Type: jwt.Service}\r\n\r\n\tnow := time.Now()\r\n\tactivation := jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"foo\"\r\n\tactivation.ImportType = jwt.Service\r\n\tactivation.IssuedAt = now.Add(-10 * time.Second).Unix()\r\n\tactivation.Expires = now.Add(time.Second).Round(time.Second).Unix()\r\n\tactJWT, err := activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\tserviceImport.Token = actJWT\r\n\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err = barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\tacc, _ = s.LookupAccount(barPub)\r\n\ts.updateAccountClaims(acc, barAC)\r\n\r\n\t// Now it should work again.\r\n\t// Send Another Request\r\n\tca.parseAsync(\"PUB foo 3\\r\\nhi2\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should receive the request. PING needed to flush.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectMsg(t, crb, \"foo\", \"hi2\")\r\n\r\n\t// Now wait for it to expire, then retry.\r\n\twaitTime := time.Duration(activation.Expires-time.Now().Unix()) * time.Second\r\n\ttime.Sleep(waitTime + 250*time.Millisecond)\r\n\r\n\t// Send Another Request\r\n\tca.parseAsync(\"PUB foo 3\\r\\nhi3\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cra)\r\n\r\n\t// We should NOT receive the request. PING needed to flush.\r\n\tcb.parseAsync(\"PING\\r\\n\")\r\n\texpectPong(t, crb)\r\n}\r\n\r\nfunc TestAccountURLResolver(t *testing.T) {\r\n\tfor _, test := range []struct {\r\n\t\tname   string\r\n\t\tuseTLS bool\r\n\t}{\r\n\t\t{\"plain\", false},\r\n\t\t{\"tls\", true},\r\n\t} {\r\n\t\tt.Run(test.name, func(t *testing.T) {\r\n\t\t\tkp, _ := nkeys.FromSeed(oSeed)\r\n\t\t\takp, _ := nkeys.CreateAccount()\r\n\t\t\tapub, _ := akp.PublicKey()\r\n\t\t\tnac := jwt.NewAccountClaims(apub)\r\n\t\t\tajwt, err := nac.Encode(kp)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t\t\t}\r\n\r\n\t\t\thf := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\t\t\tw.Write([]byte(ajwt))\r\n\t\t\t})\r\n\t\t\tvar ts *httptest.Server\r\n\t\t\tif test.useTLS {\r\n\t\t\t\tts = httptest.NewTLSServer(hf)\r\n\t\t\t} else {\r\n\t\t\t\tts = httptest.NewServer(hf)\r\n\t\t\t}\r\n\t\t\tdefer ts.Close()\r\n\r\n\t\t\tconfTemplate := `\r\n\t\t\t\toperator: %s\r\n\t\t\t\tlisten: -1\r\n\t\t\t\tresolver: URL(\"%s/ngs/v1/accounts/jwt/\")\r\n\t\t\t\tresolver_tls {\r\n\t\t\t\t\tinsecure: true\r\n\t\t\t\t}\r\n\t\t\t`\r\n\t\t\tconf := createConfFile(t, []byte(fmt.Sprintf(confTemplate, ojwt, ts.URL)))\r\n\t\t\tdefer os.Remove(conf)\r\n\r\n\t\t\ts, opts := RunServerWithConfig(conf)\r\n\t\t\tpub, _ := kp.PublicKey()\r\n\t\t\topts.TrustedKeys = []string{pub}\r\n\t\t\tdefer s.Shutdown()\r\n\r\n\t\t\tacc, _ := s.LookupAccount(apub)\r\n\t\t\tif acc == nil {\r\n\t\t\t\tt.Fatalf(\"Expected to receive an account\")\r\n\t\t\t}\r\n\t\t\tif acc.Name != apub {\r\n\t\t\t\tt.Fatalf(\"Account name did not match claim key\")\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestAccountURLResolverTimeout(t *testing.T) {\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(kp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tbasePath := \"/ngs/v1/accounts/jwt/\"\r\n\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tif r.URL.Path == basePath {\r\n\t\t\tw.Write([]byte(\"ok\"))\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// Purposely be slow on account lookup.\r\n\t\ttime.Sleep(200 * time.Millisecond)\r\n\t\tw.Write([]byte(ajwt))\r\n\t}))\r\n\tdefer ts.Close()\r\n\r\n\tconfTemplate := `\r\n\t\tlisten: -1\r\n\t\tresolver: URL(\"%s%s\")\r\n    `\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(confTemplate, ts.URL, basePath)))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tpub, _ := kp.PublicKey()\r\n\topts.TrustedKeys = []string{pub}\r\n\tdefer s.Shutdown()\r\n\r\n\t// Lower default timeout to speed-up test\r\n\ts.AccountResolver().(*URLAccResolver).c.Timeout = 50 * time.Millisecond\r\n\r\n\tacc, _ := s.LookupAccount(apub)\r\n\tif acc != nil {\r\n\t\tt.Fatalf(\"Expected to not receive an account due to timeout\")\r\n\t}\r\n}\r\n\r\nfunc TestAccountURLResolverNoFetchOnReload(t *testing.T) {\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(kp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tw.Write([]byte(ajwt))\r\n\t}))\r\n\tdefer ts.Close()\r\n\r\n\tconfTemplate := `\r\n\t\toperator: %s\r\n\t\tlisten: -1\r\n\t\tresolver: URL(\"%s/ngs/v1/accounts/jwt/\")\r\n    `\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(confTemplate, ojwt, ts.URL)))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tacc, _ := s.LookupAccount(apub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to receive an account\")\r\n\t}\r\n\r\n\t// Reload would produce a DATA race during the DeepEqual check for the account resolver,\r\n\t// so close the current one and we will create a new one that keeps track of fetch calls.\r\n\tts.Close()\r\n\r\n\tfetch := int32(0)\r\n\tts = httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tatomic.AddInt32(&fetch, 1)\r\n\t\tw.Write([]byte(ajwt))\r\n\t}))\r\n\tdefer ts.Close()\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(confTemplate, ojwt, ts.URL)))\r\n\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error on reload: %v\", err)\r\n\t}\r\n\tif atomic.LoadInt32(&fetch) != 0 {\r\n\t\tt.Fatalf(\"Fetch invoked during reload\")\r\n\t}\r\n\r\n\t// Now stop the resolver and make sure that on startup, we report URL resolver failure\r\n\ts.Shutdown()\r\n\ts = nil\r\n\tts.Close()\r\n\r\n\topts := LoadConfig(conf)\r\n\tif s, err := NewServer(opts); err == nil || !strings.Contains(err.Error(), \"could not fetch\") {\r\n\t\tif s != nil {\r\n\t\t\ts.Shutdown()\r\n\t\t}\r\n\t\tt.Fatalf(\"Expected error regarding account resolver, got %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestAccountURLResolverFetchFailureInServer1(t *testing.T) {\r\n\tconst subj = \"test\"\r\n\tconst crossAccSubj = \"test\"\r\n\t// Create Exporting Account\r\n\texpkp, _ := nkeys.CreateAccount()\r\n\texppub, _ := expkp.PublicKey()\r\n\texpac := jwt.NewAccountClaims(exppub)\r\n\texpac.Exports.Add(&jwt.Export{\r\n\t\tSubject: crossAccSubj,\r\n\t\tType:    jwt.Stream,\r\n\t})\r\n\texpjwt, err := expac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Create importing Account\r\n\timpkp, _ := nkeys.CreateAccount()\r\n\timppub, _ := impkp.PublicKey()\r\n\timpac := jwt.NewAccountClaims(imppub)\r\n\timpac.Imports.Add(&jwt.Import{\r\n\t\tAccount: exppub,\r\n\t\tSubject: crossAccSubj,\r\n\t\tType:    jwt.Stream,\r\n\t})\r\n\timpjwt, err := impac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Simulate an account server that drops the first request to exppub\r\n\tchanImpA := make(chan struct{}, 10)\r\n\tdefer close(chanImpA)\r\n\tchanExpS := make(chan struct{}, 10)\r\n\tdefer close(chanExpS)\r\n\tchanExpF := make(chan struct{}, 1)\r\n\tdefer close(chanExpF)\r\n\tfailureCnt := int32(0)\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tif r.URL.Path == \"/A/\" {\r\n\t\t\t// Server startup\r\n\t\t\tw.Write(nil)\r\n\t\t\tchanImpA <- struct{}{}\r\n\t\t} else if r.URL.Path == \"/A/\"+imppub {\r\n\t\t\tw.Write([]byte(impjwt))\r\n\t\t\tchanImpA <- struct{}{}\r\n\t\t} else if r.URL.Path == \"/A/\"+exppub {\r\n\t\t\tif atomic.AddInt32(&failureCnt, 1) <= 1 {\r\n\t\t\t\t// skip the write to simulate the failure\r\n\t\t\t\tchanExpF <- struct{}{}\r\n\t\t\t} else {\r\n\t\t\t\tw.Write([]byte(expjwt))\r\n\t\t\t\tchanExpS <- struct{}{}\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tt.Fatal(\"not expected\")\r\n\t\t}\r\n\t}))\r\n\tdefer ts.Close()\r\n\t// Create server\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(`\r\n\t\tlisten: -1\r\n\t\toperator: %s\r\n\t\tresolver: URL(\"%s/A/\")\r\n    `, ojwt, ts.URL)))\r\n\tdefer os.Remove(confA)\r\n\tsA := RunServer(LoadConfig(confA))\r\n\tdefer sA.Shutdown()\r\n\t// server observed one fetch on startup\r\n\tchanRecv(t, chanImpA, 10*time.Second)\r\n\t// Create first client\r\n\tncA := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, impkp))\r\n\tdefer ncA.Close()\r\n\t// create a test subscription\r\n\tsubA, err := ncA.SubscribeSync(subj)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected no error during subscribe: %v\", err)\r\n\t}\r\n\tdefer subA.Unsubscribe()\r\n\t// Connect of client triggered a fetch of both accounts\r\n\t// the fetch for the imported account will fail\r\n\tchanRecv(t, chanImpA, 10*time.Second)\r\n\tchanRecv(t, chanExpF, 10*time.Second)\r\n\t// create second client for user exporting\r\n\tncB := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, expkp))\r\n\tdefer ncB.Close()\r\n\tchanRecv(t, chanExpS, 10*time.Second)\r\n\t// Connect of client triggered another fetch, this time passing\r\n\tcheckSubInterest(t, sA, imppub, subj, 10*time.Second)\r\n\tcheckSubInterest(t, sA, exppub, crossAccSubj, 10*time.Second) // Will fail as a result of this issue\r\n}\r\n\r\nfunc TestAccountURLResolverFetchFailurePushReorder(t *testing.T) {\r\n\tconst subj = \"test\"\r\n\tconst crossAccSubj = \"test\"\r\n\t// Create System Account\r\n\tsyskp, _ := nkeys.CreateAccount()\r\n\tsyspub, _ := syskp.PublicKey()\r\n\tsysAc := jwt.NewAccountClaims(syspub)\r\n\tsysjwt, err := sysAc.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Create Exporting Account\r\n\texpkp, _ := nkeys.CreateAccount()\r\n\texppub, _ := expkp.PublicKey()\r\n\texpac := jwt.NewAccountClaims(exppub)\r\n\texpjwt1, err := expac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\texpac.Exports.Add(&jwt.Export{\r\n\t\tSubject: crossAccSubj,\r\n\t\tType:    jwt.Stream,\r\n\t})\r\n\texpjwt2, err := expac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Create importing Account\r\n\timpkp, _ := nkeys.CreateAccount()\r\n\timppub, _ := impkp.PublicKey()\r\n\timpac := jwt.NewAccountClaims(imppub)\r\n\timpac.Imports.Add(&jwt.Import{\r\n\t\tAccount: exppub,\r\n\t\tSubject: crossAccSubj,\r\n\t\tType:    jwt.Stream,\r\n\t})\r\n\timpjwt, err := impac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Simulate an account server that does not serve the updated jwt for exppub\r\n\tchanImpA := make(chan struct{}, 10)\r\n\tdefer close(chanImpA)\r\n\tchanExpS := make(chan struct{}, 10)\r\n\tdefer close(chanExpS)\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tif r.URL.Path == \"/A/\" {\r\n\t\t\t// Server startup\r\n\t\t\tw.Write(nil)\r\n\t\t\tchanImpA <- struct{}{}\r\n\t\t} else if r.URL.Path == \"/A/\"+imppub {\r\n\t\t\tw.Write([]byte(impjwt))\r\n\t\t\tchanImpA <- struct{}{}\r\n\t\t} else if r.URL.Path == \"/A/\"+exppub {\r\n\t\t\t// respond with jwt that does not have the export\r\n\t\t\t// this simulates an ordering issue\r\n\t\t\tw.Write([]byte(expjwt1))\r\n\t\t\tchanExpS <- struct{}{}\r\n\t\t} else if r.URL.Path == \"/A/\"+syspub {\r\n\t\t\tw.Write([]byte(sysjwt))\r\n\t\t} else {\r\n\t\t\tt.Fatal(\"not expected\")\r\n\t\t}\r\n\t}))\r\n\tdefer ts.Close()\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(`\r\n\t\tlisten: -1\r\n\t\toperator: %s\r\n\t\tresolver: URL(\"%s/A/\")\r\n\t\tsystem_account: %s\r\n    `, ojwt, ts.URL, syspub)))\r\n\tdefer os.Remove(confA)\r\n\tsA := RunServer(LoadConfig(confA))\r\n\tdefer sA.Shutdown()\r\n\t// server observed one fetch on startup\r\n\tchanRecv(t, chanImpA, 10*time.Second)\r\n\t// Create first client\r\n\tncA := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, impkp))\r\n\tdefer ncA.Close()\r\n\t// create a test subscription\r\n\tsubA, err := ncA.SubscribeSync(subj)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected no error during subscribe: %v\", err)\r\n\t}\r\n\tdefer subA.Unsubscribe()\r\n\t// Connect of client triggered a fetch of both accounts\r\n\t// the fetch for the imported account will fail\r\n\tchanRecv(t, chanImpA, 10*time.Second)\r\n\tchanRecv(t, chanExpS, 10*time.Second)\r\n\t// create second client for user exporting\r\n\tncB := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, expkp))\r\n\tdefer ncB.Close()\r\n\t// update expjwt2, this will correct the import issue\r\n\tsysc := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, syskp))\r\n\tdefer sysc.Close()\r\n\tnatsPub(t, sysc, fmt.Sprintf(accUpdateEventSubj, exppub), []byte(expjwt2))\r\n\tsysc.Flush()\r\n\t// updating expjwt should cause this to pass\r\n\tcheckSubInterest(t, sA, imppub, subj, 10*time.Second)\r\n\tcheckSubInterest(t, sA, exppub, crossAccSubj, 10*time.Second) // Will fail as a result of this issue\r\n}\r\n\r\ntype captureDebugLogger struct {\r\n\tDummyLogger\r\n\tdbgCh chan string\r\n}\r\n\r\nfunc (l *captureDebugLogger) Debugf(format string, v ...interface{}) {\r\n\tselect {\r\n\tcase l.dbgCh <- fmt.Sprintf(format, v...):\r\n\tdefault:\r\n\t}\r\n}\r\n\r\nfunc TestAccountURLResolverPermanentFetchFailure(t *testing.T) {\r\n\tconst crossAccSubj = \"test\"\r\n\texpkp, _ := nkeys.CreateAccount()\r\n\texppub, _ := expkp.PublicKey()\r\n\timpkp, _ := nkeys.CreateAccount()\r\n\timppub, _ := impkp.PublicKey()\r\n\t// Create System Account\r\n\tsyskp, _ := nkeys.CreateAccount()\r\n\tsyspub, _ := syskp.PublicKey()\r\n\tsysAc := jwt.NewAccountClaims(syspub)\r\n\tsysjwt, err := sysAc.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Create 2 Accounts. Each importing from the other, but NO matching export\r\n\texpac := jwt.NewAccountClaims(exppub)\r\n\texpac.Imports.Add(&jwt.Import{\r\n\t\tAccount: imppub,\r\n\t\tSubject: crossAccSubj,\r\n\t\tType:    jwt.Stream,\r\n\t})\r\n\texpjwt, err := expac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Create importing Account\r\n\timpac := jwt.NewAccountClaims(imppub)\r\n\timpac.Imports.Add(&jwt.Import{\r\n\t\tAccount: exppub,\r\n\t\tSubject: crossAccSubj,\r\n\t\tType:    jwt.Stream,\r\n\t})\r\n\timpjwt, err := impac.Encode(oKp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\t// Simulate an account server that does not serve the updated jwt for exppub\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tif r.URL.Path == \"/A/\" {\r\n\t\t\t// Server startup\r\n\t\t\tw.Write(nil)\r\n\t\t} else if r.URL.Path == \"/A/\"+imppub {\r\n\t\t\tw.Write([]byte(impjwt))\r\n\t\t} else if r.URL.Path == \"/A/\"+exppub {\r\n\t\t\tw.Write([]byte(expjwt))\r\n\t\t} else if r.URL.Path == \"/A/\"+syspub {\r\n\t\t\tw.Write([]byte(sysjwt))\r\n\t\t} else {\r\n\t\t\tt.Fatal(\"not expected\")\r\n\t\t}\r\n\t}))\r\n\tdefer ts.Close()\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(`\r\n\t\tlisten: -1\r\n\t\toperator: %s\r\n\t\tresolver: URL(\"%s/A/\")\r\n\t\tsystem_account: %s\r\n    `, ojwt, ts.URL, syspub)))\r\n\tdefer os.Remove(confA)\r\n\to := LoadConfig(confA)\r\n\tsA := RunServer(o)\r\n\tdefer sA.Shutdown()\r\n\tl := &captureDebugLogger{dbgCh: make(chan string, 100)} // has enough space to not block\r\n\tsA.SetLogger(l, true, false)\r\n\t// Create clients\r\n\tncA := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, impkp))\r\n\tdefer ncA.Close()\r\n\tncB := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, expkp))\r\n\tdefer ncB.Close()\r\n\tsysc := natsConnect(t, sA.ClientURL(), createUserCreds(t, nil, syskp))\r\n\tdefer sysc.Close()\r\n\t// push accounts\r\n\tnatsPub(t, sysc, fmt.Sprintf(accUpdateEventSubj, imppub), []byte(impjwt))\r\n\tnatsPub(t, sysc, fmt.Sprintf(accUpdateEventSubj, exppub), []byte(expjwt))\r\n\tsysc.Flush()\r\n\timportErrCnt := 0\r\n\ttmr := time.NewTimer(500 * time.Millisecond)\r\n\tdefer tmr.Stop()\r\n\tfor {\r\n\t\tselect {\r\n\t\tcase line := <-l.dbgCh:\r\n\t\t\tif strings.HasPrefix(line, \"Error adding stream import to account\") {\r\n\t\t\t\timportErrCnt++\r\n\t\t\t}\r\n\t\tcase <-tmr.C:\r\n\t\t\t// connecting and updating, each cause 3 traces (2 + 1 on iteration)\r\n\t\t\tif importErrCnt != 6 {\r\n\t\t\t\tt.Fatalf(\"Expected 6 debug traces, got %d\", importErrCnt)\r\n\t\t\t}\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserSigningKey(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\r\n\t// Check to make sure we would have an authTimer\r\n\tif !s.info.AuthRequired {\r\n\t\tt.Fatalf(\"Expect the server to require auth\")\r\n\t}\r\n\r\n\tc, cr, _ := newClientForServer(s)\r\n\tdefer c.close()\r\n\t// Don't send jwt field, should fail.\r\n\tc.parseAsync(\"CONNECT {\\\"verbose\\\":true,\\\"pedantic\\\":true}\\r\\nPING\\r\\n\")\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create an account\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\r\n\t// Create a signing key for the account\r\n\taskp, _ := nkeys.CreateAccount()\r\n\taspub, _ := askp.PublicKey()\r\n\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\t// Create a client with the account signing key\r\n\tc, cr, cs := createClientWithIssuer(t, s, askp, apub)\r\n\tdefer c.close()\r\n\r\n\t// PING needed to flush the +OK/-ERR to us.\r\n\t// This should fail too since no account resolver is defined.\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\r\n\t// Ok now let's walk through and make sure all is good.\r\n\t// We will set the account resolver by hand to a memory resolver.\r\n\tbuildMemAccResolver(s)\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\r\n\t// Create a client with a signing key\r\n\tc, cr, cs = createClientWithIssuer(t, s, askp, apub)\r\n\tdefer c.close()\r\n\t// should fail because the signing key is not known\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error: %v\", l)\r\n\t}\r\n\r\n\t// add a signing key\r\n\tnac.SigningKeys.Add(aspub)\r\n\t// update the memory resolver\r\n\tacc, _ := s.LookupAccount(apub)\r\n\ts.updateAccountClaims(acc, nac)\r\n\r\n\t// Create a client with a signing key\r\n\tc, cr, cs = createClientWithIssuer(t, s, askp, apub)\r\n\tdefer c.close()\r\n\r\n\t// expect this to work\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"PONG\") {\r\n\t\tt.Fatalf(\"Expected a PONG, got %q\", l)\r\n\t}\r\n\r\n\tisClosed := func() bool {\r\n\t\tc.mu.Lock()\r\n\t\tdefer c.mu.Unlock()\r\n\t\treturn c.isClosed()\r\n\t}\r\n\r\n\tif isClosed() {\r\n\t\tt.Fatal(\"expected client to be alive\")\r\n\t}\r\n\t// remove the signing key should bounce client\r\n\tnac.SigningKeys = nil\r\n\tacc, _ = s.LookupAccount(apub)\r\n\ts.updateAccountClaims(acc, nac)\r\n\r\n\tif !isClosed() {\r\n\t\tt.Fatal(\"expected client to be gone\")\r\n\t}\r\n}\r\n\r\nfunc TestJWTAccountImportSignerRemoved(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Exporter keys\r\n\tsrvKP, _ := nkeys.CreateAccount()\r\n\tsrvPK, _ := srvKP.PublicKey()\r\n\tsrvSignerKP, _ := nkeys.CreateAccount()\r\n\tsrvSignerPK, _ := srvSignerKP.PublicKey()\r\n\r\n\t// Importer keys\r\n\tclientKP, _ := nkeys.CreateAccount()\r\n\tclientPK, _ := clientKP.PublicKey()\r\n\r\n\tcreateSrvJwt := func(signingKeys ...string) (string, *jwt.AccountClaims) {\r\n\t\tac := jwt.NewAccountClaims(srvPK)\r\n\t\tac.SigningKeys.Add(signingKeys...)\r\n\t\tac.Exports.Add(&jwt.Export{Subject: \"foo\", Type: jwt.Service, TokenReq: true})\r\n\t\tac.Exports.Add(&jwt.Export{Subject: \"bar\", Type: jwt.Stream, TokenReq: true})\r\n\t\ttoken, err := ac.Encode(okp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating exporter JWT: %v\", err)\r\n\t\t}\r\n\t\treturn token, ac\r\n\t}\r\n\r\n\tcreateImportToken := func(sub string, kind jwt.ExportType) string {\r\n\t\tactC := jwt.NewActivationClaims(clientPK)\r\n\t\tactC.IssuerAccount = srvPK\r\n\t\tactC.ImportType = kind\r\n\t\tactC.ImportSubject = jwt.Subject(sub)\r\n\t\ttoken, err := actC.Encode(srvSignerKP)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatal(err)\r\n\t\t}\r\n\t\treturn token\r\n\t}\r\n\r\n\tcreateClientJwt := func() string {\r\n\t\tac := jwt.NewAccountClaims(clientPK)\r\n\t\tac.Imports.Add(&jwt.Import{Account: srvPK, Subject: \"foo\", Type: jwt.Service, Token: createImportToken(\"foo\", jwt.Service)})\r\n\t\tac.Imports.Add(&jwt.Import{Account: srvPK, Subject: \"bar\", Type: jwt.Stream, Token: createImportToken(\"bar\", jwt.Stream)})\r\n\t\ttoken, err := ac.Encode(okp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating importer JWT: %v\", err)\r\n\t\t}\r\n\t\treturn token\r\n\t}\r\n\r\n\tsrvJWT, _ := createSrvJwt(srvSignerPK)\r\n\taddAccountToMemResolver(s, srvPK, srvJWT)\r\n\r\n\tclientJWT := createClientJwt()\r\n\taddAccountToMemResolver(s, clientPK, clientJWT)\r\n\r\n\t// Create a client that will send the request\r\n\tclient, clientReader, clientCS := createClient(t, s, clientKP)\r\n\tdefer client.close()\r\n\tclient.parseAsync(clientCS)\r\n\texpectPong(t, clientReader)\r\n\r\n\tcheckShadow := func(expected int) {\r\n\t\tt.Helper()\r\n\t\tclient.mu.Lock()\r\n\t\tdefer client.mu.Unlock()\r\n\t\tsub := client.subs[\"1\"]\r\n\t\tcount := 0\r\n\t\tif sub != nil {\r\n\t\t\tcount = len(sub.shadow)\r\n\t\t}\r\n\t\tif count != expected {\r\n\t\t\tt.Fatalf(\"Expected shadows to be %d, got %d\", expected, count)\r\n\t\t}\r\n\t}\r\n\r\n\tcheckShadow(0)\r\n\t// Create the client that will respond to the requests.\r\n\tsrv, srvReader, srvCS := createClient(t, s, srvKP)\r\n\tdefer srv.close()\r\n\tsrv.parseAsync(srvCS)\r\n\texpectPong(t, srvReader)\r\n\r\n\t// Create Subscriber.\r\n\tsrv.parseAsync(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, srvReader)\r\n\r\n\t// Send Request\r\n\tclient.parseAsync(\"PUB foo 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, clientReader)\r\n\r\n\t// We should receive the request. PING needed to flush.\r\n\tsrv.parseAsync(\"PING\\r\\n\")\r\n\texpectMsg(t, srvReader, \"foo\", \"hi\")\r\n\r\n\tclient.parseAsync(\"SUB bar 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, clientReader)\r\n\tcheckShadow(1)\r\n\r\n\tsrv.parseAsync(\"PUB bar 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, srvReader)\r\n\r\n\t// We should receive from stream. PING needed to flush.\r\n\tclient.parseAsync(\"PING\\r\\n\")\r\n\texpectMsg(t, clientReader, \"bar\", \"hi\")\r\n\r\n\t// Now update the exported service no signer\r\n\tsrvJWT, srvAC := createSrvJwt()\r\n\taddAccountToMemResolver(s, srvPK, srvJWT)\r\n\tacc, _ := s.LookupAccount(srvPK)\r\n\ts.updateAccountClaims(acc, srvAC)\r\n\r\n\t// Send Another Request\r\n\tclient.parseAsync(\"PUB foo 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, clientReader)\r\n\r\n\t// We should not receive the request this time.\r\n\tsrv.parseAsync(\"PING\\r\\n\")\r\n\texpectPong(t, srvReader)\r\n\r\n\t// Publish on the stream\r\n\tsrv.parseAsync(\"PUB bar 2\\r\\nhi\\r\\nPING\\r\\n\")\r\n\texpectPong(t, srvReader)\r\n\r\n\t// We should not receive from the stream this time\r\n\tclient.parseAsync(\"PING\\r\\n\")\r\n\texpectPong(t, clientReader)\r\n\tcheckShadow(0)\r\n}\r\n\r\nfunc TestJWTAccountImportSignerDeadlock(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Exporter keys\r\n\tsrvKP, _ := nkeys.CreateAccount()\r\n\tsrvPK, _ := srvKP.PublicKey()\r\n\tsrvSignerKP, _ := nkeys.CreateAccount()\r\n\tsrvSignerPK, _ := srvSignerKP.PublicKey()\r\n\r\n\t// Importer keys\r\n\tclientKP, _ := nkeys.CreateAccount()\r\n\tclientPK, _ := clientKP.PublicKey()\r\n\r\n\tcreateSrvJwt := func(signingKeys ...string) (string, *jwt.AccountClaims) {\r\n\t\tac := jwt.NewAccountClaims(srvPK)\r\n\t\tac.SigningKeys.Add(signingKeys...)\r\n\t\tac.Exports.Add(&jwt.Export{Subject: \"foo\", Type: jwt.Service, TokenReq: true})\r\n\t\tac.Exports.Add(&jwt.Export{Subject: \"bar\", Type: jwt.Stream, TokenReq: true})\r\n\t\ttoken, err := ac.Encode(okp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating exporter JWT: %v\", err)\r\n\t\t}\r\n\t\treturn token, ac\r\n\t}\r\n\r\n\tcreateImportToken := func(sub string, kind jwt.ExportType) string {\r\n\t\tactC := jwt.NewActivationClaims(clientPK)\r\n\t\tactC.IssuerAccount = srvPK\r\n\t\tactC.ImportType = kind\r\n\t\tactC.ImportSubject = jwt.Subject(sub)\r\n\t\ttoken, err := actC.Encode(srvSignerKP)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatal(err)\r\n\t\t}\r\n\t\treturn token\r\n\t}\r\n\r\n\tcreateClientJwt := func() string {\r\n\t\tac := jwt.NewAccountClaims(clientPK)\r\n\t\tac.Imports.Add(&jwt.Import{Account: srvPK, Subject: \"foo\", Type: jwt.Service, Token: createImportToken(\"foo\", jwt.Service)})\r\n\t\tac.Imports.Add(&jwt.Import{Account: srvPK, Subject: \"bar\", Type: jwt.Stream, Token: createImportToken(\"bar\", jwt.Stream)})\r\n\t\ttoken, err := ac.Encode(okp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating importer JWT: %v\", err)\r\n\t\t}\r\n\t\treturn token\r\n\t}\r\n\r\n\tsrvJWT, _ := createSrvJwt(srvSignerPK)\r\n\taddAccountToMemResolver(s, srvPK, srvJWT)\r\n\r\n\tclientJWT := createClientJwt()\r\n\taddAccountToMemResolver(s, clientPK, clientJWT)\r\n\r\n\tacc, _ := s.LookupAccount(srvPK)\r\n\t// Have a go routine that constantly gets/releases the acc's write lock.\r\n\t// There was a bug that could cause AddServiceImportWithClaim to deadlock.\r\n\tch := make(chan bool, 1)\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(1)\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\tselect {\r\n\t\t\tcase <-ch:\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t\tacc.mu.Lock()\r\n\t\t\t\tacc.mu.Unlock()\r\n\t\t\t\ttime.Sleep(time.Millisecond)\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\t// Create a client that will send the request\r\n\tclient, clientReader, clientCS := createClient(t, s, clientKP)\r\n\tdefer client.close()\r\n\tclient.parseAsync(clientCS)\r\n\texpectPong(t, clientReader)\r\n\r\n\tclose(ch)\r\n\twg.Wait()\r\n}\r\n\r\nfunc TestJWTAccountImportWrongIssuerAccount(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tl := &captureErrorLogger{errCh: make(chan string, 2)}\r\n\ts.SetLogger(l, false, false)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Exporter keys\r\n\tsrvKP, _ := nkeys.CreateAccount()\r\n\tsrvPK, _ := srvKP.PublicKey()\r\n\tsrvSignerKP, _ := nkeys.CreateAccount()\r\n\tsrvSignerPK, _ := srvSignerKP.PublicKey()\r\n\r\n\t// Importer keys\r\n\tclientKP, _ := nkeys.CreateAccount()\r\n\tclientPK, _ := clientKP.PublicKey()\r\n\r\n\tcreateSrvJwt := func(signingKeys ...string) (string, *jwt.AccountClaims) {\r\n\t\tac := jwt.NewAccountClaims(srvPK)\r\n\t\tac.SigningKeys.Add(signingKeys...)\r\n\t\tac.Exports.Add(&jwt.Export{Subject: \"foo\", Type: jwt.Service, TokenReq: true})\r\n\t\tac.Exports.Add(&jwt.Export{Subject: \"bar\", Type: jwt.Stream, TokenReq: true})\r\n\t\ttoken, err := ac.Encode(okp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating exporter JWT: %v\", err)\r\n\t\t}\r\n\t\treturn token, ac\r\n\t}\r\n\r\n\tcreateImportToken := func(sub string, kind jwt.ExportType) string {\r\n\t\tactC := jwt.NewActivationClaims(clientPK)\r\n\t\t// Reference ourselves, which is wrong.\r\n\t\tactC.IssuerAccount = clientPK\r\n\t\tactC.ImportType = kind\r\n\t\tactC.ImportSubject = jwt.Subject(sub)\r\n\t\ttoken, err := actC.Encode(srvSignerKP)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatal(err)\r\n\t\t}\r\n\t\treturn token\r\n\t}\r\n\r\n\tcreateClientJwt := func() string {\r\n\t\tac := jwt.NewAccountClaims(clientPK)\r\n\t\tac.Imports.Add(&jwt.Import{Account: srvPK, Subject: \"foo\", Type: jwt.Service, Token: createImportToken(\"foo\", jwt.Service)})\r\n\t\tac.Imports.Add(&jwt.Import{Account: srvPK, Subject: \"bar\", Type: jwt.Stream, Token: createImportToken(\"bar\", jwt.Stream)})\r\n\t\ttoken, err := ac.Encode(okp)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error generating importer JWT: %v\", err)\r\n\t\t}\r\n\t\treturn token\r\n\t}\r\n\r\n\tsrvJWT, _ := createSrvJwt(srvSignerPK)\r\n\taddAccountToMemResolver(s, srvPK, srvJWT)\r\n\r\n\tclientJWT := createClientJwt()\r\n\taddAccountToMemResolver(s, clientPK, clientJWT)\r\n\r\n\t// Create a client that will send the request\r\n\tclient, clientReader, clientCS := createClient(t, s, clientKP)\r\n\tdefer client.close()\r\n\tclient.parseAsync(clientCS)\r\n\texpectPong(t, clientReader)\r\n\r\n\tfor i := 0; i < 2; i++ {\r\n\t\tselect {\r\n\t\tcase e := <-l.errCh:\r\n\t\t\tif !strings.HasPrefix(e, fmt.Sprintf(\"Invalid issuer account %q in activation claim\", clientPK)) {\r\n\t\t\t\tt.Fatalf(\"Unexpected error: %v\", e)\r\n\t\t\t}\r\n\t\tcase <-time.After(2 * time.Second):\r\n\t\t\tt.Fatalf(\"Did not get error regarding issuer account\")\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserRevokedOnAccountUpdate(t *testing.T) {\r\n\tnac := newJWTTestAccountClaims()\r\n\ts, akp, c, cr := setupJWTTestWitAccountClaims(t, nac, \"+OK\")\r\n\tdefer s.Shutdown()\r\n\tdefer c.close()\r\n\r\n\texpectPong(t, cr)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\tapub, _ := akp.PublicKey()\r\n\r\n\tc.mu.Lock()\r\n\tpub := c.user.Nkey\r\n\tc.mu.Unlock()\r\n\r\n\t// Now revoke the user.\r\n\tnac.Revoke(pub)\r\n\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\t// Update the account on the server.\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\tacc, err := s.LookupAccount(apub)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error looking up the account: %v\", err)\r\n\t}\r\n\r\n\t// This is simulating a system update for the account claims.\r\n\tgo s.updateAccountWithClaimJWT(acc, ajwt)\r\n\r\n\tl, _ := cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\tif !strings.Contains(l, \"Revoked\") {\r\n\t\tt.Fatalf(\"Expected 'Revoked' to be in the error\")\r\n\t}\r\n}\r\n\r\nfunc TestJWTUserRevoked(t *testing.T) {\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create a new user that we will make sure has been revoked.\r\n\tnkp, _ := nkeys.CreateUser()\r\n\tpub, _ := nkp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\t// Revoke the user right away.\r\n\tnac.Revoke(pub)\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\t// Sign for the user.\r\n\tjwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\taddAccountToMemResolver(s, apub, ajwt)\r\n\r\n\tc, cr, l := newClientForServer(s)\r\n\tdefer c.close()\r\n\r\n\t// Sign Nonce\r\n\tvar info nonceInfo\r\n\tjson.Unmarshal([]byte(l[5:]), &info)\r\n\tsigraw, _ := nkp.Sign([]byte(info.Nonce))\r\n\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\r\n\r\n\t// PING needed to flush the +OK/-ERR to us.\r\n\tcs := fmt.Sprintf(\"CONNECT {\\\"jwt\\\":%q,\\\"sig\\\":\\\"%s\\\"}\\r\\nPING\\r\\n\", jwt, sig)\r\n\r\n\tc.parseAsync(cs)\r\n\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"-ERR \") {\r\n\t\tt.Fatalf(\"Expected an error\")\r\n\t}\r\n\tif !strings.Contains(l, \"Authorization\") {\r\n\t\tt.Fatalf(\"Expected 'Revoked' to be in the error\")\r\n\t}\r\n}\r\n\r\n// Test that an account update that revokes an import authorization cancels the import.\r\nfunc TestJWTImportTokenRevokedAfter(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\r\n\t// Now create Exports.\r\n\texport := &jwt.Export{Subject: \"foo.private\", Type: jwt.Stream, TokenReq: true}\r\n\r\n\tfooAC.Exports.Add(export)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\tsimport := &jwt.Import{Account: fooPub, Subject: \"foo.private\", Type: jwt.Stream}\r\n\r\n\tactivation := jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"foo.private\"\r\n\tactivation.ImportType = jwt.Stream\r\n\tactJWT, err := activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\r\n\tsimport.Token = actJWT\r\n\tbarAC.Imports.Add(simport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\t// Now revoke the export.\r\n\tdecoded, _ := jwt.DecodeActivationClaims(actJWT)\r\n\texport.Revoke(decoded.Subject)\r\n\r\n\tfooJWT, err = fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tfooAcc, _ := s.LookupAccount(fooPub)\r\n\tif fooAcc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\t// Now lookup bar account and make sure it was revoked.\r\n\tacc, _ := s.LookupAccount(barPub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\tif les := len(acc.imports.streams); les != 0 {\r\n\t\tt.Fatalf(\"Expected imports streams len of 0, got %d\", les)\r\n\t}\r\n}\r\n\r\n// Test that an account update that revokes an import authorization cancels the import.\r\nfunc TestJWTImportTokenRevokedBefore(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\r\n\t// Now create Exports.\r\n\texport := &jwt.Export{Subject: \"foo.private\", Type: jwt.Stream, TokenReq: true}\r\n\r\n\tfooAC.Exports.Add(export)\r\n\r\n\t// Import account\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\tsimport := &jwt.Import{Account: fooPub, Subject: \"foo.private\", Type: jwt.Stream}\r\n\r\n\tactivation := jwt.NewActivationClaims(barPub)\r\n\tactivation.ImportSubject = \"foo.private\"\r\n\tactivation.ImportType = jwt.Stream\r\n\tactJWT, err := activation.Encode(fooKP)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating activation token: %v\", err)\r\n\t}\r\n\r\n\tsimport.Token = actJWT\r\n\tbarAC.Imports.Add(simport)\r\n\r\n\t// Now revoke the export.\r\n\tdecoded, _ := jwt.DecodeActivationClaims(actJWT)\r\n\texport.Revoke(decoded.Subject)\r\n\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\tfooAcc, _ := s.LookupAccount(fooPub)\r\n\tif fooAcc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\r\n\t// Now lookup bar account and make sure it was revoked.\r\n\tacc, _ := s.LookupAccount(barPub)\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to retrieve the account\")\r\n\t}\r\n\tif les := len(acc.imports.streams); les != 0 {\r\n\t\tt.Fatalf(\"Expected imports streams len of 0, got %d\", les)\r\n\t}\r\n}\r\n\r\nfunc TestJWTCircularAccountServiceImport(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\r\n\tbarKP, _ := nkeys.CreateAccount()\r\n\tbarPub, _ := barKP.PublicKey()\r\n\tbarAC := jwt.NewAccountClaims(barPub)\r\n\r\n\t// Create service export/import for account foo\r\n\tserviceExport := &jwt.Export{Subject: \"foo\", Type: jwt.Service, TokenReq: true}\r\n\tserviceImport := &jwt.Import{Account: barPub, Subject: \"bar\", Type: jwt.Service}\r\n\r\n\tfooAC.Exports.Add(serviceExport)\r\n\tfooAC.Imports.Add(serviceImport)\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\t// Create service export/import for account bar\r\n\tserviceExport = &jwt.Export{Subject: \"bar\", Type: jwt.Service, TokenReq: true}\r\n\tserviceImport = &jwt.Import{Account: fooPub, Subject: \"foo\", Type: jwt.Service}\r\n\r\n\tbarAC.Exports.Add(serviceExport)\r\n\tbarAC.Imports.Add(serviceImport)\r\n\tbarJWT, err := barAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\taddAccountToMemResolver(s, barPub, barJWT)\r\n\r\n\tc, cr, cs := createClient(t, s, fooKP)\r\n\tdefer c.close()\r\n\r\n\tc.parseAsync(cs)\r\n\texpectPong(t, cr)\r\n\r\n\tc.parseAsync(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpectPong(t, cr)\r\n}\r\n\r\n// This test ensures that connected clients are properly evicted\r\n// (no deadlock) if the max conns of an account has been lowered\r\n// and the account is being updated (following expiration during\r\n// a lookup).\r\nfunc TestJWTAccountLimitsMaxConnsAfterExpired(t *testing.T) {\r\n\ts := opTrustBasicSetup()\r\n\tdefer s.Shutdown()\r\n\tbuildMemAccResolver(s)\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create accounts and imports/exports.\r\n\tfooKP, _ := nkeys.CreateAccount()\r\n\tfooPub, _ := fooKP.PublicKey()\r\n\tfooAC := jwt.NewAccountClaims(fooPub)\r\n\tfooAC.Limits.Conn = 10\r\n\tfooJWT, err := fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\tnewClient := func(expPre string) *testAsyncClient {\r\n\t\tt.Helper()\r\n\t\t// Create a client.\r\n\t\tc, cr, cs := createClient(t, s, fooKP)\r\n\t\tc.parseAsync(cs)\r\n\t\tl, _ := cr.ReadString('\\n')\r\n\t\tif !strings.HasPrefix(l, expPre) {\r\n\t\t\tt.Fatalf(\"Expected a response starting with %q, got %q\", expPre, l)\r\n\t\t}\r\n\t\tgo func() {\r\n\t\t\tfor {\r\n\t\t\t\tif _, _, err := cr.ReadLine(); err != nil {\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}()\r\n\t\treturn c\r\n\t}\r\n\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tc := newClient(\"PONG\")\r\n\t\tdefer c.close()\r\n\t}\r\n\r\n\t// We will simulate that the account has expired. When\r\n\t// a new client will connect, the server will do a lookup\r\n\t// and find the account expired, which then will cause\r\n\t// a fetch and a rebuild of the account. Since max conns\r\n\t// is now lower, some clients should have been removed.\r\n\tacc, _ := s.LookupAccount(fooPub)\r\n\tacc.mu.Lock()\r\n\tacc.expired = true\r\n\tacc.mu.Unlock()\r\n\r\n\t// Now update with new expiration and max connections lowered to 2\r\n\tfooAC.Limits.Conn = 2\r\n\tfooJWT, err = fooAC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\taddAccountToMemResolver(s, fooPub, fooJWT)\r\n\r\n\t// Cause the lookup that will detect that account was expired\r\n\t// and rebuild it, and kick clients out.\r\n\tc := newClient(\"-ERR \")\r\n\tdefer c.close()\r\n\r\n\tacc, _ = s.LookupAccount(fooPub)\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tacc.mu.RLock()\r\n\t\tnumClients := len(acc.clients)\r\n\t\tacc.mu.RUnlock()\r\n\t\tif numClients != 2 {\r\n\t\t\treturn fmt.Errorf(\"Should have 2 clients, got %v\", numClients)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/jwt_test.go b/server/gnatsd/server/jwt_test.go
--- a/server/gnatsd/server/jwt_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/jwt_test.go	(date 1665399050129)
@@ -27,7 +27,7 @@
 	"testing"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: client/nats/nats.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2012-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\n// A Go client for the NATS messaging system (https://nats.io).\r\npackage nats\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"crypto/tls\"\r\n\t\"crypto/x509\"\r\n\t\"encoding/base64\"\r\n\t\"encoding/json\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\t\"math/rand\"\r\n\t\"net\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"runtime\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"time\"\r\n\r\n\t\"github.com/kubemq-io/broker/client/nats/util\"\r\n\t\"github.com/kubemq-io/broker/pkg/nuid\"\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\n// Default Constants\r\nconst (\r\n\tVersion                   = \"1.10.0\"\r\n\tDefaultURL                = \"nats://127.0.0.1:4222\"\r\n\tDefaultPort               = 4222\r\n\tDefaultMaxReconnect       = 60\r\n\tDefaultReconnectWait      = 2 * time.Second\r\n\tDefaultReconnectJitter    = 100 * time.Millisecond\r\n\tDefaultReconnectJitterTLS = time.Second\r\n\tDefaultTimeout            = 2 * time.Second\r\n\tDefaultPingInterval       = 2 * time.Minute\r\n\tDefaultMaxPingOut         = 2\r\n\tDefaultMaxChanLen         = 8192            // 8k\r\n\tDefaultReconnectBufSize   = 8 * 1024 * 1024 // 8MB\r\n\tRequestChanLen            = 8\r\n\tDefaultDrainTimeout       = 30 * time.Second\r\n\tLangString                = \"go\"\r\n)\r\n\r\nconst (\r\n\t// STALE_CONNECTION is for detection and proper handling of stale connections.\r\n\tSTALE_CONNECTION = \"stale connection\"\r\n\r\n\t// PERMISSIONS_ERR is for when nats server subject authorization has failed.\r\n\tPERMISSIONS_ERR = \"permissions violation\"\r\n\r\n\t// AUTHORIZATION_ERR is for when nats server user authorization has failed.\r\n\tAUTHORIZATION_ERR = \"authorization violation\"\r\n\r\n\t// AUTHENTICATION_EXPIRED_ERR is for when nats server user authorization has expired.\r\n\tAUTHENTICATION_EXPIRED_ERR = \"user authentication expired\"\r\n)\r\n\r\n// Errors\r\nvar (\r\n\tErrConnectionClosed       = errors.New(\"nats: connection closed\")\r\n\tErrConnectionDraining     = errors.New(\"nats: connection draining\")\r\n\tErrDrainTimeout           = errors.New(\"nats: draining connection timed out\")\r\n\tErrConnectionReconnecting = errors.New(\"nats: connection reconnecting\")\r\n\tErrSecureConnRequired     = errors.New(\"nats: secure connection required\")\r\n\tErrSecureConnWanted       = errors.New(\"nats: secure connection not available\")\r\n\tErrBadSubscription        = errors.New(\"nats: invalid subscription\")\r\n\tErrTypeSubscription       = errors.New(\"nats: invalid subscription type\")\r\n\tErrBadSubject             = errors.New(\"nats: invalid subject\")\r\n\tErrBadQueueName           = errors.New(\"nats: invalid queue name\")\r\n\tErrSlowConsumer           = errors.New(\"nats: slow consumer, messages dropped\")\r\n\tErrTimeout                = errors.New(\"nats: timeout\")\r\n\tErrBadTimeout             = errors.New(\"nats: timeout invalid\")\r\n\tErrAuthorization          = errors.New(\"nats: authorization violation\")\r\n\tErrAuthExpired            = errors.New(\"nats: authentication expired\")\r\n\tErrNoServers              = errors.New(\"nats: no servers available for connection\")\r\n\tErrJsonParse              = errors.New(\"nats: connect message, json parse error\")\r\n\tErrChanArg                = errors.New(\"nats: argument needs to be a channel type\")\r\n\tErrMaxPayload             = errors.New(\"nats: maximum payload exceeded\")\r\n\tErrMaxMessages            = errors.New(\"nats: maximum messages delivered\")\r\n\tErrSyncSubRequired        = errors.New(\"nats: illegal call on an async subscription\")\r\n\tErrMultipleTLSConfigs     = errors.New(\"nats: multiple tls.Configs not allowed\")\r\n\tErrNoInfoReceived         = errors.New(\"nats: protocol exception, INFO not received\")\r\n\tErrReconnectBufExceeded   = errors.New(\"nats: outbound buffer limit exceeded\")\r\n\tErrInvalidConnection      = errors.New(\"nats: invalid connection\")\r\n\tErrInvalidMsg             = errors.New(\"nats: invalid message or message nil\")\r\n\tErrInvalidArg             = errors.New(\"nats: invalid argument\")\r\n\tErrInvalidContext         = errors.New(\"nats: invalid context\")\r\n\tErrNoDeadlineContext      = errors.New(\"nats: context requires a deadline\")\r\n\tErrNoEchoNotSupported     = errors.New(\"nats: no echo option not supported by this server\")\r\n\tErrClientIDNotSupported   = errors.New(\"nats: client ID not supported by this server\")\r\n\tErrUserButNoSigCB         = errors.New(\"nats: user callback defined without a signature handler\")\r\n\tErrNkeyButNoSigCB         = errors.New(\"nats: nkey defined without a signature handler\")\r\n\tErrNoUserCB               = errors.New(\"nats: user callback not defined\")\r\n\tErrNkeyAndUser            = errors.New(\"nats: user callback and nkey defined\")\r\n\tErrNkeysNotSupported      = errors.New(\"nats: nkeys not supported by the server\")\r\n\tErrStaleConnection        = errors.New(\"nats: \" + STALE_CONNECTION)\r\n\tErrTokenAlreadySet        = errors.New(\"nats: token and token handler both set\")\r\n\tErrMsgNotBound            = errors.New(\"nats: message is not bound to subscription/connection\")\r\n\tErrMsgNoReply             = errors.New(\"nats: message does not have a reply\")\r\n\tErrClientIPNotSupported   = errors.New(\"nats: client IP not supported by this server\")\r\n\tErrDisconnected           = errors.New(\"nats: server is disconnected\")\r\n)\r\n\r\nfunc init() {\r\n\trand.Seed(time.Now().UnixNano())\r\n}\r\n\r\n// GetDefaultOptions returns default configuration options for the client.\r\nfunc GetDefaultOptions() Options {\r\n\treturn Options{\r\n\t\tAllowReconnect:     true,\r\n\t\tMaxReconnect:       DefaultMaxReconnect,\r\n\t\tReconnectWait:      DefaultReconnectWait,\r\n\t\tReconnectJitter:    DefaultReconnectJitter,\r\n\t\tReconnectJitterTLS: DefaultReconnectJitterTLS,\r\n\t\tTimeout:            DefaultTimeout,\r\n\t\tPingInterval:       DefaultPingInterval,\r\n\t\tMaxPingsOut:        DefaultMaxPingOut,\r\n\t\tSubChanLen:         DefaultMaxChanLen,\r\n\t\tReconnectBufSize:   DefaultReconnectBufSize,\r\n\t\tDrainTimeout:       DefaultDrainTimeout,\r\n\t}\r\n}\r\n\r\n// DEPRECATED: Use GetDefaultOptions() instead.\r\n// DefaultOptions is not safe for use by multiple clients.\r\n// For details see #308.\r\nvar DefaultOptions = GetDefaultOptions()\r\n\r\n// Status represents the state of the connection.\r\ntype Status int\r\n\r\nconst (\r\n\tDISCONNECTED = Status(iota)\r\n\tCONNECTED\r\n\tCLOSED\r\n\tRECONNECTING\r\n\tCONNECTING\r\n\tDRAINING_SUBS\r\n\tDRAINING_PUBS\r\n)\r\n\r\n// ConnHandler is used for asynchronous events such as\r\n// disconnected and closed connections.\r\ntype ConnHandler func(*Conn)\r\n\r\n// ConnErrHandler is used to process asynchronous events like\r\n// disconnected connection with the error (if any).\r\ntype ConnErrHandler func(*Conn, error)\r\n\r\n// ErrHandler is used to process asynchronous errors encountered\r\n// while processing inbound messages.\r\ntype ErrHandler func(*Conn, *Subscription, error)\r\n\r\n// UserJWTHandler is used to fetch and return the account signed\r\n// JWT for this user.\r\ntype UserJWTHandler func() (string, error)\r\n\r\n// SignatureHandler is used to sign a nonce from the server while\r\n// authenticating with nkeys. The user should sign the nonce and\r\n// return the raw signature. The client will base64 encode this to\r\n// send to the server.\r\ntype SignatureHandler func([]byte) ([]byte, error)\r\n\r\n// AuthTokenHandler is used to generate a new token.\r\ntype AuthTokenHandler func() string\r\n\r\n// ReconnectDelayHandler is used to get from the user the desired\r\n// delay the library should pause before attempting to reconnect\r\n// again. Note that this is invoked after the library tried the\r\n// whole list of URLs and failed to reconnect.\r\ntype ReconnectDelayHandler func(attempts int) time.Duration\r\n\r\n// asyncCB is used to preserve order for async callbacks.\r\ntype asyncCB struct {\r\n\tf    func()\r\n\tnext *asyncCB\r\n}\r\n\r\ntype asyncCallbacksHandler struct {\r\n\tmu   sync.Mutex\r\n\tcond *sync.Cond\r\n\thead *asyncCB\r\n\ttail *asyncCB\r\n}\r\n\r\n// Option is a function on the options for a connection.\r\ntype Option func(*Options) error\r\n\r\n// CustomDialer can be used to specify any dialer, not necessarily\r\n// a *net.Dialer.\r\ntype CustomDialer interface {\r\n\tDial(network, address string) (net.Conn, error)\r\n}\r\n\r\n// Options can be used to create a customized connection.\r\ntype Options struct {\r\n\r\n\t// Url represents a single NATS server url to which the client\r\n\t// will be connecting. If the Servers option is also set, it\r\n\t// then becomes the first server in the Servers array.\r\n\tUrl string\r\n\r\n\t// Servers is a configured set of servers which this client\r\n\t// will use when attempting to connect.\r\n\tServers []string\r\n\r\n\t// NoRandomize configures whether we will randomize the\r\n\t// server pool.\r\n\tNoRandomize bool\r\n\r\n\t// NoEcho configures whether the server will echo back messages\r\n\t// that are sent on this connection if we also have matching subscriptions.\r\n\t// Note this is supported on servers >= version 1.2. Proto 1 or greater.\r\n\tNoEcho bool\r\n\r\n\t// Name is an optional name label which will be sent to the server\r\n\t// on CONNECT to identify the client.\r\n\tName string\r\n\r\n\t// Verbose signals the server to send an OK ack for commands\r\n\t// successfully processed by the server.\r\n\tVerbose bool\r\n\r\n\t// Pedantic signals the server whether it should be doing further\r\n\t// validation of subjects.\r\n\tPedantic bool\r\n\r\n\t// Secure enables TLS secure connections that skip server\r\n\t// verification by default. NOT RECOMMENDED.\r\n\tSecure bool\r\n\r\n\t// TLSConfig is a custom TLS configuration to use for secure\r\n\t// transports.\r\n\tTLSConfig *tls.Config\r\n\r\n\t// AllowReconnect enables reconnection logic to be used when we\r\n\t// encounter a disconnect from the current server.\r\n\tAllowReconnect bool\r\n\r\n\t// MaxReconnect sets the number of reconnect attempts that will be\r\n\t// tried before giving up. If negative, then it will never give up\r\n\t// trying to reconnect.\r\n\tMaxReconnect int\r\n\r\n\t// ReconnectWait sets the time to backoff after attempting a reconnect\r\n\t// to a server that we were already connected to previously.\r\n\tReconnectWait time.Duration\r\n\r\n\t// CustomReconnectDelayCB is invoked after the library tried every\r\n\t// URL in the server list and failed to reconnect. It passes to the\r\n\t// user the current number of attempts. This function returns the\r\n\t// amount of time the library will sleep before attempting to reconnect\r\n\t// again. It is strongly recommended that this value contains some\r\n\t// jitter to prevent all connections to attempt reconnecting at the same time.\r\n\tCustomReconnectDelayCB ReconnectDelayHandler\r\n\r\n\t// ReconnectJitter sets the upper bound for a random delay added to\r\n\t// ReconnectWait during a reconnect when no TLS is used.\r\n\t// Note that any jitter is capped with ReconnectJitterMax.\r\n\tReconnectJitter time.Duration\r\n\r\n\t// ReconnectJitterTLS sets the upper bound for a random delay added to\r\n\t// ReconnectWait during a reconnect when TLS is used.\r\n\t// Note that any jitter is capped with ReconnectJitterMax.\r\n\tReconnectJitterTLS time.Duration\r\n\r\n\t// Timeout sets the timeout for a Dial operation on a connection.\r\n\tTimeout time.Duration\r\n\r\n\t// DrainTimeout sets the timeout for a Drain Operation to complete.\r\n\tDrainTimeout time.Duration\r\n\r\n\t// FlusherTimeout is the maximum time to wait for write operations\r\n\t// to the underlying connection to complete (including the flusher loop).\r\n\tFlusherTimeout time.Duration\r\n\r\n\t// PingInterval is the period at which the client will be sending ping\r\n\t// commands to the server, disabled if 0 or negative.\r\n\tPingInterval time.Duration\r\n\r\n\t// MaxPingsOut is the maximum number of pending ping commands that can\r\n\t// be awaiting a response before raising an ErrStaleConnection error.\r\n\tMaxPingsOut int\r\n\r\n\t// ClosedCB sets the closed handler that is called when a client will\r\n\t// no longer be connected.\r\n\tClosedCB ConnHandler\r\n\r\n\t// DisconnectedCB sets the disconnected handler that is called\r\n\t// whenever the connection is disconnected.\r\n\t// Will not be called if DisconnectedErrCB is set\r\n\t// DEPRECATED. Use DisconnectedErrCB which passes error that caused\r\n\t// the disconnect event.\r\n\tDisconnectedCB ConnHandler\r\n\r\n\t// DisconnectedErrCB sets the disconnected error handler that is called\r\n\t// whenever the connection is disconnected.\r\n\t// Disconnected error could be nil, for instance when user explicitly closes the connection.\r\n\t// DisconnectedCB will not be called if DisconnectedErrCB is set\r\n\tDisconnectedErrCB ConnErrHandler\r\n\r\n\t// ReconnectedCB sets the reconnected handler called whenever\r\n\t// the connection is successfully reconnected.\r\n\tReconnectedCB ConnHandler\r\n\r\n\t// DiscoveredServersCB sets the callback that is invoked whenever a new\r\n\t// server has joined the cluster.\r\n\tDiscoveredServersCB ConnHandler\r\n\r\n\t// AsyncErrorCB sets the async error handler (e.g. slow consumer errors)\r\n\tAsyncErrorCB ErrHandler\r\n\r\n\t// ReconnectBufSize is the size of the backing bufio during reconnect.\r\n\t// Once this has been exhausted publish operations will return an error.\r\n\tReconnectBufSize int\r\n\r\n\t// SubChanLen is the size of the buffered channel used between the socket\r\n\t// Go routine and the message delivery for SyncSubscriptions.\r\n\t// NOTE: This does not affect AsyncSubscriptions which are\r\n\t// dictated by PendingLimits()\r\n\tSubChanLen int\r\n\r\n\t// UserJWT sets the callback handler that will fetch a user's JWT.\r\n\tUserJWT UserJWTHandler\r\n\r\n\t// Nkey sets the public nkey that will be used to authenticate\r\n\t// when connecting to the server. UserJWT and Nkey are mutually exclusive\r\n\t// and if defined, UserJWT will take precedence.\r\n\tNkey string\r\n\r\n\t// SignatureCB designates the function used to sign the nonce\r\n\t// presented from the server.\r\n\tSignatureCB SignatureHandler\r\n\r\n\t// User sets the username to be used when connecting to the server.\r\n\tUser string\r\n\r\n\t// Password sets the password to be used when connecting to a server.\r\n\tPassword string\r\n\r\n\t// Token sets the token to be used when connecting to a server.\r\n\tToken string\r\n\r\n\t// TokenHandler designates the function used to generate the token to be used when connecting to a server.\r\n\tTokenHandler AuthTokenHandler\r\n\r\n\t// Dialer allows a custom net.Dialer when forming connections.\r\n\t// DEPRECATED: should use CustomDialer instead.\r\n\tDialer *net.Dialer\r\n\r\n\t// CustomDialer allows to specify a custom dialer (not necessarily\r\n\t// a *net.Dialer).\r\n\tCustomDialer CustomDialer\r\n\r\n\t// UseOldRequestStyle forces the old method of Requests that utilize\r\n\t// a new Inbox and a new Subscription for each request.\r\n\tUseOldRequestStyle bool\r\n\r\n\t// NoCallbacksAfterClientClose allows preventing the invocation of\r\n\t// callbacks after Close() is called. Client won't receive notifications\r\n\t// when Close is invoked by user code. Default is to invoke the callbacks.\r\n\tNoCallbacksAfterClientClose bool\r\n}\r\n\r\nconst (\r\n\t// Scratch storage for assembling protocol headers\r\n\tscratchSize = 512\r\n\r\n\t// The size of the bufio reader/writer on top of the socket.\r\n\tdefaultBufSize = 32768\r\n\r\n\t// The buffered size of the flush \"kick\" channel\r\n\tflushChanSize = 1\r\n\r\n\t// Default server pool size\r\n\tsrvPoolSize = 4\r\n\r\n\t// NUID size\r\n\tnuidSize = 22\r\n\r\n\t// Default port used if none is specified in given URL(s)\r\n\tdefaultPortString = \"4222\"\r\n)\r\n\r\n// A Conn represents a bare connection to a nats-server.\r\n// It can send and receive []byte payloads.\r\n// The connection is safe to use in multiple Go routines concurrently.\r\ntype Conn struct {\r\n\t// Keep all members for which we use atomic at the beginning of the\r\n\t// struct and make sure they are all 64bits (or use padding if necessary).\r\n\t// atomic.* functions crash on 32bit machines if operand is not aligned\r\n\t// at 64bit. See https://github.com/golang/go/issues/599\r\n\tStatistics\r\n\tmu sync.RWMutex\r\n\t// Opts holds the configuration of the Conn.\r\n\t// Modifying the configuration of a running Conn is a race.\r\n\tOpts    Options\r\n\twg      sync.WaitGroup\r\n\tsrvPool []*srv\r\n\tcurrent *srv\r\n\turls    map[string]struct{} // Keep track of all known URLs (used by processInfo)\r\n\tconn    net.Conn\r\n\tbw      *bufio.Writer\r\n\tpending *bytes.Buffer\r\n\tfch     chan struct{}\r\n\tinfo    serverInfo\r\n\tssid    int64\r\n\tsubsMu  sync.RWMutex\r\n\tsubs    map[int64]*Subscription\r\n\tach     *asyncCallbacksHandler\r\n\tpongs   []chan struct{}\r\n\tscratch [scratchSize]byte\r\n\tstatus  Status\r\n\tinitc   bool // true if the connection is performing the initial connect\r\n\terr     error\r\n\tps      *parseState\r\n\tptmr    *time.Timer\r\n\tpout    int\r\n\tar      bool // abort reconnect\r\n\trqch    chan struct{}\r\n\r\n\t// New style response handler\r\n\trespSub   string               // The wildcard subject\r\n\trespScanf string               // The scanf template to extract mux token\r\n\trespMux   *Subscription        // A single response subscription\r\n\trespMap   map[string]chan *Msg // Request map for the response msg channels\r\n\trespRand  *rand.Rand           // Used for generating suffix\r\n}\r\n\r\n// A Subscription represents interest in a given subject.\r\ntype Subscription struct {\r\n\tmu  sync.Mutex\r\n\tsid int64\r\n\r\n\t// Subject that represents this subscription. This can be different\r\n\t// than the received subject inside a Msg if this is a wildcard.\r\n\tSubject string\r\n\r\n\t// Optional queue group name. If present, all subscriptions with the\r\n\t// same name will form a distributed queue, and each message will\r\n\t// only be processed by one member of the group.\r\n\tQueue string\r\n\r\n\tdelivered  uint64\r\n\tmax        uint64\r\n\tconn       *Conn\r\n\tmcb        MsgHandler\r\n\tmch        chan *Msg\r\n\tclosed     bool\r\n\tsc         bool\r\n\tconnClosed bool\r\n\r\n\t// Type of Subscription\r\n\ttyp SubscriptionType\r\n\r\n\t// Async linked list\r\n\tpHead *Msg\r\n\tpTail *Msg\r\n\tpCond *sync.Cond\r\n\r\n\t// Pending stats, async subscriptions, high-speed etc.\r\n\tpMsgs       int\r\n\tpBytes      int\r\n\tpMsgsMax    int\r\n\tpBytesMax   int\r\n\tpMsgsLimit  int\r\n\tpBytesLimit int\r\n\tdropped     int\r\n}\r\n\r\n// Msg is a structure used by Subscribers and PublishMsg().\r\ntype Msg struct {\r\n\tSubject string\r\n\tReply   string\r\n\tData    []byte\r\n\tSub     *Subscription\r\n\tnext    *Msg\r\n\tbarrier *barrierInfo\r\n}\r\n\r\ntype barrierInfo struct {\r\n\trefs int64\r\n\tf    func()\r\n}\r\n\r\n// Tracks various stats received and sent on this connection,\r\n// including counts for messages and bytes.\r\ntype Statistics struct {\r\n\tInMsgs     uint64\r\n\tOutMsgs    uint64\r\n\tInBytes    uint64\r\n\tOutBytes   uint64\r\n\tReconnects uint64\r\n}\r\n\r\n// Tracks individual backend servers.\r\ntype srv struct {\r\n\turl        *url.URL\r\n\tdidConnect bool\r\n\treconnects int\r\n\tlastErr    error\r\n\tisImplicit bool\r\n\ttlsName    string\r\n}\r\n\r\ntype serverInfo struct {\r\n\tID           string   `json:\"server_id\"`\r\n\tHost         string   `json:\"host\"`\r\n\tPort         uint     `json:\"port\"`\r\n\tVersion      string   `json:\"version\"`\r\n\tAuthRequired bool     `json:\"auth_required\"`\r\n\tTLSRequired  bool     `json:\"tls_required\"`\r\n\tMaxPayload   int64    `json:\"max_payload\"`\r\n\tConnectURLs  []string `json:\"connect_urls,omitempty\"`\r\n\tProto        int      `json:\"proto,omitempty\"`\r\n\tCID          uint64   `json:\"client_id,omitempty\"`\r\n\tClientIP     string   `json:\"client_ip,omitempty\"`\r\n\tNonce        string   `json:\"nonce,omitempty\"`\r\n}\r\n\r\nconst (\r\n\t// clientProtoZero is the original client protocol from 2009.\r\n\t// http://nats.io/documentation/internals/nats-protocol/\r\n\t/* clientProtoZero */ _ = iota\r\n\t// clientProtoInfo signals a client can receive more then the original INFO block.\r\n\t// This can be used to update clients on other cluster members, etc.\r\n\tclientProtoInfo\r\n)\r\n\r\ntype connectInfo struct {\r\n\tVerbose   bool   `json:\"verbose\"`\r\n\tPedantic  bool   `json:\"pedantic\"`\r\n\tUserJWT   string `json:\"jwt,omitempty\"`\r\n\tNkey      string `json:\"nkey,omitempty\"`\r\n\tSignature string `json:\"sig,omitempty\"`\r\n\tUser      string `json:\"user,omitempty\"`\r\n\tPass      string `json:\"pass,omitempty\"`\r\n\tToken     string `json:\"auth_token,omitempty\"`\r\n\tTLS       bool   `json:\"tls_required\"`\r\n\tName      string `json:\"name\"`\r\n\tLang      string `json:\"lang\"`\r\n\tVersion   string `json:\"version\"`\r\n\tProtocol  int    `json:\"protocol\"`\r\n\tEcho      bool   `json:\"echo\"`\r\n}\r\n\r\n// MsgHandler is a callback function that processes messages delivered to\r\n// asynchronous subscribers.\r\ntype MsgHandler func(msg *Msg)\r\n\r\n// Connect will attempt to connect to the NATS system.\r\n// The url can contain username/password semantics. e.g. nats://derek:pass@localhost:4222\r\n// Comma separated arrays are also supported, e.g. urlA, urlB.\r\n// Options start with the defaults but can be overridden.\r\nfunc Connect(url string, options ...Option) (*Conn, error) {\r\n\topts := GetDefaultOptions()\r\n\topts.Servers = processUrlString(url)\r\n\tfor _, opt := range options {\r\n\t\tif opt != nil {\r\n\t\t\tif err := opt(&opts); err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn opts.Connect()\r\n}\r\n\r\n// Options that can be passed to Connect.\r\n\r\n// Name is an Option to set the client name.\r\nfunc Name(name string) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.Name = name\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// Secure is an Option to enable TLS secure connections that skip server verification by default.\r\n// Pass a TLS Configuration for proper TLS.\r\n// NOTE: This should NOT be used in a production setting.\r\nfunc Secure(tls ...*tls.Config) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.Secure = true\r\n\t\t// Use of variadic just simplifies testing scenarios. We only take the first one.\r\n\t\tif len(tls) > 1 {\r\n\t\t\treturn ErrMultipleTLSConfigs\r\n\t\t}\r\n\t\tif len(tls) == 1 {\r\n\t\t\to.TLSConfig = tls[0]\r\n\t\t}\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// RootCAs is a helper option to provide the RootCAs pool from a list of filenames.\r\n// If Secure is not already set this will set it as well.\r\nfunc RootCAs(file ...string) Option {\r\n\treturn func(o *Options) error {\r\n\t\tpool := x509.NewCertPool()\r\n\t\tfor _, f := range file {\r\n\t\t\trootPEM, err := ioutil.ReadFile(f)\r\n\t\t\tif err != nil || rootPEM == nil {\r\n\t\t\t\treturn fmt.Errorf(\"nats: error loading or parsing rootCA file: %v\", err)\r\n\t\t\t}\r\n\t\t\tok := pool.AppendCertsFromPEM(rootPEM)\r\n\t\t\tif !ok {\r\n\t\t\t\treturn fmt.Errorf(\"nats: failed to parse root certificate from %q\", f)\r\n\t\t\t}\r\n\t\t}\r\n\t\tif o.TLSConfig == nil {\r\n\t\t\to.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\r\n\t\t}\r\n\t\to.TLSConfig.RootCAs = pool\r\n\t\to.Secure = true\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ClientCert is a helper option to provide the client certificate from a file.\r\n// If Secure is not already set this will set it as well.\r\nfunc ClientCert(certFile, keyFile string) Option {\r\n\treturn func(o *Options) error {\r\n\t\tcert, err := tls.LoadX509KeyPair(certFile, keyFile)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"nats: error loading client certificate: %v\", err)\r\n\t\t}\r\n\t\tcert.Leaf, err = x509.ParseCertificate(cert.Certificate[0])\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"nats: error parsing client certificate: %v\", err)\r\n\t\t}\r\n\t\tif o.TLSConfig == nil {\r\n\t\t\to.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\r\n\t\t}\r\n\t\to.TLSConfig.Certificates = []tls.Certificate{cert}\r\n\t\to.Secure = true\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// NoReconnect is an Option to turn off reconnect behavior.\r\nfunc NoReconnect() Option {\r\n\treturn func(o *Options) error {\r\n\t\to.AllowReconnect = false\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// DontRandomize is an Option to turn off randomizing the server pool.\r\nfunc DontRandomize() Option {\r\n\treturn func(o *Options) error {\r\n\t\to.NoRandomize = true\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// NoEcho is an Option to turn off messages echoing back from a server.\r\n// Note this is supported on servers >= version 1.2. Proto 1 or greater.\r\nfunc NoEcho() Option {\r\n\treturn func(o *Options) error {\r\n\t\to.NoEcho = true\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ReconnectWait is an Option to set the wait time between reconnect attempts.\r\nfunc ReconnectWait(t time.Duration) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.ReconnectWait = t\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// MaxReconnects is an Option to set the maximum number of reconnect attempts.\r\nfunc MaxReconnects(max int) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.MaxReconnect = max\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ReconnectJitter is an Option to set the upper bound of a random delay added ReconnectWait.\r\nfunc ReconnectJitter(jitter, jitterForTLS time.Duration) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.ReconnectJitter = jitter\r\n\t\to.ReconnectJitterTLS = jitterForTLS\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// CustomReconnectDelay is an Option to set the CustomReconnectDelayCB option.\r\n// See CustomReconnectDelayCB Option for more details.\r\nfunc CustomReconnectDelay(cb ReconnectDelayHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.CustomReconnectDelayCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// PingInterval is an Option to set the period for client ping commands.\r\nfunc PingInterval(t time.Duration) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.PingInterval = t\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// MaxPingsOutstanding is an Option to set the maximum number of ping requests\r\n// that can go un-answered by the server before closing the connection.\r\nfunc MaxPingsOutstanding(max int) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.MaxPingsOut = max\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ReconnectBufSize sets the buffer size of messages kept while busy reconnecting.\r\nfunc ReconnectBufSize(size int) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.ReconnectBufSize = size\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// Timeout is an Option to set the timeout for Dial on a connection.\r\nfunc Timeout(t time.Duration) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.Timeout = t\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// FlusherTimeout is an Option to set the write (and flush) timeout on a connection.\r\nfunc FlusherTimeout(t time.Duration) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.FlusherTimeout = t\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// DrainTimeout is an Option to set the timeout for draining a connection.\r\nfunc DrainTimeout(t time.Duration) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.DrainTimeout = t\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// DisconnectErrHandler is an Option to set the disconnected error handler.\r\nfunc DisconnectErrHandler(cb ConnErrHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.DisconnectedErrCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// DisconnectHandler is an Option to set the disconnected handler.\r\n// DEPRECATED: Use DisconnectErrHandler.\r\nfunc DisconnectHandler(cb ConnHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.DisconnectedCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ReconnectHandler is an Option to set the reconnected handler.\r\nfunc ReconnectHandler(cb ConnHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.ReconnectedCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ClosedHandler is an Option to set the closed handler.\r\nfunc ClosedHandler(cb ConnHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.ClosedCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// DiscoveredServersHandler is an Option to set the new servers handler.\r\nfunc DiscoveredServersHandler(cb ConnHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.DiscoveredServersCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// ErrorHandler is an Option to set the async error  handler.\r\nfunc ErrorHandler(cb ErrHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.AsyncErrorCB = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// UserInfo is an Option to set the username and password to\r\n// use when not included directly in the URLs.\r\nfunc UserInfo(user, password string) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.User = user\r\n\t\to.Password = password\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// Token is an Option to set the token to use\r\n// when a token is not included directly in the URLs\r\n// and when a token handler is not provided.\r\nfunc Token(token string) Option {\r\n\treturn func(o *Options) error {\r\n\t\tif o.TokenHandler != nil {\r\n\t\t\treturn ErrTokenAlreadySet\r\n\t\t}\r\n\t\to.Token = token\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// TokenHandler is an Option to set the token handler to use\r\n// when a token is not included directly in the URLs\r\n// and when a token is not set.\r\nfunc TokenHandler(cb AuthTokenHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\tif o.Token != \"\" {\r\n\t\t\treturn ErrTokenAlreadySet\r\n\t\t}\r\n\t\to.TokenHandler = cb\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// UserCredentials is a convenience function that takes a filename\r\n// for a user's JWT and a filename for the user's private Nkey seed.\r\nfunc UserCredentials(userOrChainedFile string, seedFiles ...string) Option {\r\n\tuserCB := func() (string, error) {\r\n\t\treturn userFromFile(userOrChainedFile)\r\n\t}\r\n\tvar keyFile string\r\n\tif len(seedFiles) > 0 {\r\n\t\tkeyFile = seedFiles[0]\r\n\t} else {\r\n\t\tkeyFile = userOrChainedFile\r\n\t}\r\n\tsigCB := func(nonce []byte) ([]byte, error) {\r\n\t\treturn sigHandler(nonce, keyFile)\r\n\t}\r\n\treturn UserJWT(userCB, sigCB)\r\n}\r\n\r\n// UserJWT will set the callbacks to retrieve the user's JWT and\r\n// the signature callback to sign the server nonce. This an the Nkey\r\n// option are mutually exclusive.\r\nfunc UserJWT(userCB UserJWTHandler, sigCB SignatureHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\tif userCB == nil {\r\n\t\t\treturn ErrNoUserCB\r\n\t\t}\r\n\t\tif sigCB == nil {\r\n\t\t\treturn ErrUserButNoSigCB\r\n\t\t}\r\n\t\to.UserJWT = userCB\r\n\t\to.SignatureCB = sigCB\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// Nkey will set the public Nkey and the signature callback to\r\n// sign the server nonce.\r\nfunc Nkey(pubKey string, sigCB SignatureHandler) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.Nkey = pubKey\r\n\t\to.SignatureCB = sigCB\r\n\t\tif pubKey != \"\" && sigCB == nil {\r\n\t\t\treturn ErrNkeyButNoSigCB\r\n\t\t}\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// SyncQueueLen will set the maximum queue len for the internal\r\n// channel used for SubscribeSync().\r\nfunc SyncQueueLen(max int) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.SubChanLen = max\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// Dialer is an Option to set the dialer which will be used when\r\n// attempting to establish a connection.\r\n// DEPRECATED: Should use CustomDialer instead.\r\nfunc Dialer(dialer *net.Dialer) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.Dialer = dialer\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// SetCustomDialer is an Option to set a custom dialer which will be\r\n// used when attempting to establish a connection. If both Dialer\r\n// and CustomDialer are specified, CustomDialer takes precedence.\r\nfunc SetCustomDialer(dialer CustomDialer) Option {\r\n\treturn func(o *Options) error {\r\n\t\to.CustomDialer = dialer\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// UseOldRequestStyle is an Option to force usage of the old Request style.\r\nfunc UseOldRequestStyle() Option {\r\n\treturn func(o *Options) error {\r\n\t\to.UseOldRequestStyle = true\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// NoCallbacksAfterClientClose is an Option to disable callbacks when user code\r\n// calls Close(). If close is initiated by any other condition, callbacks\r\n// if any will be invoked.\r\nfunc NoCallbacksAfterClientClose() Option {\r\n\treturn func(o *Options) error {\r\n\t\to.NoCallbacksAfterClientClose = true\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n// Handler processing\r\n\r\n// SetDisconnectHandler will set the disconnect event handler.\r\n// DEPRECATED: Use SetDisconnectErrHandler\r\nfunc (nc *Conn) SetDisconnectHandler(dcb ConnHandler) {\r\n\tif nc == nil {\r\n\t\treturn\r\n\t}\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.Opts.DisconnectedCB = dcb\r\n}\r\n\r\n// SetDisconnectErrHandler will set the disconnect event handler.\r\nfunc (nc *Conn) SetDisconnectErrHandler(dcb ConnErrHandler) {\r\n\tif nc == nil {\r\n\t\treturn\r\n\t}\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.Opts.DisconnectedErrCB = dcb\r\n}\r\n\r\n// SetReconnectHandler will set the reconnect event handler.\r\nfunc (nc *Conn) SetReconnectHandler(rcb ConnHandler) {\r\n\tif nc == nil {\r\n\t\treturn\r\n\t}\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.Opts.ReconnectedCB = rcb\r\n}\r\n\r\n// SetDiscoveredServersHandler will set the discovered servers handler.\r\nfunc (nc *Conn) SetDiscoveredServersHandler(dscb ConnHandler) {\r\n\tif nc == nil {\r\n\t\treturn\r\n\t}\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.Opts.DiscoveredServersCB = dscb\r\n}\r\n\r\n// SetClosedHandler will set the reconnect event handler.\r\nfunc (nc *Conn) SetClosedHandler(cb ConnHandler) {\r\n\tif nc == nil {\r\n\t\treturn\r\n\t}\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.Opts.ClosedCB = cb\r\n}\r\n\r\n// SetErrorHandler will set the async error handler.\r\nfunc (nc *Conn) SetErrorHandler(cb ErrHandler) {\r\n\tif nc == nil {\r\n\t\treturn\r\n\t}\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.Opts.AsyncErrorCB = cb\r\n}\r\n\r\n// Process the url string argument to Connect.\r\n// Return an array of urls, even if only one.\r\nfunc processUrlString(url string) []string {\r\n\turls := strings.Split(url, \",\")\r\n\tfor i, s := range urls {\r\n\t\turls[i] = strings.TrimSpace(s)\r\n\t}\r\n\treturn urls\r\n}\r\n\r\n// Connect will attempt to connect to a NATS server with multiple options.\r\nfunc (o Options) Connect() (*Conn, error) {\r\n\tnc := &Conn{Opts: o}\r\n\r\n\t// Some default options processing.\r\n\tif nc.Opts.MaxPingsOut == 0 {\r\n\t\tnc.Opts.MaxPingsOut = DefaultMaxPingOut\r\n\t}\r\n\t// Allow old default for channel length to work correctly.\r\n\tif nc.Opts.SubChanLen == 0 {\r\n\t\tnc.Opts.SubChanLen = DefaultMaxChanLen\r\n\t}\r\n\t// Default ReconnectBufSize\r\n\tif nc.Opts.ReconnectBufSize == 0 {\r\n\t\tnc.Opts.ReconnectBufSize = DefaultReconnectBufSize\r\n\t}\r\n\t// Ensure that Timeout is not 0\r\n\tif nc.Opts.Timeout == 0 {\r\n\t\tnc.Opts.Timeout = DefaultTimeout\r\n\t}\r\n\r\n\t// Check first for user jwt callback being defined and nkey.\r\n\tif nc.Opts.UserJWT != nil && nc.Opts.Nkey != \"\" {\r\n\t\treturn nil, ErrNkeyAndUser\r\n\t}\r\n\r\n\t// Check if we have an nkey but no signature callback defined.\r\n\tif nc.Opts.Nkey != \"\" && nc.Opts.SignatureCB == nil {\r\n\t\treturn nil, ErrNkeyButNoSigCB\r\n\t}\r\n\r\n\t// Allow custom Dialer for connecting using DialTimeout by default\r\n\tif nc.Opts.Dialer == nil {\r\n\t\tnc.Opts.Dialer = &net.Dialer{\r\n\t\t\tTimeout: nc.Opts.Timeout,\r\n\t\t}\r\n\t}\r\n\r\n\tif err := nc.setupServerPool(); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\t// Create the async callback handler.\r\n\tnc.ach = &asyncCallbacksHandler{}\r\n\tnc.ach.cond = sync.NewCond(&nc.ach.mu)\r\n\r\n\tif err := nc.connect(); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\t// Spin up the async cb dispatcher on success\r\n\tgo nc.ach.asyncCBDispatcher()\r\n\r\n\treturn nc, nil\r\n}\r\n\r\nconst (\r\n\t_CRLF_  = \"\\r\\n\"\r\n\t_EMPTY_ = \"\"\r\n\t_SPC_   = \" \"\r\n\t_PUB_P_ = \"PUB \"\r\n)\r\n\r\nconst (\r\n\t_OK_OP_   = \"+OK\"\r\n\t_ERR_OP_  = \"-ERR\"\r\n\t_PONG_OP_ = \"PONG\"\r\n\t_INFO_OP_ = \"INFO\"\r\n)\r\n\r\nconst (\r\n\tconProto   = \"CONNECT %s\" + _CRLF_\r\n\tpingProto  = \"PING\" + _CRLF_\r\n\tpongProto  = \"PONG\" + _CRLF_\r\n\tsubProto   = \"SUB %s %s %d\" + _CRLF_\r\n\tunsubProto = \"UNSUB %d %s\" + _CRLF_\r\n\tokProto    = _OK_OP_ + _CRLF_\r\n)\r\n\r\n// Return the currently selected server\r\nfunc (nc *Conn) currentServer() (int, *srv) {\r\n\tfor i, s := range nc.srvPool {\r\n\t\tif s == nil {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif s == nc.current {\r\n\t\t\treturn i, s\r\n\t\t}\r\n\t}\r\n\treturn -1, nil\r\n}\r\n\r\n// Pop the current server and put onto the end of the list. Select head of list as long\r\n// as number of reconnect attempts under MaxReconnect.\r\nfunc (nc *Conn) selectNextServer() (*srv, error) {\r\n\ti, s := nc.currentServer()\r\n\tif i < 0 {\r\n\t\treturn nil, ErrNoServers\r\n\t}\r\n\tsp := nc.srvPool\r\n\tnum := len(sp)\r\n\tcopy(sp[i:num-1], sp[i+1:num])\r\n\tmaxReconnect := nc.Opts.MaxReconnect\r\n\tif maxReconnect < 0 || s.reconnects < maxReconnect {\r\n\t\tnc.srvPool[num-1] = s\r\n\t} else {\r\n\t\tnc.srvPool = sp[0 : num-1]\r\n\t}\r\n\tif len(nc.srvPool) <= 0 {\r\n\t\tnc.current = nil\r\n\t\treturn nil, ErrNoServers\r\n\t}\r\n\tnc.current = nc.srvPool[0]\r\n\treturn nc.srvPool[0], nil\r\n}\r\n\r\n// Will assign the correct server to nc.current\r\nfunc (nc *Conn) pickServer() error {\r\n\tnc.current = nil\r\n\tif len(nc.srvPool) <= 0 {\r\n\t\treturn ErrNoServers\r\n\t}\r\n\r\n\tfor _, s := range nc.srvPool {\r\n\t\tif s != nil {\r\n\t\t\tnc.current = s\r\n\t\t\treturn nil\r\n\t\t}\r\n\t}\r\n\treturn ErrNoServers\r\n}\r\n\r\nconst tlsScheme = \"tls\"\r\n\r\n// Create the server pool using the options given.\r\n// We will place a Url option first, followed by any\r\n// Server Options. We will randomize the server pool unless\r\n// the NoRandomize flag is set.\r\nfunc (nc *Conn) setupServerPool() error {\r\n\tnc.srvPool = make([]*srv, 0, srvPoolSize)\r\n\tnc.urls = make(map[string]struct{}, srvPoolSize)\r\n\r\n\t// Create srv objects from each url string in nc.Opts.Servers\r\n\t// and add them to the pool.\r\n\tfor _, urlString := range nc.Opts.Servers {\r\n\t\tif err := nc.addURLToPool(urlString, false, false); err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t}\r\n\r\n\t// Randomize if allowed to\r\n\tif !nc.Opts.NoRandomize {\r\n\t\tnc.shufflePool(0)\r\n\t}\r\n\r\n\t// Normally, if this one is set, Options.Servers should not be,\r\n\t// but we always allowed that, so continue to do so.\r\n\tif nc.Opts.Url != _EMPTY_ {\r\n\t\t// Add to the end of the array\r\n\t\tif err := nc.addURLToPool(nc.Opts.Url, false, false); err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\t// Then swap it with first to guarantee that Options.Url is tried first.\r\n\t\tlast := len(nc.srvPool) - 1\r\n\t\tif last > 0 {\r\n\t\t\tnc.srvPool[0], nc.srvPool[last] = nc.srvPool[last], nc.srvPool[0]\r\n\t\t}\r\n\t} else if len(nc.srvPool) <= 0 {\r\n\t\t// Place default URL if pool is empty.\r\n\t\tif err := nc.addURLToPool(DefaultURL, false, false); err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t}\r\n\r\n\t// Check for Scheme hint to move to TLS mode.\r\n\tfor _, srv := range nc.srvPool {\r\n\t\tif srv.url.Scheme == tlsScheme {\r\n\t\t\t// FIXME(dlc), this is for all in the pool, should be case by case.\r\n\t\t\tnc.Opts.Secure = true\r\n\t\t\tif nc.Opts.TLSConfig == nil {\r\n\t\t\t\tnc.Opts.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\treturn nc.pickServer()\r\n}\r\n\r\n// Helper function to return scheme\r\nfunc (nc *Conn) connScheme() string {\r\n\tif nc.Opts.Secure {\r\n\t\treturn tlsScheme\r\n\t}\r\n\treturn \"nats\"\r\n}\r\n\r\n// Return true iff u.Hostname() is an IP address.\r\nfunc hostIsIP(u *url.URL) bool {\r\n\treturn net.ParseIP(u.Hostname()) != nil\r\n}\r\n\r\n// addURLToPool adds an entry to the server pool\r\nfunc (nc *Conn) addURLToPool(sURL string, implicit, saveTLSName bool) error {\r\n\tif !strings.Contains(sURL, \"://\") {\r\n\t\tsURL = fmt.Sprintf(\"%s://%s\", nc.connScheme(), sURL)\r\n\t}\r\n\tvar (\r\n\t\tu   *url.URL\r\n\t\terr error\r\n\t)\r\n\tfor i := 0; i < 2; i++ {\r\n\t\tu, err = url.Parse(sURL)\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif u.Port() != \"\" {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\t// In case given URL is of the form \"localhost:\", just add\r\n\t\t// the port number at the end, otherwise, add \":4222\".\r\n\t\tif sURL[len(sURL)-1] != ':' {\r\n\t\t\tsURL += \":\"\r\n\t\t}\r\n\t\tsURL += defaultPortString\r\n\t}\r\n\r\n\tvar tlsName string\r\n\tif implicit {\r\n\t\tcurl := nc.current.url\r\n\t\t// Check to see if we do not have a url.User but current connected\r\n\t\t// url does. If so copy over.\r\n\t\tif u.User == nil && curl.User != nil {\r\n\t\t\tu.User = curl.User\r\n\t\t}\r\n\t\t// We are checking to see if we have a secure connection and are\r\n\t\t// adding an implicit server that just has an IP. If so we will remember\r\n\t\t// the current hostname we are connected to.\r\n\t\tif saveTLSName && hostIsIP(u) {\r\n\t\t\ttlsName = curl.Hostname()\r\n\t\t}\r\n\t}\r\n\r\n\ts := &srv{url: u, isImplicit: implicit, tlsName: tlsName}\r\n\tnc.srvPool = append(nc.srvPool, s)\r\n\tnc.urls[u.Host] = struct{}{}\r\n\treturn nil\r\n}\r\n\r\n// shufflePool swaps randomly elements in the server pool\r\n// The `offset` value indicates that the shuffling should start at\r\n// this offset and leave the elements from [0..offset) intact.\r\nfunc (nc *Conn) shufflePool(offset int) {\r\n\tif len(nc.srvPool) <= offset+1 {\r\n\t\treturn\r\n\t}\r\n\tsource := rand.NewSource(time.Now().UnixNano())\r\n\tr := rand.New(source)\r\n\tfor i := offset; i < len(nc.srvPool); i++ {\r\n\t\tj := offset + r.Intn(i+1-offset)\r\n\t\tnc.srvPool[i], nc.srvPool[j] = nc.srvPool[j], nc.srvPool[i]\r\n\t}\r\n}\r\n\r\nfunc (nc *Conn) newBuffer() *bufio.Writer {\r\n\tvar w io.Writer = nc.conn\r\n\tif nc.Opts.FlusherTimeout > 0 {\r\n\t\tw = &timeoutWriter{conn: nc.conn, timeout: nc.Opts.FlusherTimeout}\r\n\t}\r\n\treturn bufio.NewWriterSize(w, defaultBufSize)\r\n}\r\n\r\n// createConn will connect to the server and wrap the appropriate\r\n// bufio structures. It will do the right thing when an existing\r\n// connection is in place.\r\nfunc (nc *Conn) createConn() (err error) {\r\n\tif nc.Opts.Timeout < 0 {\r\n\t\treturn ErrBadTimeout\r\n\t}\r\n\tif _, cur := nc.currentServer(); cur == nil {\r\n\t\treturn ErrNoServers\r\n\t}\r\n\r\n\t// We will auto-expand host names if they resolve to multiple IPs\r\n\thosts := []string{}\r\n\tu := nc.current.url\r\n\r\n\tif net.ParseIP(u.Hostname()) == nil {\r\n\t\taddrs, _ := net.LookupHost(u.Hostname())\r\n\t\tfor _, addr := range addrs {\r\n\t\t\thosts = append(hosts, net.JoinHostPort(addr, u.Port()))\r\n\t\t}\r\n\t}\r\n\t// Fall back to what we were given.\r\n\tif len(hosts) == 0 {\r\n\t\thosts = append(hosts, u.Host)\r\n\t}\r\n\r\n\t// CustomDialer takes precedence. If not set, use Opts.Dialer which\r\n\t// is set to a default *net.Dialer (in Connect()) if not explicitly\r\n\t// set by the user.\r\n\tdialer := nc.Opts.CustomDialer\r\n\tif dialer == nil {\r\n\t\t// We will copy and shorten the timeout if we have multiple hosts to try.\r\n\t\tcopyDialer := *nc.Opts.Dialer\r\n\t\tcopyDialer.Timeout = copyDialer.Timeout / time.Duration(len(hosts))\r\n\t\tdialer = &copyDialer\r\n\t}\r\n\r\n\tif len(hosts) > 1 && !nc.Opts.NoRandomize {\r\n\t\trand.Shuffle(len(hosts), func(i, j int) {\r\n\t\t\thosts[i], hosts[j] = hosts[j], hosts[i]\r\n\t\t})\r\n\t}\r\n\tfor _, host := range hosts {\r\n\t\tnc.conn, err = dialer.Dial(\"tcp\", host)\r\n\t\tif err == nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif nc.pending != nil && nc.bw != nil {\r\n\t\t// Move to pending buffer.\r\n\t\tnc.bw.Flush()\r\n\t}\r\n\tnc.bw = nc.newBuffer()\r\n\treturn nil\r\n}\r\n\r\n// makeTLSConn will wrap an existing Conn using TLS\r\nfunc (nc *Conn) makeTLSConn() error {\r\n\t// Allow the user to configure their own tls.Config structure.\r\n\tvar tlsCopy *tls.Config\r\n\tif nc.Opts.TLSConfig != nil {\r\n\t\ttlsCopy = util.CloneTLSConfig(nc.Opts.TLSConfig)\r\n\t} else {\r\n\t\ttlsCopy = &tls.Config{}\r\n\t}\r\n\t// If its blank we will override it with the current host\r\n\tif tlsCopy.ServerName == _EMPTY_ {\r\n\t\tif nc.current.tlsName != _EMPTY_ {\r\n\t\t\ttlsCopy.ServerName = nc.current.tlsName\r\n\t\t} else {\r\n\t\t\th, _, _ := net.SplitHostPort(nc.current.url.Host)\r\n\t\t\ttlsCopy.ServerName = h\r\n\t\t}\r\n\t}\r\n\tnc.conn = tls.Client(nc.conn, tlsCopy)\r\n\tconn := nc.conn.(*tls.Conn)\r\n\tif err := conn.Handshake(); err != nil {\r\n\t\treturn err\r\n\t}\r\n\tnc.bw = nc.newBuffer()\r\n\treturn nil\r\n}\r\n\r\n// waitForExits will wait for all socket watcher Go routines to\r\n// be shutdown before proceeding.\r\nfunc (nc *Conn) waitForExits() {\r\n\t// Kick old flusher forcefully.\r\n\tselect {\r\n\tcase nc.fch <- struct{}{}:\r\n\tdefault:\r\n\t}\r\n\r\n\t// Wait for any previous go routines.\r\n\tnc.wg.Wait()\r\n}\r\n\r\n// Report the connected server's Url\r\nfunc (nc *Conn) ConnectedUrl() string {\r\n\tif nc == nil {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\r\n\tif nc.status != CONNECTED {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\treturn nc.current.url.String()\r\n}\r\n\r\n// ConnectedAddr returns the connected server's IP\r\nfunc (nc *Conn) ConnectedAddr() string {\r\n\tif nc == nil {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\r\n\tif nc.status != CONNECTED {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\treturn nc.conn.RemoteAddr().String()\r\n}\r\n\r\n// Report the connected server's Id\r\nfunc (nc *Conn) ConnectedServerId() string {\r\n\tif nc == nil {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\r\n\tif nc.status != CONNECTED {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\treturn nc.info.ID\r\n}\r\n\r\n// Low level setup for structs, etc\r\nfunc (nc *Conn) setup() {\r\n\tnc.subs = make(map[int64]*Subscription)\r\n\tnc.pongs = make([]chan struct{}, 0, 8)\r\n\r\n\tnc.fch = make(chan struct{}, flushChanSize)\r\n\tnc.rqch = make(chan struct{})\r\n\r\n\t// Setup scratch outbound buffer for PUB\r\n\tpub := nc.scratch[:len(_PUB_P_)]\r\n\tcopy(pub, _PUB_P_)\r\n}\r\n\r\n// Process a connected connection and initialize properly.\r\nfunc (nc *Conn) processConnectInit() error {\r\n\r\n\t// Set our deadline for the whole connect process\r\n\tnc.conn.SetDeadline(time.Now().Add(nc.Opts.Timeout))\r\n\tdefer nc.conn.SetDeadline(time.Time{})\r\n\r\n\t// Set our status to connecting.\r\n\tnc.status = CONNECTING\r\n\r\n\t// Process the INFO protocol received from the server\r\n\terr := nc.processExpectedInfo()\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Send the CONNECT protocol along with the initial PING protocol.\r\n\t// Wait for the PONG response (or any error that we get from the server).\r\n\terr = nc.sendConnect()\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Reset the number of PING sent out\r\n\tnc.pout = 0\r\n\r\n\t// Start or reset Timer\r\n\tif nc.Opts.PingInterval > 0 {\r\n\t\tif nc.ptmr == nil {\r\n\t\t\tnc.ptmr = time.AfterFunc(nc.Opts.PingInterval, nc.processPingTimer)\r\n\t\t} else {\r\n\t\t\tnc.ptmr.Reset(nc.Opts.PingInterval)\r\n\t\t}\r\n\t}\r\n\r\n\t// Start the readLoop and flusher go routines, we will wait on both on a reconnect event.\r\n\tnc.wg.Add(2)\r\n\tgo nc.readLoop()\r\n\tgo nc.flusher()\r\n\r\n\treturn nil\r\n}\r\n\r\n// Main connect function. Will connect to the nats-server\r\nfunc (nc *Conn) connect() error {\r\n\tvar returnedErr error\r\n\r\n\t// Create actual socket connection\r\n\t// For first connect we walk all servers in the pool and try\r\n\t// to connect immediately.\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tnc.initc = true\r\n\t// The pool may change inside the loop iteration due to INFO protocol.\r\n\tfor i := 0; i < len(nc.srvPool); i++ {\r\n\t\tnc.current = nc.srvPool[i]\r\n\r\n\t\tif err := nc.createConn(); err == nil {\r\n\t\t\t// This was moved out of processConnectInit() because\r\n\t\t\t// that function is now invoked from doReconnect() too.\r\n\t\t\tnc.setup()\r\n\r\n\t\t\terr = nc.processConnectInit()\r\n\r\n\t\t\tif err == nil {\r\n\t\t\t\tnc.current.didConnect = true\r\n\t\t\t\tnc.current.reconnects = 0\r\n\t\t\t\tnc.current.lastErr = nil\r\n\t\t\t\treturnedErr = nil\r\n\t\t\t\tbreak\r\n\t\t\t} else {\r\n\t\t\t\treturnedErr = err\r\n\t\t\t\tnc.mu.Unlock()\r\n\t\t\t\tnc.close(DISCONNECTED, false, err)\r\n\t\t\t\tnc.mu.Lock()\r\n\t\t\t\tnc.current = nil\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\t// Cancel out default connection refused, will trigger the\r\n\t\t\t// No servers error conditional\r\n\t\t\tif strings.Contains(err.Error(), \"connection refused\") {\r\n\t\t\t\treturnedErr = nil\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tnc.initc = false\r\n\tif returnedErr == nil && nc.status != CONNECTED {\r\n\t\treturnedErr = ErrNoServers\r\n\t}\r\n\r\n\treturn returnedErr\r\n}\r\n\r\n// This will check to see if the connection should be\r\n// secure. This can be dictated from either end and should\r\n// only be called after the INIT protocol has been received.\r\nfunc (nc *Conn) checkForSecure() error {\r\n\t// Check to see if we need to engage TLS\r\n\to := nc.Opts\r\n\r\n\t// Check for mismatch in setups\r\n\tif o.Secure && !nc.info.TLSRequired {\r\n\t\treturn ErrSecureConnWanted\r\n\t} else if nc.info.TLSRequired && !o.Secure {\r\n\t\t// Switch to Secure since server needs TLS.\r\n\t\to.Secure = true\r\n\t}\r\n\r\n\t// Need to rewrap with bufio\r\n\tif o.Secure {\r\n\t\tif err := nc.makeTLSConn(); err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// processExpectedInfo will look for the expected first INFO message\r\n// sent when a connection is established. The lock should be held entering.\r\nfunc (nc *Conn) processExpectedInfo() error {\r\n\r\n\tc := &control{}\r\n\r\n\t// Read the protocol\r\n\terr := nc.readOp(c)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// The nats protocol should send INFO first always.\r\n\tif c.op != _INFO_OP_ {\r\n\t\treturn ErrNoInfoReceived\r\n\t}\r\n\r\n\t// Parse the protocol\r\n\tif err := nc.processInfo(c.args); err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif nc.Opts.Nkey != \"\" && nc.info.Nonce == \"\" {\r\n\t\treturn ErrNkeysNotSupported\r\n\t}\r\n\r\n\treturn nc.checkForSecure()\r\n}\r\n\r\n// Sends a protocol control message by queuing into the bufio writer\r\n// and kicking the flush Go routine.  These writes are protected.\r\nfunc (nc *Conn) sendProto(proto string) {\r\n\tnc.mu.Lock()\r\n\tnc.bw.WriteString(proto)\r\n\tnc.kickFlusher()\r\n\tnc.mu.Unlock()\r\n}\r\n\r\n// Generate a connect protocol message, issuing user/password if\r\n// applicable. The lock is assumed to be held upon entering.\r\nfunc (nc *Conn) connectProto() (string, error) {\r\n\to := nc.Opts\r\n\tvar nkey, sig, user, pass, token, ujwt string\r\n\tu := nc.current.url.User\r\n\tif u != nil {\r\n\t\t// if no password, assume username is authToken\r\n\t\tif _, ok := u.Password(); !ok {\r\n\t\t\ttoken = u.Username()\r\n\t\t} else {\r\n\t\t\tuser = u.Username()\r\n\t\t\tpass, _ = u.Password()\r\n\t\t}\r\n\t} else {\r\n\t\t// Take from options (possibly all empty strings)\r\n\t\tuser = o.User\r\n\t\tpass = o.Password\r\n\t\ttoken = o.Token\r\n\t\tnkey = o.Nkey\r\n\t}\r\n\r\n\t// Look for user jwt.\r\n\tif o.UserJWT != nil {\r\n\t\tif jwt, err := o.UserJWT(); err != nil {\r\n\t\t\treturn _EMPTY_, err\r\n\t\t} else {\r\n\t\t\tujwt = jwt\r\n\t\t}\r\n\t\tif nkey != _EMPTY_ {\r\n\t\t\treturn _EMPTY_, ErrNkeyAndUser\r\n\t\t}\r\n\t}\r\n\r\n\tif ujwt != _EMPTY_ || nkey != _EMPTY_ {\r\n\t\tif o.SignatureCB == nil {\r\n\t\t\tif ujwt == _EMPTY_ {\r\n\t\t\t\treturn _EMPTY_, ErrNkeyButNoSigCB\r\n\t\t\t}\r\n\t\t\treturn _EMPTY_, ErrUserButNoSigCB\r\n\t\t}\r\n\t\tsigraw, err := o.SignatureCB([]byte(nc.info.Nonce))\r\n\t\tif err != nil {\r\n\t\t\treturn _EMPTY_, err\r\n\t\t}\r\n\t\tsig = base64.RawURLEncoding.EncodeToString(sigraw)\r\n\t}\r\n\r\n\tif nc.Opts.TokenHandler != nil {\r\n\t\tif token != _EMPTY_ {\r\n\t\t\treturn _EMPTY_, ErrTokenAlreadySet\r\n\t\t}\r\n\t\ttoken = nc.Opts.TokenHandler()\r\n\t}\r\n\r\n\tcinfo := connectInfo{o.Verbose, o.Pedantic, ujwt, nkey, sig, user, pass, token,\r\n\t\to.Secure, o.Name, LangString, Version, clientProtoInfo, !o.NoEcho}\r\n\r\n\tb, err := json.Marshal(cinfo)\r\n\tif err != nil {\r\n\t\treturn _EMPTY_, ErrJsonParse\r\n\t}\r\n\r\n\t// Check if NoEcho is set and we have a server that supports it.\r\n\tif o.NoEcho && nc.info.Proto < 1 {\r\n\t\treturn _EMPTY_, ErrNoEchoNotSupported\r\n\t}\r\n\r\n\treturn fmt.Sprintf(conProto, b), nil\r\n}\r\n\r\n// normalizeErr removes the prefix -ERR, trim spaces and remove the quotes.\r\nfunc normalizeErr(line string) string {\r\n\ts := strings.TrimSpace(strings.TrimPrefix(line, _ERR_OP_))\r\n\ts = strings.TrimLeft(strings.TrimRight(s, \"'\"), \"'\")\r\n\treturn s\r\n}\r\n\r\n// Send a connect protocol message to the server, issue user/password if\r\n// applicable. Will wait for a flush to return from the server for error\r\n// processing.\r\nfunc (nc *Conn) sendConnect() error {\r\n\t// Construct the CONNECT protocol string\r\n\tcProto, err := nc.connectProto()\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Write the protocol into the buffer\r\n\t_, err = nc.bw.WriteString(cProto)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Add to the buffer the PING protocol\r\n\t_, err = nc.bw.WriteString(pingProto)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Flush the buffer\r\n\terr = nc.bw.Flush()\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// We don't want to read more than we need here, otherwise\r\n\t// we would need to transfer the excess read data to the readLoop.\r\n\t// Since in normal situations we just are looking for a PONG\\r\\n,\r\n\t// reading byte-by-byte here is ok.\r\n\tproto, err := nc.readProto()\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// If opts.Verbose is set, handle +OK\r\n\tif nc.Opts.Verbose && proto == okProto {\r\n\t\t// Read the rest now...\r\n\t\tproto, err = nc.readProto()\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t}\r\n\r\n\t// We expect a PONG\r\n\tif proto != pongProto {\r\n\t\t// But it could be something else, like -ERR\r\n\r\n\t\t// Since we no longer use ReadLine(), trim the trailing \"\\r\\n\"\r\n\t\tproto = strings.TrimRight(proto, \"\\r\\n\")\r\n\r\n\t\t// If it's a server error...\r\n\t\tif strings.HasPrefix(proto, _ERR_OP_) {\r\n\t\t\t// Remove -ERR, trim spaces and quotes, and convert to lower case.\r\n\t\t\tproto = normalizeErr(proto)\r\n\r\n\t\t\t// Check if this is an auth error\r\n\t\t\tif authErr := checkAuthError(strings.ToLower(proto)); authErr != nil {\r\n\t\t\t\t// This will schedule an async error if we are in reconnect,\r\n\t\t\t\t// and keep track of the auth error for the current server.\r\n\t\t\t\t// If we have got the same error twice, this sets nc.ar to true to\r\n\t\t\t\t// indicate that the reconnect should be aborted (will be checked\r\n\t\t\t\t// in doReconnect()).\r\n\t\t\t\tnc.processAuthError(authErr)\r\n\t\t\t}\r\n\r\n\t\t\treturn errors.New(\"nats: \" + proto)\r\n\t\t}\r\n\r\n\t\t// Notify that we got an unexpected protocol.\r\n\t\treturn fmt.Errorf(\"nats: expected '%s', got '%s'\", _PONG_OP_, proto)\r\n\t}\r\n\r\n\t// This is where we are truly connected.\r\n\tnc.status = CONNECTED\r\n\r\n\treturn nil\r\n}\r\n\r\n// reads a protocol one byte at a time.\r\nfunc (nc *Conn) readProto() (string, error) {\r\n\tvar (\r\n\t\t_buf     = [10]byte{}\r\n\t\tbuf      = _buf[:0]\r\n\t\tb        = [1]byte{}\r\n\t\tprotoEnd = byte('\\n')\r\n\t)\r\n\tfor {\r\n\t\tif _, err := nc.conn.Read(b[:1]); err != nil {\r\n\t\t\t// Do not report EOF error\r\n\t\t\tif err == io.EOF {\r\n\t\t\t\treturn string(buf), nil\r\n\t\t\t}\r\n\t\t\treturn \"\", err\r\n\t\t}\r\n\t\tbuf = append(buf, b[0])\r\n\t\tif b[0] == protoEnd {\r\n\t\t\treturn string(buf), nil\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// A control protocol line.\r\ntype control struct {\r\n\top, args string\r\n}\r\n\r\n// Read a control line and process the intended op.\r\nfunc (nc *Conn) readOp(c *control) error {\r\n\tbr := bufio.NewReaderSize(nc.conn, defaultBufSize)\r\n\tline, err := br.ReadString('\\n')\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\tparseControl(line, c)\r\n\treturn nil\r\n}\r\n\r\n// Parse a control line from the server.\r\nfunc parseControl(line string, c *control) {\r\n\ttoks := strings.SplitN(line, _SPC_, 2)\r\n\tif len(toks) == 1 {\r\n\t\tc.op = strings.TrimSpace(toks[0])\r\n\t\tc.args = _EMPTY_\r\n\t} else if len(toks) == 2 {\r\n\t\tc.op, c.args = strings.TrimSpace(toks[0]), strings.TrimSpace(toks[1])\r\n\t} else {\r\n\t\tc.op = _EMPTY_\r\n\t}\r\n}\r\n\r\n// flushReconnectPending will push the pending items that were\r\n// gathered while we were in a RECONNECTING state to the socket.\r\nfunc (nc *Conn) flushReconnectPendingItems() {\r\n\tif nc.pending == nil {\r\n\t\treturn\r\n\t}\r\n\tif nc.pending.Len() > 0 {\r\n\t\tnc.bw.Write(nc.pending.Bytes())\r\n\t}\r\n}\r\n\r\n// Stops the ping timer if set.\r\n// Connection lock is held on entry.\r\nfunc (nc *Conn) stopPingTimer() {\r\n\tif nc.ptmr != nil {\r\n\t\tnc.ptmr.Stop()\r\n\t}\r\n}\r\n\r\n// Try to reconnect using the option parameters.\r\n// This function assumes we are allowed to reconnect.\r\nfunc (nc *Conn) doReconnect(err error) {\r\n\t// We want to make sure we have the other watchers shutdown properly\r\n\t// here before we proceed past this point.\r\n\tnc.waitForExits()\r\n\r\n\t// FIXME(dlc) - We have an issue here if we have\r\n\t// outstanding flush points (pongs) and they were not\r\n\t// sent out, but are still in the pipe.\r\n\r\n\t// Hold the lock manually and release where needed below,\r\n\t// can't do defer here.\r\n\tnc.mu.Lock()\r\n\r\n\t// Clear any queued pongs, e.g. pending flush calls.\r\n\tnc.clearPendingFlushCalls()\r\n\r\n\t// Clear any errors.\r\n\tnc.err = nil\r\n\t// Perform appropriate callback if needed for a disconnect.\r\n\t// DisconnectedErrCB has priority over deprecated DisconnectedCB\r\n\tif nc.Opts.DisconnectedErrCB != nil {\r\n\t\tnc.ach.push(func() { nc.Opts.DisconnectedErrCB(nc, err) })\r\n\t} else if nc.Opts.DisconnectedCB != nil {\r\n\t\tnc.ach.push(func() { nc.Opts.DisconnectedCB(nc) })\r\n\t}\r\n\r\n\t// This is used to wait on go routines exit if we start them in the loop\r\n\t// but an error occurs after that.\r\n\twaitForGoRoutines := false\r\n\tvar rt *time.Timer\r\n\t// Channel used to kick routine out of sleep when conn is closed.\r\n\trqch := nc.rqch\r\n\t// Counter that is increased when the whole list of servers has been tried.\r\n\tvar wlf int\r\n\r\n\tvar jitter time.Duration\r\n\tvar rw time.Duration\r\n\t// If a custom reconnect delay handler is set, this takes precedence.\r\n\tcrd := nc.Opts.CustomReconnectDelayCB\r\n\tif crd == nil {\r\n\t\trw = nc.Opts.ReconnectWait\r\n\t\t// TODO: since we sleep only after the whole list has been tried, we can't\r\n\t\t// rely on individual *srv to know if it is a TLS or non-TLS url.\r\n\t\t// We have to pick which type of jitter to use, for now, we use these hints:\r\n\t\tjitter = nc.Opts.ReconnectJitter\r\n\t\tif nc.Opts.Secure || nc.Opts.TLSConfig != nil {\r\n\t\t\tjitter = nc.Opts.ReconnectJitterTLS\r\n\t\t}\r\n\t}\r\n\r\n\tfor i := 0; len(nc.srvPool) > 0; {\r\n\t\tcur, err := nc.selectNextServer()\r\n\t\tif err != nil {\r\n\t\t\tnc.err = err\r\n\t\t\tbreak\r\n\t\t}\r\n\r\n\t\tdoSleep := i+1 >= len(nc.srvPool)\r\n\t\tnc.mu.Unlock()\r\n\r\n\t\tif !doSleep {\r\n\t\t\ti++\r\n\t\t\t// Release the lock to give a chance to a concurrent nc.Close() to break the loop.\r\n\t\t\truntime.Gosched()\r\n\t\t} else {\r\n\t\t\ti = 0\r\n\t\t\tvar st time.Duration\r\n\t\t\tif crd != nil {\r\n\t\t\t\twlf++\r\n\t\t\t\tst = crd(wlf)\r\n\t\t\t} else {\r\n\t\t\t\tst = rw\r\n\t\t\t\tif jitter > 0 {\r\n\t\t\t\t\tst += time.Duration(rand.Int63n(int64(jitter)))\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif rt == nil {\r\n\t\t\t\trt = time.NewTimer(st)\r\n\t\t\t} else {\r\n\t\t\t\trt.Reset(st)\r\n\t\t\t}\r\n\t\t\tselect {\r\n\t\t\tcase <-rqch:\r\n\t\t\t\trt.Stop()\r\n\t\t\tcase <-rt.C:\r\n\t\t\t}\r\n\t\t}\r\n\t\t// If the readLoop, etc.. go routines were started, wait for them to complete.\r\n\t\tif waitForGoRoutines {\r\n\t\t\tnc.waitForExits()\r\n\t\t\twaitForGoRoutines = false\r\n\t\t}\r\n\t\tnc.mu.Lock()\r\n\r\n\t\t// Check if we have been closed first.\r\n\t\tif nc.isClosed() {\r\n\t\t\tbreak\r\n\t\t}\r\n\r\n\t\t// Mark that we tried a reconnect\r\n\t\tcur.reconnects++\r\n\r\n\t\t// Try to create a new connection\r\n\t\terr = nc.createConn()\r\n\r\n\t\t// Not yet connected, retry...\r\n\t\t// Continue to hold the lock\r\n\t\tif err != nil {\r\n\t\t\tnc.err = nil\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// We are reconnected\r\n\t\tnc.Reconnects++\r\n\r\n\t\t// Process connect logic\r\n\t\tif nc.err = nc.processConnectInit(); nc.err != nil {\r\n\t\t\t// Check if we should abort reconnect. If so, break out\r\n\t\t\t// of the loop and connection will be closed.\r\n\t\t\tif nc.ar {\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t\tnc.status = RECONNECTING\r\n\t\t\t// Reset the buffered writer to the pending buffer\r\n\t\t\t// (was set to a buffered writer on nc.conn in createConn)\r\n\t\t\tnc.bw.Reset(nc.pending)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// Clear possible lastErr under the connection lock after\r\n\t\t// a successful processConnectInit().\r\n\t\tnc.current.lastErr = nil\r\n\r\n\t\t// Clear out server stats for the server we connected to..\r\n\t\tcur.didConnect = true\r\n\t\tcur.reconnects = 0\r\n\r\n\t\t// Send existing subscription state\r\n\t\tnc.resendSubscriptions()\r\n\r\n\t\t// Now send off and clear pending buffer\r\n\t\tnc.flushReconnectPendingItems()\r\n\r\n\t\t// Flush the buffer\r\n\t\tnc.err = nc.bw.Flush()\r\n\t\tif nc.err != nil {\r\n\t\t\tnc.status = RECONNECTING\r\n\t\t\t// Reset the buffered writer to the pending buffer (bytes.Buffer).\r\n\t\t\tnc.bw.Reset(nc.pending)\r\n\t\t\t// Stop the ping timer (if set)\r\n\t\t\tnc.stopPingTimer()\r\n\t\t\t// Since processConnectInit() returned without error, the\r\n\t\t\t// go routines were started, so wait for them to return\r\n\t\t\t// on the next iteration (after releasing the lock).\r\n\t\t\twaitForGoRoutines = true\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// Done with the pending buffer\r\n\t\tnc.pending = nil\r\n\r\n\t\t// This is where we are truly connected.\r\n\t\tnc.status = CONNECTED\r\n\r\n\t\t// Queue up the reconnect callback.\r\n\t\tif nc.Opts.ReconnectedCB != nil {\r\n\t\t\tnc.ach.push(func() { nc.Opts.ReconnectedCB(nc) })\r\n\t\t}\r\n\r\n\t\t// Release lock here, we will return below.\r\n\t\tnc.mu.Unlock()\r\n\r\n\t\t// Make sure to flush everything\r\n\t\tnc.Flush()\r\n\r\n\t\treturn\r\n\t}\r\n\r\n\t// Call into close.. We have no servers left..\r\n\tif nc.err == nil {\r\n\t\tnc.err = ErrNoServers\r\n\t}\r\n\tnc.mu.Unlock()\r\n\tnc.close(CLOSED, true, nil)\r\n}\r\n\r\n// processOpErr handles errors from reading or parsing the protocol.\r\n// The lock should not be held entering this function.\r\nfunc (nc *Conn) processOpErr(err error) {\r\n\tnc.mu.Lock()\r\n\tif nc.isConnecting() || nc.isClosed() || nc.isReconnecting() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\tif nc.Opts.AllowReconnect && nc.status == CONNECTED {\r\n\t\t// Set our new status\r\n\t\tnc.status = RECONNECTING\r\n\t\t// Stop ping timer if set\r\n\t\tnc.stopPingTimer()\r\n\t\tif nc.conn != nil {\r\n\t\t\tnc.bw.Flush()\r\n\t\t\tnc.conn.Close()\r\n\t\t\tnc.conn = nil\r\n\t\t}\r\n\r\n\t\t// Create pending buffer before reconnecting.\r\n\t\tnc.pending = new(bytes.Buffer)\r\n\t\tnc.bw.Reset(nc.pending)\r\n\r\n\t\tgo nc.doReconnect(err)\r\n\t\tnc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\tnc.status = DISCONNECTED\r\n\tnc.err = err\r\n\tnc.mu.Unlock()\r\n\tnc.close(CLOSED, true, nil)\r\n}\r\n\r\n// dispatch is responsible for calling any async callbacks\r\nfunc (ac *asyncCallbacksHandler) asyncCBDispatcher() {\r\n\tfor {\r\n\t\tac.mu.Lock()\r\n\t\t// Protect for spurious wakeups. We should get out of the\r\n\t\t// wait only if there is an element to pop from the list.\r\n\t\tfor ac.head == nil {\r\n\t\t\tac.cond.Wait()\r\n\t\t}\r\n\t\tcur := ac.head\r\n\t\tac.head = cur.next\r\n\t\tif cur == ac.tail {\r\n\t\t\tac.tail = nil\r\n\t\t}\r\n\t\tac.mu.Unlock()\r\n\r\n\t\t// This signals that the dispatcher has been closed and all\r\n\t\t// previous callbacks have been dispatched.\r\n\t\tif cur.f == nil {\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// Invoke callback outside of handler's lock\r\n\t\tcur.f()\r\n\t}\r\n}\r\n\r\n// Add the given function to the tail of the list and\r\n// signals the dispatcher.\r\nfunc (ac *asyncCallbacksHandler) push(f func()) {\r\n\tac.pushOrClose(f, false)\r\n}\r\n\r\n// Signals that we are closing...\r\nfunc (ac *asyncCallbacksHandler) close() {\r\n\tac.pushOrClose(nil, true)\r\n}\r\n\r\n// Add the given function to the tail of the list and\r\n// signals the dispatcher.\r\nfunc (ac *asyncCallbacksHandler) pushOrClose(f func(), close bool) {\r\n\tac.mu.Lock()\r\n\tdefer ac.mu.Unlock()\r\n\t// Make sure that library is not calling push with nil function,\r\n\t// since this is used to notify the dispatcher that it should stop.\r\n\tif !close && f == nil {\r\n\t\tpanic(\"pushing a nil callback\")\r\n\t}\r\n\tcb := &asyncCB{f: f}\r\n\tif ac.tail != nil {\r\n\t\tac.tail.next = cb\r\n\t} else {\r\n\t\tac.head = cb\r\n\t}\r\n\tac.tail = cb\r\n\tif close {\r\n\t\tac.cond.Broadcast()\r\n\t} else {\r\n\t\tac.cond.Signal()\r\n\t}\r\n}\r\n\r\n// readLoop() will sit on the socket reading and processing the\r\n// protocol from the server. It will dispatch appropriately based\r\n// on the op type.\r\nfunc (nc *Conn) readLoop() {\r\n\t// Release the wait group on exit\r\n\tdefer nc.wg.Done()\r\n\r\n\t// Create a parseState if needed.\r\n\tnc.mu.Lock()\r\n\tif nc.ps == nil {\r\n\t\tnc.ps = &parseState{}\r\n\t}\r\n\tconn := nc.conn\r\n\tnc.mu.Unlock()\r\n\r\n\tif conn == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Stack based buffer.\r\n\tb := make([]byte, defaultBufSize)\r\n\r\n\tfor {\r\n\t\tif n, err := conn.Read(b); err != nil {\r\n\t\t\tnc.processOpErr(err)\r\n\t\t\tbreak\r\n\t\t} else if err = nc.parse(b[:n]); err != nil {\r\n\t\t\tnc.processOpErr(err)\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\t// Clear the parseState here..\r\n\tnc.mu.Lock()\r\n\tnc.ps = nil\r\n\tnc.mu.Unlock()\r\n}\r\n\r\n// waitForMsgs waits on the conditional shared with readLoop and processMsg.\r\n// It is used to deliver messages to asynchronous subscribers.\r\nfunc (nc *Conn) waitForMsgs(s *Subscription) {\r\n\tvar closed bool\r\n\tvar delivered, max uint64\r\n\r\n\t// Used to account for adjustments to sub.pBytes when we wrap back around.\r\n\tmsgLen := -1\r\n\r\n\tfor {\r\n\t\ts.mu.Lock()\r\n\t\t// Do accounting for last msg delivered here so we only lock once\r\n\t\t// and drain state trips after callback has returned.\r\n\t\tif msgLen >= 0 {\r\n\t\t\ts.pMsgs--\r\n\t\t\ts.pBytes -= msgLen\r\n\t\t\tmsgLen = -1\r\n\t\t}\r\n\r\n\t\tif s.pHead == nil && !s.closed {\r\n\t\t\ts.pCond.Wait()\r\n\t\t}\r\n\t\t// Pop the msg off the list\r\n\t\tm := s.pHead\r\n\t\tif m != nil {\r\n\t\t\ts.pHead = m.next\r\n\t\t\tif s.pHead == nil {\r\n\t\t\t\ts.pTail = nil\r\n\t\t\t}\r\n\t\t\tif m.barrier != nil {\r\n\t\t\t\ts.mu.Unlock()\r\n\t\t\t\tif atomic.AddInt64(&m.barrier.refs, -1) == 0 {\r\n\t\t\t\t\tm.barrier.f()\r\n\t\t\t\t}\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tmsgLen = len(m.Data)\r\n\t\t}\r\n\t\tmcb := s.mcb\r\n\t\tmax = s.max\r\n\t\tclosed = s.closed\r\n\t\tif !s.closed {\r\n\t\t\ts.delivered++\r\n\t\t\tdelivered = s.delivered\r\n\t\t}\r\n\t\ts.mu.Unlock()\r\n\r\n\t\tif closed {\r\n\t\t\tbreak\r\n\t\t}\r\n\r\n\t\t// Deliver the message.\r\n\t\tif m != nil && (max == 0 || delivered <= max) {\r\n\t\t\tmcb(m)\r\n\t\t}\r\n\t\t// If we have hit the max for delivered msgs, remove sub.\r\n\t\tif max > 0 && delivered >= max {\r\n\t\t\tnc.mu.Lock()\r\n\t\t\tnc.removeSub(s)\r\n\t\t\tnc.mu.Unlock()\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\t// Check for barrier messages\r\n\ts.mu.Lock()\r\n\tfor m := s.pHead; m != nil; m = s.pHead {\r\n\t\tif m.barrier != nil {\r\n\t\t\ts.mu.Unlock()\r\n\t\t\tif atomic.AddInt64(&m.barrier.refs, -1) == 0 {\r\n\t\t\t\tm.barrier.f()\r\n\t\t\t}\r\n\t\t\ts.mu.Lock()\r\n\t\t}\r\n\t\ts.pHead = m.next\r\n\t}\r\n\ts.mu.Unlock()\r\n}\r\n\r\n// processMsg is called by parse and will place the msg on the\r\n// appropriate channel/pending queue for processing. If the channel is full,\r\n// or the pending queue is over the pending limits, the connection is\r\n// considered a slow consumer.\r\nfunc (nc *Conn) processMsg(data []byte) {\r\n\t// Don't lock the connection to avoid server cutting us off if the\r\n\t// flusher is holding the connection lock, trying to send to the server\r\n\t// that is itself trying to send data to us.\r\n\tnc.subsMu.RLock()\r\n\r\n\t// Stats\r\n\tatomic.AddUint64(&nc.InMsgs, 1)\r\n\tatomic.AddUint64(&nc.InBytes, uint64(len(data)))\r\n\r\n\tsub := nc.subs[nc.ps.ma.sid]\r\n\tif sub == nil {\r\n\t\tnc.subsMu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\r\n\t// Copy them into string\r\n\tsubj := string(nc.ps.ma.subject)\r\n\treply := string(nc.ps.ma.reply)\r\n\r\n\t// Doing message create outside of the sub's lock to reduce contention.\r\n\t// It's possible that we end-up not using the message, but that's ok.\r\n\r\n\t// FIXME(dlc): Need to copy, should/can do COW?\r\n\tmsgPayload := make([]byte, len(data))\r\n\tcopy(msgPayload, data)\r\n\r\n\t// FIXME(dlc): Should we recycle these containers?\r\n\tm := &Msg{Data: msgPayload, Subject: subj, Reply: reply, Sub: sub}\r\n\r\n\tsub.mu.Lock()\r\n\r\n\t// Subscription internal stats (applicable only for non ChanSubscription's)\r\n\tif sub.typ != ChanSubscription {\r\n\t\tsub.pMsgs++\r\n\t\tif sub.pMsgs > sub.pMsgsMax {\r\n\t\t\tsub.pMsgsMax = sub.pMsgs\r\n\t\t}\r\n\t\tsub.pBytes += len(m.Data)\r\n\t\tif sub.pBytes > sub.pBytesMax {\r\n\t\t\tsub.pBytesMax = sub.pBytes\r\n\t\t}\r\n\r\n\t\t// Check for a Slow Consumer\r\n\t\tif (sub.pMsgsLimit > 0 && sub.pMsgs > sub.pMsgsLimit) ||\r\n\t\t\t(sub.pBytesLimit > 0 && sub.pBytes > sub.pBytesLimit) {\r\n\t\t\tgoto slowConsumer\r\n\t\t}\r\n\t}\r\n\r\n\t// We have two modes of delivery. One is the channel, used by channel\r\n\t// subscribers and syncSubscribers, the other is a linked list for async.\r\n\tif sub.mch != nil {\r\n\t\tselect {\r\n\t\tcase sub.mch <- m:\r\n\t\tdefault:\r\n\t\t\tgoto slowConsumer\r\n\t\t}\r\n\t} else {\r\n\t\t// Push onto the async pList\r\n\t\tif sub.pHead == nil {\r\n\t\t\tsub.pHead = m\r\n\t\t\tsub.pTail = m\r\n\t\t\tsub.pCond.Signal()\r\n\t\t} else {\r\n\t\t\tsub.pTail.next = m\r\n\t\t\tsub.pTail = m\r\n\t\t}\r\n\t}\r\n\r\n\t// Clear SlowConsumer status.\r\n\tsub.sc = false\r\n\r\n\tsub.mu.Unlock()\r\n\tnc.subsMu.RUnlock()\r\n\treturn\r\n\r\nslowConsumer:\r\n\tsub.dropped++\r\n\tsc := !sub.sc\r\n\tsub.sc = true\r\n\t// Undo stats from above\r\n\tif sub.typ != ChanSubscription {\r\n\t\tsub.pMsgs--\r\n\t\tsub.pBytes -= len(m.Data)\r\n\t}\r\n\tsub.mu.Unlock()\r\n\tnc.subsMu.RUnlock()\r\n\tif sc {\r\n\t\t// Now we need connection's lock and we may end-up in the situation\r\n\t\t// that we were trying to avoid, except that in this case, the client\r\n\t\t// is already experiencing client-side slow consumer situation.\r\n\t\tnc.mu.Lock()\r\n\t\tnc.err = ErrSlowConsumer\r\n\t\tif nc.Opts.AsyncErrorCB != nil {\r\n\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, sub, ErrSlowConsumer) })\r\n\t\t}\r\n\t\tnc.mu.Unlock()\r\n\t}\r\n}\r\n\r\n// processPermissionsViolation is called when the server signals a subject\r\n// permissions violation on either publish or subscribe.\r\nfunc (nc *Conn) processPermissionsViolation(err string) {\r\n\tnc.mu.Lock()\r\n\t// create error here so we can pass it as a closure to the async cb dispatcher.\r\n\te := errors.New(\"nats: \" + err)\r\n\tnc.err = e\r\n\tif nc.Opts.AsyncErrorCB != nil {\r\n\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, e) })\r\n\t}\r\n\tnc.mu.Unlock()\r\n}\r\n\r\n// processAuthError generally processing for auth errors. We want to do retries\r\n// unless we get the same error again. This allows us for instance to swap credentials\r\n// and have the app reconnect, but if nothing is changing we should bail.\r\n// This function will return true if the connection should be closed, false otherwise.\r\n// Connection lock is held on entry\r\nfunc (nc *Conn) processAuthError(err error) bool {\r\n\tnc.err = err\r\n\tif !nc.initc && nc.Opts.AsyncErrorCB != nil {\r\n\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\r\n\t}\r\n\t// We should give up if we tried twice on this server and got the\r\n\t// same error.\r\n\tif nc.current.lastErr == err {\r\n\t\tnc.ar = true\r\n\t} else {\r\n\t\tnc.current.lastErr = err\r\n\t}\r\n\treturn nc.ar\r\n}\r\n\r\n// flusher is a separate Go routine that will process flush requests for the write\r\n// bufio. This allows coalescing of writes to the underlying socket.\r\nfunc (nc *Conn) flusher() {\r\n\t// Release the wait group\r\n\tdefer nc.wg.Done()\r\n\r\n\t// snapshot the bw and conn since they can change from underneath of us.\r\n\tnc.mu.Lock()\r\n\tbw := nc.bw\r\n\tconn := nc.conn\r\n\tfch := nc.fch\r\n\tnc.mu.Unlock()\r\n\r\n\tif conn == nil || bw == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\tfor {\r\n\t\tif _, ok := <-fch; !ok {\r\n\t\t\treturn\r\n\t\t}\r\n\t\tnc.mu.Lock()\r\n\r\n\t\t// Check to see if we should bail out.\r\n\t\tif !nc.isConnected() || nc.isConnecting() || bw != nc.bw || conn != nc.conn {\r\n\t\t\tnc.mu.Unlock()\r\n\t\t\treturn\r\n\t\t}\r\n\t\tif bw.Buffered() > 0 {\r\n\t\t\tif err := bw.Flush(); err != nil {\r\n\t\t\t\tif nc.err == nil {\r\n\t\t\t\t\tnc.err = err\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tnc.mu.Unlock()\r\n\t}\r\n}\r\n\r\n// processPing will send an immediate pong protocol response to the\r\n// server. The server uses this mechanism to detect dead clients.\r\nfunc (nc *Conn) processPing() {\r\n\tnc.sendProto(pongProto)\r\n}\r\n\r\n// processPong is used to process responses to the client's ping\r\n// messages. We use pings for the flush mechanism as well.\r\nfunc (nc *Conn) processPong() {\r\n\tvar ch chan struct{}\r\n\r\n\tnc.mu.Lock()\r\n\tif len(nc.pongs) > 0 {\r\n\t\tch = nc.pongs[0]\r\n\t\tnc.pongs = nc.pongs[1:]\r\n\t}\r\n\tnc.pout = 0\r\n\tnc.mu.Unlock()\r\n\tif ch != nil {\r\n\t\tch <- struct{}{}\r\n\t}\r\n}\r\n\r\n// processOK is a placeholder for processing OK messages.\r\nfunc (nc *Conn) processOK() {\r\n\t// do nothing\r\n}\r\n\r\n// processInfo is used to parse the info messages sent\r\n// from the server.\r\n// This function may update the server pool.\r\nfunc (nc *Conn) processInfo(info string) error {\r\n\tif info == _EMPTY_ {\r\n\t\treturn nil\r\n\t}\r\n\tncInfo := serverInfo{}\r\n\tif err := json.Unmarshal([]byte(info), &ncInfo); err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Copy content into connection's info structure.\r\n\tnc.info = ncInfo\r\n\t// The array could be empty/not present on initial connect,\r\n\t// if advertise is disabled on that server, or servers that\r\n\t// did not include themselves in the async INFO protocol.\r\n\t// If empty, do not remove the implicit servers from the pool.\r\n\tif len(ncInfo.ConnectURLs) == 0 {\r\n\t\treturn nil\r\n\t}\r\n\t// Note about pool randomization: when the pool was first created,\r\n\t// it was randomized (if allowed). We keep the order the same (removing\r\n\t// implicit servers that are no longer sent to us). New URLs are sent\r\n\t// to us in no specific order so don't need extra randomization.\r\n\thasNew := false\r\n\t// This is what we got from the server we are connected to.\r\n\turls := nc.info.ConnectURLs\r\n\t// Transform that to a map for easy lookups\r\n\ttmp := make(map[string]struct{}, len(urls))\r\n\tfor _, curl := range urls {\r\n\t\ttmp[curl] = struct{}{}\r\n\t}\r\n\t// Walk the pool and removed the implicit servers that are no longer in the\r\n\t// given array/map\r\n\tsp := nc.srvPool\r\n\tfor i := 0; i < len(sp); i++ {\r\n\t\tsrv := sp[i]\r\n\t\tcurl := srv.url.Host\r\n\t\t// Check if this URL is in the INFO protocol\r\n\t\t_, inInfo := tmp[curl]\r\n\t\t// Remove from the temp map so that at the end we are left with only\r\n\t\t// new (or restarted) servers that need to be added to the pool.\r\n\t\tdelete(tmp, curl)\r\n\t\t// Keep servers that were set through Options, but also the one that\r\n\t\t// we are currently connected to (even if it is a discovered server).\r\n\t\tif !srv.isImplicit || srv.url == nc.current.url {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif !inInfo {\r\n\t\t\t// Remove from server pool. Keep current order.\r\n\t\t\tcopy(sp[i:], sp[i+1:])\r\n\t\t\tnc.srvPool = sp[:len(sp)-1]\r\n\t\t\tsp = nc.srvPool\r\n\t\t\ti--\r\n\t\t}\r\n\t}\r\n\t// Figure out if we should save off the current non-IP hostname if we encounter a bare IP.\r\n\tsaveTLS := nc.current != nil && !hostIsIP(nc.current.url)\r\n\r\n\t// If there are any left in the tmp map, these are new (or restarted) servers\r\n\t// and need to be added to the pool.\r\n\tfor curl := range tmp {\r\n\t\t// Before adding, check if this is a new (as in never seen) URL.\r\n\t\t// This is used to figure out if we invoke the DiscoveredServersCB\r\n\t\tif _, present := nc.urls[curl]; !present {\r\n\t\t\thasNew = true\r\n\t\t}\r\n\t\tnc.addURLToPool(fmt.Sprintf(\"%s://%s\", nc.connScheme(), curl), true, saveTLS)\r\n\t}\r\n\tif hasNew {\r\n\t\t// Randomize the pool if allowed but leave the first URL in place.\r\n\t\tif !nc.Opts.NoRandomize {\r\n\t\t\tnc.shufflePool(1)\r\n\t\t}\r\n\t\tif !nc.initc && nc.Opts.DiscoveredServersCB != nil {\r\n\t\t\tnc.ach.push(func() { nc.Opts.DiscoveredServersCB(nc) })\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// processAsyncInfo does the same than processInfo, but is called\r\n// from the parser. Calls processInfo under connection's lock\r\n// protection.\r\nfunc (nc *Conn) processAsyncInfo(info []byte) {\r\n\tnc.mu.Lock()\r\n\t// Ignore errors, we will simply not update the server pool...\r\n\tnc.processInfo(string(info))\r\n\tnc.mu.Unlock()\r\n}\r\n\r\n// LastError reports the last error encountered via the connection.\r\n// It can be used reliably within ClosedCB in order to find out reason\r\n// why connection was closed for example.\r\nfunc (nc *Conn) LastError() error {\r\n\tif nc == nil {\r\n\t\treturn ErrInvalidConnection\r\n\t}\r\n\tnc.mu.RLock()\r\n\terr := nc.err\r\n\tnc.mu.RUnlock()\r\n\treturn err\r\n}\r\n\r\n// Check if the given error string is an auth error, and if so returns\r\n// the corresponding ErrXXX error, nil otherwise\r\nfunc checkAuthError(e string) error {\r\n\tif strings.HasPrefix(e, AUTHORIZATION_ERR) {\r\n\t\treturn ErrAuthorization\r\n\t}\r\n\tif strings.HasPrefix(e, AUTHENTICATION_EXPIRED_ERR) {\r\n\t\treturn ErrAuthExpired\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// processErr processes any error messages from the server and\r\n// sets the connection's lastError.\r\nfunc (nc *Conn) processErr(ie string) {\r\n\t// Trim, remove quotes\r\n\tne := normalizeErr(ie)\r\n\t// convert to lower case.\r\n\te := strings.ToLower(ne)\r\n\r\n\tclose := false\r\n\r\n\t// FIXME(dlc) - process Slow Consumer signals special.\r\n\tif e == STALE_CONNECTION {\r\n\t\tnc.processOpErr(ErrStaleConnection)\r\n\t} else if strings.HasPrefix(e, PERMISSIONS_ERR) {\r\n\t\tnc.processPermissionsViolation(ne)\r\n\t} else if authErr := checkAuthError(e); authErr != nil {\r\n\t\tnc.mu.Lock()\r\n\t\tclose = nc.processAuthError(authErr)\r\n\t\tnc.mu.Unlock()\r\n\t} else {\r\n\t\tclose = true\r\n\t\tnc.mu.Lock()\r\n\t\tnc.err = errors.New(\"nats: \" + ne)\r\n\t\tnc.mu.Unlock()\r\n\t}\r\n\tif close {\r\n\t\tnc.close(CLOSED, true, nil)\r\n\t}\r\n}\r\n\r\n// kickFlusher will send a bool on a channel to kick the\r\n// flush Go routine to flush data to the server.\r\nfunc (nc *Conn) kickFlusher() {\r\n\tif nc.bw != nil {\r\n\t\tselect {\r\n\t\tcase nc.fch <- struct{}{}:\r\n\t\tdefault:\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Publish publishes the data argument to the given subject. The data\r\n// argument is left untouched and needs to be correctly interpreted on\r\n// the receiver.\r\nfunc (nc *Conn) Publish(subj string, data []byte) error {\r\n\treturn nc.publish(subj, _EMPTY_, data)\r\n}\r\n\r\n// PublishMsg publishes the Msg structure, which includes the\r\n// Subject, an optional Reply and an optional Data field.\r\nfunc (nc *Conn) PublishMsg(m *Msg) error {\r\n\tif m == nil {\r\n\t\treturn ErrInvalidMsg\r\n\t}\r\n\treturn nc.publish(m.Subject, m.Reply, m.Data)\r\n}\r\n\r\n// PublishRequest will perform a Publish() excpecting a response on the\r\n// reply subject. Use Request() for automatically waiting for a response\r\n// inline.\r\nfunc (nc *Conn) PublishRequest(subj, reply string, data []byte) error {\r\n\treturn nc.publish(subj, reply, data)\r\n}\r\n\r\n// Used for handrolled itoa\r\nconst digits = \"0123456789\"\r\n\r\n// publish is the internal function to publish messages to a nats-server.\r\n// Sends a protocol data message by queuing into the bufio writer\r\n// and kicking the flush go routine. These writes should be protected.\r\nfunc (nc *Conn) publish(subj, reply string, data []byte) error {\r\n\tif nc == nil {\r\n\t\treturn ErrInvalidConnection\r\n\t}\r\n\tif subj == \"\" {\r\n\t\treturn ErrBadSubject\r\n\t}\r\n\tnc.mu.Lock()\r\n\r\n\tif nc.isClosed() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\r\n\tif nc.isDrainingPubs() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn ErrConnectionDraining\r\n\t}\r\n\r\n\t// Proactively reject payloads over the threshold set by server.\r\n\tmsgSize := int64(len(data))\r\n\tif msgSize > nc.info.MaxPayload {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn ErrMaxPayload\r\n\t}\r\n\r\n\t// Check if we are reconnecting, and if so check if\r\n\t// we have exceeded our reconnect outbound buffer limits.\r\n\tif nc.isReconnecting() {\r\n\t\t// Flush to underlying buffer.\r\n\t\tnc.bw.Flush()\r\n\t\t// Check if we are over\r\n\t\tif nc.pending.Len() >= nc.Opts.ReconnectBufSize {\r\n\t\t\tnc.mu.Unlock()\r\n\t\t\treturn ErrReconnectBufExceeded\r\n\t\t}\r\n\t}\r\n\r\n\tmsgh := nc.scratch[:len(_PUB_P_)]\r\n\tmsgh = append(msgh, subj...)\r\n\tmsgh = append(msgh, ' ')\r\n\tif reply != \"\" {\r\n\t\tmsgh = append(msgh, reply...)\r\n\t\tmsgh = append(msgh, ' ')\r\n\t}\r\n\r\n\t// We could be smarter here, but simple loop is ok,\r\n\t// just avoid strconv in fast path\r\n\t// FIXME(dlc) - Find a better way here.\r\n\t// msgh = strconv.AppendInt(msgh, int64(len(data)), 10)\r\n\r\n\tvar b [12]byte\r\n\tvar i = len(b)\r\n\tif len(data) > 0 {\r\n\t\tfor l := len(data); l > 0; l /= 10 {\r\n\t\t\ti -= 1\r\n\t\t\tb[i] = digits[l%10]\r\n\t\t}\r\n\t} else {\r\n\t\ti -= 1\r\n\t\tb[i] = digits[0]\r\n\t}\r\n\r\n\tmsgh = append(msgh, b[i:]...)\r\n\tmsgh = append(msgh, _CRLF_...)\r\n\r\n\t_, err := nc.bw.Write(msgh)\r\n\tif err == nil {\r\n\t\t_, err = nc.bw.Write(data)\r\n\t}\r\n\tif err == nil {\r\n\t\t_, err = nc.bw.WriteString(_CRLF_)\r\n\t}\r\n\tif err != nil {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn err\r\n\t}\r\n\r\n\tnc.OutMsgs++\r\n\tnc.OutBytes += uint64(len(data))\r\n\r\n\tif len(nc.fch) == 0 {\r\n\t\tnc.kickFlusher()\r\n\t}\r\n\tnc.mu.Unlock()\r\n\treturn nil\r\n}\r\n\r\n// respHandler is the global response handler. It will look up\r\n// the appropriate channel based on the last token and place\r\n// the message on the channel if possible.\r\nfunc (nc *Conn) respHandler(m *Msg) {\r\n\tnc.mu.Lock()\r\n\r\n\t// Just return if closed.\r\n\tif nc.isClosed() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\tvar mch chan *Msg\r\n\r\n\t// Grab mch\r\n\trt := nc.respToken(m.Subject)\r\n\tif rt != _EMPTY_ {\r\n\t\tmch = nc.respMap[rt]\r\n\t\t// Delete the key regardless, one response only.\r\n\t\tdelete(nc.respMap, rt)\r\n\t} else if len(nc.respMap) == 1 {\r\n\t\t// If the server has rewritten the subject, the response token (rt)\r\n\t\t// will not match (could be the case with JetStream). If that is the\r\n\t\t// case and there is a single entry, use that.\r\n\t\tfor k, v := range nc.respMap {\r\n\t\t\tmch = v\r\n\t\t\tdelete(nc.respMap, k)\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\tnc.mu.Unlock()\r\n\r\n\t// Don't block, let Request timeout instead, mch is\r\n\t// buffered and we should delete the key before a\r\n\t// second response is processed.\r\n\tselect {\r\n\tcase mch <- m:\r\n\tdefault:\r\n\t\treturn\r\n\t}\r\n}\r\n\r\n// Helper to setup and send new request style requests. Return the chan to receive the response.\r\nfunc (nc *Conn) createNewRequestAndSend(subj string, data []byte) (chan *Msg, string, error) {\r\n\t// Do setup for the new style if needed.\r\n\tif nc.respMap == nil {\r\n\t\tnc.initNewResp()\r\n\t}\r\n\t// Create new literal Inbox and map to a chan msg.\r\n\tmch := make(chan *Msg, RequestChanLen)\r\n\trespInbox := nc.newRespInbox()\r\n\ttoken := respInbox[respInboxPrefixLen:]\r\n\tnc.respMap[token] = mch\r\n\tif nc.respMux == nil {\r\n\t\t// Create the response subscription we will use for all new style responses.\r\n\t\t// This will be on an _INBOX with an additional terminal token. The subscription\r\n\t\t// will be on a wildcard.\r\n\t\ts, err := nc.subscribeLocked(nc.respSub, _EMPTY_, nc.respHandler, nil, false)\r\n\t\tif err != nil {\r\n\t\t\tnc.mu.Unlock()\r\n\t\t\treturn nil, token, err\r\n\t\t}\r\n\t\tnc.respScanf = strings.Replace(nc.respSub, \"*\", \"%s\", -1)\r\n\t\tnc.respMux = s\r\n\t}\r\n\tnc.mu.Unlock()\r\n\r\n\tif err := nc.PublishRequest(subj, respInbox, data); err != nil {\r\n\t\treturn nil, token, err\r\n\t}\r\n\r\n\treturn mch, token, nil\r\n}\r\n\r\n// Request will send a request payload and deliver the response message,\r\n// or an error, including a timeout if no message was received properly.\r\nfunc (nc *Conn) Request(subj string, data []byte, timeout time.Duration) (*Msg, error) {\r\n\tif nc == nil {\r\n\t\treturn nil, ErrInvalidConnection\r\n\t}\r\n\r\n\tnc.mu.Lock()\r\n\t// If user wants the old style.\r\n\tif nc.Opts.UseOldRequestStyle {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn nc.oldRequest(subj, data, timeout)\r\n\t}\r\n\r\n\tmch, token, err := nc.createNewRequestAndSend(subj, data)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tt := globalTimerPool.Get(timeout)\r\n\tdefer globalTimerPool.Put(t)\r\n\r\n\tvar ok bool\r\n\tvar msg *Msg\r\n\r\n\tselect {\r\n\tcase msg, ok = <-mch:\r\n\t\tif !ok {\r\n\t\t\treturn nil, ErrConnectionClosed\r\n\t\t}\r\n\tcase <-t.C:\r\n\t\tnc.mu.Lock()\r\n\t\tdelete(nc.respMap, token)\r\n\t\tnc.mu.Unlock()\r\n\t\treturn nil, ErrTimeout\r\n\t}\r\n\r\n\treturn msg, nil\r\n}\r\n\r\n// oldRequest will create an Inbox and perform a Request() call\r\n// with the Inbox reply and return the first reply received.\r\n// This is optimized for the case of multiple responses.\r\nfunc (nc *Conn) oldRequest(subj string, data []byte, timeout time.Duration) (*Msg, error) {\r\n\tinbox := NewInbox()\r\n\tch := make(chan *Msg, RequestChanLen)\r\n\r\n\ts, err := nc.subscribe(inbox, _EMPTY_, nil, ch, true)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\ts.AutoUnsubscribe(1)\r\n\tdefer s.Unsubscribe()\r\n\r\n\terr = nc.PublishRequest(subj, inbox, data)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\treturn s.NextMsg(timeout)\r\n}\r\n\r\n// InboxPrefix is the prefix for all inbox subjects.\r\nconst (\r\n\tInboxPrefix        = \"_INBOX.\"\r\n\tinboxPrefixLen     = len(InboxPrefix)\r\n\trespInboxPrefixLen = inboxPrefixLen + nuidSize + 1\r\n\treplySuffixLen     = 8 // Gives us 62^8\r\n\trdigits            = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\r\n\tbase               = 62\r\n)\r\n\r\n// NewInbox will return an inbox string which can be used for directed replies from\r\n// subscribers. These are guaranteed to be unique, but can be shared and subscribed\r\n// to by others.\r\nfunc NewInbox() string {\r\n\tvar b [inboxPrefixLen + nuidSize]byte\r\n\tpres := b[:inboxPrefixLen]\r\n\tcopy(pres, InboxPrefix)\r\n\tns := b[inboxPrefixLen:]\r\n\tcopy(ns, nuid.Next())\r\n\treturn string(b[:])\r\n}\r\n\r\n// Function to init new response structures.\r\nfunc (nc *Conn) initNewResp() {\r\n\t// _INBOX wildcard\r\n\tnc.respSub = fmt.Sprintf(\"%s.*\", NewInbox())\r\n\tnc.respMap = make(map[string]chan *Msg)\r\n\tnc.respRand = rand.New(rand.NewSource(time.Now().UnixNano()))\r\n}\r\n\r\n// newRespInbox creates a new literal response subject\r\n// that will trigger the mux subscription handler.\r\n// Lock should be held.\r\nfunc (nc *Conn) newRespInbox() string {\r\n\tif nc.respMap == nil {\r\n\t\tnc.initNewResp()\r\n\t}\r\n\tvar b [respInboxPrefixLen + replySuffixLen]byte\r\n\tpres := b[:respInboxPrefixLen]\r\n\tcopy(pres, nc.respSub)\r\n\trn := nc.respRand.Int63()\r\n\tfor i, l := respInboxPrefixLen, rn; i < len(b); i++ {\r\n\t\tb[i] = rdigits[l%base]\r\n\t\tl /= base\r\n\t}\r\n\treturn string(b[:])\r\n}\r\n\r\n// NewRespInbox is the new format used for _INBOX.\r\nfunc (nc *Conn) NewRespInbox() string {\r\n\tnc.mu.Lock()\r\n\ts := nc.newRespInbox()\r\n\tnc.mu.Unlock()\r\n\treturn s\r\n}\r\n\r\n// respToken will return the last token of a literal response inbox\r\n// which we use for the message channel lookup. This needs to do a\r\n// scan to protect itself against the server changing the subject.\r\n// Lock should be held.\r\nfunc (nc *Conn) respToken(respInbox string) string {\r\n\tvar token string\r\n\tn, err := fmt.Sscanf(respInbox, nc.respScanf, &token)\r\n\tif err != nil || n != 1 {\r\n\t\treturn \"\"\r\n\t}\r\n\treturn token\r\n}\r\n\r\n// Subscribe will express interest in the given subject. The subject\r\n// can have wildcards (partial:*, full:>). Messages will be delivered\r\n// to the associated MsgHandler.\r\nfunc (nc *Conn) Subscribe(subj string, cb MsgHandler) (*Subscription, error) {\r\n\treturn nc.subscribe(subj, _EMPTY_, cb, nil, false)\r\n}\r\n\r\n// ChanSubscribe will express interest in the given subject and place\r\n// all messages received on the channel.\r\n// You should not close the channel until sub.Unsubscribe() has been called.\r\nfunc (nc *Conn) ChanSubscribe(subj string, ch chan *Msg) (*Subscription, error) {\r\n\treturn nc.subscribe(subj, _EMPTY_, nil, ch, false)\r\n}\r\n\r\n// ChanQueueSubscribe will express interest in the given subject.\r\n// All subscribers with the same queue name will form the queue group\r\n// and only one member of the group will be selected to receive any given message,\r\n// which will be placed on the channel.\r\n// You should not close the channel until sub.Unsubscribe() has been called.\r\n// Note: This is the same than QueueSubscribeSyncWithChan.\r\nfunc (nc *Conn) ChanQueueSubscribe(subj, group string, ch chan *Msg) (*Subscription, error) {\r\n\treturn nc.subscribe(subj, group, nil, ch, false)\r\n}\r\n\r\n// SubscribeSync will express interest on the given subject. Messages will\r\n// be received synchronously using Subscription.NextMsg().\r\nfunc (nc *Conn) SubscribeSync(subj string) (*Subscription, error) {\r\n\tif nc == nil {\r\n\t\treturn nil, ErrInvalidConnection\r\n\t}\r\n\tmch := make(chan *Msg, nc.Opts.SubChanLen)\r\n\ts, e := nc.subscribe(subj, _EMPTY_, nil, mch, true)\r\n\treturn s, e\r\n}\r\n\r\n// QueueSubscribe creates an asynchronous queue subscriber on the given subject.\r\n// All subscribers with the same queue name will form the queue group and\r\n// only one member of the group will be selected to receive any given\r\n// message asynchronously.\r\nfunc (nc *Conn) QueueSubscribe(subj, queue string, cb MsgHandler) (*Subscription, error) {\r\n\treturn nc.subscribe(subj, queue, cb, nil, false)\r\n}\r\n\r\n// QueueSubscribeSync creates a synchronous queue subscriber on the given\r\n// subject. All subscribers with the same queue name will form the queue\r\n// group and only one member of the group will be selected to receive any\r\n// given message synchronously using Subscription.NextMsg().\r\nfunc (nc *Conn) QueueSubscribeSync(subj, queue string) (*Subscription, error) {\r\n\tmch := make(chan *Msg, nc.Opts.SubChanLen)\r\n\ts, e := nc.subscribe(subj, queue, nil, mch, true)\r\n\treturn s, e\r\n}\r\n\r\n// QueueSubscribeSyncWithChan will express interest in the given subject.\r\n// All subscribers with the same queue name will form the queue group\r\n// and only one member of the group will be selected to receive any given message,\r\n// which will be placed on the channel.\r\n// You should not close the channel until sub.Unsubscribe() has been called.\r\n// Note: This is the same than ChanQueueSubscribe.\r\nfunc (nc *Conn) QueueSubscribeSyncWithChan(subj, queue string, ch chan *Msg) (*Subscription, error) {\r\n\treturn nc.subscribe(subj, queue, nil, ch, false)\r\n}\r\n\r\n// badSubject will do quick test on whether a subject is acceptable.\r\n// Spaces are not allowed and all tokens should be > 0 in len.\r\nfunc badSubject(subj string) bool {\r\n\tif strings.ContainsAny(subj, \" \\t\\r\\n\") {\r\n\t\treturn true\r\n\t}\r\n\ttokens := strings.Split(subj, \".\")\r\n\tfor _, t := range tokens {\r\n\t\tif len(t) == 0 {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// badQueue will check a queue name for whitespace.\r\nfunc badQueue(qname string) bool {\r\n\treturn strings.ContainsAny(qname, \" \\t\\r\\n\")\r\n}\r\n\r\n// subscribe is the internal subscribe function that indicates interest in a subject.\r\nfunc (nc *Conn) subscribe(subj, queue string, cb MsgHandler, ch chan *Msg, isSync bool) (*Subscription, error) {\r\n\tif nc == nil {\r\n\t\treturn nil, ErrInvalidConnection\r\n\t}\r\n\tnc.mu.Lock()\r\n\ts, err := nc.subscribeLocked(subj, queue, cb, ch, isSync)\r\n\tnc.mu.Unlock()\r\n\treturn s, err\r\n}\r\n\r\nfunc (nc *Conn) subscribeLocked(subj, queue string, cb MsgHandler, ch chan *Msg, isSync bool) (*Subscription, error) {\r\n\tif nc == nil {\r\n\t\treturn nil, ErrInvalidConnection\r\n\t}\r\n\tif badSubject(subj) {\r\n\t\treturn nil, ErrBadSubject\r\n\t}\r\n\tif queue != \"\" && badQueue(queue) {\r\n\t\treturn nil, ErrBadQueueName\r\n\t}\r\n\r\n\t// Check for some error conditions.\r\n\tif nc.isClosed() {\r\n\t\treturn nil, ErrConnectionClosed\r\n\t}\r\n\tif nc.isDraining() {\r\n\t\treturn nil, ErrConnectionDraining\r\n\t}\r\n\r\n\tif cb == nil && ch == nil {\r\n\t\treturn nil, ErrBadSubscription\r\n\t}\r\n\r\n\tsub := &Subscription{Subject: subj, Queue: queue, mcb: cb, conn: nc}\r\n\t// Set pending limits.\r\n\tsub.pMsgsLimit = DefaultSubPendingMsgsLimit\r\n\tsub.pBytesLimit = DefaultSubPendingBytesLimit\r\n\r\n\t// If we have an async callback, start up a sub specific\r\n\t// Go routine to deliver the messages.\r\n\tif cb != nil {\r\n\t\tsub.typ = AsyncSubscription\r\n\t\tsub.pCond = sync.NewCond(&sub.mu)\r\n\t\tgo nc.waitForMsgs(sub)\r\n\t} else if !isSync {\r\n\t\tsub.typ = ChanSubscription\r\n\t\tsub.mch = ch\r\n\t} else { // Sync Subscription\r\n\t\tsub.typ = SyncSubscription\r\n\t\tsub.mch = ch\r\n\t}\r\n\r\n\tnc.subsMu.Lock()\r\n\tnc.ssid++\r\n\tsub.sid = nc.ssid\r\n\tnc.subs[sub.sid] = sub\r\n\tnc.subsMu.Unlock()\r\n\r\n\t// We will send these for all subs when we reconnect\r\n\t// so that we can suppress here if reconnecting.\r\n\tif !nc.isReconnecting() {\r\n\t\tfmt.Fprintf(nc.bw, subProto, subj, queue, sub.sid)\r\n\t\t// Kick flusher if needed.\r\n\t\tif len(nc.fch) == 0 {\r\n\t\t\tnc.kickFlusher()\r\n\t\t}\r\n\t}\r\n\r\n\treturn sub, nil\r\n}\r\n\r\n// NumSubscriptions returns active number of subscriptions.\r\nfunc (nc *Conn) NumSubscriptions() int {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn len(nc.subs)\r\n}\r\n\r\n// Lock for nc should be held here upon entry\r\nfunc (nc *Conn) removeSub(s *Subscription) {\r\n\tnc.subsMu.Lock()\r\n\tdelete(nc.subs, s.sid)\r\n\tnc.subsMu.Unlock()\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\t// Release callers on NextMsg for SyncSubscription only\r\n\tif s.mch != nil && s.typ == SyncSubscription {\r\n\t\tclose(s.mch)\r\n\t}\r\n\ts.mch = nil\r\n\r\n\t// Mark as invalid\r\n\ts.closed = true\r\n\tif s.pCond != nil {\r\n\t\ts.pCond.Broadcast()\r\n\t}\r\n}\r\n\r\n// SubscriptionType is the type of the Subscription.\r\ntype SubscriptionType int\r\n\r\n// The different types of subscription types.\r\nconst (\r\n\tAsyncSubscription = SubscriptionType(iota)\r\n\tSyncSubscription\r\n\tChanSubscription\r\n\tNilSubscription\r\n)\r\n\r\n// Type returns the type of Subscription.\r\nfunc (s *Subscription) Type() SubscriptionType {\r\n\tif s == nil {\r\n\t\treturn NilSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.typ\r\n}\r\n\r\n// IsValid returns a boolean indicating whether the subscription\r\n// is still active. This will return false if the subscription has\r\n// already been closed.\r\nfunc (s *Subscription) IsValid() bool {\r\n\tif s == nil {\r\n\t\treturn false\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.conn != nil && !s.closed\r\n}\r\n\r\n// Drain will remove interest but continue callbacks until all messages\r\n// have been processed.\r\nfunc (s *Subscription) Drain() error {\r\n\tif s == nil {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tconn := s.conn\r\n\ts.mu.Unlock()\r\n\tif conn == nil {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\treturn conn.unsubscribe(s, 0, true)\r\n}\r\n\r\n// Unsubscribe will remove interest in the given subject.\r\nfunc (s *Subscription) Unsubscribe() error {\r\n\tif s == nil {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tconn := s.conn\r\n\tclosed := s.closed\r\n\ts.mu.Unlock()\r\n\tif conn == nil || conn.IsClosed() {\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\tif closed {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\tif conn.IsDraining() {\r\n\t\treturn ErrConnectionDraining\r\n\t}\r\n\treturn conn.unsubscribe(s, 0, false)\r\n}\r\n\r\n// checkDrained will watch for a subscription to be fully drained\r\n// and then remove it.\r\nfunc (nc *Conn) checkDrained(sub *Subscription) {\r\n\tif nc == nil || sub == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\t// This allows us to know that whatever we have in the client pending\r\n\t// is correct and the server will not send additional information.\r\n\tnc.Flush()\r\n\r\n\t// Once we are here we just wait for Pending to reach 0 or\r\n\t// any other state to exit this go routine.\r\n\tfor {\r\n\t\t// check connection is still valid.\r\n\t\tif nc.IsClosed() {\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\t// Check subscription state\r\n\t\tsub.mu.Lock()\r\n\t\tconn := sub.conn\r\n\t\tclosed := sub.closed\r\n\t\tpMsgs := sub.pMsgs\r\n\t\tsub.mu.Unlock()\r\n\r\n\t\tif conn == nil || closed || pMsgs == 0 {\r\n\t\t\tnc.mu.Lock()\r\n\t\t\tnc.removeSub(sub)\r\n\t\t\tnc.mu.Unlock()\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\ttime.Sleep(100 * time.Millisecond)\r\n\t}\r\n}\r\n\r\n// AutoUnsubscribe will issue an automatic Unsubscribe that is\r\n// processed by the server when max messages have been received.\r\n// This can be useful when sending a request to an unknown number\r\n// of subscribers.\r\nfunc (s *Subscription) AutoUnsubscribe(max int) error {\r\n\tif s == nil {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tconn := s.conn\r\n\tclosed := s.closed\r\n\ts.mu.Unlock()\r\n\tif conn == nil || closed {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\treturn conn.unsubscribe(s, max, false)\r\n}\r\n\r\n// unsubscribe performs the low level unsubscribe to the server.\r\n// Use Subscription.Unsubscribe()\r\nfunc (nc *Conn) unsubscribe(sub *Subscription, max int, drainMode bool) error {\r\n\tnc.mu.Lock()\r\n\t// ok here, but defer is expensive\r\n\tdefer nc.mu.Unlock()\r\n\tdefer nc.kickFlusher()\r\n\r\n\tif nc.isClosed() {\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\r\n\tnc.subsMu.RLock()\r\n\ts := nc.subs[sub.sid]\r\n\tnc.subsMu.RUnlock()\r\n\t// Already unsubscribed\r\n\tif s == nil {\r\n\t\treturn nil\r\n\t}\r\n\r\n\tmaxStr := _EMPTY_\r\n\tif max > 0 {\r\n\t\ts.max = uint64(max)\r\n\t\tmaxStr = strconv.Itoa(max)\r\n\t} else if !drainMode {\r\n\t\tnc.removeSub(s)\r\n\t}\r\n\r\n\tif drainMode {\r\n\t\tgo nc.checkDrained(sub)\r\n\t}\r\n\r\n\t// We will send these for all subs when we reconnect\r\n\t// so that we can suppress here.\r\n\tif !nc.isReconnecting() {\r\n\t\tfmt.Fprintf(nc.bw, unsubProto, s.sid, maxStr)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// NextMsg will return the next message available to a synchronous subscriber\r\n// or block until one is available. An error is returned if the subscription is invalid (ErrBadSubscription),\r\n// the connection is closed (ErrConnectionClosed), or the timeout is reached (ErrTimeout).\r\nfunc (s *Subscription) NextMsg(timeout time.Duration) (*Msg, error) {\r\n\tif s == nil {\r\n\t\treturn nil, ErrBadSubscription\r\n\t}\r\n\r\n\ts.mu.Lock()\r\n\terr := s.validateNextMsgState()\r\n\tif err != nil {\r\n\t\ts.mu.Unlock()\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\t// snapshot\r\n\tmch := s.mch\r\n\ts.mu.Unlock()\r\n\r\n\tvar ok bool\r\n\tvar msg *Msg\r\n\r\n\t// If something is available right away, let's optimize that case.\r\n\tselect {\r\n\tcase msg, ok = <-mch:\r\n\t\tif !ok {\r\n\t\t\treturn nil, s.getNextMsgErr()\r\n\t\t}\r\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t} else {\r\n\t\t\treturn msg, nil\r\n\t\t}\r\n\tdefault:\r\n\t}\r\n\r\n\t// If we are here a message was not immediately available, so lets loop\r\n\t// with a timeout.\r\n\r\n\tt := globalTimerPool.Get(timeout)\r\n\tdefer globalTimerPool.Put(t)\r\n\r\n\tselect {\r\n\tcase msg, ok = <-mch:\r\n\t\tif !ok {\r\n\t\t\treturn nil, s.getNextMsgErr()\r\n\t\t}\r\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\tcase <-t.C:\r\n\t\treturn nil, ErrTimeout\r\n\t}\r\n\r\n\treturn msg, nil\r\n}\r\n\r\n// validateNextMsgState checks whether the subscription is in a valid\r\n// state to call NextMsg and be delivered another message synchronously.\r\n// This should be called while holding the lock.\r\nfunc (s *Subscription) validateNextMsgState() error {\r\n\tif s.connClosed {\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\tif s.mch == nil {\r\n\t\tif s.max > 0 && s.delivered >= s.max {\r\n\t\t\treturn ErrMaxMessages\r\n\t\t} else if s.closed {\r\n\t\t\treturn ErrBadSubscription\r\n\t\t}\r\n\t}\r\n\tif s.mcb != nil {\r\n\t\treturn ErrSyncSubRequired\r\n\t}\r\n\tif s.sc {\r\n\t\ts.sc = false\r\n\t\treturn ErrSlowConsumer\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// This is called when the sync channel has been closed.\r\n// The error returned will be either connection or subscription\r\n// closed depending on what caused NextMsg() to fail.\r\nfunc (s *Subscription) getNextMsgErr() error {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.connClosed {\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\treturn ErrBadSubscription\r\n}\r\n\r\n// processNextMsgDelivered takes a message and applies the needed\r\n// accounting to the stats from the subscription, returning an\r\n// error in case we have the maximum number of messages have been\r\n// delivered already. It should not be called while holding the lock.\r\nfunc (s *Subscription) processNextMsgDelivered(msg *Msg) error {\r\n\ts.mu.Lock()\r\n\tnc := s.conn\r\n\tmax := s.max\r\n\r\n\t// Update some stats.\r\n\ts.delivered++\r\n\tdelivered := s.delivered\r\n\tif s.typ == SyncSubscription {\r\n\t\ts.pMsgs--\r\n\t\ts.pBytes -= len(msg.Data)\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\tif max > 0 {\r\n\t\tif delivered > max {\r\n\t\t\treturn ErrMaxMessages\r\n\t\t}\r\n\t\t// Remove subscription if we have reached max.\r\n\t\tif delivered == max {\r\n\t\t\tnc.mu.Lock()\r\n\t\t\tnc.removeSub(s)\r\n\t\t\tnc.mu.Unlock()\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// Queued returns the number of queued messages in the client for this subscription.\r\n// DEPRECATED: Use Pending()\r\nfunc (s *Subscription) QueuedMsgs() (int, error) {\r\n\tm, _, err := s.Pending()\r\n\treturn int(m), err\r\n}\r\n\r\n// Pending returns the number of queued messages and queued bytes in the client for this subscription.\r\nfunc (s *Subscription) Pending() (int, int, error) {\r\n\tif s == nil {\r\n\t\treturn -1, -1, ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn -1, -1, ErrBadSubscription\r\n\t}\r\n\tif s.typ == ChanSubscription {\r\n\t\treturn -1, -1, ErrTypeSubscription\r\n\t}\r\n\treturn s.pMsgs, s.pBytes, nil\r\n}\r\n\r\n// MaxPending returns the maximum number of queued messages and queued bytes seen so far.\r\nfunc (s *Subscription) MaxPending() (int, int, error) {\r\n\tif s == nil {\r\n\t\treturn -1, -1, ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn -1, -1, ErrBadSubscription\r\n\t}\r\n\tif s.typ == ChanSubscription {\r\n\t\treturn -1, -1, ErrTypeSubscription\r\n\t}\r\n\treturn s.pMsgsMax, s.pBytesMax, nil\r\n}\r\n\r\n// ClearMaxPending resets the maximums seen so far.\r\nfunc (s *Subscription) ClearMaxPending() error {\r\n\tif s == nil {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\tif s.typ == ChanSubscription {\r\n\t\treturn ErrTypeSubscription\r\n\t}\r\n\ts.pMsgsMax, s.pBytesMax = 0, 0\r\n\treturn nil\r\n}\r\n\r\n// Pending Limits\r\nconst (\r\n\tDefaultSubPendingMsgsLimit  = 65536\r\n\tDefaultSubPendingBytesLimit = 65536 * 1024\r\n)\r\n\r\n// PendingLimits returns the current limits for this subscription.\r\n// If no error is returned, a negative value indicates that the\r\n// given metric is not limited.\r\nfunc (s *Subscription) PendingLimits() (int, int, error) {\r\n\tif s == nil {\r\n\t\treturn -1, -1, ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn -1, -1, ErrBadSubscription\r\n\t}\r\n\tif s.typ == ChanSubscription {\r\n\t\treturn -1, -1, ErrTypeSubscription\r\n\t}\r\n\treturn s.pMsgsLimit, s.pBytesLimit, nil\r\n}\r\n\r\n// SetPendingLimits sets the limits for pending msgs and bytes for this subscription.\r\n// Zero is not allowed. Any negative value means that the given metric is not limited.\r\nfunc (s *Subscription) SetPendingLimits(msgLimit, bytesLimit int) error {\r\n\tif s == nil {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn ErrBadSubscription\r\n\t}\r\n\tif s.typ == ChanSubscription {\r\n\t\treturn ErrTypeSubscription\r\n\t}\r\n\tif msgLimit == 0 || bytesLimit == 0 {\r\n\t\treturn ErrInvalidArg\r\n\t}\r\n\ts.pMsgsLimit, s.pBytesLimit = msgLimit, bytesLimit\r\n\treturn nil\r\n}\r\n\r\n// Delivered returns the number of delivered messages for this subscription.\r\nfunc (s *Subscription) Delivered() (int64, error) {\r\n\tif s == nil {\r\n\t\treturn -1, ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn -1, ErrBadSubscription\r\n\t}\r\n\treturn int64(s.delivered), nil\r\n}\r\n\r\n// Dropped returns the number of known dropped messages for this subscription.\r\n// This will correspond to messages dropped by violations of PendingLimits. If\r\n// the server declares the connection a SlowConsumer, this number may not be\r\n// valid.\r\nfunc (s *Subscription) Dropped() (int, error) {\r\n\tif s == nil {\r\n\t\treturn -1, ErrBadSubscription\r\n\t}\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.conn == nil || s.closed {\r\n\t\treturn -1, ErrBadSubscription\r\n\t}\r\n\treturn s.dropped, nil\r\n}\r\n\r\n// Respond allows a convenient way to respond to requests in service based subscriptions.\r\nfunc (m *Msg) Respond(data []byte) error {\r\n\tif m == nil || m.Sub == nil {\r\n\t\treturn ErrMsgNotBound\r\n\t}\r\n\tif m.Reply == \"\" {\r\n\t\treturn ErrMsgNoReply\r\n\t}\r\n\tm.Sub.mu.Lock()\r\n\tnc := m.Sub.conn\r\n\tm.Sub.mu.Unlock()\r\n\t// No need to check the connection here since the call to publish will do all the checking.\r\n\treturn nc.Publish(m.Reply, data)\r\n}\r\n\r\n// FIXME: This is a hack\r\n// removeFlushEntry is needed when we need to discard queued up responses\r\n// for our pings as part of a flush call. This happens when we have a flush\r\n// call outstanding and we call close.\r\nfunc (nc *Conn) removeFlushEntry(ch chan struct{}) bool {\r\n\tnc.mu.Lock()\r\n\tdefer nc.mu.Unlock()\r\n\tif nc.pongs == nil {\r\n\t\treturn false\r\n\t}\r\n\tfor i, c := range nc.pongs {\r\n\t\tif c == ch {\r\n\t\t\tnc.pongs[i] = nil\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// The lock must be held entering this function.\r\nfunc (nc *Conn) sendPing(ch chan struct{}) {\r\n\tnc.pongs = append(nc.pongs, ch)\r\n\tnc.bw.WriteString(pingProto)\r\n\t// Flush in place.\r\n\tnc.bw.Flush()\r\n}\r\n\r\n// This will fire periodically and send a client origin\r\n// ping to the server. Will also check that we have received\r\n// responses from the server.\r\nfunc (nc *Conn) processPingTimer() {\r\n\tnc.mu.Lock()\r\n\r\n\tif nc.status != CONNECTED {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\t// Check for violation\r\n\tnc.pout++\r\n\tif nc.pout > nc.Opts.MaxPingsOut {\r\n\t\tnc.mu.Unlock()\r\n\t\tnc.processOpErr(ErrStaleConnection)\r\n\t\treturn\r\n\t}\r\n\r\n\tnc.sendPing(nil)\r\n\tnc.ptmr.Reset(nc.Opts.PingInterval)\r\n\tnc.mu.Unlock()\r\n}\r\n\r\n// FlushTimeout allows a Flush operation to have an associated timeout.\r\nfunc (nc *Conn) FlushTimeout(timeout time.Duration) (err error) {\r\n\tif nc == nil {\r\n\t\treturn ErrInvalidConnection\r\n\t}\r\n\tif timeout <= 0 {\r\n\t\treturn ErrBadTimeout\r\n\t}\r\n\r\n\tnc.mu.Lock()\r\n\tif nc.isClosed() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\tt := globalTimerPool.Get(timeout)\r\n\tdefer globalTimerPool.Put(t)\r\n\r\n\t// Create a buffered channel to prevent chan send to block\r\n\t// in processPong() if this code here times out just when\r\n\t// PONG was received.\r\n\tch := make(chan struct{}, 1)\r\n\tnc.sendPing(ch)\r\n\tnc.mu.Unlock()\r\n\r\n\tselect {\r\n\tcase _, ok := <-ch:\r\n\t\tif !ok {\r\n\t\t\terr = ErrConnectionClosed\r\n\t\t} else {\r\n\t\t\tclose(ch)\r\n\t\t}\r\n\tcase <-t.C:\r\n\t\terr = ErrTimeout\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\tnc.removeFlushEntry(ch)\r\n\t}\r\n\treturn\r\n}\r\n\r\n// RTT calculates the round trip time between this client and the server.\r\nfunc (nc *Conn) RTT() (time.Duration, error) {\r\n\tif nc.IsClosed() {\r\n\t\treturn 0, ErrConnectionClosed\r\n\t}\r\n\tif nc.IsReconnecting() {\r\n\t\treturn 0, ErrDisconnected\r\n\t}\r\n\tstart := time.Now()\r\n\tif err := nc.FlushTimeout(10 * time.Second); err != nil {\r\n\t\treturn 0, err\r\n\t}\r\n\treturn time.Since(start), nil\r\n}\r\n\r\n// Flush will perform a round trip to the server and return when it\r\n// receives the internal reply.\r\nfunc (nc *Conn) Flush() error {\r\n\treturn nc.FlushTimeout(10 * time.Second)\r\n}\r\n\r\n// Buffered will return the number of bytes buffered to be sent to the server.\r\n// FIXME(dlc) take into account disconnected state.\r\nfunc (nc *Conn) Buffered() (int, error) {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\tif nc.isClosed() || nc.bw == nil {\r\n\t\treturn -1, ErrConnectionClosed\r\n\t}\r\n\treturn nc.bw.Buffered(), nil\r\n}\r\n\r\n// resendSubscriptions will send our subscription state back to the\r\n// server. Used in reconnects\r\nfunc (nc *Conn) resendSubscriptions() {\r\n\t// Since we are going to send protocols to the server, we don't want to\r\n\t// be holding the subsMu lock (which is used in processMsg). So copy\r\n\t// the subscriptions in a temporary array.\r\n\tnc.subsMu.RLock()\r\n\tsubs := make([]*Subscription, 0, len(nc.subs))\r\n\tfor _, s := range nc.subs {\r\n\t\tsubs = append(subs, s)\r\n\t}\r\n\tnc.subsMu.RUnlock()\r\n\tfor _, s := range subs {\r\n\t\tadjustedMax := uint64(0)\r\n\t\ts.mu.Lock()\r\n\t\tif s.max > 0 {\r\n\t\t\tif s.delivered < s.max {\r\n\t\t\t\tadjustedMax = s.max - s.delivered\r\n\t\t\t}\r\n\t\t\t// adjustedMax could be 0 here if the number of delivered msgs\r\n\t\t\t// reached the max, if so unsubscribe.\r\n\t\t\tif adjustedMax == 0 {\r\n\t\t\t\ts.mu.Unlock()\r\n\t\t\t\tfmt.Fprintf(nc.bw, unsubProto, s.sid, _EMPTY_)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t}\r\n\t\ts.mu.Unlock()\r\n\r\n\t\tfmt.Fprintf(nc.bw, subProto, s.Subject, s.Queue, s.sid)\r\n\t\tif adjustedMax > 0 {\r\n\t\t\tmaxStr := strconv.Itoa(int(adjustedMax))\r\n\t\t\tfmt.Fprintf(nc.bw, unsubProto, s.sid, maxStr)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// This will clear any pending flush calls and release pending calls.\r\n// Lock is assumed to be held by the caller.\r\nfunc (nc *Conn) clearPendingFlushCalls() {\r\n\t// Clear any queued pongs, e.g. pending flush calls.\r\n\tfor _, ch := range nc.pongs {\r\n\t\tif ch != nil {\r\n\t\t\tclose(ch)\r\n\t\t}\r\n\t}\r\n\tnc.pongs = nil\r\n}\r\n\r\n// This will clear any pending Request calls.\r\n// Lock is assumed to be held by the caller.\r\nfunc (nc *Conn) clearPendingRequestCalls() {\r\n\tif nc.respMap == nil {\r\n\t\treturn\r\n\t}\r\n\tfor key, ch := range nc.respMap {\r\n\t\tif ch != nil {\r\n\t\t\tclose(ch)\r\n\t\t\tdelete(nc.respMap, key)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Low level close call that will do correct cleanup and set\r\n// desired status. Also controls whether user defined callbacks\r\n// will be triggered. The lock should not be held entering this\r\n// function. This function will handle the locking manually.\r\nfunc (nc *Conn) close(status Status, doCBs bool, err error) {\r\n\tnc.mu.Lock()\r\n\tif nc.isClosed() {\r\n\t\tnc.status = status\r\n\t\tnc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\tnc.status = CLOSED\r\n\r\n\t// Kick the Go routines so they fall out.\r\n\tnc.kickFlusher()\r\n\r\n\t// If the reconnect timer is waiting between a reconnect attempt,\r\n\t// this will kick it out.\r\n\tif nc.rqch != nil {\r\n\t\tclose(nc.rqch)\r\n\t\tnc.rqch = nil\r\n\t}\r\n\r\n\t// Clear any queued pongs, e.g. pending flush calls.\r\n\tnc.clearPendingFlushCalls()\r\n\r\n\t// Clear any queued and blocking Requests.\r\n\tnc.clearPendingRequestCalls()\r\n\r\n\t// Stop ping timer if set.\r\n\tnc.stopPingTimer()\r\n\tnc.ptmr = nil\r\n\r\n\t// Need to close and set tcp conn to nil if reconnect loop has stopped,\r\n\t// otherwise we would incorrectly invoke Disconnect handler (if set)\r\n\t// down below.\r\n\tif nc.ar && nc.conn != nil {\r\n\t\tnc.conn.Close()\r\n\t\tnc.conn = nil\r\n\t} else if nc.conn != nil {\r\n\t\t// Go ahead and make sure we have flushed the outbound\r\n\t\tnc.bw.Flush()\r\n\t\tdefer nc.conn.Close()\r\n\t}\r\n\r\n\t// Close sync subscriber channels and release any\r\n\t// pending NextMsg() calls.\r\n\tnc.subsMu.Lock()\r\n\tfor _, s := range nc.subs {\r\n\t\ts.mu.Lock()\r\n\r\n\t\t// Release callers on NextMsg for SyncSubscription only\r\n\t\tif s.mch != nil && s.typ == SyncSubscription {\r\n\t\t\tclose(s.mch)\r\n\t\t}\r\n\t\ts.mch = nil\r\n\t\t// Mark as invalid, for signaling to deliverMsgs\r\n\t\ts.closed = true\r\n\t\t// Mark connection closed in subscription\r\n\t\ts.connClosed = true\r\n\t\t// If we have an async subscription, signals it to exit\r\n\t\tif s.typ == AsyncSubscription && s.pCond != nil {\r\n\t\t\ts.pCond.Signal()\r\n\t\t}\r\n\r\n\t\ts.mu.Unlock()\r\n\t}\r\n\tnc.subs = nil\r\n\tnc.subsMu.Unlock()\r\n\r\n\tnc.status = status\r\n\r\n\t// Perform appropriate callback if needed for a disconnect.\r\n\tif doCBs {\r\n\t\tif nc.conn != nil {\r\n\t\t\tif nc.Opts.DisconnectedErrCB != nil {\r\n\t\t\t\tnc.ach.push(func() { nc.Opts.DisconnectedErrCB(nc, err) })\r\n\t\t\t} else if nc.Opts.DisconnectedCB != nil {\r\n\t\t\t\tnc.ach.push(func() { nc.Opts.DisconnectedCB(nc) })\r\n\t\t\t}\r\n\t\t}\r\n\t\tif nc.Opts.ClosedCB != nil {\r\n\t\t\tnc.ach.push(func() { nc.Opts.ClosedCB(nc) })\r\n\t\t}\r\n\t}\r\n\t// If this is terminal, then we have to notify the asyncCB handler that\r\n\t// it can exit once all async cbs have been dispatched.\r\n\tif status == CLOSED {\r\n\t\tnc.ach.close()\r\n\t}\r\n\tnc.mu.Unlock()\r\n}\r\n\r\n// Close will close the connection to the server. This call will release\r\n// all blocking calls, such as Flush() and NextMsg()\r\nfunc (nc *Conn) Close() {\r\n\tif nc != nil {\r\n\t\tnc.close(CLOSED, !nc.Opts.NoCallbacksAfterClientClose, nil)\r\n\t}\r\n}\r\n\r\n// IsClosed tests if a Conn has been closed.\r\nfunc (nc *Conn) IsClosed() bool {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.isClosed()\r\n}\r\n\r\n// IsReconnecting tests if a Conn is reconnecting.\r\nfunc (nc *Conn) IsReconnecting() bool {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.isReconnecting()\r\n}\r\n\r\n// IsConnected tests if a Conn is connected.\r\nfunc (nc *Conn) IsConnected() bool {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.isConnected()\r\n}\r\n\r\n// drainConnection will run in a separate Go routine and will\r\n// flush all publishes and drain all active subscriptions.\r\nfunc (nc *Conn) drainConnection() {\r\n\t// Snapshot subs list.\r\n\tnc.mu.Lock()\r\n\r\n\t// Check again here if we are in a state to not process.\r\n\tif nc.isClosed() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\tif nc.isConnecting() || nc.isReconnecting() {\r\n\t\tnc.mu.Unlock()\r\n\t\t// Move to closed state.\r\n\t\tnc.close(CLOSED, true, nil)\r\n\t\treturn\r\n\t}\r\n\r\n\tsubs := make([]*Subscription, 0, len(nc.subs))\r\n\tfor _, s := range nc.subs {\r\n\t\tsubs = append(subs, s)\r\n\t}\r\n\terrCB := nc.Opts.AsyncErrorCB\r\n\tdrainWait := nc.Opts.DrainTimeout\r\n\tnc.mu.Unlock()\r\n\r\n\t// for pushing errors with context.\r\n\tpushErr := func(err error) {\r\n\t\tnc.mu.Lock()\r\n\t\tnc.err = err\r\n\t\tif errCB != nil {\r\n\t\t\tnc.ach.push(func() { errCB(nc, nil, err) })\r\n\t\t}\r\n\t\tnc.mu.Unlock()\r\n\t}\r\n\r\n\t// Do subs first\r\n\tfor _, s := range subs {\r\n\t\tif err := s.Drain(); err != nil {\r\n\t\t\t// We will notify about these but continue.\r\n\t\t\tpushErr(err)\r\n\t\t}\r\n\t}\r\n\r\n\t// Wait for the subscriptions to drop to zero.\r\n\ttimeout := time.Now().Add(drainWait)\r\n\tfor time.Now().Before(timeout) {\r\n\t\tif nc.NumSubscriptions() == 0 {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\ttime.Sleep(10 * time.Millisecond)\r\n\t}\r\n\r\n\t// Check if we timed out.\r\n\tif nc.NumSubscriptions() != 0 {\r\n\t\tpushErr(ErrDrainTimeout)\r\n\t}\r\n\r\n\t// Flip State\r\n\tnc.mu.Lock()\r\n\tnc.status = DRAINING_PUBS\r\n\tnc.mu.Unlock()\r\n\r\n\t// Do publish drain via Flush() call.\r\n\terr := nc.FlushTimeout(5 * time.Second)\r\n\tif err != nil {\r\n\t\tpushErr(err)\r\n\t\tnc.close(CLOSED, true, nil)\r\n\t\treturn\r\n\t}\r\n\r\n\t// Move to closed state.\r\n\tnc.close(CLOSED, true, nil)\r\n}\r\n\r\n// Drain will put a connection into a drain state. All subscriptions will\r\n// immediately be put into a drain state. Upon completion, the publishers\r\n// will be drained and can not publish any additional messages. Upon draining\r\n// of the publishers, the connection will be closed. Use the ClosedCB()\r\n// option to know when the connection has moved from draining to closed.\r\nfunc (nc *Conn) Drain() error {\r\n\tnc.mu.Lock()\r\n\tif nc.isClosed() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\tif nc.isConnecting() || nc.isReconnecting() {\r\n\t\tnc.mu.Unlock()\r\n\t\tnc.close(CLOSED, true, nil)\r\n\t\treturn ErrConnectionReconnecting\r\n\t}\r\n\tif nc.isDraining() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn nil\r\n\t}\r\n\tnc.status = DRAINING_SUBS\r\n\tgo nc.drainConnection()\r\n\tnc.mu.Unlock()\r\n\r\n\treturn nil\r\n}\r\n\r\n// IsDraining tests if a Conn is in the draining state.\r\nfunc (nc *Conn) IsDraining() bool {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.isDraining()\r\n}\r\n\r\n// caller must lock\r\nfunc (nc *Conn) getServers(implicitOnly bool) []string {\r\n\tpoolSize := len(nc.srvPool)\r\n\tvar servers = make([]string, 0)\r\n\tfor i := 0; i < poolSize; i++ {\r\n\t\tif implicitOnly && !nc.srvPool[i].isImplicit {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\turl := nc.srvPool[i].url\r\n\t\tservers = append(servers, fmt.Sprintf(\"%s://%s\", url.Scheme, url.Host))\r\n\t}\r\n\treturn servers\r\n}\r\n\r\n// Servers returns the list of known server urls, including additional\r\n// servers discovered after a connection has been established.  If\r\n// authentication is enabled, use UserInfo or Token when connecting with\r\n// these urls.\r\nfunc (nc *Conn) Servers() []string {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.getServers(false)\r\n}\r\n\r\n// DiscoveredServers returns only the server urls that have been discovered\r\n// after a connection has been established. If authentication is enabled,\r\n// use UserInfo or Token when connecting with these urls.\r\nfunc (nc *Conn) DiscoveredServers() []string {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.getServers(true)\r\n}\r\n\r\n// Status returns the current state of the connection.\r\nfunc (nc *Conn) Status() Status {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.status\r\n}\r\n\r\n// Test if Conn has been closed Lock is assumed held.\r\nfunc (nc *Conn) isClosed() bool {\r\n\treturn nc.status == CLOSED\r\n}\r\n\r\n// Test if Conn is in the process of connecting\r\nfunc (nc *Conn) isConnecting() bool {\r\n\treturn nc.status == CONNECTING\r\n}\r\n\r\n// Test if Conn is being reconnected.\r\nfunc (nc *Conn) isReconnecting() bool {\r\n\treturn nc.status == RECONNECTING\r\n}\r\n\r\n// Test if Conn is connected or connecting.\r\nfunc (nc *Conn) isConnected() bool {\r\n\treturn nc.status == CONNECTED || nc.isDraining()\r\n}\r\n\r\n// Test if Conn is in the draining state.\r\nfunc (nc *Conn) isDraining() bool {\r\n\treturn nc.status == DRAINING_SUBS || nc.status == DRAINING_PUBS\r\n}\r\n\r\n// Test if Conn is in the draining state for pubs.\r\nfunc (nc *Conn) isDrainingPubs() bool {\r\n\treturn nc.status == DRAINING_PUBS\r\n}\r\n\r\n// Stats will return a race safe copy of the Statistics section for the connection.\r\nfunc (nc *Conn) Stats() Statistics {\r\n\t// Stats are updated either under connection's mu or with atomic operations\r\n\t// for inbound stats in processMsg().\r\n\tnc.mu.Lock()\r\n\tstats := Statistics{\r\n\t\tInMsgs:     atomic.LoadUint64(&nc.InMsgs),\r\n\t\tInBytes:    atomic.LoadUint64(&nc.InBytes),\r\n\t\tOutMsgs:    nc.OutMsgs,\r\n\t\tOutBytes:   nc.OutBytes,\r\n\t\tReconnects: nc.Reconnects,\r\n\t}\r\n\tnc.mu.Unlock()\r\n\treturn stats\r\n}\r\n\r\n// MaxPayload returns the size limit that a message payload can have.\r\n// This is set by the server configuration and delivered to the client\r\n// upon connect.\r\nfunc (nc *Conn) MaxPayload() int64 {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.info.MaxPayload\r\n}\r\n\r\n// AuthRequired will return if the connected server requires authorization.\r\nfunc (nc *Conn) AuthRequired() bool {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.info.AuthRequired\r\n}\r\n\r\n// TLSRequired will return if the connected server requires TLS connections.\r\nfunc (nc *Conn) TLSRequired() bool {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\treturn nc.info.TLSRequired\r\n}\r\n\r\n// Barrier schedules the given function `f` to all registered asynchronous\r\n// subscriptions.\r\n// Only the last subscription to see this barrier will invoke the function.\r\n// If no subscription is registered at the time of this call, `f()` is invoked\r\n// right away.\r\n// ErrConnectionClosed is returned if the connection is closed prior to\r\n// the call.\r\nfunc (nc *Conn) Barrier(f func()) error {\r\n\tnc.mu.Lock()\r\n\tif nc.isClosed() {\r\n\t\tnc.mu.Unlock()\r\n\t\treturn ErrConnectionClosed\r\n\t}\r\n\tnc.subsMu.Lock()\r\n\t// Need to figure out how many non chan subscriptions there are\r\n\tnumSubs := 0\r\n\tfor _, sub := range nc.subs {\r\n\t\tif sub.typ == AsyncSubscription {\r\n\t\t\tnumSubs++\r\n\t\t}\r\n\t}\r\n\tif numSubs == 0 {\r\n\t\tnc.subsMu.Unlock()\r\n\t\tnc.mu.Unlock()\r\n\t\tf()\r\n\t\treturn nil\r\n\t}\r\n\tbarrier := &barrierInfo{refs: int64(numSubs), f: f}\r\n\tfor _, sub := range nc.subs {\r\n\t\tsub.mu.Lock()\r\n\t\tif sub.mch == nil {\r\n\t\t\tmsg := &Msg{barrier: barrier}\r\n\t\t\t// Push onto the async pList\r\n\t\t\tif sub.pTail != nil {\r\n\t\t\t\tsub.pTail.next = msg\r\n\t\t\t} else {\r\n\t\t\t\tsub.pHead = msg\r\n\t\t\t\tsub.pCond.Signal()\r\n\t\t\t}\r\n\t\t\tsub.pTail = msg\r\n\t\t}\r\n\t\tsub.mu.Unlock()\r\n\t}\r\n\tnc.subsMu.Unlock()\r\n\tnc.mu.Unlock()\r\n\treturn nil\r\n}\r\n\r\n// GetClientIP returns the client IP as known by the server.\r\n// Supported as of server version 2.1.6.\r\nfunc (nc *Conn) GetClientIP() (net.IP, error) {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\tif nc.isClosed() {\r\n\t\treturn nil, ErrConnectionClosed\r\n\t}\r\n\tif nc.info.ClientIP == \"\" {\r\n\t\treturn nil, ErrClientIPNotSupported\r\n\t}\r\n\tip := net.ParseIP(nc.info.ClientIP)\r\n\treturn ip, nil\r\n}\r\n\r\n// GetClientID returns the client ID assigned by the server to which\r\n// the client is currently connected to. Note that the value may change if\r\n// the client reconnects.\r\n// This function returns ErrClientIDNotSupported if the server is of a\r\n// version prior to 1.2.0.\r\nfunc (nc *Conn) GetClientID() (uint64, error) {\r\n\tnc.mu.RLock()\r\n\tdefer nc.mu.RUnlock()\r\n\tif nc.isClosed() {\r\n\t\treturn 0, ErrConnectionClosed\r\n\t}\r\n\tif nc.info.CID == 0 {\r\n\t\treturn 0, ErrClientIDNotSupported\r\n\t}\r\n\treturn nc.info.CID, nil\r\n}\r\n\r\n// NkeyOptionFromSeed will load an nkey pair from a seed file.\r\n// It will return the NKey Option and will handle\r\n// signing of nonce challenges from the server. It will take\r\n// care to not hold keys in memory and to wipe memory.\r\nfunc NkeyOptionFromSeed(seedFile string) (Option, error) {\r\n\tkp, err := nkeyPairFromSeedFile(seedFile)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\t// Wipe our key on exit.\r\n\tdefer kp.Wipe()\r\n\r\n\tpub, err := kp.PublicKey()\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tif !nkeys.IsValidPublicUserKey(pub) {\r\n\t\treturn nil, fmt.Errorf(\"nats: Not a valid nkey user seed\")\r\n\t}\r\n\tsigCB := func(nonce []byte) ([]byte, error) {\r\n\t\treturn sigHandler(nonce, seedFile)\r\n\t}\r\n\treturn Nkey(string(pub), sigCB), nil\r\n}\r\n\r\n// Just wipe slice with 'x', for clearing contents of creds or nkey seed file.\r\nfunc wipeSlice(buf []byte) {\r\n\tfor i := range buf {\r\n\t\tbuf[i] = 'x'\r\n\t}\r\n}\r\n\r\nfunc userFromFile(userFile string) (string, error) {\r\n\tpath, err := expandPath(userFile)\r\n\tif err != nil {\r\n\t\treturn _EMPTY_, fmt.Errorf(\"nats: %v\", err)\r\n\t}\r\n\r\n\tcontents, err := ioutil.ReadFile(path)\r\n\tif err != nil {\r\n\t\treturn _EMPTY_, fmt.Errorf(\"nats: %v\", err)\r\n\t}\r\n\tdefer wipeSlice(contents)\r\n\treturn jwt.ParseDecoratedJWT(contents)\r\n}\r\n\r\nfunc homeDir() (string, error) {\r\n\tif runtime.GOOS == \"windows\" {\r\n\t\thomeDrive, homePath := os.Getenv(\"HOMEDRIVE\"), os.Getenv(\"HOMEPATH\")\r\n\t\tuserProfile := os.Getenv(\"USERPROFILE\")\r\n\r\n\t\tvar home string\r\n\t\tif homeDrive == \"\" || homePath == \"\" {\r\n\t\t\tif userProfile == \"\" {\r\n\t\t\t\treturn _EMPTY_, errors.New(\"nats: failed to get home dir, require %HOMEDRIVE% and %HOMEPATH% or %USERPROFILE%\")\r\n\t\t\t}\r\n\t\t\thome = userProfile\r\n\t\t} else {\r\n\t\t\thome = filepath.Join(homeDrive, homePath)\r\n\t\t}\r\n\r\n\t\treturn home, nil\r\n\t}\r\n\r\n\thome := os.Getenv(\"HOME\")\r\n\tif home == \"\" {\r\n\t\treturn _EMPTY_, errors.New(\"nats: failed to get home dir, require $HOME\")\r\n\t}\r\n\treturn home, nil\r\n}\r\n\r\nfunc expandPath(p string) (string, error) {\r\n\tp = os.ExpandEnv(p)\r\n\r\n\tif !strings.HasPrefix(p, \"~\") {\r\n\t\treturn p, nil\r\n\t}\r\n\r\n\thome, err := homeDir()\r\n\tif err != nil {\r\n\t\treturn _EMPTY_, err\r\n\t}\r\n\r\n\treturn filepath.Join(home, p[1:]), nil\r\n}\r\n\r\nfunc nkeyPairFromSeedFile(seedFile string) (nkeys.KeyPair, error) {\r\n\tcontents, err := ioutil.ReadFile(seedFile)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"nats: %v\", err)\r\n\t}\r\n\tdefer wipeSlice(contents)\r\n\treturn jwt.ParseDecoratedNKey(contents)\r\n}\r\n\r\n// Sign authentication challenges from the server.\r\n// Do not keep private seed in memory.\r\nfunc sigHandler(nonce []byte, seedFile string) ([]byte, error) {\r\n\tkp, err := nkeyPairFromSeedFile(seedFile)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\t// Wipe our key on exit.\r\n\tdefer kp.Wipe()\r\n\r\n\tsig, _ := kp.Sign(nonce)\r\n\treturn sig, nil\r\n}\r\n\r\ntype timeoutWriter struct {\r\n\ttimeout time.Duration\r\n\tconn    net.Conn\r\n\terr     error\r\n}\r\n\r\n// Write implements the io.Writer interface.\r\nfunc (tw *timeoutWriter) Write(p []byte) (int, error) {\r\n\tif tw.err != nil {\r\n\t\treturn 0, tw.err\r\n\t}\r\n\r\n\tvar n int\r\n\ttw.conn.SetWriteDeadline(time.Now().Add(tw.timeout))\r\n\tn, tw.err = tw.conn.Write(p)\r\n\ttw.conn.SetWriteDeadline(time.Time{})\r\n\treturn n, tw.err\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/client/nats/nats.go b/client/nats/nats.go
--- a/client/nats/nats.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/client/nats/nats.go	(date 1665399050264)
@@ -39,7 +39,7 @@
 
 	"github.com/kubemq-io/broker/client/nats/util"
 	"github.com/kubemq-io/broker/pkg/nuid"
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/test/leafnode_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage test\r\n\r\nimport (\r\n\t\"crypto/tls\"\r\n\t\"crypto/x509\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"math/rand\"\r\n\t\"net\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"regexp\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/kubemq-io/broker/client/nats\"\r\n\t\"github.com/kubemq-io/broker/pkg/nuid\"\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/logger\"\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/server\"\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nfunc createLeafConn(t tLogger, host string, port int) net.Conn {\r\n\treturn createClientConn(t, host, port)\r\n}\r\n\r\nfunc testDefaultOptionsForLeafNodes() *server.Options {\r\n\to := DefaultTestOptions\r\n\to.Port = -1\r\n\to.LeafNode.Host = o.Host\r\n\to.LeafNode.Port = -1\r\n\treturn &o\r\n}\r\n\r\nfunc runLeafServer() (*server.Server, *server.Options) {\r\n\to := testDefaultOptionsForLeafNodes()\r\n\treturn RunServer(o), o\r\n}\r\n\r\nfunc runLeafServerOnPort(port int) (*server.Server, *server.Options) {\r\n\to := testDefaultOptionsForLeafNodes()\r\n\to.LeafNode.Port = port\r\n\treturn RunServer(o), o\r\n}\r\n\r\nfunc runSolicitLeafServer(lso *server.Options) (*server.Server, *server.Options) {\r\n\treturn runSolicitLeafServerToURL(fmt.Sprintf(\"nats-leaf://%s:%d\", lso.LeafNode.Host, lso.LeafNode.Port))\r\n}\r\n\r\nfunc runSolicitLeafServerToURL(surl string) (*server.Server, *server.Options) {\r\n\to := DefaultTestOptions\r\n\to.Port = -1\r\n\trurl, _ := url.Parse(surl)\r\n\to.LeafNode.Remotes = []*server.RemoteLeafOpts{{URLs: []*url.URL{rurl}}}\r\n\to.LeafNode.ReconnectInterval = 100 * time.Millisecond\r\n\treturn RunServer(&o), &o\r\n}\r\n\r\nfunc TestLeafNodeInfo(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tinfo := checkInfoMsg(t, lc)\r\n\tif !info.AuthRequired {\r\n\t\tt.Fatalf(\"AuthRequired should always be true for leaf nodes\")\r\n\t}\r\n\tsendProto(t, lc, \"CONNECT {}\\r\\n\")\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\t// Now close connection, make sure we are doing the right accounting in the server.\r\n\tlc.Close()\r\n\r\n\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\tif nln := s.NumLeafNodes(); nln != 0 {\r\n\t\t\treturn fmt.Errorf(\"Number of leaf nodes is %d\", nln)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestLeafNodeSplitBuffer(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tnc, err := nats.Connect(s.ClientURL())\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tnc.QueueSubscribe(\"foo\", \"bar\", func(m *nats.Msg) {\r\n\t\tm.Respond([]byte(\"ok\"))\r\n\t})\r\n\tnc.Flush()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\tsendProto(t, lc, \"CONNECT {}\\r\\n\")\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 2)\r\n\r\n\tleafSend(\"LS+ reply\\r\\nPING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tleafSend(\"LMSG foo \")\r\n\ttime.Sleep(time.Millisecond)\r\n\tleafSend(\"+ reply bar 2\\r\\n\")\r\n\ttime.Sleep(time.Millisecond)\r\n\tleafSend(\"OK\\r\")\r\n\ttime.Sleep(time.Millisecond)\r\n\tleafSend(\"\\n\")\r\n\tleafExpect(lmsgRe)\r\n}\r\n\r\nfunc TestNumLeafNodes(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tcreateNewLeafNode := func() net.Conn {\r\n\t\tt.Helper()\r\n\t\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\t\tcheckInfoMsg(t, lc)\r\n\t\tsendProto(t, lc, \"CONNECT {}\\r\\n\")\r\n\t\treturn lc\r\n\t}\r\n\tcheckLFCount := func(n int) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\t\tif nln := s.NumLeafNodes(); nln != n {\r\n\t\t\t\treturn fmt.Errorf(\"Number of leaf nodes is %d\", nln)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\tcheckLFCount(0)\r\n\r\n\tlc1 := createNewLeafNode()\r\n\tdefer lc1.Close()\r\n\tcheckLFCount(1)\r\n\r\n\tlc2 := createNewLeafNode()\r\n\tdefer lc2.Close()\r\n\tcheckLFCount(2)\r\n\r\n\t// Now test remove works.\r\n\tlc1.Close()\r\n\tcheckLFCount(1)\r\n\r\n\tlc2.Close()\r\n\tcheckLFCount(0)\r\n}\r\n\r\nfunc TestLeafNodeRequiresConnect(t *testing.T) {\r\n\topts := testDefaultOptionsForLeafNodes()\r\n\topts.LeafNode.AuthTimeout = 0.001\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tinfo := checkInfoMsg(t, lc)\r\n\tif !info.AuthRequired {\r\n\t\tt.Fatalf(\"Expected AuthRequired to force CONNECT\")\r\n\t}\r\n\tif info.TLSRequired {\r\n\t\tt.Fatalf(\"Expected TLSRequired to be false\")\r\n\t}\r\n\tif info.TLSVerify {\r\n\t\tt.Fatalf(\"Expected TLSVerify to be false\")\r\n\t}\r\n\r\n\t// Now wait and make sure we get disconnected.\r\n\terrBuf := expectResult(t, lc, errRe)\r\n\r\n\tif !strings.Contains(string(errBuf), \"Authentication Timeout\") {\r\n\t\tt.Fatalf(\"Authentication Timeout response incorrect: %q\", errBuf)\r\n\t}\r\n\texpectDisconnect(t, lc)\r\n}\r\n\r\nfunc setupLeaf(t *testing.T, lc net.Conn, expectedSubs int) (sendFun, expectFun) {\r\n\tt.Helper()\r\n\tsend, expect := setupConn(t, lc)\r\n\t// A loop detection subscription is sent, so consume this here, along\r\n\t// with the ones that caller expect on setup.\r\n\texpectNumberOfProtos(t, expect, lsubRe, expectedSubs)\r\n\treturn send, expect\r\n}\r\n\r\nfunc TestLeafNodeSendsSubsAfterConnect(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"SUB foo 1\\r\\n\")\r\n\tsend(\"SUB bar 2\\r\\n\")\r\n\tsend(\"SUB foo baz 3\\r\\n\")\r\n\tsend(\"SUB foo baz 4\\r\\n\")\r\n\tsend(\"SUB bar 5\\r\\n\")\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This should compress down to 1 for foo, 1 for bar, and 1 for foo [baz]\r\n\t// and one for the loop detection subject.\r\n\tsetupLeaf(t, lc, 4)\r\n}\r\n\r\nfunc TestLeafNodeSendsSubsOngoing(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 1)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"SUB foo 1\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\r\n\t// Check queues send updates each time.\r\n\t// TODO(dlc) - If we decide to suppress this with a timer approach this test will break.\r\n\tsend(\"SUB foo bar 2\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\tsend(\"SUB foo bar 3\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\tsend(\"SUB foo bar 4\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\r\n\t// Now check more normal subs do nothing.\r\n\tsend(\"SUB foo 5\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\r\n\t// Check going back down does nothing til we hit 0.\r\n\tsend(\"UNSUB 5\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tsend(\"UNSUB 1\\r\\n\")\r\n\tleafExpect(lunsubRe)\r\n\r\n\t// Queues going down should always send updates.\r\n\tsend(\"UNSUB 2\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\tsend(\"UNSUB 3\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\tsend(\"UNSUB 4\\r\\n\")\r\n\tleafExpect(lunsubRe)\r\n}\r\n\r\nfunc TestLeafNodeSubs(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 1)\r\n\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tleafSend(\"LS+ foo\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tsend(\"PUB foo 2\\r\\nOK\\r\\n\")\r\n\tmatches := lmsgRe.FindAllSubmatch(leafExpect(lmsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckLmsg(t, matches[0], \"foo\", \"\", \"2\", \"OK\")\r\n\r\n\t// Second sub should not change delivery\r\n\tleafSend(\"LS+ foo\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"PUB foo 3\\r\\nOK!\\r\\n\")\r\n\tmatches = lmsgRe.FindAllSubmatch(leafExpect(lmsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckLmsg(t, matches[0], \"foo\", \"\", \"3\", \"OK!\")\r\n\r\n\t// Now add in a queue sub with weight 4.\r\n\tleafSend(\"LS+ foo bar 4\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"PUB foo 4\\r\\nOKOK\\r\\n\")\r\n\tmatches = lmsgRe.FindAllSubmatch(leafExpect(lmsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckLmsg(t, matches[0], \"foo\", \"| bar\", \"4\", \"OKOK\")\r\n\r\n\t// Now add in a queue sub with weight 4.\r\n\tleafSend(\"LS+ foo baz 2\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"PUB foo 5\\r\\nHELLO\\r\\n\")\r\n\tmatches = lmsgRe.FindAllSubmatch(leafExpect(lmsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckLmsg(t, matches[0], \"foo\", \"| bar baz\", \"5\", \"HELLO\")\r\n\r\n\t// Test Unsub\r\n\tleafSend(\"LS- foo\\r\\n\")\r\n\tleafSend(\"LS- foo bar\\r\\n\")\r\n\tleafSend(\"LS- foo baz\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"PUB foo 5\\r\\nHELLO\\r\\n\")\r\n\texpectNothing(t, lc)\r\n}\r\n\r\nfunc TestLeafNodeMsgDelivery(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 1)\r\n\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tsend(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lsubRe)\r\n\r\n\t// Now send from leaf side.\r\n\tleafSend(\"LMSG foo 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\r\n\tmatches := msgRe.FindAllSubmatch(expect(msgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckMsg(t, matches[0], \"foo\", \"1\", \"\", \"2\", \"OK\")\r\n\r\n\tsend(\"UNSUB 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lunsubRe)\r\n\tsend(\"SUB foo bar 2\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lsubRe)\r\n\r\n\t// Now send again from leaf side. This is targeted so this should\r\n\t// not be delivered.\r\n\tleafSend(\"LMSG foo 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\texpectNothing(t, c)\r\n\r\n\t// Now send targeted, and we should receive it.\r\n\tleafSend(\"LMSG foo | bar 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\r\n\tmatches = msgRe.FindAllSubmatch(expect(msgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckMsg(t, matches[0], \"foo\", \"2\", \"\", \"2\", \"OK\")\r\n\r\n\t// Check reply + queues\r\n\tleafSend(\"LMSG foo + myreply bar 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\r\n\tmatches = msgRe.FindAllSubmatch(expect(msgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckMsg(t, matches[0], \"foo\", \"2\", \"myreply\", \"2\", \"OK\")\r\n}\r\n\r\nfunc TestLeafNodeAndRoutes(t *testing.T) {\r\n\tsrvA, optsA := RunServerWithConfig(\"./configs/srv_a_leaf.conf\")\r\n\tsrvB, optsB := RunServerWithConfig(\"./configs/srv_b.conf\")\r\n\tcheckClusterFormed(t, srvA, srvB)\r\n\tdefer srvA.Shutdown()\r\n\tdefer srvB.Shutdown()\r\n\r\n\tlc := createLeafConn(t, optsA.LeafNode.Host, optsA.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 1)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tc := createClientConn(t, optsB.Host, optsB.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tsend(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lsubRe)\r\n\r\n\tsend(\"SUB foo 2\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\texpectNothing(t, lc)\r\n\r\n\tsend(\"UNSUB 2\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\texpectNothing(t, lc)\r\n\tsend(\"UNSUB 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lunsubRe)\r\n\r\n\t// Now put it back and test msg flow.\r\n\tsend(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lsubRe)\r\n\r\n\tleafSend(\"LMSG foo + myreply bar 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\r\n\tmatches := msgRe.FindAllSubmatch(expect(msgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckMsg(t, matches[0], \"foo\", \"1\", \"myreply\", \"2\", \"OK\")\r\n\r\n\t// Now check reverse.\r\n\tleafSend(\"LS+ bar\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"PUB bar 2\\r\\nOK\\r\\n\")\r\n\tmatches = lmsgRe.FindAllSubmatch(leafExpect(lmsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckLmsg(t, matches[0], \"bar\", \"\", \"2\", \"OK\")\r\n}\r\n\r\n// Helper function to check that a leaf node has connected to our server.\r\nfunc checkLeafNodeConnected(t *testing.T, s *server.Server) {\r\n\tt.Helper()\r\n\tcheckLeafNodeConnections(t, s, 1)\r\n}\r\n\r\nfunc checkLeafNodeConnections(t *testing.T, s *server.Server, expected int) {\r\n\tt.Helper()\r\n\tcheckFor(t, 5*time.Second, 100*time.Millisecond, func() error {\r\n\t\tif nln := s.NumLeafNodes(); nln != expected {\r\n\t\t\treturn fmt.Errorf(\"Expected a connected leafnode for server %q, got %d\", s.ID(), nln)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestLeafNodeSolicit(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tsl, _ := runSolicitLeafServer(opts)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\t// Now test reconnect.\r\n\ts.Shutdown()\r\n\t// Need to restart it on the same port.\r\n\ts, _ = runLeafServerOnPort(opts.LeafNode.Port)\r\n\tdefer s.Shutdown()\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\nfunc TestLeafNodeNoEcho(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 1)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\t// We should not echo back to ourselves. Set up 'foo' subscriptions\r\n\t// on both sides and send message across the leafnode connection. It\r\n\t// should not come back.\r\n\r\n\tsend(\"SUB foo 1\\r\\n\")\r\n\tleafExpect(lsubRe)\r\n\r\n\tleafSend(\"LS+ foo\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tleafSend(\"LMSG foo 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n}\r\n\r\nfunc TestLeafNodeLoop(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\tlc1 := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc1.Close()\r\n\r\n\tleaf1Send, leaf1Expect := setupLeaf(t, lc1, 1)\r\n\tleaf1Send(\"PING\\r\\n\")\r\n\tleaf1Expect(pongRe)\r\n\r\n\tleaf1Send(\"LS+ $lds.foo\\r\\n\")\r\n\texpectNothing(t, lc1)\r\n\r\n\t// Same loop detection subscription from same client\r\n\tleaf1Send(\"LS+ $lds.foo\\r\\n\")\r\n\texpectNothing(t, lc1)\r\n\r\n\tlc2 := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc2.Close()\r\n\r\n\tleaf2Send, leaf2Expect := setupLeaf(t, lc2, 2)\r\n\tleaf2Send(\"PING\\r\\n\")\r\n\tleaf2Expect(pongRe)\r\n\r\n\t// Same loop detection subscription from different client\r\n\tleaf2Send(\"LS+ $lds.foo\\r\\n\")\r\n\tleaf2Expect(regexp.MustCompile(\r\n\t\t\"-ERR 'Loop detected for leafnode account=\\\".G\\\". \" +\r\n\t\t\t\"Delaying attempt to reconnect for .*\"))\r\n}\r\n\r\n// Used to setup clusters of clusters for tests.\r\ntype cluster struct {\r\n\tservers []*server.Server\r\n\topts    []*server.Options\r\n\tname    string\r\n}\r\n\r\nfunc testDefaultClusterOptionsForLeafNodes() *server.Options {\r\n\to := DefaultTestOptions\r\n\to.Port = -1\r\n\to.Cluster.Host = o.Host\r\n\to.Cluster.Port = -1\r\n\to.Gateway.Host = o.Host\r\n\to.Gateway.Port = -1\r\n\to.LeafNode.Host = o.Host\r\n\to.LeafNode.Port = -1\r\n\treturn &o\r\n}\r\n\r\nfunc shutdownCluster(c *cluster) {\r\n\tif c == nil {\r\n\t\treturn\r\n\t}\r\n\tfor _, s := range c.servers {\r\n\t\ts.Shutdown()\r\n\t}\r\n}\r\n\r\nfunc (c *cluster) totalSubs() int {\r\n\ttotalSubs := 0\r\n\tfor _, s := range c.servers {\r\n\t\ttotalSubs += int(s.NumSubscriptions())\r\n\t}\r\n\treturn totalSubs\r\n}\r\n\r\n// Wait for the expected number of outbound gateways, or fails.\r\nfunc waitForOutboundGateways(t *testing.T, s *server.Server, expected int, timeout time.Duration) {\r\n\tt.Helper()\r\n\tif timeout < 2*time.Second {\r\n\t\ttimeout = 2 * time.Second\r\n\t}\r\n\tcheckFor(t, timeout, 15*time.Millisecond, func() error {\r\n\t\tif n := s.NumOutboundGateways(); n != expected {\r\n\t\t\treturn fmt.Errorf(\"Expected %v outbound gateway(s), got %v (ulimt -n too low?)\",\r\n\t\t\t\texpected, n)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\n// Creates a full cluster with numServers and given name and makes sure its well formed.\r\n// Will have Gateways and Leaf Node connections active.\r\nfunc createClusterWithName(t *testing.T, clusterName string, numServers int, connectTo ...*cluster) *cluster {\r\n\tt.Helper()\r\n\treturn createClusterEx(t, false, clusterName, numServers, connectTo...)\r\n}\r\n\r\n// Creates a cluster and optionally additional accounts and users.\r\n// Will have Gateways and Leaf Node connections active.\r\nfunc createClusterEx(t *testing.T, doAccounts bool, clusterName string, numServers int, connectTo ...*cluster) *cluster {\r\n\tt.Helper()\r\n\r\n\tif clusterName == \"\" || numServers < 1 {\r\n\t\tt.Fatalf(\"Bad params\")\r\n\t}\r\n\r\n\t// Setup some accounts and users.\r\n\t// $SYS is always the system account. And we have default FOO and BAR accounts, as well\r\n\t// as DLC and NGS which do a service import.\r\n\tcreateAccountsAndUsers := func() ([]*server.Account, []*server.User) {\r\n\t\tif !doAccounts {\r\n\t\t\treturn []*server.Account{server.NewAccount(\"$SYS\")}, nil\r\n\t\t}\r\n\r\n\t\tsys := server.NewAccount(\"$SYS\")\r\n\t\tngs := server.NewAccount(\"NGS\")\r\n\t\tdlc := server.NewAccount(\"DLC\")\r\n\t\tfoo := server.NewAccount(\"FOO\")\r\n\t\tbar := server.NewAccount(\"BAR\")\r\n\r\n\t\taccounts := []*server.Account{sys, foo, bar, ngs, dlc}\r\n\r\n\t\tngs.AddServiceExport(\"ngs.usage.*\", nil)\r\n\t\tdlc.AddServiceImport(ngs, \"ngs.usage\", \"ngs.usage.dlc\")\r\n\r\n\t\t// Setup users\r\n\t\tusers := []*server.User{\r\n\t\t\t&server.User{Username: \"dlc\", Password: \"pass\", Permissions: nil, Account: dlc},\r\n\t\t\t&server.User{Username: \"ngs\", Password: \"pass\", Permissions: nil, Account: ngs},\r\n\t\t\t&server.User{Username: \"foo\", Password: \"pass\", Permissions: nil, Account: foo},\r\n\t\t\t&server.User{Username: \"bar\", Password: \"pass\", Permissions: nil, Account: bar},\r\n\t\t\t&server.User{Username: \"sys\", Password: \"pass\", Permissions: nil, Account: sys},\r\n\t\t}\r\n\t\treturn accounts, users\r\n\t}\r\n\r\n\tbindGlobal := func(s *server.Server) {\r\n\t\tngs, _ := s.LookupAccount(\"NGS\")\r\n\t\t// Bind global to service import\r\n\t\tgacc, _ := s.LookupAccount(\"$G\")\r\n\t\tgacc.AddServiceImport(ngs, \"ngs.usage\", \"ngs.usage.$G\")\r\n\t}\r\n\r\n\t// If we are going to connect to another cluster set that up now for options.\r\n\tvar gws []*server.RemoteGatewayOpts\r\n\tfor _, c := range connectTo {\r\n\t\t// Gateways autodiscover here too, so just need one address from the set.\r\n\t\tgwAddr := fmt.Sprintf(\"nats-gw://%s:%d\", c.opts[0].Gateway.Host, c.opts[0].Gateway.Port)\r\n\t\tgwurl, _ := url.Parse(gwAddr)\r\n\t\tgws = append(gws, &server.RemoteGatewayOpts{Name: c.name, URLs: []*url.URL{gwurl}})\r\n\t}\r\n\r\n\t// Make the GWs form faster for the tests.\r\n\tserver.SetGatewaysSolicitDelay(5 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\t// Create seed first.\r\n\to := testDefaultClusterOptionsForLeafNodes()\r\n\to.Gateway.Name = clusterName\r\n\to.Gateway.Gateways = gws\r\n\t// All of these need system accounts.\r\n\to.Accounts, o.Users = createAccountsAndUsers()\r\n\to.SystemAccount = \"$SYS\"\r\n\t// Run the server\r\n\ts := RunServer(o)\r\n\tbindGlobal(s)\r\n\r\n\tc := &cluster{servers: make([]*server.Server, 0, 3), opts: make([]*server.Options, 0, 3), name: clusterName}\r\n\tc.servers = append(c.servers, s)\r\n\tc.opts = append(c.opts, o)\r\n\r\n\t// For connecting to seed server above.\r\n\trouteAddr := fmt.Sprintf(\"nats-route://%s:%d\", o.Cluster.Host, o.Cluster.Port)\r\n\trurl, _ := url.Parse(routeAddr)\r\n\troutes := []*url.URL{rurl}\r\n\r\n\tfor i := 1; i < numServers; i++ {\r\n\t\to := testDefaultClusterOptionsForLeafNodes()\r\n\t\to.Gateway.Name = clusterName\r\n\t\to.Gateway.Gateways = gws\r\n\t\to.Routes = routes\r\n\t\t// All of these need system accounts.\r\n\t\to.Accounts, o.Users = createAccountsAndUsers()\r\n\t\to.SystemAccount = \"$SYS\"\r\n\t\ts := RunServer(o)\r\n\t\tbindGlobal(s)\r\n\r\n\t\tc.servers = append(c.servers, s)\r\n\t\tc.opts = append(c.opts, o)\r\n\t}\r\n\tcheckClusterFormed(t, c.servers...)\r\n\r\n\t// Wait on gateway connections if we were asked to connect to other gateways.\r\n\tif numGWs := len(connectTo); numGWs > 0 {\r\n\t\tfor _, s := range c.servers {\r\n\t\t\twaitForOutboundGateways(t, s, numGWs, 2*time.Second)\r\n\t\t}\r\n\t}\r\n\r\n\treturn c\r\n}\r\n\r\nfunc TestLeafNodeGatewayRequiresSystemAccount(t *testing.T) {\r\n\to := testDefaultClusterOptionsForLeafNodes()\r\n\to.Gateway.Name = \"CLUSTER-A\"\r\n\t_, err := server.NewServer(o)\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expected an error with no system account defined\")\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeGatewaySendsSystemEvent(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(50 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterWithName(t, \"A\", 1)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterWithName(t, \"B\", 1, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Create client on a server in cluster A\r\n\topts := ca.opts[0]\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\t// Listen for the leaf node event.\r\n\tsend, expect := setupConnWithAccount(t, c, \"$SYS\")\r\n\tsend(\"SUB $SYS.ACCOUNT.*.LEAFNODE.CONNECT 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\topts = cb.opts[0]\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This is for our global responses since we are setting up GWs above.\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 3)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tmatches := rawMsgRe.FindAllSubmatch(expect(rawMsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tm := matches[0]\r\n\tif string(m[subIndex]) != \"$SYS.ACCOUNT.$G.LEAFNODE.CONNECT\" {\r\n\t\tt.Fatalf(\"Got wrong subject for leaf node event, got %q, wanted %q\",\r\n\t\t\tm[subIndex], \"$SYS.ACCOUNT.$G.LEAFNODE.CONNECT\")\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeGatewayInterestPropagation(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(10 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterWithName(t, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterWithName(t, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\tsl1, sl1Opts := runSolicitLeafServer(ca.opts[1])\r\n\tdefer sl1.Shutdown()\r\n\r\n\tc := createClientConn(t, sl1Opts.Host, sl1Opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"SUB foo 1\\r\\n\")\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\t// Now we will create a new leaf node on cluster B, expect to get the\r\n\t// interest for \"foo\".\r\n\topts := cb.opts[0]\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\t_, leafExpect := setupConn(t, lc)\r\n\tvar totalBuf []byte\r\n\tfor count := 0; count != 4; {\r\n\t\tbuf := leafExpect(lsubRe)\r\n\t\ttotalBuf = append(totalBuf, buf...)\r\n\t\tcount += len(lsubRe.FindAllSubmatch(buf, -1))\r\n\t\tif count > 4 {\r\n\t\t\tt.Fatalf(\"Expected %v matches, got %v (buf=%s)\", 3, count, totalBuf)\r\n\t\t}\r\n\t}\r\n\tif !strings.Contains(string(totalBuf), \"foo\") {\r\n\t\tt.Fatalf(\"Expected interest for 'foo' as 'LS+ foo\\\\r\\\\n', got %q\", totalBuf)\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeAuthSystemEventNoCrash(t *testing.T) {\r\n\tca := createClusterWithName(t, \"A\", 1)\r\n\tdefer shutdownCluster(ca)\r\n\r\n\topts := ca.opts[0]\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend := sendCommand(t, lc)\r\n\tleafSend(\"LS+ foo\\r\\n\")\r\n\tcheckInfoMsg(t, lc)\r\n}\r\n\r\nfunc TestLeafNodeWithRouteAndGateway(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(50 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterWithName(t, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterWithName(t, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Create client on a server in cluster A\r\n\topts := ca.opts[0]\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\t// Create a leaf node connection on a server in cluster B\r\n\topts = cb.opts[0]\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This is for our global responses since we are setting up GWs above.\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 3)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\t// Make sure we see interest graph propagation on the leaf node\r\n\t// connection. This is required since leaf nodes only send data\r\n\t// in the presence of interest.\r\n\tsend(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lsubRe)\r\n\r\n\tsend(\"SUB foo 2\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\texpectNothing(t, lc)\r\n\r\n\tsend(\"UNSUB 2\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tsend(\"UNSUB 1\\r\\n\")\r\n\tleafExpect(lunsubRe)\r\n\r\n\t// Now put it back and test msg flow.\r\n\tsend(\"SUB foo 1\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\tleafExpect(lsubRe)\r\n\r\n\tleafSend(\"LMSG foo 2\\r\\nOK\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\r\n\tmatches := msgRe.FindAllSubmatch(expect(msgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckMsg(t, matches[0], \"foo\", \"1\", \"\", \"2\", \"OK\")\r\n\r\n\t// Now check reverse.\r\n\tleafSend(\"LS+ bar\\r\\n\")\r\n\texpectNothing(t, lc)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tsend(\"PUB bar 2\\r\\nOK\\r\\n\")\r\n\tmatches = lmsgRe.FindAllSubmatch(leafExpect(lmsgRe), -1)\r\n\tif len(matches) != 1 {\r\n\t\tt.Fatalf(\"Expected only 1 msg, got %d\", len(matches))\r\n\t}\r\n\tcheckLmsg(t, matches[0], \"bar\", \"\", \"2\", \"OK\")\r\n}\r\n\r\nfunc TestLeafNodeLocalizedDQ(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tsl, slOpts := runSolicitLeafServer(opts)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\tc := createClientConn(t, slOpts.Host, slOpts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"SUB foo bar 1\\r\\n\")\r\n\tsend(\"SUB foo bar 2\\r\\n\")\r\n\tsend(\"SUB foo bar 3\\r\\n\")\r\n\tsend(\"SUB foo bar 4\\r\\n\")\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\t// Now create another client on the main leaf server.\r\n\tsc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer sc.Close()\r\n\r\n\tsendL, expectL := setupConn(t, sc)\r\n\tsendL(\"SUB foo bar 11\\r\\n\")\r\n\tsendL(\"SUB foo bar 12\\r\\n\")\r\n\tsendL(\"SUB foo bar 13\\r\\n\")\r\n\tsendL(\"SUB foo bar 14\\r\\n\")\r\n\tsendL(\"PING\\r\\n\")\r\n\texpectL(pongRe)\r\n\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tsend(\"PUB foo 2\\r\\nOK\\r\\n\")\r\n\t}\r\n\texpectNothing(t, sc)\r\n\r\n\tmatches := msgRe.FindAllSubmatch(expect(msgRe), -1)\r\n\tif len(matches) != 10 {\r\n\t\tt.Fatalf(\"Expected 10 msgs, got %d\", len(matches))\r\n\t}\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tcheckMsg(t, matches[i], \"foo\", \"\", \"\", \"2\", \"OK\")\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeBasicAuth(t *testing.T) {\r\n\tcontent := `\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t\tauthorization {\r\n\t\t\tuser: \"derek\"\r\n\t\t\tpassword: \"s3cr3t!\"\r\n\t\t\ttimeout: 2.2\r\n\t\t}\r\n\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This should fail since we want u/p\r\n\tsetupConn(t, lc)\r\n\terrBuf := expectResult(t, lc, errRe)\r\n\tif !strings.Contains(string(errBuf), \"Authorization Violation\") {\r\n\t\tt.Fatalf(\"Authentication Timeout response incorrect: %q\", errBuf)\r\n\t}\r\n\texpectDisconnect(t, lc)\r\n\r\n\t// Try bad password as well.\r\n\tlc = createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This should fail since we want u/p\r\n\tsetupConnWithUserPass(t, lc, \"derek\", \"badpassword\")\r\n\terrBuf = expectResult(t, lc, errRe)\r\n\tif !strings.Contains(string(errBuf), \"Authorization Violation\") {\r\n\t\tt.Fatalf(\"Authentication Timeout response incorrect: %q\", errBuf)\r\n\t}\r\n\texpectDisconnect(t, lc)\r\n\r\n\t// This one should work.\r\n\tlc = createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\tleafSend, leafExpect := setupConnWithUserPass(t, lc, \"derek\", \"s3cr3t!\")\r\n\tleafExpect(lsubRe)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\nfunc runTLSSolicitLeafServer(lso *server.Options) (*server.Server, *server.Options) {\r\n\to := DefaultTestOptions\r\n\to.Port = -1\r\n\trurl, _ := url.Parse(fmt.Sprintf(\"nats-leaf://%s:%d\", lso.LeafNode.Host, lso.LeafNode.Port))\r\n\tremote := &server.RemoteLeafOpts{URLs: []*url.URL{rurl}}\r\n\tremote.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\r\n\thost, _, _ := net.SplitHostPort(lso.LeafNode.Host)\r\n\tremote.TLSConfig.ServerName = host\r\n\tremote.TLSConfig.InsecureSkipVerify = true\r\n\to.LeafNode.Remotes = []*server.RemoteLeafOpts{remote}\r\n\treturn RunServer(&o), &o\r\n}\r\n\r\nfunc TestLeafNodeTLS(t *testing.T) {\r\n\tcontent := `\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t\ttls {\r\n\t\t\tcert_file: \"./configs/certs/server-cert.pem\"\r\n\t\t\tkey_file: \"./configs/certs/server-key.pem\"\r\n\t\t\ttimeout: 0.1\r\n\t\t}\r\n\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tinfo := checkInfoMsg(t, lc)\r\n\tif !info.TLSRequired {\r\n\t\tt.Fatalf(\"Expected TLSRequired to be true\")\r\n\t}\r\n\tif info.TLSVerify {\r\n\t\tt.Fatalf(\"Expected TLSVerify to be false\")\r\n\t}\r\n\t// We should get a disconnect here since we have not upgraded to TLS.\r\n\texpectDisconnect(t, lc)\r\n\r\n\t// This should work ok.\r\n\tsl, _ := runTLSSolicitLeafServer(opts)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\ntype captureLeafNodeErrLogger struct {\r\n\tdummyLogger\r\n\tch chan string\r\n}\r\n\r\nfunc (c *captureLeafNodeErrLogger) Errorf(format string, v ...interface{}) {\r\n\tmsg := fmt.Sprintf(format, v...)\r\n\tselect {\r\n\tcase c.ch <- msg:\r\n\tdefault:\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeTLSMixIP(t *testing.T) {\r\n\tcontent := `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n        authorization {\r\n\t\t\tuser: dlc\r\n\t\t\tpass: monkey\r\n\t\t}\r\n\t\ttls {\r\n\t\t\tcert_file: \"./configs/certs/server-noip.pem\"\r\n\t\t\tkey_file:  \"./configs/certs/server-key-noip.pem\"\r\n\t\t\tca_file:   \"./configs/certs/ca.pem\"\r\n\t\t}\r\n\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tslContent := `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\tleafnodes {\r\n\t\treconnect: 1\r\n\t\tremotes: [\r\n\t\t\t{\r\n\t\t\t\turl: [tls://127.0.0.1:%d, \"tls://localhost:%d\"]\r\n\t\t\t\ttls { ca_file: \"./configs/certs/ca.pem\" }\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n\t`\r\n\tslconf := createConfFile(t, []byte(fmt.Sprintf(slContent, opts.LeafNode.Port, opts.LeafNode.Port)))\r\n\tdefer os.Remove(slconf)\r\n\r\n\t// This will fail but we want to make sure in the correct way, not with\r\n\t// TLS issue because we used an IP for serverName.\r\n\tsl, _ := RunServerWithConfig(slconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tll := &captureLeafNodeErrLogger{ch: make(chan string, 2)}\r\n\tsl.SetLogger(ll, false, false)\r\n\r\n\t// We may or may not get an error depending on timing. For the handshake bug\r\n\t// we would always get it, so make sure if we have anything it is not that.\r\n\tselect {\r\n\tcase msg := <-ll.ch:\r\n\t\tif strings.Contains(msg, \"TLS handshake error\") && strings.Contains(msg, \"doesn't contain any IP SANs\") {\r\n\t\t\tt.Fatalf(\"Got bad error about TLS handshake\")\r\n\t\t}\r\n\tdefault:\r\n\t}\r\n}\r\n\r\nfunc runLeafNodeOperatorServer(t *testing.T) (*server.Server, *server.Options, string) {\r\n\tt.Helper()\r\n\tcontent := `\r\n\tport: -1\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tresolver = MEMORY\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\ts, opts := RunServerWithConfig(conf)\r\n\treturn s, opts, conf\r\n}\r\n\r\nfunc genCredsFile(t *testing.T, jwt string, seed []byte) string {\r\n\tcreds := `\r\n\t\t-----BEGIN NATS USER JWT-----\r\n\t\t%s\r\n\t\t------END NATS USER JWT------\r\n\r\n\t\t************************* IMPORTANT *************************\r\n\t\tNKEY Seed printed below can be used to sign and prove identity.\r\n\t\tNKEYs are sensitive and should be treated as secrets.\r\n\r\n\t\t-----BEGIN USER NKEY SEED-----\r\n\t\t%s\r\n\t\t------END USER NKEY SEED------\r\n\r\n\t\t*************************************************************\r\n\t\t`\r\n\treturn createConfFile(t, []byte(strings.Replace(fmt.Sprintf(creds, jwt, seed), \"\\t\\t\", \"\", -1)))\r\n}\r\n\r\nfunc runSolicitWithCredentials(t *testing.T, opts *server.Options, creds string) (*server.Server, *server.Options, string) {\r\n\tcontent := `\r\n\t\tport: -1\r\n\t\tleafnodes {\r\n\t\t\tremotes = [\r\n\t\t\t\t{\r\n\t\t\t\t\turl: nats-leaf://127.0.0.1:%d\r\n\t\t\t\t\tcredentials: '%s'\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t\t`\r\n\tconfig := fmt.Sprintf(content, opts.LeafNode.Port, creds)\r\n\tconf := createConfFile(t, []byte(config))\r\n\ts, opts := RunServerWithConfig(conf)\r\n\treturn s, opts, conf\r\n}\r\n\r\nfunc TestLeafNodeOperatorModel(t *testing.T) {\r\n\ts, opts, conf := runLeafNodeOperatorServer(t)\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Make sure we get disconnected without proper credentials etc.\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This should fail since we want user jwt, signed nonce etc.\r\n\tsetupConn(t, lc)\r\n\terrBuf := expectResult(t, lc, errRe)\r\n\tif !strings.Contains(string(errBuf), \"Authorization Violation\") {\r\n\t\tt.Fatalf(\"Authentication Timeout response incorrect: %q\", errBuf)\r\n\t}\r\n\texpectDisconnect(t, lc)\r\n\r\n\t// Setup account and a user that will be used by the remote leaf node server.\r\n\t// createAccount automatically registers with resolver etc..\r\n\t_, akp := createAccount(t, s)\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tseed, _ := kp.Seed()\r\n\tmycreds := genCredsFile(t, ujwt, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tsl, _, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\nfunc TestLeafNodeMultipleAccounts(t *testing.T) {\r\n\t// So we will create a main server with two accounts. The remote server, acting as a leaf node, will simply have\r\n\t// the $G global account and no auth. Make sure things work properly here.\r\n\ts, opts, conf := runLeafNodeOperatorServer(t)\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Setup the two accounts for this server.\r\n\t_, akp1 := createAccount(t, s)\r\n\tkp1, _ := nkeys.CreateUser()\r\n\tpub1, _ := kp1.PublicKey()\r\n\tnuc1 := jwt.NewUserClaims(pub1)\r\n\tujwt1, err := nuc1.Encode(akp1)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\r\n\t// Create second account.\r\n\tcreateAccount(t, s)\r\n\r\n\t// Create the leaf node server using the first account.\r\n\tseed, _ := kp1.Seed()\r\n\tmycreds := genCredsFile(t, ujwt1, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tsl, lopts, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\t// To connect to main server.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\tnc1, err := nats.Connect(url, createUserCreds(t, s, akp1))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc1.Close()\r\n\r\n\t// This is a client connected to the leaf node with no auth,\r\n\t// binding to account1 via leafnode connection.\r\n\t// To connect to leafnode server.\r\n\tlurl := fmt.Sprintf(\"nats://%s:%d\", lopts.Host, lopts.Port)\r\n\tncl, err := nats.Connect(lurl)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl.Close()\r\n\r\n\tlsub, _ := ncl.SubscribeSync(\"foo.test\")\r\n\r\n\t// Wait for the sub to propagate.\r\n\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\tif subs := s.NumSubscriptions(); subs < 1 {\r\n\t\t\treturn fmt.Errorf(\"Number of subs is %d\", subs)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Now send from nc1 with account 1, should be received by our leafnode subscriber.\r\n\tnc1.Publish(\"foo.test\", nil)\r\n\r\n\t_, err = lsub.NextMsg(1 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error during wait for next message: %s\", err)\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeSignerUser(t *testing.T) {\r\n\ts, opts, conf := runLeafNodeOperatorServer(t)\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Setup the two accounts for this server.\r\n\t_, akp1 := createAccount(t, s)\r\n\tapk1, _ := akp1.PublicKey()\r\n\r\n\t// add a signing key to the account\r\n\takp2, err := nkeys.CreateAccount()\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tapk2, _ := akp2.PublicKey()\r\n\r\n\ttoken, err := s.AccountResolver().Fetch(apk1)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tac, err := jwt.DecodeAccountClaims(token)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tac.SigningKeys.Add(apk2)\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\ttoken, err = ac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\t// update the resolver\r\n\taccount, _ := s.LookupAccount(apk1)\r\n\terr = s.AccountResolver().Store(apk1, token)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\ts.UpdateAccountClaims(account, ac)\r\n\r\n\ttt, err := s.AccountResolver().Fetch(apk1)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tac2, err := jwt.DecodeAccountClaims(tt)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tif len(ac2.SigningKeys) != 1 && ac2.SigningKeys[0] != apk2 {\r\n\t\tt.Fatal(\"signing key is not added\")\r\n\t}\r\n\r\n\t// create an user signed by the signing key\r\n\tkp1, _ := nkeys.CreateUser()\r\n\tpub1, _ := kp1.PublicKey()\r\n\tnuc1 := jwt.NewUserClaims(pub1)\r\n\tnuc1.IssuerAccount = apk1\r\n\tujwt1, err := nuc1.Encode(akp2)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\r\n\t// Create the leaf node server using the first account.\r\n\tseed, _ := kp1.Seed()\r\n\tmycreds := genCredsFile(t, ujwt1, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tsl, _, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\nfunc TestLeafNodeExportsImports(t *testing.T) {\r\n\t// So we will create a main server with two accounts. The remote server, acting as a leaf node, will simply have\r\n\t// the $G global account and no auth. Make sure things work properly here.\r\n\ts, opts, conf := runLeafNodeOperatorServer(t)\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Setup the two accounts for this server.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create second account with exports\r\n\tacc2, akp2 := createAccount(t, s)\r\n\takp2Pub, _ := akp2.PublicKey()\r\n\takp2AC := jwt.NewAccountClaims(akp2Pub)\r\n\tstreamExport := &jwt.Export{Subject: \"foo.stream\", Type: jwt.Stream}\r\n\tserviceExport := &jwt.Export{Subject: \"req.echo\", Type: jwt.Service}\r\n\takp2AC.Exports.Add(streamExport, serviceExport)\r\n\takp2ACJWT, err := akp2AC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tif err := s.AccountResolver().Store(akp2Pub, akp2ACJWT); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\ts.UpdateAccountClaims(acc2, akp2AC)\r\n\r\n\t// Now create the first account and add on the imports. This will be what is used in the leafnode.\r\n\tacc1, akp1 := createAccount(t, s)\r\n\takp1Pub, _ := akp1.PublicKey()\r\n\takp1AC := jwt.NewAccountClaims(akp1Pub)\r\n\tstreamImport := &jwt.Import{Account: akp2Pub, Subject: \"foo.stream\", To: \"import\", Type: jwt.Stream}\r\n\tserviceImport := &jwt.Import{Account: akp2Pub, Subject: \"import.request\", To: \"req.echo\", Type: jwt.Service}\r\n\takp1AC.Imports.Add(streamImport, serviceImport)\r\n\takp1ACJWT, err := akp1AC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\tif err := s.AccountResolver().Store(akp1Pub, akp1ACJWT); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\ts.UpdateAccountClaims(acc1, akp1AC)\r\n\r\n\t// Create the user will we use to connect the leafnode.\r\n\tkp1, _ := nkeys.CreateUser()\r\n\tpub1, _ := kp1.PublicKey()\r\n\tnuc1 := jwt.NewUserClaims(pub1)\r\n\tujwt1, err := nuc1.Encode(akp1)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\r\n\t// Create the leaf node server using the first account.\r\n\tseed, _ := kp1.Seed()\r\n\tmycreds := genCredsFile(t, ujwt1, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tsl, lopts, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\t// To connect to main server.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\t// Imported\r\n\tnc1, err := nats.Connect(url, createUserCreds(t, s, akp1))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc1.Close()\r\n\r\n\t// Exported\r\n\tnc2, err := nats.Connect(url, createUserCreds(t, s, akp2))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Leaf node connection.\r\n\tlurl := fmt.Sprintf(\"nats://%s:%d\", lopts.Host, lopts.Port)\r\n\tncl, err := nats.Connect(lurl)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl.Close()\r\n\r\n\t// So everything should be setup here. So let's test streams first.\r\n\tlsub, _ := ncl.SubscribeSync(\"import.foo.stream\")\r\n\r\n\t// Wait for the sub to propagate.\r\n\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\tif subs := s.NumSubscriptions(); subs < 1 {\r\n\t\t\treturn fmt.Errorf(\"Number of subs is %d\", subs)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Pub to other account with export on original subject.\r\n\tnc2.Publish(\"foo.stream\", nil)\r\n\r\n\t_, err = lsub.NextMsg(1 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error during wait for next message: %s\", err)\r\n\t}\r\n\r\n\t// Services\r\n\t// Create listener on nc2\r\n\tnc2.Subscribe(\"req.echo\", func(msg *nats.Msg) {\r\n\t\tnc2.Publish(msg.Reply, []byte(\"WORKED\"))\r\n\t})\r\n\tnc2.Flush()\r\n\r\n\t// Now send the request on the leaf node client.\r\n\tif _, err := ncl.Request(\"import.request\", []byte(\"fingers crossed\"), 500*time.Millisecond); err != nil {\r\n\t\tt.Fatalf(\"Did not receive response: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestLeadNodeExportImportComplexSetup(t *testing.T) {\r\n\tcontent := `\r\n\tport: -1\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tresolver = MEMORY\r\n\tcluster {\r\n\t\tport: -1\r\n\t}\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\tdefer os.Remove(conf)\r\n\ts1, s1Opts := RunServerWithConfig(conf)\r\n\tdefer s1.Shutdown()\r\n\r\n\tcontent = fmt.Sprintf(`\r\n\tport: -1\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tresolver = MEMORY\r\n\tcluster {\r\n\t\tport: -1\r\n\t\troutes: [\"nats://%s:%d\"]\r\n\t}\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t}\r\n\t`, s1Opts.Cluster.Host, s1Opts.Cluster.Port)\r\n\tconf = createConfFile(t, []byte(content))\r\n\ts2, s2Opts := RunServerWithConfig(conf)\r\n\tdefer s2.Shutdown()\r\n\r\n\t// Setup the two accounts for this server.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Create second account with exports\r\n\tacc2, akp2 := createAccount(t, s1)\r\n\takp2Pub, _ := akp2.PublicKey()\r\n\takp2AC := jwt.NewAccountClaims(akp2Pub)\r\n\tstreamExport := &jwt.Export{Subject: \"foo.stream\", Type: jwt.Stream}\r\n\tserviceExport := &jwt.Export{Subject: \"req.echo\", Type: jwt.Service}\r\n\takp2AC.Exports.Add(streamExport, serviceExport)\r\n\takp2ACJWT, err := akp2AC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tif err := s1.AccountResolver().Store(akp2Pub, akp2ACJWT); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\ts1.UpdateAccountClaims(acc2, akp2AC)\r\n\r\n\t// Now create the first account and add on the imports. This will be what is used in the leafnode.\r\n\tacc1, akp1 := createAccount(t, s1)\r\n\takp1Pub, _ := akp1.PublicKey()\r\n\takp1AC := jwt.NewAccountClaims(akp1Pub)\r\n\tstreamImport := &jwt.Import{Account: akp2Pub, Subject: \"foo.stream\", To: \"import\", Type: jwt.Stream}\r\n\tserviceImport := &jwt.Import{Account: akp2Pub, Subject: \"import.request\", To: \"req.echo\", Type: jwt.Service}\r\n\takp1AC.Imports.Add(streamImport, serviceImport)\r\n\takp1ACJWT, err := akp1AC.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\tif err := s1.AccountResolver().Store(akp1Pub, akp1ACJWT); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\ts1.UpdateAccountClaims(acc1, akp1AC)\r\n\r\n\tif err := s2.AccountResolver().Store(akp2Pub, akp2ACJWT); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\t// Just make sure that account object registered in S2 is not acc2\r\n\tif a, err := s2.LookupAccount(acc2.Name); err != nil || a == acc2 {\r\n\t\tt.Fatalf(\"Lookup account error: %v - accounts are same: %v\", err, a == acc2)\r\n\t}\r\n\r\n\tif err := s2.AccountResolver().Store(akp1Pub, akp1ACJWT); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\t// Just make sure that account object registered in S2 is not acc1\r\n\tif a, err := s2.LookupAccount(acc1.Name); err != nil || a == acc1 {\r\n\t\tt.Fatalf(\"Lookup account error: %v - accounts are same: %v\", err, a == acc1)\r\n\t}\r\n\r\n\t// Create the user will we use to connect the leafnode.\r\n\tkp1, _ := nkeys.CreateUser()\r\n\tpub1, _ := kp1.PublicKey()\r\n\tnuc1 := jwt.NewUserClaims(pub1)\r\n\tujwt1, err := nuc1.Encode(akp1)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\r\n\t// Create the leaf node server using the first account.\r\n\tseed, _ := kp1.Seed()\r\n\tmycreds := genCredsFile(t, ujwt1, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tsl, lopts, lnconf := runSolicitWithCredentials(t, s1Opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s1)\r\n\r\n\t// Url to server s2\r\n\ts2URL := fmt.Sprintf(\"nats://%s:%d\", s2Opts.Host, s2Opts.Port)\r\n\r\n\t// Imported\r\n\tnc1, err := nats.Connect(s2URL, createUserCreds(t, s2, akp1))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc1.Close()\r\n\r\n\t// Exported\r\n\tnc2, err := nats.Connect(s2URL, createUserCreds(t, s2, akp2))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Leaf node connection.\r\n\tlurl := fmt.Sprintf(\"nats://%s:%d\", lopts.Host, lopts.Port)\r\n\tncl, err := nats.Connect(lurl)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl.Close()\r\n\r\n\t// So everything should be setup here. So let's test streams first.\r\n\tlsub, _ := ncl.SubscribeSync(\"import.foo.stream\")\r\n\r\n\t// Wait for the sub to propagate to s2.\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tif acc1.RoutedSubs() == 0 {\r\n\t\t\treturn fmt.Errorf(\"Still no routed subscription\")\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Pub to other account with export on original subject.\r\n\tnc2.Publish(\"foo.stream\", nil)\r\n\r\n\tif _, err = lsub.NextMsg(1 * time.Second); err != nil {\r\n\t\tt.Fatalf(\"Did not receive stream message: %s\", err)\r\n\t}\r\n\r\n\t// Services\r\n\t// Create listener on nc2 (which connects to s2)\r\n\tgotIt := int32(0)\r\n\tnc2.Subscribe(\"req.echo\", func(msg *nats.Msg) {\r\n\t\tatomic.AddInt32(&gotIt, 1)\r\n\t\tnc2.Publish(msg.Reply, []byte(\"WORKED\"))\r\n\t})\r\n\tnc2.Flush()\r\n\r\n\t// Now send the request on the leaf node client.\r\n\tif _, err := ncl.Request(\"import.request\", []byte(\"fingers crossed\"), 5500*time.Millisecond); err != nil {\r\n\t\tif atomic.LoadInt32(&gotIt) == 0 {\r\n\t\t\tt.Fatalf(\"Request was not received\")\r\n\t\t}\r\n\t\tt.Fatalf(\"Did not receive response: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeInfoURLs(t *testing.T) {\r\n\tfor _, test := range []struct {\r\n\t\tname         string\r\n\t\tuseAdvertise bool\r\n\t}{\r\n\t\t{\r\n\t\t\t\"without advertise\",\r\n\t\t\tfalse,\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"with advertise\",\r\n\t\t\ttrue,\r\n\t\t},\r\n\t} {\r\n\t\tt.Run(test.name, func(t *testing.T) {\r\n\t\t\topts := testDefaultOptionsForLeafNodes()\r\n\t\t\topts.Cluster.Port = -1\r\n\t\t\topts.LeafNode.Host = \"127.0.0.1\"\r\n\t\t\tif test.useAdvertise {\r\n\t\t\t\topts.LeafNode.Advertise = \"me:1\"\r\n\t\t\t}\r\n\t\t\ts1 := RunServer(opts)\r\n\t\t\tdefer s1.Shutdown()\r\n\r\n\t\t\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\t\t\tdefer lc.Close()\r\n\t\t\tinfo := checkInfoMsg(t, lc)\r\n\t\t\tif sz := len(info.LeafNodeURLs); sz != 1 {\r\n\t\t\t\tt.Fatalf(\"Expected LeafNodeURLs array to be size 1, got %v\", sz)\r\n\t\t\t}\r\n\t\t\tvar s1LNURL string\r\n\t\t\tif test.useAdvertise {\r\n\t\t\t\ts1LNURL = \"me:1\"\r\n\t\t\t} else {\r\n\t\t\t\ts1LNURL = net.JoinHostPort(opts.LeafNode.Host, strconv.Itoa(opts.LeafNode.Port))\r\n\t\t\t}\r\n\t\t\tif url := info.LeafNodeURLs[0]; url != s1LNURL {\r\n\t\t\t\tt.Fatalf(\"Expected URL to be %s, got %s\", s1LNURL, url)\r\n\t\t\t}\r\n\t\t\tlc.Close()\r\n\r\n\t\t\topts2 := testDefaultOptionsForLeafNodes()\r\n\t\t\topts2.Cluster.Port = -1\r\n\t\t\topts2.Routes = server.RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", opts.Cluster.Host, opts.Cluster.Port))\r\n\t\t\topts2.LeafNode.Host = \"127.0.0.1\"\r\n\t\t\tif test.useAdvertise {\r\n\t\t\t\topts2.LeafNode.Advertise = \"me:2\"\r\n\t\t\t}\r\n\t\t\ts2 := RunServer(opts2)\r\n\t\t\tdefer s2.Shutdown()\r\n\r\n\t\t\tcheckClusterFormed(t, s1, s2)\r\n\r\n\t\t\tlc = createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\t\t\tdefer lc.Close()\r\n\t\t\tinfo = checkInfoMsg(t, lc)\r\n\t\t\tif sz := len(info.LeafNodeURLs); sz != 2 {\r\n\t\t\t\tt.Fatalf(\"Expected LeafNodeURLs array to be size 2, got %v\", sz)\r\n\t\t\t}\r\n\t\t\tvar s2LNURL string\r\n\t\t\tif test.useAdvertise {\r\n\t\t\t\ts2LNURL = \"me:2\"\r\n\t\t\t} else {\r\n\t\t\t\ts2LNURL = net.JoinHostPort(opts2.LeafNode.Host, strconv.Itoa(opts2.LeafNode.Port))\r\n\t\t\t}\r\n\t\t\tvar ok [2]int\r\n\t\t\tfor _, url := range info.LeafNodeURLs {\r\n\t\t\t\tif url == s1LNURL {\r\n\t\t\t\t\tok[0]++\r\n\t\t\t\t} else if url == s2LNURL {\r\n\t\t\t\t\tok[1]++\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tfor i, res := range ok {\r\n\t\t\t\tif res != 1 {\r\n\t\t\t\t\tt.Fatalf(\"URL from server %v was found %v times\", i+1, res)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tlc.Close()\r\n\r\n\t\t\t// Remove s2, and wait for route to be lost on s1.\r\n\t\t\ts2.Shutdown()\r\n\t\t\tcheckNumRoutes(t, s1, 0)\r\n\r\n\t\t\t// Now check that s1 returns only itself in the URLs array.\r\n\t\t\tlc = createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\t\t\tdefer lc.Close()\r\n\t\t\tinfo = checkInfoMsg(t, lc)\r\n\t\t\tif sz := len(info.LeafNodeURLs); sz != 1 {\r\n\t\t\t\tt.Fatalf(\"Expected LeafNodeURLs array to be size 1, got %v\", sz)\r\n\t\t\t}\r\n\t\t\tif url := info.LeafNodeURLs[0]; url != s1LNURL {\r\n\t\t\t\tt.Fatalf(\"Expected URL to be %s, got %s\", s1LNURL, url)\r\n\t\t\t}\r\n\t\t\tlc.Close()\r\n\r\n\t\t\ts1.Shutdown()\r\n\r\n\t\t\t// Now we need a configuration where both s1 and s2 have a route\r\n\t\t\t// to each other, so we need explicit configuration. We are trying\r\n\t\t\t// to get S1->S2 and S2->S1 so one of the route is dropped. This\r\n\t\t\t// should not affect the number of URLs reported in INFO.\r\n\r\n\t\t\topts.Cluster.Port = 5223\r\n\t\t\topts.Routes = server.RoutesFromStr(fmt.Sprintf(\"nats://%s:5224\", opts2.Host))\r\n\t\t\ts1, _ = server.NewServer(opts)\r\n\t\t\tdefer s1.Shutdown()\r\n\r\n\t\t\topts2.Cluster.Port = 5224\r\n\t\t\topts2.Routes = server.RoutesFromStr(fmt.Sprintf(\"nats://%s:5223\", opts.Host))\r\n\t\t\ts2, _ = server.NewServer(opts2)\r\n\t\t\tdefer s2.Shutdown()\r\n\r\n\t\t\t// Start this way to increase chance of having the two connect\r\n\t\t\t// to each other at the same time. This will cause one of the\r\n\t\t\t// route to be dropped.\r\n\t\t\twg := &sync.WaitGroup{}\r\n\t\t\twg.Add(2)\r\n\t\t\tgo func() {\r\n\t\t\t\ts1.Start()\r\n\t\t\t\twg.Done()\r\n\t\t\t}()\r\n\t\t\tgo func() {\r\n\t\t\t\ts2.Start()\r\n\t\t\t\twg.Done()\r\n\t\t\t}()\r\n\r\n\t\t\tcheckClusterFormed(t, s1, s2)\r\n\r\n\t\t\tlc = createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\t\t\tdefer lc.Close()\r\n\t\t\tinfo = checkInfoMsg(t, lc)\r\n\t\t\tif sz := len(info.LeafNodeURLs); sz != 2 {\r\n\t\t\t\tt.Fatalf(\"Expected LeafNodeURLs array to be size 2, got %v\", sz)\r\n\t\t\t}\r\n\t\t\tok[0], ok[1] = 0, 0\r\n\t\t\tfor _, url := range info.LeafNodeURLs {\r\n\t\t\t\tif url == s1LNURL {\r\n\t\t\t\t\tok[0]++\r\n\t\t\t\t} else if url == s2LNURL {\r\n\t\t\t\t\tok[1]++\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tfor i, res := range ok {\r\n\t\t\t\tif res != 1 {\r\n\t\t\t\t\tt.Fatalf(\"URL from server %v was found %v times\", i+1, res)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeFailover(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(50 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterWithName(t, \"A\", 2)\r\n\tdefer shutdownCluster(ca)\r\n\r\n\tcb := createClusterWithName(t, \"B\", 1, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Start a server that creates LeafNode connection to first\r\n\t// server in cluster A.\r\n\ts, opts := runSolicitLeafServer(ca.opts[0])\r\n\tdefer s.Shutdown()\r\n\r\n\t// Shutdown that server on A.\r\n\tca.servers[0].Shutdown()\r\n\r\n\t// Make sure that s reconnects its LN connection\r\n\tcheckLNConnected := func(t *testing.T, s *server.Server) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, 3*time.Second, 15*time.Millisecond, func() error {\r\n\t\t\tif s.NumLeafNodes() == 1 {\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\t\treturn fmt.Errorf(\"Server did not reconnect to second server in cluster A\")\r\n\t\t})\r\n\t}\r\n\tcheckLNConnected(t, ca.servers[1])\r\n\r\n\t// Verify that LeafNode info protocol is sent to the server `s`\r\n\t// with list of new servers. To do that, we will restart\r\n\t// ca[0] but with a different LN listen port.\r\n\tca.opts[0].Port = -1\r\n\tca.opts[0].Cluster.Port = -1\r\n\tca.opts[0].Routes = server.RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", ca.opts[1].Cluster.Host, ca.opts[1].Cluster.Port))\r\n\tca.opts[0].LeafNode.Port = -1\r\n\tnewa0 := RunServer(ca.opts[0])\r\n\tdefer newa0.Shutdown()\r\n\r\n\tcheckClusterFormed(t, newa0, ca.servers[1])\r\n\r\n\t// Shutdown the server the LN is currently connected to. It should\r\n\t// reconnect to newa0.\r\n\tca.servers[1].Shutdown()\r\n\tcheckLNConnected(t, newa0)\r\n\r\n\t// Now shutdown newa0 and make sure `s` does not reconnect\r\n\t// to server in gateway.\r\n\tnewa0.Shutdown()\r\n\r\n\t// Wait for more than the reconnect attempts.\r\n\ttime.Sleep(opts.LeafNode.ReconnectInterval + 50*time.Millisecond)\r\n\r\n\tif cb.servers[0].NumLeafNodes() != 0 {\r\n\t\tt.Fatalf(\"Server reconnected to server in cluster B\")\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeAdvertise(t *testing.T) {\r\n\t// Create a dummy listener which will we use for the advertise address.\r\n\t// We will then stop the server and the test will be a success if\r\n\t// this listener accepts a connection.\r\n\tch := make(chan struct{}, 1)\r\n\tl, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error starting listener: %v\", err)\r\n\t}\r\n\tdefer l.Close()\r\n\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(1)\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tc, _ := l.Accept()\r\n\t\tif c != nil {\r\n\t\t\tc.Close()\r\n\t\t}\r\n\t\tl.Close()\r\n\t\tch <- struct{}{}\r\n\t}()\r\n\r\n\tport := l.Addr().(*net.TCPAddr).Port\r\n\r\n\to2 := testDefaultOptionsForLeafNodes()\r\n\to2.LeafNode.Advertise = fmt.Sprintf(\"127.0.0.1:%d\", port)\r\n\to2.Cluster.Port = -1\r\n\ts2 := RunServer(o2)\r\n\tdefer s2.Shutdown()\r\n\r\n\to1 := testDefaultOptionsForLeafNodes()\r\n\to1.Cluster.Port = -1\r\n\to1.Routes = server.RoutesFromStr(fmt.Sprintf(\"nats://127.0.0.1:%d\", o2.Cluster.Port))\r\n\ts1 := RunServer(o1)\r\n\tdefer s1.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s1, s2)\r\n\r\n\t// Start a server that connects to s1. It should be made aware\r\n\t// of s2 (and its advertise address).\r\n\ts, _ := runSolicitLeafServer(o1)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Wait for leaf node connection to be established on s1.\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\tif s1.NumLeafNodes() == 1 {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\treturn fmt.Errorf(\"Leaf node connection still not established\")\r\n\t})\r\n\r\n\t// Shutdown s1. The listener that we created should be the one\r\n\t// receiving the connection from s.\r\n\ts1.Shutdown()\r\n\r\n\tselect {\r\n\tcase <-ch:\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatalf(\"Server did not reconnect to advertised address\")\r\n\t}\r\n\twg.Wait()\r\n}\r\n\r\nfunc TestLeafNodeConnectionLimitsSingleServer(t *testing.T) {\r\n\ts, opts, conf := runLeafNodeOperatorServer(t)\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Setup account and a user that will be used by the remote leaf node server.\r\n\t// createAccount automatically registers with resolver etc..\r\n\tacc, akp := createAccount(t, s)\r\n\r\n\t// Now update with limits for lead node connections.\r\n\tconst maxleafs = 2\r\n\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tnac.Limits.LeafNodeConn = maxleafs\r\n\ts.UpdateAccountClaims(acc, nac)\r\n\r\n\t// Make sure we have the limits updated in acc.\r\n\tif mleafs := acc.MaxActiveLeafNodes(); mleafs != maxleafs {\r\n\t\tt.Fatalf(\"Expected to have max leafnodes of %d, got %d\", maxleafs, mleafs)\r\n\t}\r\n\r\n\t// Create the user credentials for the leadnode connection.\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tseed, _ := kp.Seed()\r\n\tmycreds := genCredsFile(t, ujwt, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tcheckLFCount := func(n int) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\t\tif nln := s.NumLeafNodes(); nln != n {\r\n\t\t\t\treturn fmt.Errorf(\"Number of leaf nodes is %d\", nln)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\r\n\tsl, _, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\tcheckLFCount(1)\r\n\r\n\t// Make sure we are accounting properly here.\r\n\tif nln := acc.NumLeafNodes(); nln != 1 {\r\n\t\tt.Fatalf(\"Expected 1 leaf node, got %d\", nln)\r\n\t}\r\n\t// clients and leafnodes counted together.\r\n\tif nc := acc.NumConnections(); nc != 1 {\r\n\t\tt.Fatalf(\"Expected 1 for total connections, got %d\", nc)\r\n\t}\r\n\r\n\ts2, _, lnconf2 := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf2)\r\n\tdefer s2.Shutdown()\r\n\tcheckLFCount(2)\r\n\r\n\t// Make sure we are accounting properly here.\r\n\tif nln := acc.NumLeafNodes(); nln != 2 {\r\n\t\tt.Fatalf(\"Expected 2 leaf nodes, got %d\", nln)\r\n\t}\r\n\t// clients and leafnodes counted together.\r\n\tif nc := acc.NumConnections(); nc != 2 {\r\n\t\tt.Fatalf(\"Expected 2 total connections, got %d\", nc)\r\n\t}\r\n\ts2.Shutdown()\r\n\tcheckLFCount(1)\r\n\r\n\t// Make sure we are accounting properly here.\r\n\tif nln := acc.NumLeafNodes(); nln != 1 {\r\n\t\tt.Fatalf(\"Expected 1 leaf node, got %d\", nln)\r\n\t}\r\n\t// clients and leafnodes counted together.\r\n\tif nc := acc.NumConnections(); nc != 1 {\r\n\t\tt.Fatalf(\"Expected 1 for total connections, got %d\", nc)\r\n\t}\r\n\r\n\t// Now add back the second one as #3.\r\n\ts3, _, lnconf3 := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf3)\r\n\tdefer s3.Shutdown()\r\n\tcheckLFCount(2)\r\n\r\n\tif nln := acc.NumLeafNodes(); nln != 2 {\r\n\t\tt.Fatalf(\"Expected 2 leaf nodes, got %d\", nln)\r\n\t}\r\n\r\n\t// Once we are here we should not be able to create anymore. Limit == 2.\r\n\ts4, _, lnconf4 := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf4)\r\n\tdefer s4.Shutdown()\r\n\r\n\tif nln := acc.NumLeafNodes(); nln != 2 {\r\n\t\tt.Fatalf(\"Expected 2 leaf nodes, got %d\", nln)\r\n\t}\r\n\r\n\t// Make sure s4 has 0 still.\r\n\tif nln := s4.NumLeafNodes(); nln != 0 {\r\n\t\tt.Fatalf(\"Expected no leafnodes accounted for in s4, got %d\", nln)\r\n\t}\r\n\r\n\t// Make sure this is still 2.\r\n\tcheckLFCount(2)\r\n}\r\n\r\nfunc TestLeafNodeConnectionLimitsCluster(t *testing.T) {\r\n\tcontent := `\r\n\tport: -1\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n    system_account = \"AD2VB6C25DQPEUUQ7KJBUFX2J4ZNVBPOHSCBISC7VFZXVWXZA7VASQZG\"\r\n\tresolver = MEMORY\r\n\tcluster {\r\n\t\tport: -1\r\n\t}\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t}\r\n    resolver_preload = {\r\n        AD2VB6C25DQPEUUQ7KJBUFX2J4ZNVBPOHSCBISC7VFZXVWXZA7VASQZG : \"eyJ0eXAiOiJqd3QiLCJhbGciOiJlZDI1NTE5In0.eyJqdGkiOiJDSzU1UERKSUlTWU5QWkhLSUpMVURVVTdJT1dINlM3UkE0RUc2TTVGVUQzUEdGQ1RWWlJRIiwiaWF0IjoxNTQzOTU4NjU4LCJpc3MiOiJPQ0FUMzNNVFZVMlZVT0lNR05HVU5YSjY2QUgyUkxTREFGM01VQkNZQVk1UU1JTDY1TlFNNlhRRyIsInN1YiI6IkFEMlZCNkMyNURRUEVVVVE3S0pCVUZYMko0Wk5WQlBPSFNDQklTQzdWRlpYVldYWkE3VkFTUVpHIiwidHlwZSI6ImFjY291bnQiLCJuYXRzIjp7ImxpbWl0cyI6e319fQ.7m1fysYUsBw15Lj88YmYoHxOI4HlOzu6qgP8Zg-1q9mQXUURijuDGVZrtb7gFYRlo-nG9xZyd2ZTRpMA-b0xCQ\"\r\n    }\r\n\t`\r\n\tconf := createConfFile(t, []byte(content))\r\n\tdefer os.Remove(conf)\r\n\ts1, s1Opts := RunServerWithConfig(conf)\r\n\tdefer s1.Shutdown()\r\n\r\n\tcontent = fmt.Sprintf(`\r\n\tport: -1\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n    system_account = \"AD2VB6C25DQPEUUQ7KJBUFX2J4ZNVBPOHSCBISC7VFZXVWXZA7VASQZG\"\r\n\tresolver = MEMORY\r\n\tcluster {\r\n\t\tport: -1\r\n\t\troutes: [\"nats://%s:%d\"]\r\n\t}\r\n\tleafnodes {\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t}\r\n    resolver_preload = {\r\n        AD2VB6C25DQPEUUQ7KJBUFX2J4ZNVBPOHSCBISC7VFZXVWXZA7VASQZG : \"eyJ0eXAiOiJqd3QiLCJhbGciOiJlZDI1NTE5In0.eyJqdGkiOiJDSzU1UERKSUlTWU5QWkhLSUpMVURVVTdJT1dINlM3UkE0RUc2TTVGVUQzUEdGQ1RWWlJRIiwiaWF0IjoxNTQzOTU4NjU4LCJpc3MiOiJPQ0FUMzNNVFZVMlZVT0lNR05HVU5YSjY2QUgyUkxTREFGM01VQkNZQVk1UU1JTDY1TlFNNlhRRyIsInN1YiI6IkFEMlZCNkMyNURRUEVVVVE3S0pCVUZYMko0Wk5WQlBPSFNDQklTQzdWRlpYVldYWkE3VkFTUVpHIiwidHlwZSI6ImFjY291bnQiLCJuYXRzIjp7ImxpbWl0cyI6e319fQ.7m1fysYUsBw15Lj88YmYoHxOI4HlOzu6qgP8Zg-1q9mQXUURijuDGVZrtb7gFYRlo-nG9xZyd2ZTRpMA-b0xCQ\"\r\n    }\r\n\t`, s1Opts.Cluster.Host, s1Opts.Cluster.Port)\r\n\tconf = createConfFile(t, []byte(content))\r\n\ts2, s2Opts := RunServerWithConfig(conf)\r\n\tdefer s2.Shutdown()\r\n\r\n\t// Setup the two accounts for this server.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\t// Setup account and a user that will be used by the remote leaf node server.\r\n\t// createAccount automatically registers with resolver etc..\r\n\tacc, akp := createAccount(t, s1)\r\n\r\n\t// Now update with limits for lead node connections.\r\n\tconst maxleafs = 10\r\n\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tnac.Limits.LeafNodeConn = maxleafs\r\n\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\tif err := s1.AccountResolver().Store(apub, ajwt); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\ts1.UpdateAccountClaims(acc, nac)\r\n\r\n\tif err := s2.AccountResolver().Store(apub, ajwt); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\t// Make sure that account object registered in S2 is not acc2\r\n\tacc2, err := s2.LookupAccount(acc.Name)\r\n\tif err != nil || acc == acc2 {\r\n\t\tt.Fatalf(\"Lookup account error: %v - accounts are same: %v\", err, acc == acc2)\r\n\t}\r\n\r\n\t// Create the user credentials for the leadnode connection.\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tseed, _ := kp.Seed()\r\n\tmycreds := genCredsFile(t, ujwt, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tloop := maxleafs / 2\r\n\r\n\t// Now create maxleafs/2 leaf node servers on each operator server.\r\n\tfor i := 0; i < loop; i++ {\r\n\t\tsl1, _, lnconf1 := runSolicitWithCredentials(t, s1Opts, mycreds)\r\n\t\tdefer os.Remove(lnconf1)\r\n\t\tdefer sl1.Shutdown()\r\n\r\n\t\tsl2, _, lnconf2 := runSolicitWithCredentials(t, s2Opts, mycreds)\r\n\t\tdefer os.Remove(lnconf2)\r\n\t\tdefer sl2.Shutdown()\r\n\t}\r\n\r\n\tcheckLFCount := func(s *server.Server, n int) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\t\tif nln := s.NumLeafNodes(); nln != n {\r\n\t\t\t\treturn fmt.Errorf(\"Number of leaf nodes is %d\", nln)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\tcheckLFCount(s1, loop)\r\n\tcheckLFCount(s2, loop)\r\n\r\n\t// Now check that we have the remotes registered. This will prove we are sending\r\n\t// and processing the leaf node connect events properly etc.\r\n\tcheckAccRemoteLFCount := func(acc *server.Account, n int) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, time.Second, 10*time.Millisecond, func() error {\r\n\t\t\tif nrln := acc.NumRemoteLeafNodes(); nrln != n {\r\n\t\t\t\treturn fmt.Errorf(\"Number of remote leaf nodes is %d\", nrln)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\tcheckAccRemoteLFCount(acc, loop)\r\n\tcheckAccRemoteLFCount(acc2, loop)\r\n\r\n\t// Now that we are here we should not be allowed anymore leaf nodes.\r\n\tl, _, lnconf := runSolicitWithCredentials(t, s1Opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer l.Shutdown()\r\n\r\n\tif nln := acc.NumLeafNodes(); nln != maxleafs {\r\n\t\tt.Fatalf(\"Expected %d leaf nodes, got %d\", maxleafs, nln)\r\n\t}\r\n\t// Should still be at loop size.\r\n\tcheckLFCount(s1, loop)\r\n\r\n\tl, _, lnconf = runSolicitWithCredentials(t, s2Opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer l.Shutdown()\r\n\tif nln := acc2.NumLeafNodes(); nln != maxleafs {\r\n\t\tt.Fatalf(\"Expected %d leaf nodes, got %d\", maxleafs, nln)\r\n\t}\r\n\t// Should still be at loop size.\r\n\tcheckLFCount(s2, loop)\r\n}\r\n\r\nfunc TestLeafNodeSwitchGatewayToInterestModeOnly(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(50 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterWithName(t, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterWithName(t, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Create client on a server in cluster A\r\n\topts := ca.opts[0]\r\n\tc := createClientConn(t, opts.Host, opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\t// Send a message from this client on \"foo\" so that B\r\n\t// registers a no-interest for account \"$G\"\r\n\tsend(\"PUB foo 2\\r\\nok\\r\\nPING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\t// Create a leaf node connection on a server in cluster B\r\n\topts = cb.opts[0]\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\t// This is for our global responses since we are setting up GWs above.\r\n\tleafSend, leafExpect := setupLeaf(t, lc, 3)\r\n\tleafSend(\"PING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n}\r\n\r\n// The MSG proto for routes and gateways is RMSG, and we have an\r\n// optimization that a scratch buffer has RMSG and when doing a\r\n// client we just start at scratch[1]. For leaf nodes its LMSG and we\r\n// rewrite scratch[0], but never reset it which causes protocol\r\n// errors when used with routes or gateways after use to send\r\n// to a leafnode.\r\n// We will create a server with a leafnode connection and a route\r\n// and a gateway connection.\r\n\r\n// route connections to simulate.\r\nfunc TestLeafNodeResetsMSGProto(t *testing.T) {\r\n\topts := testDefaultOptionsForLeafNodes()\r\n\topts.Cluster.Host = opts.Host\r\n\topts.Cluster.Port = -1\r\n\topts.Gateway.Name = \"lproto\"\r\n\topts.Gateway.Host = opts.Host\r\n\topts.Gateway.Port = -1\r\n\topts.Accounts = []*server.Account{server.NewAccount(\"$SYS\")}\r\n\topts.SystemAccount = \"$SYS\"\r\n\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tleafSend, leafExpect := setupConn(t, lc)\r\n\r\n\t// To avoid possible INFO when switching to interest mode only,\r\n\t// delay start of gateway.\r\n\ttime.Sleep(500 * time.Millisecond)\r\n\r\n\tgw := createGatewayConn(t, opts.Gateway.Host, opts.Gateway.Port)\r\n\tdefer gw.Close()\r\n\r\n\tgwSend, gwExpect := setupGatewayConn(t, gw, \"A\", \"lproto\")\r\n\tgwSend(\"PING\\r\\n\")\r\n\tgwExpect(pongRe)\r\n\r\n\t// This is for our global responses since we are setting up GWs above.\r\n\tleafExpect(lsubRe)\r\n\r\n\t// Now setup interest in the leaf node for 'foo'.\r\n\tleafSend(\"LS+ foo\\r\\nPING\\r\\n\")\r\n\tleafExpect(pongRe)\r\n\r\n\t// Send msg from the gateway.\r\n\tgwSend(\"RMSG $G foo 2\\r\\nok\\r\\nPING\\r\\n\")\r\n\tgwExpect(pongRe)\r\n\r\n\tleafExpect(lmsgRe)\r\n\r\n\t// At this point the gw inside our main server's scratch buffer is LMSG. When we do\r\n\t// same with a connected route with interest it should fail.\r\n\trc := createRouteConn(t, opts.Cluster.Host, opts.Cluster.Port)\r\n\tdefer rc.Close()\r\n\tcheckInfoMsg(t, rc)\r\n\trouteSend, routeExpect := setupRouteEx(t, rc, opts, \"RC\")\r\n\r\n\trouteSend(\"RS+ $G foo\\r\\nPING\\r\\n\")\r\n\trouteExpect(pongRe)\r\n\r\n\t// This is for the route interest we just created.\r\n\tleafExpect(lsubRe)\r\n\r\n\t// Send msg from the gateway.\r\n\tgwSend(\"RMSG $G foo 2\\r\\nok\\r\\nPING\\r\\n\")\r\n\tgwExpect(pongRe)\r\n\r\n\tleafExpect(lmsgRe)\r\n\r\n\t// Now make sure we get it on route. This will fail with the proto bug.\r\n\trouteExpect(rmsgRe)\r\n}\r\n\r\n// We need to make sure that as a remote server we also send our local subs on connect.\r\nfunc TestLeafNodeSendsRemoteSubsOnConnect(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tsl, slOpts := runSolicitLeafServer(opts)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\ts.Shutdown()\r\n\r\n\tc := createClientConn(t, slOpts.Host, slOpts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"SUB foo 1\\r\\n\")\r\n\tsend(\"PING\\r\\n\")\r\n\texpect(pongRe)\r\n\r\n\t// Need to restart it on the same port.\r\n\ts, _ = runLeafServerOnPort(opts.LeafNode.Port)\r\n\tdefer s.Shutdown()\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\tdefer lc.Close()\r\n\r\n\tsetupLeaf(t, lc, 2)\r\n}\r\n\r\nfunc TestLeafNodeServiceImportLikeNGS(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(10 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterEx(t, true, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterEx(t, true, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Hang a responder off of cluster A.\r\n\topts := ca.opts[0]\r\n\turl := fmt.Sprintf(\"nats://ngs:pass@%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Create a queue subscriber to send results\r\n\tnc.QueueSubscribe(\"ngs.usage.*\", \"ngs\", func(m *nats.Msg) {\r\n\t\tm.Respond([]byte(\"22\"))\r\n\t})\r\n\tnc.Flush()\r\n\r\n\t// Now create a leafnode server on B.\r\n\topts = cb.opts[1]\r\n\tsl, slOpts := runSolicitLeafServer(opts)\r\n\tdefer sl.Shutdown()\r\n\r\n\t// Create a normal direct connect client on B.\r\n\turl = fmt.Sprintf(\"nats://dlc:pass@%s:%d\", opts.Host, opts.Port)\r\n\tnc2, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\tif _, err := nc2.Request(\"ngs.usage\", []byte(\"fingers crossed\"), 500*time.Millisecond); err != nil {\r\n\t\tt.Fatalf(\"Did not receive response: %v\", err)\r\n\t}\r\n\r\n\t// Now create a client on the leafnode.\r\n\turl = fmt.Sprintf(\"nats://%s:%d\", slOpts.Host, slOpts.Port)\r\n\tncl, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl.Close()\r\n\r\n\tif _, err := ncl.Request(\"ngs.usage\", []byte(\"fingers crossed\"), 500*time.Millisecond); err != nil {\r\n\t\tt.Fatalf(\"Did not receive response: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeSendsAccountingEvents(t *testing.T) {\r\n\ts, opts, conf := runLeafNodeOperatorServer(t)\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// System account\r\n\tacc, akp := createAccount(t, s)\r\n\tif err := s.SetSystemAccount(acc.Name); err != nil {\r\n\t\tt.Fatalf(\"Expected this succeed, got %v\", err)\r\n\t}\r\n\r\n\t// Leafnode Account\r\n\tlacc, lakp := createAccount(t, s)\r\n\r\n\t// Create a system account user and connect a client to listen for the events.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Watch only for our leaf node account.\r\n\tcSub, _ := nc.SubscribeSync(fmt.Sprintf(\"$SYS.ACCOUNT.%s.CONNECT\", lacc.Name))\r\n\tdSub, _ := nc.SubscribeSync(fmt.Sprintf(\"$SYS.ACCOUNT.%s.DISCONNECT\", lacc.Name))\r\n\tnc.Flush()\r\n\r\n\t// Now create creds for the leafnode and connect the server.\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(lakp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tseed, _ := kp.Seed()\r\n\tmycreds := genCredsFile(t, ujwt, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\tsl, _, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\t// Wait for connect event\r\n\tmsg, err := cSub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error waiting for account connect event: %v\", err)\r\n\t}\r\n\tm := server.ConnectEventMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &m); err != nil {\r\n\t\tt.Fatal(\"Did not get correctly formatted event\")\r\n\t}\r\n\r\n\t// Shutdown leafnode to generate disconnect event.\r\n\tsl.Shutdown()\r\n\r\n\tmsg, err = dSub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error waiting for account disconnect event: %v\", err)\r\n\t}\r\n\tdm := server.DisconnectEventMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &dm); err != nil {\r\n\t\tt.Fatal(\"Did not get correctly formatted event\")\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeDistributedQueueAcrossGWs(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(10 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterEx(t, true, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterEx(t, true, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\tcc := createClusterEx(t, true, \"C\", 3, ca, cb)\r\n\tdefer shutdownCluster(cc)\r\n\r\n\t// Create queue subscribers\r\n\tcreateQS := func(c *cluster) *nats.Conn {\r\n\t\tt.Helper()\r\n\t\topts := c.opts[rand.Intn(len(c.opts))]\r\n\t\turl := fmt.Sprintf(\"nats://ngs:pass@%s:%d\", opts.Host, opts.Port)\r\n\t\tnc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tnc.QueueSubscribe(\"ngs.usage.*\", \"dq\", func(m *nats.Msg) {\r\n\t\t\tm.Respond([]byte(c.name))\r\n\t\t})\r\n\t\tnc.Flush()\r\n\t\treturn nc\r\n\t}\r\n\r\n\tncA := createQS(ca)\r\n\tdefer ncA.Close()\r\n\tncB := createQS(cb)\r\n\tdefer ncB.Close()\r\n\tncC := createQS(cc)\r\n\tdefer ncC.Close()\r\n\r\n\tconnectAndRequest := func(url, clusterName string, nreqs int) {\r\n\t\tt.Helper()\r\n\t\tnc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t\tfor i := 0; i < nreqs; i++ {\r\n\t\t\tm, err := nc.Request(\"ngs.usage\", nil, 500*time.Millisecond)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Did not receive a response: %v\", err)\r\n\t\t\t}\r\n\t\t\tif string(m.Data) != clusterName {\r\n\t\t\t\tt.Fatalf(\"Expected to prefer %q, but got response from %q\", clusterName, m.Data)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tcheckClientDQ := func(c *cluster, nreqs int) {\r\n\t\tt.Helper()\r\n\t\t// Pick one at random.\r\n\t\topts := c.opts[rand.Intn(len(c.opts))]\r\n\t\turl := fmt.Sprintf(\"nats://dlc:pass@%s:%d\", opts.Host, opts.Port)\r\n\t\tconnectAndRequest(url, c.name, nreqs)\r\n\t}\r\n\r\n\t// First check that this works with direct connected clients.\r\n\tcheckClientDQ(ca, 100)\r\n\tcheckClientDQ(cb, 100)\r\n\tcheckClientDQ(cc, 100)\r\n\r\n\tcreateLNS := func(c *cluster) (*server.Server, *server.Options) {\r\n\t\tt.Helper()\r\n\t\t// Pick one at random.\r\n\t\ts, opts := runSolicitLeafServer(c.opts[rand.Intn(len(c.servers))])\r\n\t\tcheckLeafNodeConnected(t, s)\r\n\t\treturn s, opts\r\n\t}\r\n\r\n\tcheckLeafDQ := func(opts *server.Options, clusterName string, nreqs int) {\r\n\t\tt.Helper()\r\n\t\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\t\tconnectAndRequest(url, clusterName, nreqs)\r\n\t}\r\n\r\n\t// Test leafnodes to all clusters.\r\n\tfor _, c := range []*cluster{ca, cb, cc} {\r\n\t\t// Now create a leafnode on cluster.\r\n\t\tsl, slOpts := createLNS(c)\r\n\t\tdefer sl.Shutdown()\r\n\t\t// Now connect to the leafnode server and run test.\r\n\t\tcheckLeafDQ(slOpts, c.name, 100)\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeDistributedQueueEvenly(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(10 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterEx(t, true, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterEx(t, true, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Create queue subscribers\r\n\tcreateQS := func(c *cluster) *nats.Conn {\r\n\t\tt.Helper()\r\n\t\topts := c.opts[rand.Intn(len(c.opts))]\r\n\t\turl := fmt.Sprintf(\"nats://ngs:pass@%s:%d\", opts.Host, opts.Port)\r\n\t\tnc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tcid, _ := nc.GetClientID()\r\n\t\tresponse := []byte(fmt.Sprintf(\"%s:%d:%d\", c.name, opts.Port, cid))\r\n\t\tnc.QueueSubscribe(\"ngs.usage.*\", \"dq\", func(m *nats.Msg) {\r\n\t\t\tm.Respond(response)\r\n\t\t})\r\n\t\tnc.Flush()\r\n\t\treturn nc\r\n\t}\r\n\r\n\tncA1 := createQS(ca)\r\n\tdefer ncA1.Close()\r\n\r\n\tncA2 := createQS(ca)\r\n\tdefer ncA2.Close()\r\n\r\n\tncA3 := createQS(ca)\r\n\tdefer ncA3.Close()\r\n\r\n\tresp := make(map[string]int)\r\n\r\n\tconnectAndRequest := func(url, clusterName string, nreqs int) {\r\n\t\tt.Helper()\r\n\t\tnc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t\tfor i := 0; i < nreqs; i++ {\r\n\t\t\tm, err := nc.Request(\"ngs.usage\", nil, 500*time.Millisecond)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Did not receive a response: %v\", err)\r\n\t\t\t}\r\n\t\t\tif string(m.Data[0]) != clusterName {\r\n\t\t\t\tt.Fatalf(\"Expected to prefer %q, but got response from %q\", clusterName, m.Data[0])\r\n\t\t\t}\r\n\t\t\tresp[string(m.Data)]++\r\n\t\t}\r\n\t}\r\n\r\n\tcreateLNS := func(c *cluster) (*server.Server, *server.Options) {\r\n\t\tt.Helper()\r\n\t\t// Pick one at random.\r\n\t\tcopts := c.opts[rand.Intn(len(c.servers))]\r\n\t\ts, opts := runSolicitLeafServer(copts)\r\n\t\tcheckLeafNodeConnected(t, s)\r\n\t\treturn s, opts\r\n\t}\r\n\r\n\tcheckLeafDQ := func(opts *server.Options, clusterName string, nreqs int) {\r\n\t\tt.Helper()\r\n\t\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\t\tconnectAndRequest(url, clusterName, nreqs)\r\n\t}\r\n\r\n\t// Now create a leafnode on cluster A.\r\n\tsl, slOpts := createLNS(ca)\r\n\tdefer sl.Shutdown()\r\n\r\n\t// Now connect to the leafnode server and run test.\r\n\tcheckLeafDQ(slOpts, ca.name, 100)\r\n\r\n\t// Should have some for all 3 QS [ncA1, ncA2, ncA3]\r\n\tif lr := len(resp); lr != 3 {\r\n\t\tt.Fatalf(\"Expected all 3 queue subscribers to have received some messages, only got %d\", lr)\r\n\t}\r\n\t// Now check that we have at least 10% for each subscriber.\r\n\tfor _, r := range resp {\r\n\t\tif r < 10 {\r\n\t\t\tt.Fatalf(\"Got a subscriber with less than 10 responses: %d\", r)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestLeafNodeDefaultPort(t *testing.T) {\r\n\to := testDefaultOptionsForLeafNodes()\r\n\to.LeafNode.Port = server.DEFAULT_LEAFNODE_PORT\r\n\ts := RunServer(o)\r\n\tdefer s.Shutdown()\r\n\r\n\tconf := createConfFile(t, []byte(`\r\n\t\tport: -1\r\n\t\tleaf {\r\n\t\t\tremotes = [\r\n\t\t\t\t{\r\n\t\t\t\t\turl: \"leafnode://127.0.0.1\"\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\r\n\tsl, _ := RunServerWithConfig(conf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\n// Since leafnode's are true interest only we need to make sure that we\r\n// register the proper interest with global routing $GR.xxxxxx._INBOX.>\r\nfunc TestLeafNodeAndGatewayGlobalRouting(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(50 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\tca := createClusterWithName(t, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterWithName(t, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\tsl, slOpts := runSolicitLeafServer(ca.opts[1])\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, ca.servers[1])\r\n\r\n\t// Create a client on the leafnode. This will listen for requests.\r\n\tncl, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", slOpts.Host, slOpts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl.Close()\r\n\r\n\tncl.Subscribe(\"foo\", func(m *nats.Msg) {\r\n\t\tm.Respond([]byte(\"World\"))\r\n\t})\r\n\tncl.Flush()\r\n\r\n\t// Since for leafnodes the account becomes interest-only mode,\r\n\t// let's make sure that the interest on \"foo\" has time to propagate\r\n\t// to cluster B.\r\n\ttime.Sleep(250 * time.Millisecond)\r\n\r\n\t// Create a direct connect requestor. Try with all possible\r\n\t// servers in cluster B to make sure that we also receive the\r\n\t// reply when the accepting leafnode server does not have\r\n\t// its outbound GW connection to the requestor's server.\r\n\tfor i := 0; i < 3; i++ {\r\n\t\topts := cb.opts[i]\r\n\t\turl := fmt.Sprintf(\"nats://ngs:pass@%s:%d\", opts.Host, opts.Port)\r\n\t\tnc, err := nats.Connect(url)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\r\n\t\t// We don't use an INBOX here because we had a bug where\r\n\t\t// leafnode would subscribe to _GR_.*.*.*.> instead of\r\n\t\t// _GR_.>, and inbox masked that because of their number\r\n\t\t// of tokens.\r\n\t\treply := nuid.Next()\r\n\t\tsub, err := nc.SubscribeSync(reply)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t\t}\r\n\t\tif err := nc.PublishRequest(\"foo\", reply, []byte(\"Hello\")); err != nil {\r\n\t\t\tt.Fatalf(\"Failed to send request: %v\", err)\r\n\t\t}\r\n\t\tif _, err := sub.NextMsg(250 * time.Millisecond); err != nil {\r\n\t\t\tt.Fatalf(\"Did not get reply from server %d: %v\", i, err)\r\n\t\t}\r\n\t\tnc.Close()\r\n\t}\r\n}\r\n\r\nfunc checkLeafNode2Connected(t *testing.T, s *server.Server) {\r\n\tt.Helper()\r\n\tcheckFor(t, 5*time.Second, 100*time.Millisecond, func() error {\r\n\t\tif nln := s.NumLeafNodes(); nln != 2 {\r\n\t\t\treturn fmt.Errorf(\"Expected a connected leafnode for server %q, got none\", s.ID())\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestLeafNodesStaggeredSubPub(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tsl1, sl1Opts := runSolicitLeafServer(opts)\r\n\tdefer sl1.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\t// Create a client on the leafnode and a subscription.\r\n\tncl, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", sl1Opts.Host, sl1Opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl.Close()\r\n\r\n\tsub, err := ncl.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Failed to subscribe: %v\", err)\r\n\t}\r\n\r\n\tsl2, sl2Opts := runSolicitLeafServer(opts)\r\n\tdefer sl2.Shutdown()\r\n\r\n\tcheckLeafNode2Connected(t, s)\r\n\r\n\t// Create a client on the second leafnode and publish a message.\r\n\tncl2, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", sl2Opts.Host, sl2Opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncl2.Close()\r\n\r\n\tncl2.Publish(\"foo\", nil)\r\n\r\n\tcheckFor(t, 250*time.Millisecond, 10*time.Millisecond, func() error {\r\n\t\tif nmsgs, _, err := sub.Pending(); err != nil || nmsgs != 1 {\r\n\t\t\treturn fmt.Errorf(\"Did not receive the message: %v\", err)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestLeafNodeMultipleRemoteURLs(t *testing.T) {\r\n\ts, opts := runLeafServer()\r\n\tdefer s.Shutdown()\r\n\r\n\tcontent := `\r\n\t\tport: -1\r\n\t\tleafnodes {\r\n\t\t\tremotes = [\r\n\t\t\t\t{\r\n\t\t\t\t\turls: [nats-leaf://127.0.0.1:%d,nats-leaf://localhost:%d]\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t\t`\r\n\r\n\tconfig := fmt.Sprintf(content, opts.LeafNode.Port, opts.LeafNode.Port)\r\n\tconf := createConfFile(t, []byte(config))\r\n\tsl, _ := RunServerWithConfig(conf)\r\n\tdefer os.Remove(conf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n}\r\n\r\nfunc runSolicitLeafCluster(t *testing.T, clusterName string, d1, d2 *cluster) *cluster {\r\n\tc := &cluster{servers: make([]*server.Server, 0, 2), opts: make([]*server.Options, 0, 2), name: clusterName}\r\n\r\n\t// Who we will solicit for server 1\r\n\tci := rand.Intn(len(d1.opts))\r\n\topts := d1.opts[ci]\r\n\tsurl := fmt.Sprintf(\"nats-leaf://%s:%d\", opts.LeafNode.Host, opts.LeafNode.Port)\r\n\r\n\to := DefaultTestOptions\r\n\to.Port = -1\r\n\trurl, _ := url.Parse(surl)\r\n\to.LeafNode.Remotes = []*server.RemoteLeafOpts{{URLs: []*url.URL{rurl}}}\r\n\to.LeafNode.ReconnectInterval = 100 * time.Millisecond\r\n\to.Cluster.Host = o.Host\r\n\to.Cluster.Port = -1\r\n\ts := RunServer(&o)\r\n\tcheckLeafNodeConnected(t, d1.servers[ci])\r\n\r\n\tc.servers = append(c.servers, s)\r\n\tc.opts = append(c.opts, &o)\r\n\r\n\t// Grab route info\r\n\trouteAddr := fmt.Sprintf(\"nats-route://%s:%d\", o.Cluster.Host, o.Cluster.Port)\r\n\tcurl, _ := url.Parse(routeAddr)\r\n\r\n\t// Who we will solicit for server 2\r\n\tci = rand.Intn(len(d2.opts))\r\n\topts = d2.opts[ci]\r\n\tsurl = fmt.Sprintf(\"nats-leaf://%s:%d\", opts.LeafNode.Host, opts.LeafNode.Port)\r\n\r\n\t// This is for the case were d1 == d2 and we select the same server.\r\n\tplfn := d2.servers[ci].NumLeafNodes()\r\n\r\n\to2 := DefaultTestOptions\r\n\to2.Port = -1\r\n\trurl, _ = url.Parse(surl)\r\n\to2.LeafNode.Remotes = []*server.RemoteLeafOpts{{URLs: []*url.URL{rurl}}}\r\n\to2.LeafNode.ReconnectInterval = 100 * time.Millisecond\r\n\to2.Cluster.Host = o.Host\r\n\to2.Cluster.Port = -1\r\n\to2.Routes = []*url.URL{curl}\r\n\ts = RunServer(&o2)\r\n\r\n\tif plfn == 0 {\r\n\t\tcheckLeafNodeConnected(t, d2.servers[ci])\r\n\t} else {\r\n\t\tcheckLeafNode2Connected(t, d2.servers[ci])\r\n\t}\r\n\r\n\tc.servers = append(c.servers, s)\r\n\tc.opts = append(c.opts, &o2)\r\n\r\n\tcheckClusterFormed(t, c.servers...)\r\n\r\n\treturn c\r\n}\r\n\r\nfunc clientForCluster(t *testing.T, c *cluster) *nats.Conn {\r\n\tt.Helper()\r\n\topts := c.opts[rand.Intn(len(c.opts))]\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\treturn nc\r\n}\r\n\r\nfunc TestLeafNodeCycleWithSolicited(t *testing.T) {\r\n\tserver.SetGatewaysSolicitDelay(10 * time.Millisecond)\r\n\tdefer server.ResetGatewaysSolicitDelay()\r\n\r\n\t// Accepting leafnode cluster, e.g. NGS\r\n\tca := createClusterWithName(t, \"A\", 3)\r\n\tdefer shutdownCluster(ca)\r\n\tcb := createClusterWithName(t, \"B\", 3, ca)\r\n\tdefer shutdownCluster(cb)\r\n\r\n\t// Create the responders.\r\n\trequestsReceived := int32(0)\r\n\r\n\tnc := clientForCluster(t, ca)\r\n\tdefer nc.Close()\r\n\tnc.QueueSubscribe(\"request\", \"cycles\", func(m *nats.Msg) {\r\n\t\tatomic.AddInt32(&requestsReceived, 1)\r\n\t\tm.Respond([]byte(\"22\"))\r\n\t})\r\n\tnc.Flush()\r\n\r\n\tnc = clientForCluster(t, cb)\r\n\tdefer nc.Close()\r\n\tnc.QueueSubscribe(\"request\", \"cycles\", func(m *nats.Msg) {\r\n\t\tatomic.AddInt32(&requestsReceived, 1)\r\n\t\tm.Respond([]byte(\"33\"))\r\n\t})\r\n\tnc.Flush()\r\n\r\n\t// Soliciting cluster, both solicited connected to the \"A\" cluster\r\n\tsc := runSolicitLeafCluster(t, \"SC\", ca, ca)\r\n\tdefer shutdownCluster(sc)\r\n\r\n\t// Connect a client to a random server in sc\r\n\tcreateClientAndRequest := func(c *cluster) (*nats.Conn, *nats.Subscription) {\r\n\t\tnc := clientForCluster(t, c)\r\n\t\treply := nats.NewInbox()\r\n\t\tsub, err := nc.SubscribeSync(reply)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Could not subscribe: %v\", err)\r\n\t\t}\r\n\t\tif err := nc.PublishRequest(\"request\", reply, []byte(\"fingers crossed\")); err != nil {\r\n\t\t\tt.Fatalf(\"Error sending request: %v\", err)\r\n\t\t}\r\n\t\treturn nc, sub\r\n\t}\r\n\r\n\tverifyOneResponse := func(sub *nats.Subscription) {\r\n\t\ttime.Sleep(250 * time.Millisecond)\r\n\t\tm, _, err := sub.Pending()\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error calling Pending(): %v\", err)\r\n\t\t}\r\n\t\tif m > 1 {\r\n\t\t\tt.Fatalf(\"Received more then one response, cycle indicated: %d\", m)\r\n\t\t}\r\n\t}\r\n\r\n\tverifyRequestTotal := func(nre int32) {\r\n\t\tif nr := atomic.LoadInt32(&requestsReceived); nr != nre {\r\n\t\t\tt.Fatalf(\"Expected %d requests received, got %d\", nre, nr)\r\n\t\t}\r\n\t}\r\n\r\n\t// This should pass to here, but if we have a cycle things will be spinning and we will receive\r\n\t// too many responses when it should only be 1.\r\n\tnc, rsub := createClientAndRequest(sc)\r\n\tdefer nc.Close()\r\n\tverifyOneResponse(rsub)\r\n\tverifyRequestTotal(1)\r\n\r\n\t// Do a solicit across GW, so shut this one down.\r\n\tnc.Close()\r\n\tshutdownCluster(sc)\r\n\r\n\t// Soliciting cluster, connect to different clusters across a GW.\r\n\tsc = runSolicitLeafCluster(t, \"SC\", ca, cb)\r\n\tdefer shutdownCluster(sc)\r\n\r\n\tnc, rsub = createClientAndRequest(sc)\r\n\tdefer nc.Close()\r\n\tverifyOneResponse(rsub)\r\n\tverifyRequestTotal(2) // This is total since use same responders.\r\n}\r\n\r\nfunc TestLeafNodeNoRaceGeneratingNonce(t *testing.T) {\r\n\topts := testDefaultOptionsForLeafNodes()\r\n\topts.Cluster.Port = -1\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tquitCh := make(chan struct{})\r\n\twg := sync.WaitGroup{}\r\n\twg.Add(2)\r\n\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\tlc := createLeafConn(t, opts.LeafNode.Host, opts.LeafNode.Port)\r\n\t\t\tcheckInfoMsg(t, lc)\r\n\t\t\tlc.Close()\r\n\t\t\tselect {\r\n\t\t\tcase <-quitCh:\r\n\t\t\t\treturn\r\n\t\t\tcase <-time.After(time.Millisecond):\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\tgo func() {\r\n\t\tdefer wg.Done()\r\n\t\tfor {\r\n\t\t\trc := createRouteConn(t, opts.Cluster.Host, opts.Cluster.Port)\r\n\t\t\tcheckInfoMsg(t, rc)\r\n\t\t\trc.Close()\r\n\t\t\tselect {\r\n\t\t\tcase <-quitCh:\r\n\t\t\t\treturn\r\n\t\t\tcase <-time.After(time.Millisecond):\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\t// Let this run for a bit to see if we get data race\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\tclose(quitCh)\r\n\twg.Wait()\r\n}\r\n\r\nfunc runSolicitAndAcceptLeafServer(lso *server.Options) (*server.Server, *server.Options) {\r\n\tsurl := fmt.Sprintf(\"nats-leaf://%s:%d\", lso.LeafNode.Host, lso.LeafNode.Port)\r\n\to := testDefaultOptionsForLeafNodes()\r\n\to.Port = -1\r\n\trurl, _ := url.Parse(surl)\r\n\to.LeafNode.Remotes = []*server.RemoteLeafOpts{{URLs: []*url.URL{rurl}}}\r\n\to.LeafNode.ReconnectInterval = 100 * time.Millisecond\r\n\treturn RunServer(o), o\r\n}\r\n\r\nfunc TestLeafNodeDaisyChain(t *testing.T) {\r\n\t// To quickly enable trace and debug logging\r\n\t// doLog, doTrace, doDebug = true, true, true\r\n\ts1, opts1 := runLeafServer()\r\n\tdefer s1.Shutdown()\r\n\r\n\ts2, opts2 := runSolicitAndAcceptLeafServer(opts1)\r\n\tdefer s2.Shutdown()\r\n\tcheckLeafNodeConnected(t, s1)\r\n\r\n\ts3, _ := runSolicitLeafServer(opts2)\r\n\tdefer s3.Shutdown()\r\n\tcheckLeafNodeConnections(t, s2, 2)\r\n\r\n\t// Make so we can tell the two apart since in same PID.\r\n\tif doLog {\r\n\t\ts1.SetLogger(logger.NewTestLogger(\"[S-1] - \", false), true, true)\r\n\t\ts2.SetLogger(logger.NewTestLogger(\"[S-2] - \", false), true, true)\r\n\t\ts3.SetLogger(logger.NewTestLogger(\"[S-3] - \", false), true, true)\r\n\t}\r\n\r\n\tnc1, err := nats.Connect(s1.ClientURL())\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc1.Close()\r\n\r\n\tnc1.Subscribe(\"ngs.usage\", func(msg *nats.Msg) {\r\n\t\tmsg.Respond([]byte(\"22 msgs\"))\r\n\t})\r\n\tnc1.Flush()\r\n\r\n\tnc2, err := nats.Connect(s3.ClientURL())\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\tif _, err = nc2.Request(\"ngs.usage\", []byte(\"1h\"), time.Second); err != nil {\r\n\t\tt.Fatalf(\"Expected a response\")\r\n\t}\r\n}\r\n\r\n// This will test failover to a server with a cert with only an IP after successfully connecting\r\n// to a server with a cert with both.\r\nfunc TestClusterTLSMixedIPAndDNS(t *testing.T) {\r\n\tconfA := createConfFile(t, []byte(`\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tleafnodes {\r\n\t\t\tlisten: \"127.0.0.1:-1\"\r\n\t\t\ttls {\r\n\t\t\t\tcert_file: \"./configs/certs/server-iponly.pem\"\r\n\t\t\t\tkey_file:  \"./configs/certs/server-key-iponly.pem\"\r\n\t\t\t\tca_file:   \"./configs/certs/ca.pem\"\r\n\t\t\t\ttimeout: 2\r\n\t\t\t}\r\n\t\t}\r\n\t\tcluster {\r\n\t\t\tlisten: \"127.0.0.1:-1\"\r\n\t\t}\r\n\t`))\r\n\tsrvA, optsA := RunServerWithConfig(confA)\r\n\tdefer srvA.Shutdown()\r\n\r\n\tbConfigTemplate := `\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tleafnodes {\r\n\t\t\tlisten: \"localhost:-1\"\r\n\t\t\ttls {\r\n\t\t\t\tcert_file: \"./configs/certs/server-cert.pem\"\r\n\t\t\t\tkey_file:  \"./configs/certs/server-key.pem\"\r\n\t\t\t\tca_file:   \"./configs/certs/ca.pem\"\r\n\t\t\t\ttimeout: 2\r\n\t\t\t}\r\n\t\t}\r\n\t\tcluster {\r\n\t\t\tlisten: \"127.0.0.1:-1\"\r\n\t\t\troutes [\r\n\t\t\t\t\"nats://%s:%d\"\r\n\t\t\t]\r\n\t\t}\r\n\t`\r\n\tconfB := createConfFile(t, []byte(fmt.Sprintf(bConfigTemplate,\r\n\t\toptsA.Cluster.Host, optsA.Cluster.Port)))\r\n\tsrvB, optsB := RunServerWithConfig(confB)\r\n\tdefer srvB.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srvA, srvB)\r\n\r\n\t// Solicit a leafnode server here. Don't use the helper since we need verification etc.\r\n\to := DefaultTestOptions\r\n\to.Port = -1\r\n\trurl, _ := url.Parse(fmt.Sprintf(\"nats-leaf://%s:%d\", optsB.LeafNode.Host, optsB.LeafNode.Port))\r\n\to.LeafNode.ReconnectInterval = 10 * time.Millisecond\r\n\tremote := &server.RemoteLeafOpts{URLs: []*url.URL{rurl}}\r\n\tremote.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\r\n\tpool := x509.NewCertPool()\r\n\trootPEM, err := ioutil.ReadFile(\"./configs/certs/ca.pem\")\r\n\tif err != nil || rootPEM == nil {\r\n\t\tt.Fatalf(\"Error loading or parsing rootCA file: %v\", err)\r\n\t}\r\n\tok := pool.AppendCertsFromPEM(rootPEM)\r\n\tif !ok {\r\n\t\tt.Fatalf(\"Failed to parse root certificate from %q\", \"./configs/certs/ca.pem\")\r\n\t}\r\n\tremote.TLSConfig.RootCAs = pool\r\n\to.LeafNode.Remotes = []*server.RemoteLeafOpts{remote}\r\n\tsl, _ := RunServer(&o), &o\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, srvB)\r\n\r\n\t// Now kill off srvB and force client to connect to srvA.\r\n\tsrvB.Shutdown()\r\n\r\n\t// Make sure this works.\r\n\tcheckLeafNodeConnected(t, srvA)\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/test/leafnode_test.go b/server/gnatsd/test/leafnode_test.go
--- a/server/gnatsd/test/leafnode_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/test/leafnode_test.go	(date 1665399050174)
@@ -35,7 +35,7 @@
 	"github.com/kubemq-io/broker/pkg/nuid"
 	"github.com/kubemq-io/broker/server/gnatsd/logger"
 	"github.com/kubemq-io/broker/server/gnatsd/server"
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/server/accounts.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2018-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"math/rand\"\r\n\t\"net/http\"\r\n\t\"net/url\"\r\n\t\"reflect\"\r\n\t\"sort\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n)\r\n\r\n// For backwards compatibility with NATS < 2.0, users who are not explicitly defined into an\r\n// account will be grouped in the default global account.\r\nconst globalAccountName = \"$G\"\r\n\r\n// Account are subject namespace definitions. By default no messages are shared between accounts.\r\n// You can share via Exports and Imports of Streams and Services.\r\ntype Account struct {\r\n\tName         string\r\n\tNkey         string\r\n\tIssuer       string\r\n\tclaimJWT     string\r\n\tupdated      time.Time\r\n\tmu           sync.RWMutex\r\n\tsqmu         sync.Mutex\r\n\tsl           *Sublist\r\n\tetmr         *time.Timer\r\n\tctmr         *time.Timer\r\n\tstrack       map[string]sconns\r\n\tnrclients    int32\r\n\tsysclients   int32\r\n\tnleafs       int32\r\n\tnrleafs      int32\r\n\tclients      map[*client]struct{}\r\n\trm           map[string]int32\r\n\tlqws         map[string]int32\r\n\tusersRevoked map[string]int64\r\n\tactsRevoked  map[string]int64\r\n\trespMap      map[string][]*serviceRespEntry\r\n\tlleafs       []*client\r\n\timports      importMap\r\n\texports      exportMap\r\n\tlimits\r\n\tnae           int32\r\n\tpruning       bool\r\n\trmPruning     bool\r\n\texpired       bool\r\n\tincomplete    bool\r\n\tsigningKeys   []string\r\n\tsrv           *Server // server this account is registered with (possibly nil)\r\n\tlds           string  // loop detection subject for leaf nodes\r\n\tsiReply       []byte  // service reply prefix, will form wildcard subscription.\r\n\tsiReplyClient *client\r\n\tprand         *rand.Rand\r\n}\r\n\r\n// Account based limits.\r\ntype limits struct {\r\n\tmpay     int32\r\n\tmsubs    int32\r\n\tmconns   int32\r\n\tmleafs   int32\r\n\tmaxnae   int32\r\n\tmaxnrm   int32\r\n\tmaxaettl time.Duration\r\n}\r\n\r\n// Used to track remote clients and leafnodes per remote server.\r\ntype sconns struct {\r\n\tconns int32\r\n\tleafs int32\r\n}\r\n\r\n// Import stream mapping struct\r\ntype streamImport struct {\r\n\tacc     *Account\r\n\tfrom    string\r\n\tprefix  string\r\n\tclaim   *jwt.Import\r\n\tinvalid bool\r\n}\r\n\r\n// Import service mapping struct\r\ntype serviceImport struct {\r\n\tacc      *Account\r\n\tclaim    *jwt.Import\r\n\tfrom     string\r\n\tto       string\r\n\tts       int64\r\n\trt       ServiceRespType\r\n\tlatency  *serviceLatency\r\n\tm1       *ServiceLatency\r\n\tae       bool\r\n\tinternal bool\r\n\tinvalid  bool\r\n\ttracking bool\r\n}\r\n\r\n// This is used to record when we create a mapping for implicit service\r\n// imports. We use this to clean up entries that are not singletons when\r\n// we detect that interest is no longer present. The key to the map will\r\n// be the actual interest. We record the mapped subject and the serviceImport\r\ntype serviceRespEntry struct {\r\n\tacc  *Account\r\n\tmsub string\r\n}\r\n\r\n// ServiceRespType represents the types of service request response types.\r\ntype ServiceRespType uint8\r\n\r\n// Service response types. Defaults to a singleton.\r\nconst (\r\n\tSingleton ServiceRespType = iota\r\n\tStream\r\n\tChunked\r\n)\r\n\r\n// String helper.\r\nfunc (rt ServiceRespType) String() string {\r\n\tswitch rt {\r\n\tcase Singleton:\r\n\t\treturn \"Singleton\"\r\n\tcase Stream:\r\n\t\treturn \"Stream\"\r\n\tcase Chunked:\r\n\t\treturn \"Chunked\"\r\n\t}\r\n\treturn \"Unknown ServiceResType\"\r\n}\r\n\r\n// exportAuth holds configured approvals or boolean indicating an\r\n// auth token is required for import.\r\ntype exportAuth struct {\r\n\ttokenReq bool\r\n\tapproved map[string]*Account\r\n}\r\n\r\n// streamExport\r\ntype streamExport struct {\r\n\texportAuth\r\n}\r\n\r\n// serviceExport holds additional information for exported services.\r\ntype serviceExport struct {\r\n\texportAuth\r\n\trespType ServiceRespType\r\n\tlatency  *serviceLatency\r\n}\r\n\r\n// Used to track service latency.\r\ntype serviceLatency struct {\r\n\tsampling int8\r\n\tsubject  string\r\n}\r\n\r\n// exportMap tracks the exported streams and services.\r\ntype exportMap struct {\r\n\tstreams  map[string]*streamExport\r\n\tservices map[string]*serviceExport\r\n}\r\n\r\n// importMap tracks the imported streams and services.\r\ntype importMap struct {\r\n\tstreams  []*streamImport\r\n\tservices map[string]*serviceImport // TODO(dlc) sync.Map may be better.\r\n}\r\n\r\n// NewAccount creates a new unlimited account with the given name.\r\nfunc NewAccount(name string) *Account {\r\n\ta := &Account{\r\n\t\tName:   name,\r\n\t\tlimits: limits{-1, -1, -1, -1, 0, 0, 0},\r\n\t}\r\n\treturn a\r\n}\r\n\r\n// Used to create shallow copies of accounts for transfer\r\n// from opts to real accounts in server struct.\r\nfunc (a *Account) shallowCopy() *Account {\r\n\tna := NewAccount(a.Name)\r\n\tna.Nkey = a.Nkey\r\n\tna.Issuer = a.Issuer\r\n\r\n\tif a.imports.streams != nil {\r\n\t\tna.imports.streams = make([]*streamImport, 0, len(a.imports.streams))\r\n\t\tfor _, v := range a.imports.streams {\r\n\t\t\tsi := *v\r\n\t\t\tna.imports.streams = append(na.imports.streams, &si)\r\n\t\t}\r\n\t}\r\n\tif a.imports.services != nil {\r\n\t\tna.imports.services = make(map[string]*serviceImport)\r\n\t\tfor k, v := range a.imports.services {\r\n\t\t\tsi := *v\r\n\t\t\tna.imports.services[k] = &si\r\n\t\t}\r\n\t}\r\n\tif a.exports.streams != nil {\r\n\t\tna.exports.streams = make(map[string]*streamExport)\r\n\t\tfor k, v := range a.exports.streams {\r\n\t\t\tif v != nil {\r\n\t\t\t\tse := *v\r\n\t\t\t\tna.exports.streams[k] = &se\r\n\t\t\t} else {\r\n\t\t\t\tna.exports.streams[k] = nil\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tif a.exports.services != nil {\r\n\t\tna.exports.services = make(map[string]*serviceExport)\r\n\t\tfor k, v := range a.exports.services {\r\n\t\t\tif v != nil {\r\n\t\t\t\tse := *v\r\n\t\t\t\tna.exports.services[k] = &se\r\n\t\t\t} else {\r\n\t\t\t\tna.exports.services[k] = nil\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn na\r\n}\r\n\r\n// Called to track a remote server and connections and leafnodes it\r\n// has for this account.\r\nfunc (a *Account) updateRemoteServer(m *AccountNumConns) {\r\n\ta.mu.Lock()\r\n\tif a.strack == nil {\r\n\t\ta.strack = make(map[string]sconns)\r\n\t}\r\n\t// This does not depend on receiving all updates since each one is idempotent.\r\n\t// FIXME(dlc) - We should cleanup when these both go to zero.\r\n\tprev := a.strack[m.Server.ID]\r\n\ta.strack[m.Server.ID] = sconns{conns: int32(m.Conns), leafs: int32(m.LeafNodes)}\r\n\ta.nrclients += int32(m.Conns) - prev.conns\r\n\ta.nrleafs += int32(m.LeafNodes) - prev.leafs\r\n\ta.mu.Unlock()\r\n}\r\n\r\n// Removes tracking for a remote server that has shutdown.\r\nfunc (a *Account) removeRemoteServer(sid string) {\r\n\ta.mu.Lock()\r\n\tif a.strack != nil {\r\n\t\tprev := a.strack[sid]\r\n\t\tdelete(a.strack, sid)\r\n\t\ta.nrclients -= prev.conns\r\n\t\ta.nrleafs -= prev.leafs\r\n\t}\r\n\ta.mu.Unlock()\r\n}\r\n\r\n// When querying for subject interest this is the number of\r\n// expected responses. We need to actually check that the entry\r\n// has active connections.\r\nfunc (a *Account) expectedRemoteResponses() (expected int32) {\r\n\ta.mu.RLock()\r\n\tfor _, sc := range a.strack {\r\n\t\tif sc.conns > 0 || sc.leafs > 0 {\r\n\t\t\texpected++\r\n\t\t}\r\n\t}\r\n\ta.mu.RUnlock()\r\n\treturn\r\n}\r\n\r\n// Clears eventing and tracking for this account.\r\nfunc (a *Account) clearEventing() {\r\n\ta.mu.Lock()\r\n\ta.nrclients = 0\r\n\t// Now clear state\r\n\tclearTimer(&a.etmr)\r\n\tclearTimer(&a.ctmr)\r\n\ta.clients = nil\r\n\ta.strack = nil\r\n\ta.mu.Unlock()\r\n}\r\n\r\n// NumConnections returns active number of clients for this account for\r\n// all known servers.\r\nfunc (a *Account) NumConnections() int {\r\n\ta.mu.RLock()\r\n\tnc := len(a.clients) + int(a.nrclients)\r\n\ta.mu.RUnlock()\r\n\treturn nc\r\n}\r\n\r\n// NumRemoteConnections returns the number of client or leaf connections that\r\n// are not on this server.\r\nfunc (a *Account) NumRemoteConnections() int {\r\n\ta.mu.RLock()\r\n\tnc := int(a.nrclients + a.nrleafs)\r\n\ta.mu.RUnlock()\r\n\treturn nc\r\n}\r\n\r\n// NumLocalConnections returns active number of clients for this account\r\n// on this server.\r\nfunc (a *Account) NumLocalConnections() int {\r\n\ta.mu.RLock()\r\n\tnlc := a.numLocalConnections()\r\n\ta.mu.RUnlock()\r\n\treturn nlc\r\n}\r\n\r\n// Do not account for the system accounts.\r\nfunc (a *Account) numLocalConnections() int {\r\n\treturn len(a.clients) - int(a.sysclients) - int(a.nleafs)\r\n}\r\n\r\n// This is for extended local interest.\r\n// Lock should not be held.\r\nfunc (a *Account) numLocalAndLeafConnections() int {\r\n\ta.mu.RLock()\r\n\tnlc := len(a.clients) - int(a.sysclients)\r\n\ta.mu.RUnlock()\r\n\treturn nlc\r\n}\r\n\r\nfunc (a *Account) numLocalLeafNodes() int {\r\n\treturn int(a.nleafs)\r\n}\r\n\r\n// MaxTotalConnectionsReached returns if we have reached our limit for number of connections.\r\nfunc (a *Account) MaxTotalConnectionsReached() bool {\r\n\tvar mtc bool\r\n\ta.mu.RLock()\r\n\tif a.mconns != jwt.NoLimit {\r\n\t\tmtc = len(a.clients)-int(a.sysclients)+int(a.nrclients) >= int(a.mconns)\r\n\t}\r\n\ta.mu.RUnlock()\r\n\treturn mtc\r\n}\r\n\r\n// MaxActiveConnections return the set limit for the account system\r\n// wide for total number of active connections.\r\nfunc (a *Account) MaxActiveConnections() int {\r\n\ta.mu.RLock()\r\n\tmconns := int(a.mconns)\r\n\ta.mu.RUnlock()\r\n\treturn mconns\r\n}\r\n\r\n// MaxTotalLeafNodesReached returns if we have reached our limit for number of leafnodes.\r\nfunc (a *Account) MaxTotalLeafNodesReached() bool {\r\n\ta.mu.RLock()\r\n\tmtc := a.maxTotalLeafNodesReached()\r\n\ta.mu.RUnlock()\r\n\treturn mtc\r\n}\r\n\r\nfunc (a *Account) maxTotalLeafNodesReached() bool {\r\n\tif a.mleafs != jwt.NoLimit {\r\n\t\treturn a.nleafs+a.nrleafs >= a.mleafs\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// NumLeafNodes returns the active number of local and remote\r\n// leaf node connections.\r\nfunc (a *Account) NumLeafNodes() int {\r\n\ta.mu.RLock()\r\n\tnln := int(a.nleafs + a.nrleafs)\r\n\ta.mu.RUnlock()\r\n\treturn nln\r\n}\r\n\r\n// NumRemoteLeafNodes returns the active number of remote\r\n// leaf node connections.\r\nfunc (a *Account) NumRemoteLeafNodes() int {\r\n\ta.mu.RLock()\r\n\tnrn := int(a.nrleafs)\r\n\ta.mu.RUnlock()\r\n\treturn nrn\r\n}\r\n\r\n// MaxActiveLeafNodes return the set limit for the account system\r\n// wide for total number of leavenode connections.\r\n// NOTE: these are tracked separately.\r\nfunc (a *Account) MaxActiveLeafNodes() int {\r\n\ta.mu.RLock()\r\n\tmleafs := int(a.mleafs)\r\n\ta.mu.RUnlock()\r\n\treturn mleafs\r\n}\r\n\r\n// RoutedSubs returns how many subjects we would send across a route when first\r\n// connected or expressing interest. Local client subs.\r\nfunc (a *Account) RoutedSubs() int {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn len(a.rm)\r\n}\r\n\r\n// TotalSubs returns total number of Subscriptions for this account.\r\nfunc (a *Account) TotalSubs() int {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn int(a.sl.Count())\r\n}\r\n\r\n// SubscriptionInterest returns true if this account has a matching subscription\r\n// for the given `subject`. Works only for literal subjects.\r\n// TODO: Add support for wildcards\r\nfunc (a *Account) SubscriptionInterest(subject string) bool {\r\n\tvar interest bool\r\n\ta.mu.RLock()\r\n\tif a.sl != nil {\r\n\t\tif res := a.sl.Match(subject); len(res.psubs)+len(res.qsubs) > 0 {\r\n\t\t\tinterest = true\r\n\t\t}\r\n\t}\r\n\ta.mu.RUnlock()\r\n\treturn interest\r\n}\r\n\r\n// addClient keeps our accounting of local active clients or leafnodes updated.\r\n// Returns previous total.\r\nfunc (a *Account) addClient(c *client) int {\r\n\ta.mu.Lock()\r\n\tn := len(a.clients)\r\n\tif a.clients != nil {\r\n\t\ta.clients[c] = struct{}{}\r\n\t}\r\n\tadded := n != len(a.clients)\r\n\tif added {\r\n\t\tif c.kind == SYSTEM {\r\n\t\t\ta.sysclients++\r\n\t\t} else if c.kind == LEAF {\r\n\t\t\ta.nleafs++\r\n\t\t\ta.lleafs = append(a.lleafs, c)\r\n\t\t}\r\n\t}\r\n\ta.mu.Unlock()\r\n\r\n\tif c != nil && c.srv != nil && added {\r\n\t\tc.srv.accConnsUpdate(a)\r\n\t}\r\n\r\n\treturn n\r\n}\r\n\r\n// Helper function to remove leaf nodes. If number of leafnodes gets large\r\n// this may need to be optimized out of linear search but believe number\r\n// of active leafnodes per account scope to be small and therefore cache friendly.\r\n// Lock should be held on account.\r\nfunc (a *Account) removeLeafNode(c *client) {\r\n\tll := len(a.lleafs)\r\n\tfor i, l := range a.lleafs {\r\n\t\tif l == c {\r\n\t\t\ta.lleafs[i] = a.lleafs[ll-1]\r\n\t\t\tif ll == 1 {\r\n\t\t\t\ta.lleafs = nil\r\n\t\t\t} else {\r\n\t\t\t\ta.lleafs = a.lleafs[:ll-1]\r\n\t\t\t}\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// removeClient keeps our accounting of local active clients updated.\r\nfunc (a *Account) removeClient(c *client) int {\r\n\ta.mu.Lock()\r\n\tn := len(a.clients)\r\n\tdelete(a.clients, c)\r\n\tremoved := n != len(a.clients)\r\n\tif removed {\r\n\t\tif c.kind == SYSTEM {\r\n\t\t\ta.sysclients--\r\n\t\t} else if c.kind == LEAF {\r\n\t\t\ta.nleafs--\r\n\t\t\ta.removeLeafNode(c)\r\n\t\t}\r\n\t}\r\n\ta.mu.Unlock()\r\n\r\n\tif c != nil && c.srv != nil && removed {\r\n\t\tc.srv.mu.Lock()\r\n\t\tdoRemove := a != c.srv.gacc\r\n\t\tc.srv.mu.Unlock()\r\n\t\tif doRemove {\r\n\t\t\tc.srv.accConnsUpdate(a)\r\n\t\t}\r\n\t}\r\n\treturn n\r\n}\r\n\r\nfunc (a *Account) randomClient() *client {\r\n\tvar c *client\r\n\tif a.siReplyClient != nil {\r\n\t\treturn a.siReplyClient\r\n\t}\r\n\tfor c = range a.clients {\r\n\t\tbreak\r\n\t}\r\n\treturn c\r\n}\r\n\r\n// AddServiceExport will configure the account with the defined export.\r\nfunc (a *Account) AddServiceExport(subject string, accounts []*Account) error {\r\n\treturn a.AddServiceExportWithResponse(subject, Singleton, accounts)\r\n}\r\n\r\n// AddServiceExportWithresponse will configure the account with the defined export and response type.\r\nfunc (a *Account) AddServiceExportWithResponse(subject string, respType ServiceRespType, accounts []*Account) error {\r\n\tif a == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\ta.mu.Lock()\r\n\tdefer a.mu.Unlock()\r\n\tif a.exports.services == nil {\r\n\t\ta.exports.services = make(map[string]*serviceExport)\r\n\t}\r\n\r\n\tea := a.exports.services[subject]\r\n\r\n\tif respType != Singleton {\r\n\t\tif ea == nil {\r\n\t\t\tea = &serviceExport{}\r\n\t\t}\r\n\t\tea.respType = respType\r\n\t}\r\n\r\n\tif accounts != nil {\r\n\t\tif ea == nil {\r\n\t\t\tea = &serviceExport{}\r\n\t\t}\r\n\t\t// empty means auth required but will be import token.\r\n\t\tif len(accounts) == 0 {\r\n\t\t\tea.tokenReq = true\r\n\t\t} else {\r\n\t\t\tif ea.approved == nil {\r\n\t\t\t\tea.approved = make(map[string]*Account, len(accounts))\r\n\t\t\t}\r\n\t\t\tfor _, acc := range accounts {\r\n\t\t\t\tea.approved[acc.Name] = acc\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\ta.exports.services[subject] = ea\r\n\treturn nil\r\n}\r\n\r\n// TrackServiceExport will enable latency tracking of the named service.\r\n// Results will be published in this account to the given results subject.\r\nfunc (a *Account) TrackServiceExport(service, results string) error {\r\n\treturn a.TrackServiceExportWithSampling(service, results, DEFAULT_SERVICE_LATENCY_SAMPLING)\r\n}\r\n\r\n// TrackServiceExportWithSampling will enable latency tracking of the named service for the given\r\n// sampling rate (1-100). Results will be published in this account to the given results subject.\r\nfunc (a *Account) TrackServiceExportWithSampling(service, results string, sampling int) error {\r\n\tif a == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\r\n\tif sampling < 1 || sampling > 100 {\r\n\t\treturn ErrBadSampling\r\n\t}\r\n\tif !IsValidPublishSubject(results) {\r\n\t\treturn ErrBadPublishSubject\r\n\t}\r\n\t// Don't loop back on outselves.\r\n\tif a.IsExportService(results) {\r\n\t\treturn ErrBadPublishSubject\r\n\t}\r\n\r\n\tif a.srv != nil && !a.srv.EventsEnabled() {\r\n\t\treturn ErrNoSysAccount\r\n\t}\r\n\r\n\ta.mu.Lock()\r\n\tif a.exports.services == nil {\r\n\t\ta.mu.Unlock()\r\n\t\treturn ErrMissingService\r\n\t}\r\n\tea, ok := a.exports.services[service]\r\n\tif !ok {\r\n\t\ta.mu.Unlock()\r\n\t\treturn ErrMissingService\r\n\t}\r\n\tif ea == nil {\r\n\t\tea = &serviceExport{}\r\n\t\ta.exports.services[service] = ea\r\n\t} else if ea.respType != Singleton {\r\n\t\ta.mu.Unlock()\r\n\t\treturn ErrBadServiceType\r\n\t}\r\n\tea.latency = &serviceLatency{\r\n\t\tsampling: int8(sampling),\r\n\t\tsubject:  results,\r\n\t}\r\n\ts := a.srv\r\n\ta.mu.Unlock()\r\n\r\n\tif s == nil {\r\n\t\treturn nil\r\n\t}\r\n\r\n\t// Now track down the imports and add in latency as needed to enable.\r\n\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\tacc := v.(*Account)\r\n\t\tacc.mu.Lock()\r\n\t\tfor _, im := range acc.imports.services {\r\n\t\t\tif im != nil && im.acc.Name == a.Name && subjectIsSubsetMatch(im.to, service) {\r\n\t\t\t\tim.latency = ea.latency\r\n\t\t\t}\r\n\t\t}\r\n\t\tacc.mu.Unlock()\r\n\t\treturn true\r\n\t})\r\n\r\n\treturn nil\r\n}\r\n\r\n// UnTrackServiceExport will disable latency tracking of the named service.\r\nfunc (a *Account) UnTrackServiceExport(service string) {\r\n\tif a == nil || (a.srv != nil && !a.srv.EventsEnabled()) {\r\n\t\treturn\r\n\t}\r\n\r\n\ta.mu.Lock()\r\n\tif a == nil || a.exports.services == nil {\r\n\t\ta.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\tea, ok := a.exports.services[service]\r\n\tif !ok || ea == nil || ea.latency == nil {\r\n\t\ta.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\t// We have latency here.\r\n\tea.latency = nil\r\n\ts := a.srv\r\n\ta.mu.Unlock()\r\n\r\n\tif s == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Now track down the imports and clean them up.\r\n\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\tacc := v.(*Account)\r\n\t\tacc.mu.Lock()\r\n\t\tfor _, im := range acc.imports.services {\r\n\t\t\tif im != nil && im.acc.Name == a.Name {\r\n\t\t\t\tif subjectIsSubsetMatch(im.to, service) {\r\n\t\t\t\t\tim.latency, im.m1 = nil, nil\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tacc.mu.Unlock()\r\n\t\treturn true\r\n\t})\r\n}\r\n\r\n// IsExportService will indicate if this service exists. Will check wildcard scenarios.\r\nfunc (a *Account) IsExportService(service string) bool {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\t_, ok := a.exports.services[service]\r\n\tif ok {\r\n\t\treturn true\r\n\t}\r\n\ttokens := strings.Split(service, tsep)\r\n\tfor subj := range a.exports.services {\r\n\t\tif isSubsetMatch(tokens, subj) {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// IsExportServiceTracking will indicate if given publish subject is an export service with tracking enabled.\r\nfunc (a *Account) IsExportServiceTracking(service string) bool {\r\n\ta.mu.RLock()\r\n\tea, ok := a.exports.services[service]\r\n\tif ok && ea == nil {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn false\r\n\t}\r\n\tif ok && ea != nil && ea.latency != nil {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn true\r\n\t}\r\n\t// FIXME(dlc) - Might want to cache this is in the hot path checking for latency tracking.\r\n\ttokens := strings.Split(service, tsep)\r\n\tfor subj, ea := range a.exports.services {\r\n\t\tif isSubsetMatch(tokens, subj) && ea != nil && ea.latency != nil {\r\n\t\t\ta.mu.RUnlock()\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\ta.mu.RUnlock()\r\n\treturn false\r\n}\r\n\r\n// NATSLatency represents the internal NATS latencies, including RTTs to clients.\r\ntype NATSLatency struct {\r\n\tRequestor time.Duration `json:\"req\"`\r\n\tResponder time.Duration `json:\"resp\"`\r\n\tSystem    time.Duration `json:\"sys\"`\r\n}\r\n\r\n// TotalTime is a helper function that totals the NATS latencies.\r\nfunc (nl *NATSLatency) TotalTime() time.Duration {\r\n\treturn nl.Requestor + nl.Responder + nl.System\r\n}\r\n\r\n// ServiceLatency is the JSON message sent out in response to latency tracking for\r\n// exported services.\r\ntype ServiceLatency struct {\r\n\tAppName        string        `json:\"app,omitempty\"`\r\n\tRequestStart   time.Time     `json:\"start\"`\r\n\tServiceLatency time.Duration `json:\"svc\"`\r\n\tNATSLatency    NATSLatency   `json:\"nats\"`\r\n\tTotalLatency   time.Duration `json:\"total\"`\r\n}\r\n\r\n// Merge function to merge m1 and m2 (requestor and responder) measurements\r\n// when there are two samples. This happens when the requestor and responder\r\n// are on different servers.\r\n//\r\n// m2 ServiceLatency is correct, so use that.\r\n// m1 TotalLatency is correct, so use that.\r\n// Will use those to back into NATS latency.\r\nfunc (m1 *ServiceLatency) merge(m2 *ServiceLatency) {\r\n\tm1.AppName = m2.AppName\r\n\tm1.NATSLatency.System = m1.ServiceLatency - (m2.ServiceLatency + m2.NATSLatency.Responder)\r\n\tm1.ServiceLatency = m2.ServiceLatency\r\n\tm1.NATSLatency.Responder = m2.NATSLatency.Responder\r\n\tsanitizeLatencyMetric(m1)\r\n}\r\n\r\n// sanitizeLatencyMetric adjusts latency metric values that could go\r\n// negative in some edge conditions since we estimate client RTT\r\n// for both requestor and responder.\r\n// These numbers are never meant to be negative, it just could be\r\n// how we back into the values based on estimated RTT.\r\nfunc sanitizeLatencyMetric(sl *ServiceLatency) {\r\n\tif sl.ServiceLatency < 0 {\r\n\t\tsl.ServiceLatency = 0\r\n\t}\r\n\tif sl.NATSLatency.System < 0 {\r\n\t\tsl.NATSLatency.System = 0\r\n\t}\r\n}\r\n\r\n// Used for transporting remote latency measurements.\r\ntype remoteLatency struct {\r\n\tAccount string         `json:\"account\"`\r\n\tReqId   string         `json:\"req_id\"`\r\n\tM2      ServiceLatency `json:\"m2\"`\r\n}\r\n\r\n// sendTrackingMessage will send out the appropriate tracking information for the\r\n// service request/response latency. This is called when the requestor's server has\r\n// received the response.\r\n// TODO(dlc) - holding locks for RTTs may be too much long term. Should revisit.\r\nfunc (a *Account) sendTrackingLatency(si *serviceImport, requestor, responder *client) bool {\r\n\tts := time.Now()\r\n\tserviceRTT := time.Duration(ts.UnixNano() - si.ts)\r\n\r\n\tvar reqClientRTT = requestor.getRTTValue()\r\n\tvar respClientRTT time.Duration\r\n\tvar appName string\r\n\r\n\tif responder != nil && responder.kind == CLIENT {\r\n\t\trespClientRTT = responder.getRTTValue()\r\n\t\tappName = responder.GetName()\r\n\t}\r\n\r\n\t// We will estimate time when request left the requestor by time we received\r\n\t// and the client RTT for the requestor.\r\n\treqStart := time.Unix(0, si.ts-int64(reqClientRTT))\r\n\tsl := &ServiceLatency{\r\n\t\tAppName:        appName,\r\n\t\tRequestStart:   reqStart,\r\n\t\tServiceLatency: serviceRTT - respClientRTT,\r\n\t\tNATSLatency: NATSLatency{\r\n\t\t\tRequestor: reqClientRTT,\r\n\t\t\tResponder: respClientRTT,\r\n\t\t\tSystem:    0,\r\n\t\t},\r\n\t\tTotalLatency: reqClientRTT + serviceRTT,\r\n\t}\r\n\tif respClientRTT > 0 {\r\n\t\tsl.NATSLatency.System = time.Since(ts)\r\n\t\tsl.TotalLatency += sl.NATSLatency.System\r\n\t}\r\n\r\n\tsanitizeLatencyMetric(sl)\r\n\r\n\t// If we are expecting a remote measurement, store our sl here.\r\n\t// We need to account for the race between this and us receiving the\r\n\t// remote measurement.\r\n\t// FIXME(dlc) - We need to clean these up but this should happen\r\n\t// already with the auto-expire logic.\r\n\tif responder != nil && responder.kind != CLIENT {\r\n\t\tsi.acc.mu.Lock()\r\n\t\tif si.m1 != nil {\r\n\t\t\tm1, m2 := sl, si.m1\r\n\t\t\tm1.merge(m2)\r\n\t\t\tsi.acc.mu.Unlock()\r\n\t\t\ta.srv.sendInternalAccountMsg(a, si.latency.subject, m1)\r\n\t\t\treturn true\r\n\t\t}\r\n\t\tsi.m1 = sl\r\n\t\tsi.acc.mu.Unlock()\r\n\t\treturn false\r\n\t} else {\r\n\t\ta.srv.sendInternalAccountMsg(a, si.latency.subject, sl)\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// numServiceRoutes returns the number of service routes on this account.\r\nfunc (a *Account) numServiceRoutes() int {\r\n\ta.mu.RLock()\r\n\tnum := len(a.imports.services)\r\n\ta.mu.RUnlock()\r\n\treturn num\r\n}\r\n\r\n// AddServiceImportWithClaim will add in the service import via the jwt claim.\r\nfunc (a *Account) AddServiceImportWithClaim(destination *Account, from, to string, imClaim *jwt.Import) error {\r\n\tif destination == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\t// Empty means use from.\r\n\tif to == \"\" {\r\n\t\tto = from\r\n\t}\r\n\tif !IsValidLiteralSubject(from) || !IsValidLiteralSubject(to) {\r\n\t\treturn ErrInvalidSubject\r\n\t}\r\n\t// First check to see if the account has authorized us to route to the \"to\" subject.\r\n\tif !destination.checkServiceImportAuthorized(a, to, imClaim) {\r\n\t\treturn ErrServiceImportAuthorization\r\n\t}\r\n\r\n\t_, err := a.addServiceImport(destination, from, to, imClaim)\r\n\treturn err\r\n}\r\n\r\n// AddServiceImport will add a route to an account to send published messages / requests\r\n// to the destination account. From is the local subject to map, To is the\r\n// subject that will appear on the destination account. Destination will need\r\n// to have an import rule to allow access via addService.\r\nfunc (a *Account) AddServiceImport(destination *Account, from, to string) error {\r\n\treturn a.AddServiceImportWithClaim(destination, from, to, nil)\r\n}\r\n\r\n// NumServiceImports return number of service imports we have.\r\nfunc (a *Account) NumServiceImports() int {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn len(a.imports.services)\r\n}\r\n\r\n// removeServiceImport will remove the route by subject.\r\nfunc (a *Account) removeServiceImport(subject string) {\r\n\ta.mu.Lock()\r\n\tsi, ok := a.imports.services[subject]\r\n\tif ok && si != nil && si.ae {\r\n\t\ta.nae--\r\n\t}\r\n\tdelete(a.imports.services, subject)\r\n\ta.mu.Unlock()\r\n}\r\n\r\n// This tracks responses to service requests mappings. This is used for cleanup.\r\nfunc (a *Account) addRespMapEntry(acc *Account, reply, from string) {\r\n\ta.mu.Lock()\r\n\tif a.respMap == nil {\r\n\t\ta.respMap = make(map[string][]*serviceRespEntry)\r\n\t}\r\n\tsre := &serviceRespEntry{acc, from}\r\n\tsra := a.respMap[reply]\r\n\ta.respMap[reply] = append(sra, sre)\r\n\tif len(a.respMap) > int(a.maxnrm) && !a.rmPruning {\r\n\t\ta.rmPruning = true\r\n\t\tgo a.pruneNonAutoExpireResponseMaps()\r\n\t}\r\n\ta.mu.Unlock()\r\n}\r\n\r\n// This checks for any response map entries.\r\nfunc (a *Account) checkForRespEntry(reply string) {\r\n\ta.mu.RLock()\r\n\tif len(a.imports.services) == 0 || len(a.respMap) == 0 {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\tsra := a.respMap[reply]\r\n\tif sra == nil {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\t// If we are here we have an entry we should check. We will first check\r\n\t// if there is any interest for this subject for the entire account. If\r\n\t// there is we can not delete any entries yet.\r\n\trr := a.sl.Match(reply)\r\n\ta.mu.RUnlock()\r\n\r\n\t// No interest.\r\n\tif len(rr.psubs)+len(rr.qsubs) > 0 {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Delete all the entries here.\r\n\ta.mu.Lock()\r\n\tdelete(a.respMap, reply)\r\n\ta.mu.Unlock()\r\n\r\n\t// If we are here we no longer have interest and we have a respMap entry\r\n\t// that we should clean up.\r\n\tfor _, sre := range sra {\r\n\t\tsre.acc.removeServiceImport(sre.msub)\r\n\t}\r\n}\r\n\r\n// Return the number of AutoExpireResponseMaps for request/reply. These are mapped to the account that\r\n// has the service import.\r\nfunc (a *Account) numAutoExpireResponseMaps() int {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn int(a.nae)\r\n}\r\n\r\n// MaxAutoExpireResponseMaps return the maximum number of\r\n// auto expire response maps we will allow.\r\nfunc (a *Account) MaxAutoExpireResponseMaps() int {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn int(a.maxnae)\r\n}\r\n\r\n// SetMaxAutoExpireResponseMaps sets the max outstanding auto expire response maps.\r\nfunc (a *Account) SetMaxAutoExpireResponseMaps(max int) {\r\n\ta.mu.Lock()\r\n\tdefer a.mu.Unlock()\r\n\ta.maxnae = int32(max)\r\n}\r\n\r\n// AutoExpireTTL returns the ttl for response maps.\r\nfunc (a *Account) AutoExpireTTL() time.Duration {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn a.maxaettl\r\n}\r\n\r\n// SetAutoExpireTTL sets the ttl for response maps.\r\nfunc (a *Account) SetAutoExpireTTL(ttl time.Duration) {\r\n\ta.mu.Lock()\r\n\tdefer a.mu.Unlock()\r\n\ta.maxaettl = ttl\r\n}\r\n\r\n// Return a list of the current autoExpireResponseMaps.\r\nfunc (a *Account) autoExpireResponseMaps() []*serviceImport {\r\n\ta.mu.RLock()\r\n\tif len(a.imports.services) == 0 {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn nil\r\n\t}\r\n\taesis := make([]*serviceImport, 0, len(a.imports.services))\r\n\tfor _, si := range a.imports.services {\r\n\t\tif si.ae {\r\n\t\t\taesis = append(aesis, si)\r\n\t\t}\r\n\t}\r\n\tsort.Slice(aesis, func(i, j int) bool {\r\n\t\treturn aesis[i].ts < aesis[j].ts\r\n\t})\r\n\r\n\ta.mu.RUnlock()\r\n\treturn aesis\r\n}\r\n\r\n// MaxResponseMaps return the maximum number of\r\n// non auto-expire response maps we will allow.\r\nfunc (a *Account) MaxResponseMaps() int {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn int(a.maxnrm)\r\n}\r\n\r\n// SetMaxResponseMaps sets the max outstanding non auto-expire response maps.\r\nfunc (a *Account) SetMaxResponseMaps(max int) {\r\n\ta.mu.Lock()\r\n\tdefer a.mu.Unlock()\r\n\ta.maxnrm = int32(max)\r\n}\r\n\r\n// Add a route to connect from an implicit route created for a response to a request.\r\n// This does no checks and should be only called by the msg processing code. Use\r\n// AddServiceImport from above if responding to user input or config changes, etc.\r\nfunc (a *Account) addServiceImport(dest *Account, from, to string, claim *jwt.Import) (*serviceImport, error) {\r\n\trt := Singleton\r\n\tvar lat *serviceLatency\r\n\r\n\tdest.mu.Lock()\r\n\tif ea := dest.getServiceExport(to); ea != nil {\r\n\t\trt = ea.respType\r\n\t\tlat = ea.latency\r\n\t}\r\n\tdest.mu.Unlock()\r\n\r\n\ta.mu.Lock()\r\n\tif a.imports.services == nil {\r\n\t\ta.imports.services = make(map[string]*serviceImport)\r\n\t} else if dup := a.imports.services[from]; dup != nil {\r\n\t\ta.mu.Unlock()\r\n\t\treturn nil, fmt.Errorf(\"duplicate service import subject %q, previously used in import for account %q, subject %q\",\r\n\t\t\tfrom, dup.acc.Name, dup.to)\r\n\t}\r\n\tsi := &serviceImport{dest, claim, from, to, 0, rt, lat, nil, false, false, false, false}\r\n\ta.imports.services[from] = si\r\n\ta.mu.Unlock()\r\n\r\n\treturn si, nil\r\n}\r\n\r\n// Helper to detrmine when to sample.\r\nfunc shouldSample(l *serviceLatency) bool {\r\n\tif l == nil || l.sampling <= 0 {\r\n\t\treturn false\r\n\t}\r\n\tif l.sampling >= 100 {\r\n\t\treturn true\r\n\t}\r\n\treturn rand.Int31n(100) <= int32(l.sampling)\r\n}\r\n\r\n// Used to mimic client like replies.\r\nconst (\r\n\treplyPrefix    = \"_R_.\"\r\n\ttrackSuffix    = \".T\"\r\n\treplyPrefixLen = len(replyPrefix)\r\n\tbaseServerLen  = 10\r\n\treplyLen       = 6\r\n\tminReplyLen    = 15\r\n\tdigits         = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\r\n\tbase           = 62\r\n)\r\n\r\n// Will create a wildcard subscription to handle interest graph propagation for all\r\n// service replies.\r\n// Lock should not be held.\r\nfunc (a *Account) createRespWildcard() []byte {\r\n\ta.mu.Lock()\r\n\tif a.prand == nil {\r\n\t\ta.prand = rand.New(rand.NewSource(time.Now().UnixNano()))\r\n\t}\r\n\tvar b = [baseServerLen]byte{'_', 'R', '_', '.'}\r\n\trn := a.prand.Int63()\r\n\tfor i, l := replyPrefixLen, rn; i < len(b); i++ {\r\n\t\tb[i] = digits[l%base]\r\n\t\tl /= base\r\n\t}\r\n\ta.siReply = append(b[:], '.')\r\n\ts := a.srv\r\n\taName := a.Name\r\n\tpre := a.siReply\r\n\twcsub := append(a.siReply, '>')\r\n\ta.mu.Unlock()\r\n\r\n\t// Check to see if we need to propagate interest.\r\n\tif s != nil {\r\n\t\tnow := time.Now()\r\n\t\tc := &client{srv: a.srv, acc: a, kind: SYSTEM, opts: internalOpts, msubs: -1, mpay: -1, start: now, last: now}\r\n\t\tsub := &subscription{client: c, subject: wcsub}\r\n\t\ts.updateRouteSubscriptionMap(a, sub, 1)\r\n\t\tif s.gateway.enabled {\r\n\t\t\ts.gatewayUpdateSubInterest(aName, sub, 1)\r\n\t\t\ta.mu.Lock()\r\n\t\t\ta.siReplyClient = c\r\n\t\t\ta.mu.Unlock()\r\n\t\t}\r\n\t\t// Now check on leafnode updates.\r\n\t\ts.updateLeafNodes(a, sub, 1)\r\n\t}\r\n\r\n\treturn pre\r\n}\r\n\r\nfunc (a *Account) replyClient() *client {\r\n\ta.mu.RLock()\r\n\tc := a.siReplyClient\r\n\ta.mu.RUnlock()\r\n\treturn c\r\n}\r\n\r\n// Test whether this is a tracked reply.\r\nfunc isTrackedReply(reply []byte) bool {\r\n\tlreply := len(reply) - 1\r\n\treturn lreply > 3 && reply[lreply-1] == '.' && reply[lreply] == 'T'\r\n}\r\n\r\n// Generate a new service reply from the wildcard prefix.\r\n// FIXME(dlc) - probably do not have to use rand here. about 25ns per.\r\nfunc (a *Account) newServiceReply(tracking bool) []byte {\r\n\ta.mu.RLock()\r\n\treplyPre := a.siReply\r\n\ts := a.srv\r\n\ta.mu.RUnlock()\r\n\r\n\tif replyPre == nil {\r\n\t\treplyPre = a.createRespWildcard()\r\n\t}\r\n\r\n\tvar b [replyLen]byte\r\n\trn := a.prand.Int63()\r\n\tfor i, l := 0, rn; i < len(b); i++ {\r\n\t\tb[i] = digits[l%base]\r\n\t\tl /= base\r\n\t}\r\n\t// Make sure to copy.\r\n\treply := make([]byte, 0, len(replyPre)+len(b))\r\n\treply = append(reply, replyPre...)\r\n\treply = append(reply, b[:]...)\r\n\r\n\tif tracking && s.sys != nil {\r\n\t\t// Add in our tracking identifier. This allows the metrics to get back to only\r\n\t\t// this server without needless SUBS/UNSUBS.\r\n\t\treply = append(reply, '.')\r\n\t\treply = append(reply, s.sys.shash...)\r\n\t\treply = append(reply, '.', 'T')\r\n\t}\r\n\treturn reply\r\n}\r\n\r\n// This is for internal responses.\r\nfunc (a *Account) addRespServiceImport(dest *Account, from, to string, rt ServiceRespType, lat *serviceLatency) *serviceImport {\r\n\ta.mu.Lock()\r\n\tif a.imports.services == nil {\r\n\t\ta.imports.services = make(map[string]*serviceImport)\r\n\t}\r\n\t// dest is the requestor's account. a is the service responder with the export.\r\n\tae := rt == Singleton\r\n\tsi := &serviceImport{dest, nil, from, to, 0, rt, nil, nil, ae, true, false, false}\r\n\ta.imports.services[from] = si\r\n\tif ae {\r\n\t\ta.nae++\r\n\t\tsi.ts = time.Now().UnixNano()\r\n\t\tif lat != nil {\r\n\t\t\tsi.latency = lat\r\n\t\t\tsi.tracking = true\r\n\t\t}\r\n\t\tif a.nae > a.maxnae && !a.pruning {\r\n\t\t\ta.pruning = true\r\n\t\t\tgo a.pruneAutoExpireResponseMaps()\r\n\t\t}\r\n\t}\r\n\ta.mu.Unlock()\r\n\treturn si\r\n}\r\n\r\n// This will prune off the non auto-expire (non singleton) response maps.\r\nfunc (a *Account) pruneNonAutoExpireResponseMaps() {\r\n\tvar sres []*serviceRespEntry\r\n\ta.mu.Lock()\r\n\tfor subj, sra := range a.respMap {\r\n\t\trr := a.sl.Match(subj)\r\n\t\tif len(rr.psubs)+len(rr.qsubs) == 0 {\r\n\t\t\tdelete(a.respMap, subj)\r\n\t\t\tsres = append(sres, sra...)\r\n\t\t}\r\n\t}\r\n\ta.rmPruning = false\r\n\ta.mu.Unlock()\r\n\r\n\tfor _, sre := range sres {\r\n\t\tsre.acc.removeServiceImport(sre.msub)\r\n\t}\r\n}\r\n\r\n// This will prune the list to below the threshold and remove all ttl'd maps.\r\nfunc (a *Account) pruneAutoExpireResponseMaps() {\r\n\tdefer func() {\r\n\t\ta.mu.Lock()\r\n\t\ta.pruning = false\r\n\t\ta.mu.Unlock()\r\n\t}()\r\n\r\n\ta.mu.RLock()\r\n\tttl := int64(a.maxaettl)\r\n\ta.mu.RUnlock()\r\n\r\n\tfor {\r\n\t\tsis := a.autoExpireResponseMaps()\r\n\r\n\t\t// Check ttl items.\r\n\t\tnow := time.Now().UnixNano()\r\n\t\tfor i, si := range sis {\r\n\t\t\tif now-si.ts >= ttl {\r\n\t\t\t\ta.removeServiceImport(si.from)\r\n\t\t\t} else {\r\n\t\t\t\tsis = sis[i:]\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\ta.mu.RLock()\r\n\t\tnumOver := int(a.nae - a.maxnae)\r\n\t\ta.mu.RUnlock()\r\n\r\n\t\tif numOver <= 0 {\r\n\t\t\treturn\r\n\t\t} else if numOver >= len(sis) {\r\n\t\t\tnumOver = len(sis) - 1\r\n\t\t}\r\n\t\t// These are in sorted order, remove at least numOver\r\n\t\tfor _, si := range sis[:numOver] {\r\n\t\t\ta.removeServiceImport(si.from)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// AddStreamImportWithClaim will add in the stream import from a specific account with optional token.\r\nfunc (a *Account) AddStreamImportWithClaim(account *Account, from, prefix string, imClaim *jwt.Import) error {\r\n\tif account == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\r\n\t// First check to see if the account has authorized export of the subject.\r\n\tif !account.checkStreamImportAuthorized(a, from, imClaim) {\r\n\t\treturn ErrStreamImportAuthorization\r\n\t}\r\n\r\n\t// Check prefix if it exists and make sure its a literal.\r\n\t// Append token separator if not already present.\r\n\tif prefix != \"\" {\r\n\t\t// Make sure there are no wildcards here, this prefix needs to be a literal\r\n\t\t// since it will be prepended to a publish subject.\r\n\t\tif !subjectIsLiteral(prefix) {\r\n\t\t\treturn ErrStreamImportBadPrefix\r\n\t\t}\r\n\t\tif prefix[len(prefix)-1] != btsep {\r\n\t\t\tprefix = prefix + string(btsep)\r\n\t\t}\r\n\t}\r\n\ta.mu.Lock()\r\n\tif a.isStreamImportDuplicate(account, from) {\r\n\t\ta.mu.Unlock()\r\n\t\treturn ErrStreamImportDuplicate\r\n\t}\r\n\ta.imports.streams = append(a.imports.streams, &streamImport{account, from, prefix, imClaim, false})\r\n\ta.mu.Unlock()\r\n\treturn nil\r\n}\r\n\r\n// isStreamImportDuplicate checks for duplicate.\r\n// Lock should be held.\r\nfunc (a *Account) isStreamImportDuplicate(acc *Account, from string) bool {\r\n\tfor _, si := range a.imports.streams {\r\n\t\tif si.acc == acc && si.from == from {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// AddStreamImport will add in the stream import from a specific account.\r\nfunc (a *Account) AddStreamImport(account *Account, from, prefix string) error {\r\n\treturn a.AddStreamImportWithClaim(account, from, prefix, nil)\r\n}\r\n\r\n// IsPublicExport is a placeholder to denote a public export.\r\nvar IsPublicExport = []*Account(nil)\r\n\r\n// AddStreamExport will add an export to the account. If accounts is nil\r\n// it will signify a public export, meaning anyone can impoort.\r\nfunc (a *Account) AddStreamExport(subject string, accounts []*Account) error {\r\n\tif a == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\ta.mu.Lock()\r\n\tdefer a.mu.Unlock()\r\n\tif a.exports.streams == nil {\r\n\t\ta.exports.streams = make(map[string]*streamExport)\r\n\t}\r\n\tea := a.exports.streams[subject]\r\n\tif accounts != nil {\r\n\t\tif ea == nil {\r\n\t\t\tea = &streamExport{}\r\n\t\t}\r\n\t\t// empty means auth required but will be import token.\r\n\t\tif len(accounts) == 0 {\r\n\t\t\tea.tokenReq = true\r\n\t\t} else {\r\n\t\t\tif ea.approved == nil {\r\n\t\t\t\tea.approved = make(map[string]*Account, len(accounts))\r\n\t\t\t}\r\n\t\t\tfor _, acc := range accounts {\r\n\t\t\t\tea.approved[acc.Name] = acc\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\ta.exports.streams[subject] = ea\r\n\treturn nil\r\n}\r\n\r\n// Check if another account is authorized to import from us.\r\nfunc (a *Account) checkStreamImportAuthorized(account *Account, subject string, imClaim *jwt.Import) bool {\r\n\t// Find the subject in the exports list.\r\n\ta.mu.RLock()\r\n\tauth := a.checkStreamImportAuthorizedNoLock(account, subject, imClaim)\r\n\ta.mu.RUnlock()\r\n\treturn auth\r\n}\r\n\r\nfunc (a *Account) checkStreamImportAuthorizedNoLock(account *Account, subject string, imClaim *jwt.Import) bool {\r\n\tif a.exports.streams == nil || !IsValidSubject(subject) {\r\n\t\treturn false\r\n\t}\r\n\treturn a.checkStreamExportApproved(account, subject, imClaim)\r\n}\r\n\r\nfunc (a *Account) checkAuth(ea *exportAuth, account *Account, imClaim *jwt.Import) bool {\r\n\t// if ea is nil or ea.approved is nil, that denotes a public export\r\n\tif ea == nil || (ea.approved == nil && !ea.tokenReq) {\r\n\t\treturn true\r\n\t}\r\n\t// Check if token required\r\n\tif ea.tokenReq {\r\n\t\treturn a.checkActivation(account, imClaim, true)\r\n\t}\r\n\t// If we have a matching account we are authorized\r\n\t_, ok := ea.approved[account.Name]\r\n\treturn ok\r\n}\r\n\r\nfunc (a *Account) checkStreamExportApproved(account *Account, subject string, imClaim *jwt.Import) bool {\r\n\t// Check direct match of subject first\r\n\tea, ok := a.exports.streams[subject]\r\n\tif ok {\r\n\t\tif ea == nil {\r\n\t\t\treturn true\r\n\t\t}\r\n\t\treturn a.checkAuth(&ea.exportAuth, account, imClaim)\r\n\t}\r\n\t// ok if we are here we did not match directly so we need to test each one.\r\n\t// The import subject arg has to take precedence, meaning the export\r\n\t// has to be a true subset of the import claim. We already checked for\r\n\t// exact matches above.\r\n\ttokens := strings.Split(subject, tsep)\r\n\tfor subj, ea := range a.exports.streams {\r\n\t\tif isSubsetMatch(tokens, subj) {\r\n\t\t\tif ea == nil {\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t\treturn a.checkAuth(&ea.exportAuth, account, imClaim)\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\nfunc (a *Account) checkServiceExportApproved(account *Account, subject string, imClaim *jwt.Import) bool {\r\n\t// Check direct match of subject first\r\n\tea, ok := a.exports.services[subject]\r\n\tif ok {\r\n\t\t// if ea is nil or eq.approved is nil, that denotes a public export\r\n\t\tif ea == nil || (ea.approved == nil && !ea.tokenReq) {\r\n\t\t\treturn true\r\n\t\t}\r\n\t\t// Check if token required\r\n\t\tif ea.tokenReq {\r\n\t\t\treturn a.checkActivation(account, imClaim, true)\r\n\t\t}\r\n\t\t// If we have a matching account we are authorized\r\n\t\t_, ok := ea.approved[account.Name]\r\n\t\treturn ok\r\n\t}\r\n\t// ok if we are here we did not match directly so we need to test each one.\r\n\t// The import subject arg has to take precedence, meaning the export\r\n\t// has to be a true subset of the import claim. We already checked for\r\n\t// exact matches above.\r\n\ttokens := strings.Split(subject, tsep)\r\n\tfor subj, ea := range a.exports.services {\r\n\t\tif isSubsetMatch(tokens, subj) {\r\n\t\t\tif ea == nil || ea.approved == nil && !ea.tokenReq {\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t\t// Check if token required\r\n\t\t\tif ea.tokenReq {\r\n\t\t\t\treturn a.checkActivation(account, imClaim, true)\r\n\t\t\t}\r\n\t\t\t_, ok := ea.approved[account.Name]\r\n\t\t\treturn ok\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// Helper function to get a serviceExport.\r\n// Lock should be held on entry.\r\nfunc (a *Account) getServiceExport(subj string) *serviceExport {\r\n\tea, ok := a.exports.services[subj]\r\n\t// The export probably has a wildcard, so lookup that up.\r\n\tif !ok {\r\n\t\tea = a.getWildcardServiceExport(subj)\r\n\t}\r\n\treturn ea\r\n}\r\n\r\n// This helper is used when trying to match a serviceExport record that is\r\n// represented by a wildcard.\r\n// Lock should be held on entry.\r\nfunc (a *Account) getWildcardServiceExport(to string) *serviceExport {\r\n\ttokens := strings.Split(to, tsep)\r\n\tfor subj, ea := range a.exports.services {\r\n\t\tif isSubsetMatch(tokens, subj) {\r\n\t\t\treturn ea\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// Will fetch the activation token for an import.\r\nfunc fetchActivation(url string) string {\r\n\t// FIXME(dlc) - Make configurable.\r\n\tc := &http.Client{Timeout: 2 * time.Second}\r\n\tresp, err := c.Get(url)\r\n\tif err != nil || resp == nil {\r\n\t\treturn \"\"\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\treturn \"\"\r\n\t}\r\n\treturn string(body)\r\n}\r\n\r\n// These are import stream specific versions for when an activation expires.\r\nfunc (a *Account) streamActivationExpired(exportAcc *Account, subject string) {\r\n\ta.mu.RLock()\r\n\tif a.expired || a.imports.streams == nil {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\tvar si *streamImport\r\n\tfor _, si = range a.imports.streams {\r\n\t\tif si.acc == exportAcc && si.from == subject {\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\r\n\tif si == nil || si.invalid {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\ta.mu.RUnlock()\r\n\r\n\tif si.acc.checkActivation(a, si.claim, false) {\r\n\t\t// The token has been updated most likely and we are good to go.\r\n\t\treturn\r\n\t}\r\n\r\n\ta.mu.Lock()\r\n\tsi.invalid = true\r\n\tclients := make([]*client, 0, len(a.clients))\r\n\tfor c := range a.clients {\r\n\t\tclients = append(clients, c)\r\n\t}\r\n\tawcsti := map[string]struct{}{a.Name: {}}\r\n\ta.mu.Unlock()\r\n\tfor _, c := range clients {\r\n\t\tc.processSubsOnConfigReload(awcsti)\r\n\t}\r\n}\r\n\r\n// These are import service specific versions for when an activation expires.\r\nfunc (a *Account) serviceActivationExpired(subject string) {\r\n\ta.mu.RLock()\r\n\tif a.expired || a.imports.services == nil {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\tsi := a.imports.services[subject]\r\n\tif si == nil || si.invalid {\r\n\t\ta.mu.RUnlock()\r\n\t\treturn\r\n\t}\r\n\ta.mu.RUnlock()\r\n\r\n\tif si.acc.checkActivation(a, si.claim, false) {\r\n\t\t// The token has been updated most likely and we are good to go.\r\n\t\treturn\r\n\t}\r\n\r\n\ta.mu.Lock()\r\n\tsi.invalid = true\r\n\ta.mu.Unlock()\r\n}\r\n\r\n// Fires for expired activation tokens. We could track this with timers etc.\r\n// Instead we just re-analyze where we are and if we need to act.\r\nfunc (a *Account) activationExpired(exportAcc *Account, subject string, kind jwt.ExportType) {\r\n\tswitch kind {\r\n\tcase jwt.Stream:\r\n\t\ta.streamActivationExpired(exportAcc, subject)\r\n\tcase jwt.Service:\r\n\t\ta.serviceActivationExpired(subject)\r\n\t}\r\n}\r\n\r\nfunc isRevoked(revocations map[string]int64, subject string, issuedAt int64) bool {\r\n\tif revocations == nil {\r\n\t\treturn false\r\n\t}\r\n\tif t, ok := revocations[subject]; !ok || t < issuedAt {\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// checkActivation will check the activation token for validity.\r\nfunc (a *Account) checkActivation(importAcc *Account, claim *jwt.Import, expTimer bool) bool {\r\n\tif claim == nil || claim.Token == \"\" {\r\n\t\treturn false\r\n\t}\r\n\t// Create a quick clone so we can inline Token JWT.\r\n\tclone := *claim\r\n\r\n\t// We grab the token from a URL by hand here since we need expiration etc.\r\n\tif url, err := url.Parse(clone.Token); err == nil && url.Scheme != \"\" {\r\n\t\tclone.Token = fetchActivation(url.String())\r\n\t}\r\n\tvr := jwt.CreateValidationResults()\r\n\tclone.Validate(a.Name, vr)\r\n\tif vr.IsBlocking(true) {\r\n\t\treturn false\r\n\t}\r\n\tact, err := jwt.DecodeActivationClaims(clone.Token)\r\n\tif err != nil {\r\n\t\treturn false\r\n\t}\r\n\tif !a.isIssuerClaimTrusted(act) {\r\n\t\treturn false\r\n\t}\r\n\tvr = jwt.CreateValidationResults()\r\n\tact.Validate(vr)\r\n\tif vr.IsBlocking(true) {\r\n\t\treturn false\r\n\t}\r\n\tif act.Expires != 0 {\r\n\t\ttn := time.Now().Unix()\r\n\t\tif act.Expires <= tn {\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif expTimer {\r\n\t\t\texpiresAt := time.Duration(act.Expires - tn)\r\n\t\t\ttime.AfterFunc(expiresAt*time.Second, func() {\r\n\t\t\t\timportAcc.activationExpired(a, string(act.ImportSubject), claim.Type)\r\n\t\t\t})\r\n\t\t}\r\n\t}\r\n\t// Check for token revocation..\r\n\treturn !isRevoked(a.actsRevoked, act.Subject, act.IssuedAt)\r\n}\r\n\r\n// Returns true if the activation claim is trusted. That is the issuer matches\r\n// the account or is an entry in the signing keys.\r\nfunc (a *Account) isIssuerClaimTrusted(claims *jwt.ActivationClaims) bool {\r\n\t// if no issuer account, issuer is the account\r\n\tif claims.IssuerAccount == \"\" {\r\n\t\treturn true\r\n\t}\r\n\t// If the IssuerAccount is not us, then this is considered an error.\r\n\tif a.Name != claims.IssuerAccount {\r\n\t\tif a.srv != nil {\r\n\t\t\ta.srv.Errorf(\"Invalid issuer account %q in activation claim (subject: %q - type: %q) for account %q\",\r\n\t\t\t\tclaims.IssuerAccount, claims.Activation.ImportSubject, claims.Activation.ImportType, a.Name)\r\n\t\t}\r\n\t\treturn false\r\n\t}\r\n\treturn a.hasIssuerNoLock(claims.Issuer)\r\n}\r\n\r\n// Returns true if `a` and `b` stream imports are the same. Note that the\r\n// check is done with the account's name, not the pointer. This is used\r\n// during config reload where we are comparing current and new config\r\n// in which pointers are different.\r\n// No lock is acquired in this function, so it is assumed that the\r\n// import maps are not changed while this executes.\r\nfunc (a *Account) checkStreamImportsEqual(b *Account) bool {\r\n\tif len(a.imports.streams) != len(b.imports.streams) {\r\n\t\treturn false\r\n\t}\r\n\t// Load the b imports into a map index by what we are looking for.\r\n\tbm := make(map[string]*streamImport, len(b.imports.streams))\r\n\tfor _, bim := range b.imports.streams {\r\n\t\tbm[bim.acc.Name+bim.from+bim.prefix] = bim\r\n\t}\r\n\tfor _, aim := range a.imports.streams {\r\n\t\tif _, ok := bm[aim.acc.Name+aim.from+aim.prefix]; !ok {\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\treturn true\r\n}\r\n\r\nfunc (a *Account) checkStreamExportsEqual(b *Account) bool {\r\n\tif len(a.exports.streams) != len(b.exports.streams) {\r\n\t\treturn false\r\n\t}\r\n\tfor subj, aea := range a.exports.streams {\r\n\t\tbea, ok := b.exports.streams[subj]\r\n\t\tif !ok {\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif !reflect.DeepEqual(aea, bea) {\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\treturn true\r\n}\r\n\r\nfunc (a *Account) checkServiceExportsEqual(b *Account) bool {\r\n\tif len(a.exports.services) != len(b.exports.services) {\r\n\t\treturn false\r\n\t}\r\n\tfor subj, aea := range a.exports.services {\r\n\t\tbea, ok := b.exports.services[subj]\r\n\t\tif !ok {\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif !reflect.DeepEqual(aea, bea) {\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// Check if another account is authorized to route requests to this service.\r\nfunc (a *Account) checkServiceImportAuthorized(account *Account, subject string, imClaim *jwt.Import) bool {\r\n\ta.mu.RLock()\r\n\tauthorized := a.checkServiceImportAuthorizedNoLock(account, subject, imClaim)\r\n\ta.mu.RUnlock()\r\n\treturn authorized\r\n}\r\n\r\n// Check if another account is authorized to route requests to this service.\r\nfunc (a *Account) checkServiceImportAuthorizedNoLock(account *Account, subject string, imClaim *jwt.Import) bool {\r\n\t// Find the subject in the services list.\r\n\tif a.exports.services == nil || !IsValidLiteralSubject(subject) {\r\n\t\treturn false\r\n\t}\r\n\treturn a.checkServiceExportApproved(account, subject, imClaim)\r\n}\r\n\r\n// IsExpired returns expiration status.\r\nfunc (a *Account) IsExpired() bool {\r\n\ta.mu.RLock()\r\n\texp := a.expired\r\n\ta.mu.RUnlock()\r\n\treturn exp\r\n}\r\n\r\n// Called when an account has expired.\r\nfunc (a *Account) expiredTimeout() {\r\n\t// Mark expired first.\r\n\ta.mu.Lock()\r\n\ta.expired = true\r\n\ta.mu.Unlock()\r\n\r\n\t// Collect the clients and expire them.\r\n\tcs := make([]*client, 0, len(a.clients))\r\n\ta.mu.RLock()\r\n\tfor c := range a.clients {\r\n\t\tcs = append(cs, c)\r\n\t}\r\n\ta.mu.RUnlock()\r\n\r\n\tfor _, c := range cs {\r\n\t\tc.accountAuthExpired()\r\n\t}\r\n}\r\n\r\n// Sets the expiration timer for an account JWT that has it set.\r\nfunc (a *Account) setExpirationTimer(d time.Duration) {\r\n\ta.etmr = time.AfterFunc(d, a.expiredTimeout)\r\n}\r\n\r\n// Lock should be held\r\nfunc (a *Account) clearExpirationTimer() bool {\r\n\tif a.etmr == nil {\r\n\t\treturn true\r\n\t}\r\n\tstopped := a.etmr.Stop()\r\n\ta.etmr = nil\r\n\treturn stopped\r\n}\r\n\r\n// checkUserRevoked will check if a user has been revoked.\r\nfunc (a *Account) checkUserRevoked(nkey string, issuedAt int64) bool {\r\n\ta.mu.RLock()\r\n\tdefer a.mu.RUnlock()\r\n\treturn isRevoked(a.usersRevoked, nkey, issuedAt)\r\n}\r\n\r\n// Check expiration and set the proper state as needed.\r\nfunc (a *Account) checkExpiration(claims *jwt.ClaimsData) {\r\n\ta.mu.Lock()\r\n\tdefer a.mu.Unlock()\r\n\r\n\ta.clearExpirationTimer()\r\n\tif claims.Expires == 0 {\r\n\t\ta.expired = false\r\n\t\treturn\r\n\t}\r\n\ttn := time.Now().Unix()\r\n\tif claims.Expires <= tn {\r\n\t\ta.expired = true\r\n\t\treturn\r\n\t}\r\n\texpiresAt := time.Duration(claims.Expires - tn)\r\n\ta.setExpirationTimer(expiresAt * time.Second)\r\n\ta.expired = false\r\n}\r\n\r\n// hasIssuer returns true if the issuer matches the account\r\n// issuer or it is a signing key for the account.\r\nfunc (a *Account) hasIssuer(issuer string) bool {\r\n\ta.mu.RLock()\r\n\thi := a.hasIssuerNoLock(issuer)\r\n\ta.mu.RUnlock()\r\n\treturn hi\r\n}\r\n\r\n// hasIssuerNoLock is the unlocked version of hasIssuer\r\nfunc (a *Account) hasIssuerNoLock(issuer string) bool {\r\n\t// same issuer\r\n\tif a.Issuer == issuer {\r\n\t\treturn true\r\n\t}\r\n\tfor i := 0; i < len(a.signingKeys); i++ {\r\n\t\tif a.signingKeys[i] == issuer {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// Returns the loop detection subject used for leafnodes\r\nfunc (a *Account) getLDSubject() string {\r\n\ta.mu.RLock()\r\n\tlds := a.lds\r\n\ta.mu.RUnlock()\r\n\treturn lds\r\n}\r\n\r\n// Placeholder for signaling token auth required.\r\nvar tokenAuthReq = []*Account{}\r\n\r\nfunc authAccounts(tokenReq bool) []*Account {\r\n\tif tokenReq {\r\n\t\treturn tokenAuthReq\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// SetAccountResolver will assign the account resolver.\r\nfunc (s *Server) SetAccountResolver(ar AccountResolver) {\r\n\ts.mu.Lock()\r\n\ts.accResolver = ar\r\n\ts.mu.Unlock()\r\n}\r\n\r\n// AccountResolver returns the registered account resolver.\r\nfunc (s *Server) AccountResolver() AccountResolver {\r\n\ts.mu.Lock()\r\n\tar := s.accResolver\r\n\ts.mu.Unlock()\r\n\treturn ar\r\n}\r\n\r\n// UpdateAccountClaims will call updateAccountClaims.\r\nfunc (s *Server) UpdateAccountClaims(a *Account, ac *jwt.AccountClaims) {\r\n\ts.updateAccountClaims(a, ac)\r\n}\r\n\r\n// updateAccountClaims will update an existing account with new claims.\r\n// This will replace any exports or imports previously defined.\r\n// Lock MUST NOT be held upon entry.\r\nfunc (s *Server) updateAccountClaims(a *Account, ac *jwt.AccountClaims) {\r\n\ts.updateAccountClaimsWithRefresh(a, ac, true)\r\n}\r\n\r\n// updateAccountClaimsWithRefresh will update an existing account with new claims.\r\n// If refreshImportingAccounts is true it will also update incomplete dependent accounts\r\n// This will replace any exports or imports previously defined.\r\n// Lock MUST NOT be held upon entry.\r\nfunc (s *Server) updateAccountClaimsWithRefresh(a *Account, ac *jwt.AccountClaims, refreshImportingAccounts bool) {\r\n\tif a == nil {\r\n\t\treturn\r\n\t}\r\n\ts.Debugf(\"Updating account claims: %s\", a.Name)\r\n\ta.checkExpiration(ac.Claims())\r\n\r\n\ta.mu.Lock()\r\n\t// Clone to update, only select certain fields.\r\n\told := &Account{Name: a.Name, exports: a.exports, limits: a.limits, signingKeys: a.signingKeys}\r\n\r\n\t// Reset exports and imports here.\r\n\ta.exports = exportMap{}\r\n\r\n\t// Imports are checked unlocked in processInbound, so we can't change out the struct here. Need to process inline.\r\n\tif a.imports.streams != nil {\r\n\t\told.imports.streams = a.imports.streams\r\n\t\ta.imports.streams = nil\r\n\t}\r\n\tif a.imports.services != nil {\r\n\t\told.imports.services = make(map[string]*serviceImport, len(a.imports.services))\r\n\t}\r\n\tfor k, v := range a.imports.services {\r\n\t\told.imports.services[k] = v\r\n\t\tdelete(a.imports.services, k)\r\n\t}\r\n\t// Reset any notion of export revocations.\r\n\ta.actsRevoked = nil\r\n\r\n\t// update account signing keys\r\n\ta.signingKeys = nil\r\n\tsignersChanged := false\r\n\tif len(ac.SigningKeys) > 0 {\r\n\t\t// insure copy the new keys and sort\r\n\t\ta.signingKeys = append(a.signingKeys, ac.SigningKeys...)\r\n\t\tsort.Strings(a.signingKeys)\r\n\t}\r\n\tif len(a.signingKeys) != len(old.signingKeys) {\r\n\t\tsignersChanged = true\r\n\t} else {\r\n\t\tfor i := 0; i < len(old.signingKeys); i++ {\r\n\t\t\tif a.signingKeys[i] != old.signingKeys[i] {\r\n\t\t\t\tsignersChanged = true\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\ta.mu.Unlock()\r\n\r\n\tgatherClients := func() []*client {\r\n\t\ta.mu.RLock()\r\n\t\tclients := make([]*client, 0, len(a.clients))\r\n\t\tfor c := range a.clients {\r\n\t\t\tclients = append(clients, c)\r\n\t\t}\r\n\t\ta.mu.RUnlock()\r\n\t\treturn clients\r\n\t}\r\n\r\n\tfor _, e := range ac.Exports {\r\n\t\tswitch e.Type {\r\n\t\tcase jwt.Stream:\r\n\t\t\ts.Debugf(\"Adding stream export %q for %s\", e.Subject, a.Name)\r\n\t\t\tif err := a.AddStreamExport(string(e.Subject), authAccounts(e.TokenReq)); err != nil {\r\n\t\t\t\ts.Debugf(\"Error adding stream export to account [%s]: %v\", a.Name, err.Error())\r\n\t\t\t}\r\n\t\tcase jwt.Service:\r\n\t\t\ts.Debugf(\"Adding service export %q for %s\", e.Subject, a.Name)\r\n\t\t\trt := Singleton\r\n\t\t\tswitch e.ResponseType {\r\n\t\t\tcase jwt.ResponseTypeStream:\r\n\t\t\t\trt = Stream\r\n\t\t\tcase jwt.ResponseTypeChunked:\r\n\t\t\t\trt = Chunked\r\n\t\t\t}\r\n\t\t\tif err := a.AddServiceExportWithResponse(string(e.Subject), rt, authAccounts(e.TokenReq)); err != nil {\r\n\t\t\t\ts.Debugf(\"Error adding service export to account [%s]: %v\", a.Name, err)\r\n\t\t\t}\r\n\t\t\tif e.Latency != nil {\r\n\t\t\t\tif err := a.TrackServiceExportWithSampling(string(e.Subject), string(e.Latency.Results), e.Latency.Sampling); err != nil {\r\n\t\t\t\t\ts.Debugf(\"Error adding latency tracking for service export to account [%s]: %v\", a.Name, err)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\t// We will track these at the account level. Should not have any collisions.\r\n\t\tif e.Revocations != nil {\r\n\t\t\ta.mu.Lock()\r\n\t\t\tif a.actsRevoked == nil {\r\n\t\t\t\ta.actsRevoked = make(map[string]int64)\r\n\t\t\t}\r\n\t\t\tfor k, t := range e.Revocations {\r\n\t\t\t\ta.actsRevoked[k] = t\r\n\t\t\t}\r\n\t\t\ta.mu.Unlock()\r\n\t\t}\r\n\t}\r\n\tvar incompleteImports []*jwt.Import\r\n\tfor _, i := range ac.Imports {\r\n\t\tacc, err := s.lookupAccount(i.Account)\r\n\t\tif acc == nil || err != nil {\r\n\t\t\ts.Errorf(\"Can't locate account [%s] for import of [%v] %s (err=%v)\", i.Account, i.Subject, i.Type, err)\r\n\t\t\tincompleteImports = append(incompleteImports, i)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tswitch i.Type {\r\n\t\tcase jwt.Stream:\r\n\t\t\ts.Debugf(\"Adding stream import %s:%q for %s:%q\", acc.Name, i.Subject, a.Name, i.To)\r\n\t\t\tif err := a.AddStreamImportWithClaim(acc, string(i.Subject), string(i.To), i); err != nil {\r\n\t\t\t\ts.Debugf(\"Error adding stream import to account [%s]: %v\", a.Name, err.Error())\r\n\t\t\t\tincompleteImports = append(incompleteImports, i)\r\n\t\t\t}\r\n\t\tcase jwt.Service:\r\n\t\t\ts.Debugf(\"Adding service import %s:%q for %s:%q\", acc.Name, i.Subject, a.Name, i.To)\r\n\t\t\tif err := a.AddServiceImportWithClaim(acc, string(i.Subject), string(i.To), i); err != nil {\r\n\t\t\t\ts.Debugf(\"Error adding service import to account [%s]: %v\", a.Name, err.Error())\r\n\t\t\t\tincompleteImports = append(incompleteImports, i)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t// Now let's apply any needed changes from import/export changes.\r\n\tif !a.checkStreamImportsEqual(old) {\r\n\t\tawcsti := map[string]struct{}{a.Name: {}}\r\n\t\tfor _, c := range gatherClients() {\r\n\t\t\tc.processSubsOnConfigReload(awcsti)\r\n\t\t}\r\n\t}\r\n\t// Now check if stream exports have changed.\r\n\tif !a.checkStreamExportsEqual(old) || signersChanged {\r\n\t\tclients := map[*client]struct{}{}\r\n\t\t// We need to check all accounts that have an import claim from this account.\r\n\t\tawcsti := map[string]struct{}{}\r\n\t\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\t\tacc := v.(*Account)\r\n\t\t\t// Move to the next if this account is actually account \"a\".\r\n\t\t\tif acc.Name == a.Name {\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t\t// TODO: checkStreamImportAuthorized() stack should not be trying\r\n\t\t\t// to lock \"acc\". If we find that to be needed, we will need to\r\n\t\t\t// rework this to ensure we don't lock acc.\r\n\t\t\tacc.mu.Lock()\r\n\t\t\tfor _, im := range acc.imports.streams {\r\n\t\t\t\tif im != nil && im.acc.Name == a.Name {\r\n\t\t\t\t\t// Check for if we are still authorized for an import.\r\n\t\t\t\t\tim.invalid = !a.checkStreamImportAuthorized(acc, im.from, im.claim)\r\n\t\t\t\t\tawcsti[acc.Name] = struct{}{}\r\n\t\t\t\t\tfor c := range acc.clients {\r\n\t\t\t\t\t\tclients[c] = struct{}{}\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tacc.mu.Unlock()\r\n\t\t\treturn true\r\n\t\t})\r\n\t\t// Now walk clients.\r\n\t\tfor c := range clients {\r\n\t\t\tc.processSubsOnConfigReload(awcsti)\r\n\t\t}\r\n\t}\r\n\t// Now check if service exports have changed.\r\n\tif !a.checkServiceExportsEqual(old) || signersChanged {\r\n\t\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\t\tacc := v.(*Account)\r\n\t\t\t// Move to the next if this account is actually account \"a\".\r\n\t\t\tif acc.Name == a.Name {\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t\t// TODO: checkServiceImportAuthorized() stack should not be trying\r\n\t\t\t// to lock \"acc\". If we find that to be needed, we will need to\r\n\t\t\t// rework this to ensure we don't lock acc.\r\n\t\t\tacc.mu.Lock()\r\n\t\t\tfor _, im := range acc.imports.services {\r\n\t\t\t\tif im != nil && im.acc.Name == a.Name {\r\n\t\t\t\t\t// Check for if we are still authorized for an import.\r\n\t\t\t\t\tim.invalid = !a.checkServiceImportAuthorized(acc, im.to, im.claim)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tacc.mu.Unlock()\r\n\t\t\treturn true\r\n\t\t})\r\n\t}\r\n\r\n\t// Now do limits if they are present.\r\n\ta.mu.Lock()\r\n\ta.msubs = int32(ac.Limits.Subs)\r\n\ta.mpay = int32(ac.Limits.Payload)\r\n\ta.mconns = int32(ac.Limits.Conn)\r\n\ta.mleafs = int32(ac.Limits.LeafNodeConn)\r\n\t// Check for any revocations\r\n\tif len(ac.Revocations) > 0 {\r\n\t\t// We will always replace whatever we had with most current, so no\r\n\t\t// need to look at what we have.\r\n\t\ta.usersRevoked = make(map[string]int64, len(ac.Revocations))\r\n\t\tfor pk, t := range ac.Revocations {\r\n\t\t\ta.usersRevoked[pk] = t\r\n\t\t}\r\n\t} else {\r\n\t\ta.usersRevoked = nil\r\n\t}\r\n\ta.incomplete = len(incompleteImports) != 0\r\n\tfor _, i := range incompleteImports {\r\n\t\ts.incompleteAccExporterMap.Store(i.Account, struct{}{})\r\n\t}\r\n\ta.mu.Unlock()\r\n\r\n\tclients := gatherClients()\r\n\t// Sort if we are over the limit.\r\n\tif a.MaxTotalConnectionsReached() {\r\n\t\tsort.Slice(clients, func(i, j int) bool {\r\n\t\t\treturn clients[i].start.After(clients[j].start)\r\n\t\t})\r\n\t}\r\n\r\n\tfor i, c := range clients {\r\n\t\ta.mu.RLock()\r\n\t\texceeded := a.mconns != jwt.NoLimit && i >= int(a.mconns)\r\n\t\ta.mu.RUnlock()\r\n\t\tif exceeded {\r\n\t\t\tc.maxAccountConnExceeded()\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tc.mu.Lock()\r\n\t\tc.applyAccountLimits()\r\n\t\ttheJWT := c.opts.JWT\r\n\t\tc.mu.Unlock()\r\n\r\n\t\t// Check for being revoked here. We use ac one to avoid the account lock.\r\n\t\tif ac.Revocations != nil && theJWT != \"\" {\r\n\t\t\tif juc, err := jwt.DecodeUserClaims(theJWT); err != nil {\r\n\t\t\t\tc.Debugf(\"User JWT not valid: %v\", err)\r\n\t\t\t\tc.authViolation()\r\n\t\t\t\tcontinue\r\n\t\t\t} else if ok := ac.IsClaimRevoked(juc); ok {\r\n\t\t\t\tc.sendErrAndDebug(\"User Authentication Revoked\")\r\n\t\t\t\tc.closeConnection(Revocation)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Check if the signing keys changed, might have to evict\r\n\tif signersChanged {\r\n\t\tfor _, c := range clients {\r\n\t\t\tc.mu.Lock()\r\n\t\t\tsk := c.user.SigningKey\r\n\t\t\tc.mu.Unlock()\r\n\t\t\tif sk != \"\" && !a.hasIssuer(sk) {\r\n\t\t\t\tc.closeConnection(AuthenticationViolation)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tif _, ok := s.incompleteAccExporterMap.Load(old.Name); ok && refreshImportingAccounts {\r\n\t\ts.incompleteAccExporterMap.Delete(old.Name)\r\n\t\ts.accounts.Range(func(key, value interface{}) bool {\r\n\t\t\tacc := value.(*Account)\r\n\t\t\tacc.mu.RLock()\r\n\t\t\tincomplete := acc.incomplete\r\n\t\t\tname := acc.Name\r\n\t\t\t// Must use jwt in account or risk failing on fetch\r\n\t\t\t// This jwt may not be the same that caused exportingAcc to be in incompleteAccExporterMap\r\n\t\t\tclaimJWT := acc.claimJWT\r\n\t\t\tacc.mu.RUnlock()\r\n\t\t\tif incomplete && name != old.Name {\r\n\t\t\t\tif accClaims, _, err := s.verifyAccountClaims(claimJWT); err == nil {\r\n\t\t\t\t\t// Since claimJWT has not changed, acc can become complete\r\n\t\t\t\t\t// but it won't alter incomplete for it's dependents accounts.\r\n\t\t\t\t\ts.updateAccountClaimsWithRefresh(acc, accClaims, false)\r\n\t\t\t\t\t// old.Name was deleted before ranging over accounts\r\n\t\t\t\t\t// If it exists again, UpdateAccountClaims set it for failed imports of acc.\r\n\t\t\t\t\t// So there was one import of acc that imported this account and failed again.\r\n\t\t\t\t\t// Since this account just got updated, the import itself may be in error. So trace that.\r\n\t\t\t\t\tif _, ok := s.incompleteAccExporterMap.Load(old.Name); ok {\r\n\t\t\t\t\t\ts.incompleteAccExporterMap.Delete(old.Name)\r\n\t\t\t\t\t\ts.Errorf(\"Account %s has issues importing account %s\", name, old.Name)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\treturn true\r\n\t\t})\r\n\t}\r\n}\r\n\r\n// Helper to build an internal account structure from a jwt.AccountClaims.\r\n// Lock MUST NOT be held upon entry.\r\nfunc (s *Server) buildInternalAccount(ac *jwt.AccountClaims) *Account {\r\n\tacc := NewAccount(ac.Subject)\r\n\tacc.Issuer = ac.Issuer\r\n\t// Set this here since we are placing in s.tmpAccounts below and may be\r\n\t// referenced by an route RS+, etc.\r\n\ts.setAccountSublist(acc)\r\n\r\n\t// We don't want to register an account that is in the process of\r\n\t// being built, however, to solve circular import dependencies, we\r\n\t// need to store it here.\r\n\ts.tmpAccounts.Store(ac.Subject, acc)\r\n\ts.updateAccountClaims(acc, ac)\r\n\treturn acc\r\n}\r\n\r\n// Helper to build internal NKeyUser.\r\nfunc buildInternalNkeyUser(uc *jwt.UserClaims, acc *Account) *NkeyUser {\r\n\tnu := &NkeyUser{Nkey: uc.Subject, Account: acc}\r\n\tif uc.IssuerAccount != \"\" {\r\n\t\tnu.SigningKey = uc.Issuer\r\n\t}\r\n\r\n\t// Now check for permissions.\r\n\tvar p *Permissions\r\n\r\n\tif len(uc.Pub.Allow) > 0 || len(uc.Pub.Deny) > 0 {\r\n\t\tif p == nil {\r\n\t\t\tp = &Permissions{}\r\n\t\t}\r\n\t\tp.Publish = &SubjectPermission{}\r\n\t\tp.Publish.Allow = uc.Pub.Allow\r\n\t\tp.Publish.Deny = uc.Pub.Deny\r\n\t}\r\n\tif len(uc.Sub.Allow) > 0 || len(uc.Sub.Deny) > 0 {\r\n\t\tif p == nil {\r\n\t\t\tp = &Permissions{}\r\n\t\t}\r\n\t\tp.Subscribe = &SubjectPermission{}\r\n\t\tp.Subscribe.Allow = uc.Sub.Allow\r\n\t\tp.Subscribe.Deny = uc.Sub.Deny\r\n\t}\r\n\tif uc.Resp != nil {\r\n\t\tif p == nil {\r\n\t\t\tp = &Permissions{}\r\n\t\t}\r\n\t\tp.Response = &ResponsePermission{\r\n\t\t\tMaxMsgs: uc.Resp.MaxMsgs,\r\n\t\t\tExpires: uc.Resp.Expires,\r\n\t\t}\r\n\t\tvalidateResponsePermissions(p)\r\n\t}\r\n\tnu.Permissions = p\r\n\treturn nu\r\n}\r\n\r\n// AccountResolver interface. This is to fetch Account JWTs by public nkeys\r\ntype AccountResolver interface {\r\n\tFetch(name string) (string, error)\r\n\tStore(name, jwt string) error\r\n}\r\n\r\n// MemAccResolver is a memory only resolver.\r\n// Mostly for testing.\r\ntype MemAccResolver struct {\r\n\tsm sync.Map\r\n}\r\n\r\n// Fetch will fetch the account jwt claims from the internal sync.Map.\r\nfunc (m *MemAccResolver) Fetch(name string) (string, error) {\r\n\tif j, ok := m.sm.Load(name); ok {\r\n\t\treturn j.(string), nil\r\n\t}\r\n\treturn _EMPTY_, ErrMissingAccount\r\n}\r\n\r\n// Store will store the account jwt claims in the internal sync.Map.\r\nfunc (m *MemAccResolver) Store(name, jwt string) error {\r\n\tm.sm.Store(name, jwt)\r\n\treturn nil\r\n}\r\n\r\n// URLAccResolver implements an http fetcher.\r\ntype URLAccResolver struct {\r\n\turl string\r\n\tc   *http.Client\r\n}\r\n\r\n// NewURLAccResolver returns a new resolver for the given base URL.\r\nfunc NewURLAccResolver(url string) (*URLAccResolver, error) {\r\n\tif !strings.HasSuffix(url, \"/\") {\r\n\t\turl += \"/\"\r\n\t}\r\n\r\n\t// FIXME(dlc) - Make timeout and others configurable.\r\n\t// We create our own transport to amortize TLS.\r\n\ttr := &http.Transport{\r\n\t\tMaxIdleConns:    10,\r\n\t\tIdleConnTimeout: 30 * time.Second,\r\n\t}\r\n\tur := &URLAccResolver{\r\n\t\turl: url,\r\n\t\tc:   &http.Client{Timeout: 2 * time.Second, Transport: tr},\r\n\t}\r\n\treturn ur, nil\r\n}\r\n\r\n// Fetch will fetch the account jwt claims from the base url, appending the\r\n// account name onto the end.\r\nfunc (ur *URLAccResolver) Fetch(name string) (string, error) {\r\n\turl := ur.url + name\r\n\tresp, err := ur.c.Get(url)\r\n\tif err != nil {\r\n\t\treturn _EMPTY_, fmt.Errorf(\"could not fetch <%q>: %v\", url, err)\r\n\t} else if resp == nil {\r\n\t\treturn _EMPTY_, fmt.Errorf(\"could not fetch <%q>: no response\", url)\r\n\t} else if resp.StatusCode != http.StatusOK {\r\n\t\treturn _EMPTY_, fmt.Errorf(\"could not fetch <%q>: %v\", url, resp.Status)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\treturn _EMPTY_, err\r\n\t}\r\n\treturn string(body), nil\r\n}\r\n\r\n// Store is not implemented for URL Resolver.\r\nfunc (ur *URLAccResolver) Store(name, jwt string) error {\r\n\treturn fmt.Errorf(\"Store operation not supported for URL Resolver\")\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/accounts.go b/server/gnatsd/server/accounts.go
--- a/server/gnatsd/server/accounts.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/accounts.go	(date 1665399749257)
@@ -25,7 +25,7 @@
 	"sync"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 )
 
 // For backwards compatibility with NATS < 2.0, users who are not explicitly defined into an
@@ -1846,7 +1846,11 @@
 	signersChanged := false
 	if len(ac.SigningKeys) > 0 {
 		// insure copy the new keys and sort
-		a.signingKeys = append(a.signingKeys, ac.SigningKeys...)
+		// Added by Lior to accomodate for the new signing keys
+		for key, _ := range ac.SigningKeys {
+			a.signingKeys = append(a.signingKeys, key)
+		}
+
 		sort.Strings(a.signingKeys)
 	}
 	if len(a.signingKeys) != len(old.signingKeys) {
@@ -1891,7 +1895,8 @@
 				s.Debugf("Error adding service export to account [%s]: %v", a.Name, err)
 			}
 			if e.Latency != nil {
-				if err := a.TrackServiceExportWithSampling(string(e.Subject), string(e.Latency.Results), e.Latency.Sampling); err != nil {
+				// fix by Lior to accomodate for the new latency
+				if err := a.TrackServiceExportWithSampling(string(e.Subject), string(e.Latency.Results), int(e.Latency.Sampling)); err != nil {
 					s.Debugf("Error adding latency tracking for service export to account [%s]: %v", a.Name, err)
 				}
 			}
Index: server/gnatsd/server/auth.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2012-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"crypto/tls\"\r\n\t\"crypto/x509/pkix\"\r\n\t\"encoding/asn1\"\r\n\t\"encoding/base64\"\r\n\t\"fmt\"\r\n\t\"net\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/internal/ldap\"\r\n\t\"github.com/nats-io/nkeys\"\r\n\t\"golang.org/x/crypto/bcrypt\"\r\n)\r\n\r\n// Authentication is an interface for implementing authentication\r\ntype Authentication interface {\r\n\t// Check if a client is authorized to connect\r\n\tCheck(c ClientAuthentication) bool\r\n}\r\n\r\n// ClientAuthentication is an interface for client authentication\r\ntype ClientAuthentication interface {\r\n\t// Get options associated with a client\r\n\tGetOpts() *clientOpts\r\n\t// If TLS is enabled, TLS ConnectionState, nil otherwise\r\n\tGetTLSConnectionState() *tls.ConnectionState\r\n\t// Optionally map a user after auth.\r\n\tRegisterUser(*User)\r\n\t// RemoteAddress expose the connection information of the client\r\n\tRemoteAddress() net.Addr\r\n}\r\n\r\n// NkeyUser is for multiple nkey based users\r\ntype NkeyUser struct {\r\n\tNkey        string       `json:\"user\"`\r\n\tPermissions *Permissions `json:\"permissions,omitempty\"`\r\n\tAccount     *Account     `json:\"account,omitempty\"`\r\n\tSigningKey  string       `json:\"signing_key,omitempty\"`\r\n}\r\n\r\n// User is for multiple accounts/users.\r\ntype User struct {\r\n\tUsername    string       `json:\"user\"`\r\n\tPassword    string       `json:\"password\"`\r\n\tPermissions *Permissions `json:\"permissions,omitempty\"`\r\n\tAccount     *Account     `json:\"account,omitempty\"`\r\n}\r\n\r\n// clone performs a deep copy of the User struct, returning a new clone with\r\n// all values copied.\r\nfunc (u *User) clone() *User {\r\n\tif u == nil {\r\n\t\treturn nil\r\n\t}\r\n\tclone := &User{}\r\n\t*clone = *u\r\n\tclone.Permissions = u.Permissions.clone()\r\n\treturn clone\r\n}\r\n\r\n// clone performs a deep copy of the NkeyUser struct, returning a new clone with\r\n// all values copied.\r\nfunc (n *NkeyUser) clone() *NkeyUser {\r\n\tif n == nil {\r\n\t\treturn nil\r\n\t}\r\n\tclone := &NkeyUser{}\r\n\t*clone = *n\r\n\tclone.Permissions = n.Permissions.clone()\r\n\treturn clone\r\n}\r\n\r\n// SubjectPermission is an individual allow and deny struct for publish\r\n// and subscribe authorizations.\r\ntype SubjectPermission struct {\r\n\tAllow []string `json:\"allow,omitempty\"`\r\n\tDeny  []string `json:\"deny,omitempty\"`\r\n}\r\n\r\n// ResponsePermission can be used to allow responses to any reply subject\r\n// that is received on a valid subscription.\r\ntype ResponsePermission struct {\r\n\tMaxMsgs int           `json:\"max\"`\r\n\tExpires time.Duration `json:\"ttl\"`\r\n}\r\n\r\n// Permissions are the allowed subjects on a per\r\n// publish or subscribe basis.\r\ntype Permissions struct {\r\n\tPublish   *SubjectPermission  `json:\"publish\"`\r\n\tSubscribe *SubjectPermission  `json:\"subscribe\"`\r\n\tResponse  *ResponsePermission `json:\"responses,omitempty\"`\r\n}\r\n\r\n// RoutePermissions are similar to user permissions\r\n// but describe what a server can import/export from and to\r\n// another server.\r\ntype RoutePermissions struct {\r\n\tImport *SubjectPermission `json:\"import\"`\r\n\tExport *SubjectPermission `json:\"export\"`\r\n}\r\n\r\n// clone will clone an individual subject permission.\r\nfunc (p *SubjectPermission) clone() *SubjectPermission {\r\n\tif p == nil {\r\n\t\treturn nil\r\n\t}\r\n\tclone := &SubjectPermission{}\r\n\tif p.Allow != nil {\r\n\t\tclone.Allow = make([]string, len(p.Allow))\r\n\t\tcopy(clone.Allow, p.Allow)\r\n\t}\r\n\tif p.Deny != nil {\r\n\t\tclone.Deny = make([]string, len(p.Deny))\r\n\t\tcopy(clone.Deny, p.Deny)\r\n\t}\r\n\treturn clone\r\n}\r\n\r\n// clone performs a deep copy of the Permissions struct, returning a new clone\r\n// with all values copied.\r\nfunc (p *Permissions) clone() *Permissions {\r\n\tif p == nil {\r\n\t\treturn nil\r\n\t}\r\n\tclone := &Permissions{}\r\n\tif p.Publish != nil {\r\n\t\tclone.Publish = p.Publish.clone()\r\n\t}\r\n\tif p.Subscribe != nil {\r\n\t\tclone.Subscribe = p.Subscribe.clone()\r\n\t}\r\n\tif p.Response != nil {\r\n\t\tclone.Response = &ResponsePermission{\r\n\t\t\tMaxMsgs: p.Response.MaxMsgs,\r\n\t\t\tExpires: p.Response.Expires,\r\n\t\t}\r\n\t}\r\n\treturn clone\r\n}\r\n\r\n// checkAuthforWarnings will look for insecure settings and log concerns.\r\n// Lock is assumed held.\r\nfunc (s *Server) checkAuthforWarnings() {\r\n\twarn := false\r\n\tif s.opts.Password != \"\" && !isBcrypt(s.opts.Password) {\r\n\t\twarn = true\r\n\t}\r\n\tfor _, u := range s.users {\r\n\t\t// Skip warn if using TLS certs based auth\r\n\t\t// unless a password has been left in the config.\r\n\t\tif u.Password == \"\" && s.opts.TLSMap {\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tif !isBcrypt(u.Password) {\r\n\t\t\twarn = true\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\tif warn {\r\n\t\t// Warning about using plaintext passwords.\r\n\t\ts.Warnf(\"Plaintext passwords detected, use nkeys or bcrypt.\")\r\n\t}\r\n}\r\n\r\n// If opts.Users or opts.Nkeys have definitions without an account\r\n// defined assign them to the default global account.\r\n// Lock should be held.\r\nfunc (s *Server) assignGlobalAccountToOrphanUsers() {\r\n\tfor _, u := range s.users {\r\n\t\tif u.Account == nil {\r\n\t\t\tu.Account = s.gacc\r\n\t\t}\r\n\t}\r\n\tfor _, u := range s.nkeys {\r\n\t\tif u.Account == nil {\r\n\t\t\tu.Account = s.gacc\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// If the given permissions has a ResponsePermission\r\n// set, ensure that defaults are set (if values are 0)\r\n// and that a Publish permission is set, and Allow\r\n// is disabled if not explicitly set.\r\nfunc validateResponsePermissions(p *Permissions) {\r\n\tif p == nil || p.Response == nil {\r\n\t\treturn\r\n\t}\r\n\tif p.Publish == nil {\r\n\t\tp.Publish = &SubjectPermission{}\r\n\t}\r\n\tif p.Publish.Allow == nil {\r\n\t\t// We turn off the blanket allow statement.\r\n\t\tp.Publish.Allow = []string{}\r\n\t}\r\n\t// If there is a response permission, ensure\r\n\t// that if value is 0, we set the default value.\r\n\tif p.Response.MaxMsgs == 0 {\r\n\t\tp.Response.MaxMsgs = DEFAULT_ALLOW_RESPONSE_MAX_MSGS\r\n\t}\r\n\tif p.Response.Expires == 0 {\r\n\t\tp.Response.Expires = DEFAULT_ALLOW_RESPONSE_EXPIRATION\r\n\t}\r\n}\r\n\r\n// configureAuthorization will do any setup needed for authorization.\r\n// Lock is assumed held.\r\nfunc (s *Server) configureAuthorization() {\r\n\tif s.opts == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\t// Check for multiple users first\r\n\t// This just checks and sets up the user map if we have multiple users.\r\n\tif opts.CustomClientAuthentication != nil {\r\n\t\ts.info.AuthRequired = true\r\n\t} else if len(s.trustedKeys) > 0 {\r\n\t\ts.info.AuthRequired = true\r\n\t} else if opts.Nkeys != nil || opts.Users != nil {\r\n\t\t// Support both at the same time.\r\n\t\tif opts.Nkeys != nil {\r\n\t\t\ts.nkeys = make(map[string]*NkeyUser)\r\n\t\t\tfor _, u := range opts.Nkeys {\r\n\t\t\t\tcopy := u.clone()\r\n\t\t\t\tif u.Account != nil {\r\n\t\t\t\t\tif v, ok := s.accounts.Load(u.Account.Name); ok {\r\n\t\t\t\t\t\tcopy.Account = v.(*Account)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tif copy.Permissions != nil {\r\n\t\t\t\t\tvalidateResponsePermissions(copy.Permissions)\r\n\t\t\t\t}\r\n\t\t\t\ts.nkeys[u.Nkey] = copy\r\n\t\t\t}\r\n\t\t}\r\n\t\tif opts.Users != nil {\r\n\t\t\ts.users = make(map[string]*User)\r\n\t\t\tfor _, u := range opts.Users {\r\n\t\t\t\tcopy := u.clone()\r\n\t\t\t\tif u.Account != nil {\r\n\t\t\t\t\tif v, ok := s.accounts.Load(u.Account.Name); ok {\r\n\t\t\t\t\t\tcopy.Account = v.(*Account)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tif copy.Permissions != nil {\r\n\t\t\t\t\tvalidateResponsePermissions(copy.Permissions)\r\n\t\t\t\t}\r\n\t\t\t\ts.users[u.Username] = copy\r\n\t\t\t}\r\n\t\t}\r\n\t\ts.assignGlobalAccountToOrphanUsers()\r\n\t\ts.info.AuthRequired = true\r\n\t} else if opts.Username != \"\" || opts.Authorization != \"\" {\r\n\t\ts.info.AuthRequired = true\r\n\t} else {\r\n\t\ts.users = nil\r\n\t\ts.info.AuthRequired = false\r\n\t}\r\n}\r\n\r\n// checkAuthentication will check based on client type and\r\n// return boolean indicating if client is authorized.\r\nfunc (s *Server) checkAuthentication(c *client) bool {\r\n\tswitch c.kind {\r\n\tcase CLIENT:\r\n\t\treturn s.isClientAuthorized(c)\r\n\tcase ROUTER:\r\n\t\treturn s.isRouterAuthorized(c)\r\n\tcase GATEWAY:\r\n\t\treturn s.isGatewayAuthorized(c)\r\n\tcase LEAF:\r\n\t\treturn s.isLeafNodeAuthorized(c)\r\n\tdefault:\r\n\t\treturn false\r\n\t}\r\n}\r\n\r\n// isClientAuthorized will check the client against the proper authorization method and data.\r\n// This could be nkey, token, or username/password based.\r\nfunc (s *Server) isClientAuthorized(c *client) bool {\r\n\topts := s.getOpts()\r\n\r\n\t// Check custom auth first, then jwts, then nkeys, then\r\n\t// multiple users with TLS map if enabled, then token,\r\n\t// then single user/pass.\r\n\tif opts.CustomClientAuthentication != nil {\r\n\t\treturn opts.CustomClientAuthentication.Check(c)\r\n\t}\r\n\r\n\treturn s.processClientOrLeafAuthentication(c)\r\n}\r\n\r\nfunc (s *Server) processClientOrLeafAuthentication(c *client) bool {\r\n\tvar (\r\n\t\tnkey *NkeyUser\r\n\t\tjuc  *jwt.UserClaims\r\n\t\tacc  *Account\r\n\t\tuser *User\r\n\t\tok   bool\r\n\t\terr  error\r\n\t\topts = s.getOpts()\r\n\t)\r\n\r\n\ts.mu.Lock()\r\n\tusers := s.users\r\n\ttlsMap := opts.TLSMap\r\n\tauthRequired := s.info.AuthRequired\r\n\tif !authRequired {\r\n\t\t// TODO(dlc) - If they send us credentials should we fail?\r\n\t\ts.mu.Unlock()\r\n\t\treturn true\r\n\t}\r\n\r\n\t// Check if we have trustedKeys defined in the server. If so we require a user jwt.\r\n\tif s.trustedKeys != nil {\r\n\t\tif c.opts.JWT == \"\" {\r\n\t\t\ts.mu.Unlock()\r\n\t\t\tc.Debugf(\"Authentication requires a user JWT\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\t// So we have a valid user jwt here.\r\n\t\tjuc, err = jwt.DecodeUserClaims(c.opts.JWT)\r\n\t\tif err != nil {\r\n\t\t\ts.mu.Unlock()\r\n\t\t\tc.Debugf(\"User JWT not valid: %v\", err)\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tvr := jwt.CreateValidationResults()\r\n\t\tjuc.Validate(vr)\r\n\t\tif vr.IsBlocking(true) {\r\n\t\t\ts.mu.Unlock()\r\n\t\t\tc.Debugf(\"User JWT no longer valid: %+v\", vr)\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\r\n\t// Check if we have nkeys or users for client.\r\n\thasNkeys := s.nkeys != nil\r\n\thasUsers := s.users != nil\r\n\tif hasNkeys && c.opts.Nkey != \"\" {\r\n\t\tnkey, ok = s.nkeys[c.opts.Nkey]\r\n\t\tif !ok {\r\n\t\t\ts.mu.Unlock()\r\n\t\t\treturn false\r\n\t\t}\r\n\t} else if hasUsers {\r\n\t\t// Check if we are tls verify and are mapping users from the client_certificate.\r\n\t\tif tlsMap {\r\n\t\t\tauthorized := checkClientTLSCertSubject(c, func(u string, certRDN *ldap.DN) (string, bool) {\r\n\t\t\t\t// First do literal lookup using the resulting string representation\r\n\t\t\t\t// of RDNSequence as implemented by the pkix package from Go.\r\n\t\t\t\tif u != \"\" {\r\n\t\t\t\t\tusr, ok := users[u]\r\n\t\t\t\t\tif !ok {\r\n\t\t\t\t\t\treturn \"\", ok\r\n\t\t\t\t\t}\r\n\t\t\t\t\tuser = usr\r\n\t\t\t\t\treturn usr.Username, ok\r\n\t\t\t\t}\r\n\r\n\t\t\t\tif certRDN == nil {\r\n\t\t\t\t\treturn \"\", false\r\n\t\t\t\t}\r\n\r\n\t\t\t\t// Look through the accounts for an RDN that is equal to the one\r\n\t\t\t\t// presented by the certificate.\r\n\t\t\t\tfor _, usr := range users {\r\n\t\t\t\t\t// TODO: Use this utility to make a full validation pass\r\n\t\t\t\t\t// on start in case tlsmap feature is being used.\r\n\t\t\t\t\tinputRDN, err := ldap.ParseDN(usr.Username)\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n\t\t\t\t\tif inputRDN.Equal(certRDN) {\r\n\t\t\t\t\t\tuser = usr\r\n\t\t\t\t\t\treturn usr.Username, true\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\treturn \"\", false\r\n\t\t\t})\r\n\t\t\tif !authorized {\r\n\t\t\t\ts.mu.Unlock()\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t\tif c.opts.Username != \"\" {\r\n\t\t\t\ts.Warnf(\"User found in connect proto, but user required from cert - %v\", c)\r\n\t\t\t}\r\n\t\t\t// Already checked that the client didn't send a user in connect\r\n\t\t\t// but we set it here to be able to identify it in the logs.\r\n\t\t\tc.opts.Username = user.Username\r\n\t\t} else {\r\n\t\t\tif c.kind == CLIENT && c.opts.Username == \"\" && s.opts.NoAuthUser != \"\" {\r\n\t\t\t\tif u, exists := s.users[s.opts.NoAuthUser]; exists {\r\n\t\t\t\t\tc.opts.Username = u.Username\r\n\t\t\t\t\tc.opts.Password = u.Password\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif c.opts.Username != \"\" {\r\n\t\t\t\tuser, ok = s.users[c.opts.Username]\r\n\t\t\t\tif !ok {\r\n\t\t\t\t\ts.mu.Unlock()\r\n\t\t\t\t\treturn false\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\t// If we have a jwt and a userClaim, make sure we have the Account, etc associated.\r\n\t// We need to look up the account. This will use an account resolver if one is present.\r\n\tif juc != nil {\r\n\t\tissuer := juc.Issuer\r\n\t\tif juc.IssuerAccount != \"\" {\r\n\t\t\tissuer = juc.IssuerAccount\r\n\t\t}\r\n\t\tif acc, err = s.LookupAccount(issuer); acc == nil {\r\n\t\t\tc.Debugf(\"Account JWT lookup error: %v\", err)\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif !s.isTrustedIssuer(acc.Issuer) {\r\n\t\t\tc.Debugf(\"Account JWT not signed by trusted operator\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif juc.IssuerAccount != \"\" && !acc.hasIssuer(juc.Issuer) {\r\n\t\t\tc.Debugf(\"User JWT issuer is not known\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif acc.IsExpired() {\r\n\t\t\tc.Debugf(\"Account JWT has expired\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\t// Verify the signature against the nonce.\r\n\t\tif c.opts.Sig == \"\" {\r\n\t\t\tc.Debugf(\"Signature missing\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tsig, err := base64.RawURLEncoding.DecodeString(c.opts.Sig)\r\n\t\tif err != nil {\r\n\t\t\t// Allow fallback to normal base64.\r\n\t\t\tsig, err = base64.StdEncoding.DecodeString(c.opts.Sig)\r\n\t\t\tif err != nil {\r\n\t\t\t\tc.Debugf(\"Signature not valid base64\")\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t}\r\n\t\tpub, err := nkeys.FromPublicKey(juc.Subject)\r\n\t\tif err != nil {\r\n\t\t\tc.Debugf(\"User nkey not valid: %v\", err)\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif err := pub.Verify(c.nonce, sig); err != nil {\r\n\t\t\tc.Debugf(\"Signature not verified\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif acc.checkUserRevoked(juc.Subject, juc.IssuedAt) {\r\n\t\t\tc.Debugf(\"User authentication revoked\")\r\n\t\t\treturn false\r\n\t\t}\r\n\r\n\t\tnkey = buildInternalNkeyUser(juc, acc)\r\n\t\tif err := c.RegisterNkeyUser(nkey); err != nil {\r\n\t\t\treturn false\r\n\t\t}\r\n\r\n\t\t// Generate an event if we have a system account.\r\n\t\ts.accountConnectEvent(c)\r\n\r\n\t\t// Check if we need to set an auth timer if the user jwt expires.\r\n\t\tc.checkExpiration(juc.Claims())\r\n\t\treturn true\r\n\t}\r\n\r\n\tif nkey != nil {\r\n\t\tif c.opts.Sig == \"\" {\r\n\t\t\tc.Debugf(\"Signature missing\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tsig, err := base64.RawURLEncoding.DecodeString(c.opts.Sig)\r\n\t\tif err != nil {\r\n\t\t\t// Allow fallback to normal base64.\r\n\t\t\tsig, err = base64.StdEncoding.DecodeString(c.opts.Sig)\r\n\t\t\tif err != nil {\r\n\t\t\t\tc.Debugf(\"Signature not valid base64\")\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t}\r\n\t\tpub, err := nkeys.FromPublicKey(c.opts.Nkey)\r\n\t\tif err != nil {\r\n\t\t\tc.Debugf(\"User nkey not valid: %v\", err)\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif err := pub.Verify(c.nonce, sig); err != nil {\r\n\t\t\tc.Debugf(\"Signature not verified\")\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif err := c.RegisterNkeyUser(nkey); err != nil {\r\n\t\t\treturn false\r\n\t\t}\r\n\t\treturn true\r\n\t}\r\n\tif user != nil {\r\n\t\tok = comparePasswords(user.Password, c.opts.Password)\r\n\t\t// If we are authorized, register the user which will properly setup any permissions\r\n\t\t// for pub/sub authorizations.\r\n\t\tif ok {\r\n\t\t\tc.RegisterUser(user)\r\n\t\t\t// Generate an event if we have a system account and this is not the $G account.\r\n\t\t\ts.accountConnectEvent(c)\r\n\t\t}\r\n\t\treturn ok\r\n\t}\r\n\r\n\tif c.kind == CLIENT {\r\n\t\tif opts.Authorization != \"\" {\r\n\t\t\treturn comparePasswords(opts.Authorization, c.opts.Authorization)\r\n\t\t} else if opts.Username != \"\" {\r\n\t\t\tif opts.Username != c.opts.Username {\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t\treturn comparePasswords(opts.Password, c.opts.Password)\r\n\t\t}\r\n\t} else if c.kind == LEAF {\r\n\t\t// There is no required username/password to connect and\r\n\t\t// there was no u/p in the CONNECT or none that matches the\r\n\t\t// know users. Register the leaf connection with global account\r\n\t\t// or the one specified in config (if provided).\r\n\t\treturn s.registerLeafWithAccount(c, opts.LeafNode.Account)\r\n\t}\r\n\treturn false\r\n}\r\n\r\nfunc getTLSAuthDCs(rdns *pkix.RDNSequence) string {\r\n\tdcOID := asn1.ObjectIdentifier{0, 9, 2342, 19200300, 100, 1, 25}\r\n\tdcs := []string{}\r\n\tfor _, rdn := range *rdns {\r\n\t\tif len(rdn) == 0 {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tfor _, atv := range rdn {\r\n\t\t\tvalue, ok := atv.Value.(string)\r\n\t\t\tif !ok {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif atv.Type.Equal(dcOID) {\r\n\t\t\t\tdcs = append(dcs, \"DC=\"+value)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn strings.Join(dcs, \",\")\r\n}\r\n\r\ntype tlsMapAuthFn func(string, *ldap.DN) (string, bool)\r\n\r\nfunc checkClientTLSCertSubject(c *client, fn tlsMapAuthFn) bool {\r\n\ttlsState := c.GetTLSConnectionState()\r\n\tif tlsState == nil {\r\n\t\tc.Debugf(\"User required in cert, no TLS connection state\")\r\n\t\treturn false\r\n\t}\r\n\tif len(tlsState.PeerCertificates) == 0 {\r\n\t\tc.Debugf(\"User required in cert, no peer certificates found\")\r\n\t\treturn false\r\n\t}\r\n\tcert := tlsState.PeerCertificates[0]\r\n\tif len(tlsState.PeerCertificates) > 1 {\r\n\t\tc.Debugf(\"Multiple peer certificates found, selecting first\")\r\n\t}\r\n\r\n\thasSANs := len(cert.DNSNames) > 0\r\n\thasEmailAddresses := len(cert.EmailAddresses) > 0\r\n\thasSubject := len(cert.Subject.String()) > 0\r\n\tif !hasEmailAddresses && !hasSubject {\r\n\t\tc.Debugf(\"User required in cert, none found\")\r\n\t\treturn false\r\n\t}\r\n\r\n\tswitch {\r\n\tcase hasEmailAddresses:\r\n\t\tfor _, u := range cert.EmailAddresses {\r\n\t\t\tif match, ok := fn(u, nil); ok {\r\n\t\t\t\tc.Debugf(\"Using email found in cert for auth [%q]\", match)\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t}\r\n\t\tfallthrough\r\n\tcase hasSANs:\r\n\t\tfor _, u := range cert.DNSNames {\r\n\t\t\tif match, ok := fn(u, nil); ok {\r\n\t\t\t\tc.Debugf(\"Using SAN found in cert for auth [%q]\", match)\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Use the string representation of the full RDN Sequence including\r\n\t// the domain components in case there are any.\r\n\trdn := cert.Subject.ToRDNSequence().String()\r\n\r\n\t// Match that follows original order from the subject takes precedence.\r\n\tdn, err := ldap.FromCertSubject(cert.Subject)\r\n\tif err == nil {\r\n\t\tif match, ok := fn(\"\", dn); ok {\r\n\t\t\tc.Debugf(\"Using DistinguishedNameMatch for auth [%q]\", match)\r\n\t\t\treturn true\r\n\t\t}\r\n\t\tc.Debugf(\"DistinguishedNameMatch could not be used for auth [%q]\", rdn)\r\n\t}\r\n\r\n\tvar rdns pkix.RDNSequence\r\n\tif _, err := asn1.Unmarshal(cert.RawSubject, &rdns); err == nil {\r\n\t\t// If found domain components then include roughly following\r\n\t\t// the order from https://tools.ietf.org/html/rfc2253\r\n\t\t//\r\n\t\t// NOTE: The original sequence from string representation by ToRDNSequence does not follow\r\n\t\t// the correct ordering, so this addition ofdomainComponents would likely be deprecated in\r\n\t\t// another release in favor of using the correct ordered as parsed by the go-ldap library.\r\n\t\t//\r\n\t\tdcs := getTLSAuthDCs(&rdns)\r\n\t\tif len(dcs) > 0 {\r\n\t\t\tu := strings.Join([]string{rdn, dcs}, \",\")\r\n\t\t\tif match, ok := fn(u, nil); ok {\r\n\t\t\t\tc.Debugf(\"Using RDNSequence for auth [%q]\", match)\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t\tc.Debugf(\"RDNSequence could not be used for auth [%q]\", u)\r\n\t\t}\r\n\t}\r\n\r\n\t// If no match, then use the string representation of the RDNSequence\r\n\t// from the subject without the domainComponents.\r\n\tif match, ok := fn(rdn, nil); ok {\r\n\t\tc.Debugf(\"Using certificate subject for auth [%q]\", match)\r\n\t\treturn true\r\n\t}\r\n\r\n\tc.Debugf(\"User in cert [%q], not found\", rdn)\r\n\treturn false\r\n}\r\n\r\n// checkRouterAuth checks optional router authorization which can be nil or username/password.\r\nfunc (s *Server) isRouterAuthorized(c *client) bool {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\t// Check custom auth first, then TLS map if enabled\r\n\t// then single user/pass.\r\n\tif s.opts.CustomRouterAuthentication != nil {\r\n\t\treturn s.opts.CustomRouterAuthentication.Check(c)\r\n\t}\r\n\r\n\tif opts.Cluster.Username == \"\" {\r\n\t\treturn true\r\n\t}\r\n\r\n\tif opts.Cluster.TLSMap {\r\n\t\treturn checkClientTLSCertSubject(c, func(user string, _ *ldap.DN) (string, bool) {\r\n\t\t\treturn \"\", opts.Cluster.Username == user\r\n\t\t})\r\n\t}\r\n\r\n\tif opts.Cluster.Username != c.opts.Username {\r\n\t\treturn false\r\n\t}\r\n\tif !comparePasswords(opts.Cluster.Password, c.opts.Password) {\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// isGatewayAuthorized checks optional gateway authorization which can be nil or username/password.\r\nfunc (s *Server) isGatewayAuthorized(c *client) bool {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\tif opts.Gateway.Username == \"\" {\r\n\t\treturn true\r\n\t}\r\n\r\n\t// Check whether TLS map is enabled, otherwise use single user/pass.\r\n\tif opts.Gateway.TLSMap {\r\n\t\treturn checkClientTLSCertSubject(c, func(user string, _ *ldap.DN) (string, bool) {\r\n\t\t\treturn \"\", opts.Gateway.Username == user\r\n\t\t})\r\n\t}\r\n\r\n\tif opts.Gateway.Username != c.opts.Username {\r\n\t\treturn false\r\n\t}\r\n\treturn comparePasswords(opts.Gateway.Password, c.opts.Password)\r\n}\r\n\r\nfunc (s *Server) registerLeafWithAccount(c *client, account string) bool {\r\n\tvar err error\r\n\tacc := s.globalAccount()\r\n\tif account != _EMPTY_ {\r\n\t\tacc, err = s.lookupAccount(account)\r\n\t\tif err != nil {\r\n\t\t\ts.Errorf(\"authentication of user %q failed, unable to lookup account %q: %v\",\r\n\t\t\t\tc.opts.Username, account, err)\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\tif err = c.registerWithAccount(acc); err != nil {\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// isLeafNodeAuthorized will check for auth for an inbound leaf node connection.\r\nfunc (s *Server) isLeafNodeAuthorized(c *client) bool {\r\n\topts := s.getOpts()\r\n\r\n\tisAuthorized := func(username, password, account string) bool {\r\n\t\tif username != c.opts.Username {\r\n\t\t\treturn false\r\n\t\t}\r\n\t\tif !comparePasswords(password, c.opts.Password) {\r\n\t\t\treturn false\r\n\t\t}\r\n\t\treturn s.registerLeafWithAccount(c, account)\r\n\t}\r\n\r\n\t// If leafnodes config has an authorization{} stanza, this takes precedence.\r\n\t// The user in CONNECT mutch match. We will bind to the account associated\r\n\t// with that user (from the leafnode's authorization{} config).\r\n\tif opts.LeafNode.Username != _EMPTY_ {\r\n\t\treturn isAuthorized(opts.LeafNode.Username, opts.LeafNode.Password, opts.LeafNode.Account)\r\n\t} else if len(opts.LeafNode.Users) > 0 {\r\n\t\tif opts.LeafNode.TLSMap {\r\n\t\t\tvar user *User\r\n\t\t\tfound := checkClientTLSCertSubject(c, func(u string, _ *ldap.DN) (string, bool) {\r\n\t\t\t\t// This is expected to be a very small array.\r\n\t\t\t\tfor _, usr := range opts.LeafNode.Users {\r\n\t\t\t\t\tif u == usr.Username {\r\n\t\t\t\t\t\tuser = usr\r\n\t\t\t\t\t\treturn u, true\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\treturn \"\", false\r\n\t\t\t})\r\n\t\t\tif !found {\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t\tif c.opts.Username != \"\" {\r\n\t\t\t\ts.Warnf(\"User %q found in connect proto, but user required from cert\", c.opts.Username)\r\n\t\t\t}\r\n\t\t\tc.opts.Username = user.Username\r\n\t\t\t// This will authorize since are using an existing user,\r\n\t\t\t// but it will also register with proper account.\r\n\t\t\treturn isAuthorized(user.Username, user.Password, user.Account.Name)\r\n\t\t}\r\n\r\n\t\t// This is expected to be a very small array.\r\n\t\tfor _, u := range opts.LeafNode.Users {\r\n\t\t\tif u.Username == c.opts.Username {\r\n\t\t\t\tvar accName string\r\n\t\t\t\tif u.Account != nil {\r\n\t\t\t\t\taccName = u.Account.Name\r\n\t\t\t\t}\r\n\t\t\t\treturn isAuthorized(u.Username, u.Password, accName)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn false\r\n\t}\r\n\r\n\t// We are here if we accept leafnode connections without any credential.\r\n\r\n\t// Still, if the CONNECT has some user info, we will bind to the\r\n\t// user's account or to the specified default account (if provided)\r\n\t// or to the global account.\r\n\treturn s.processClientOrLeafAuthentication(c)\r\n}\r\n\r\n// Support for bcrypt stored passwords and tokens.\r\nconst bcryptPrefix = \"$2a$\"\r\n\r\n// isBcrypt checks whether the given password or token is bcrypted.\r\nfunc isBcrypt(password string) bool {\r\n\treturn strings.HasPrefix(password, bcryptPrefix)\r\n}\r\n\r\nfunc comparePasswords(serverPassword, clientPassword string) bool {\r\n\t// Check to see if the server password is a bcrypt hash\r\n\tif isBcrypt(serverPassword) {\r\n\t\tif err := bcrypt.CompareHashAndPassword([]byte(serverPassword), []byte(clientPassword)); err != nil {\r\n\t\t\treturn false\r\n\t\t}\r\n\t} else if serverPassword != clientPassword {\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\nfunc validateAuth(o *Options) error {\r\n\tif o.NoAuthUser == \"\" {\r\n\t\treturn nil\r\n\t}\r\n\tif len(o.TrustedOperators) > 0 {\r\n\t\treturn fmt.Errorf(\"no_auth_user not compatible with Trusted Operator\")\r\n\t}\r\n\tif o.Users == nil {\r\n\t\treturn fmt.Errorf(`no_auth_user: \"%s\" present, but users are not defined`, o.NoAuthUser)\r\n\t}\r\n\tfor _, u := range o.Users {\r\n\t\tif u.Username == o.NoAuthUser {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t}\r\n\treturn fmt.Errorf(\r\n\t\t`no_auth_user: \"%s\" not present as user in authorization block or account configuration`,\r\n\t\to.NoAuthUser)\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/auth.go b/server/gnatsd/server/auth.go
--- a/server/gnatsd/server/auth.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/auth.go	(date 1665399058048)
@@ -23,8 +23,8 @@
 	"strings"
 	"time"
 
-	"github.com/nats-io/jwt"
 	"github.com/kubemq-io/broker/server/gnatsd/internal/ldap"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 	"golang.org/x/crypto/bcrypt"
 )
Index: server/gnatsd/server/server.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2012-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"context\"\r\n\t\"crypto/tls\"\r\n\t\"encoding/json\"\r\n\t\"flag\"\r\n\t\"fmt\"\r\n\t\"github.com/kubemq-io/broker/pkg/pipe\"\r\n\t\"io/ioutil\"\r\n\t\"math/rand\"\r\n\t\"net\"\r\n\t\"net/http\"\r\n\t\"os\"\r\n\t\"path\"\r\n\t\"path/filepath\"\r\n\t\"runtime\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"time\"\r\n\r\n\t// Allow dynamic profiling.\r\n\t_ \"net/http/pprof\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nconst (\r\n\t// Time to wait before starting closing clients when in LD mode.\r\n\tlameDuckModeDefaultInitialDelay = int64(10 * time.Second)\r\n\r\n\t// Interval for the first PING for non client connections.\r\n\tfirstPingInterval = time.Second\r\n\r\n\t// This is for the first ping for client connections.\r\n\tfirstClientPingInterval = 2 * time.Second\r\n)\r\n\r\n// Make this a variable so that we can change during tests\r\nvar lameDuckModeInitialDelay = int64(lameDuckModeDefaultInitialDelay)\r\n\r\n// Info is the information sent to clients, routes, gateways, and leaf nodes,\r\n// to help them understand information about this server.\r\ntype Info struct {\r\n\tID                string   `json:\"server_id\"`\r\n\tName              string   `json:\"server_name\"`\r\n\tVersion           string   `json:\"version\"`\r\n\tProto             int      `json:\"proto\"`\r\n\tGitCommit         string   `json:\"git_commit,omitempty\"`\r\n\tGoVersion         string   `json:\"go\"`\r\n\tHost              string   `json:\"host\"`\r\n\tPort              int      `json:\"port\"`\r\n\tAuthRequired      bool     `json:\"auth_required,omitempty\"`\r\n\tTLSRequired       bool     `json:\"tls_required,omitempty\"`\r\n\tTLSVerify         bool     `json:\"tls_verify,omitempty\"`\r\n\tMaxPayload        int32    `json:\"max_payload\"`\r\n\tIP                string   `json:\"ip,omitempty\"`\r\n\tCID               uint64   `json:\"client_id,omitempty\"`\r\n\tClientIP          string   `json:\"client_ip,omitempty\"`\r\n\tNonce             string   `json:\"nonce,omitempty\"`\r\n\tCluster           string   `json:\"cluster,omitempty\"`\r\n\tClientConnectURLs []string `json:\"connect_urls,omitempty\"` // Contains URLs a client can connect to.\r\n\r\n\t// Route Specific\r\n\tImport *SubjectPermission `json:\"import,omitempty\"`\r\n\tExport *SubjectPermission `json:\"export,omitempty\"`\r\n\r\n\t// Gateways Specific\r\n\tGateway           string   `json:\"gateway,omitempty\"`             // Name of the origin Gateway (sent by gateway's INFO)\r\n\tGatewayURLs       []string `json:\"gateway_urls,omitempty\"`        // Gateway URLs in the originating cluster (sent by gateway's INFO)\r\n\tGatewayURL        string   `json:\"gateway_url,omitempty\"`         // Gateway URL on that server (sent by route's INFO)\r\n\tGatewayCmd        byte     `json:\"gateway_cmd,omitempty\"`         // Command code for the receiving server to know what to do\r\n\tGatewayCmdPayload []byte   `json:\"gateway_cmd_payload,omitempty\"` // Command payload when needed\r\n\tGatewayNRP        bool     `json:\"gateway_nrp,omitempty\"`         // Uses new $GNR. prefix for mapped replies\r\n\r\n\t// LeafNode Specific\r\n\tLeafNodeURLs []string `json:\"leafnode_urls,omitempty\"` // LeafNode URLs that the server can reconnect to.\r\n}\r\n\r\n// Server is our main struct.\r\ntype Server struct {\r\n\tgcid uint64\r\n\tstats\r\n\tmu               sync.Mutex\r\n\tkp               nkeys.KeyPair\r\n\tprand            *rand.Rand\r\n\tinfo             Info\r\n\tconfigFile       string\r\n\toptsMu           sync.RWMutex\r\n\topts             *Options\r\n\trunning          bool\r\n\tshutdown         bool\r\n\tlistener         net.Listener\r\n\tgacc             *Account\r\n\tsys              *internal\r\n\taccounts         sync.Map\r\n\ttmpAccounts      sync.Map // Temporarily stores accounts that are being built\r\n\tactiveAccounts   int32\r\n\taccResolver      AccountResolver\r\n\tclients          map[uint64]*client\r\n\troutes           map[uint64]*client\r\n\troutesByHash     sync.Map\r\n\thash             []byte\r\n\tremotes          map[string]*client\r\n\tleafs            map[uint64]*client\r\n\tusers            map[string]*User\r\n\tnkeys            map[string]*NkeyUser\r\n\ttotalClients     uint64\r\n\tclosed           *closedRingBuffer\r\n\tdone             chan bool\r\n\tstart            time.Time\r\n\thttp             net.Listener\r\n\thttpHandler      http.Handler\r\n\thttpBasePath     string\r\n\tprofiler         net.Listener\r\n\thttpReqStats     map[string]uint64\r\n\trouteListener    net.Listener\r\n\trouteInfo        Info\r\n\trouteInfoJSON    []byte\r\n\tleafNodeListener net.Listener\r\n\tleafNodeInfo     Info\r\n\tleafNodeInfoJSON []byte\r\n\tleafNodeOpts     struct {\r\n\t\tresolver    netResolver\r\n\t\tdialTimeout time.Duration\r\n\t}\r\n\r\n\tquitCh           chan struct{}\r\n\tshutdownComplete chan struct{}\r\n\t// memory client server pipe\r\n\tpipeListener *pipe.Pipe\r\n\r\n\t// Tracking Go routines\r\n\tgrMu         sync.Mutex\r\n\tgrTmpClients map[uint64]*client\r\n\tgrRunning    bool\r\n\tgrWG         sync.WaitGroup // to wait on various go routines\r\n\r\n\tcproto     int64     // number of clients supporting async INFO\r\n\tconfigTime time.Time // last time config was loaded\r\n\r\n\tlogging struct {\r\n\t\tsync.RWMutex\r\n\t\tlogger      Logger\r\n\t\ttrace       int32\r\n\t\tdebug       int32\r\n\t\ttraceSysAcc int32\r\n\t}\r\n\r\n\tclientConnectURLs []string\r\n\r\n\t// Used internally for quick look-ups.\r\n\tclientConnectURLsMap map[string]struct{}\r\n\r\n\tlastCURLsUpdate int64\r\n\r\n\t// For Gateways\r\n\tgatewayListener net.Listener // Accept listener\r\n\tgateway         *srvGateway\r\n\r\n\t// Used by tests to check that http.Servers do\r\n\t// not set any timeout.\r\n\tmonitoringServer *http.Server\r\n\tprofilingServer  *http.Server\r\n\r\n\t// LameDuck mode\r\n\tldm   bool\r\n\tldmCh chan bool\r\n\r\n\t// Trusted public operator keys.\r\n\ttrustedKeys []string\r\n\r\n\t// We use this to minimize mem copies for request to monitoring\r\n\t// endpoint /varz (when it comes from http).\r\n\tvarzMu sync.Mutex\r\n\tvarz   *Varz\r\n\t// This is set during a config reload if we detect that we have\r\n\t// added/removed routes. The monitoring code then check that\r\n\t// to know if it should update the cluster's URLs array.\r\n\tvarzUpdateRouteURLs bool\r\n\r\n\t// Keeps a sublist of of subscriptions attached to leafnode connections\r\n\t// for the $GNR.*.*.*.> subject so that a server can send back a mapped\r\n\t// gateway reply.\r\n\tgwLeafSubs *Sublist\r\n\r\n\t// Used for expiration of mapped GW replies\r\n\tgwrm struct {\r\n\t\tw  int32\r\n\t\tch chan time.Duration\r\n\t\tm  sync.Map\r\n\t}\r\n\r\n\t// exporting account name the importer experienced issues with\r\n\tincompleteAccExporterMap sync.Map\r\n}\r\n\r\n// Make sure all are 64bits for atomic use\r\ntype stats struct {\r\n\tinMsgs        int64\r\n\toutMsgs       int64\r\n\tinBytes       int64\r\n\toutBytes      int64\r\n\tslowConsumers int64\r\n}\r\n\r\n// New will setup a new server struct after parsing the options.\r\n// DEPRECATED: Use NewServer(opts)\r\nfunc New(opts *Options) *Server {\r\n\ts, _ := NewServer(opts)\r\n\treturn s\r\n}\r\n\r\n// NewServer will setup a new server struct after parsing the options.\r\n// Could return an error if options can not be validated.\r\nfunc NewServer(opts *Options) (*Server, error) {\r\n\tsetBaselineOptions(opts)\r\n\r\n\t// Process TLS options, including whether we require client certificates.\r\n\ttlsReq := opts.TLSConfig != nil\r\n\tverify := (tlsReq && opts.TLSConfig.ClientAuth == tls.RequireAndVerifyClientCert)\r\n\r\n\t// Created server's nkey identity.\r\n\tkp, _ := nkeys.CreateServer()\r\n\tpub, _ := kp.PublicKey()\r\n\r\n\tserverName := pub\r\n\tif opts.ServerName != \"\" {\r\n\t\tserverName = opts.ServerName\r\n\t}\r\n\r\n\thttpBasePath := normalizeBasePath(opts.HTTPBasePath)\r\n\r\n\t// Validate some options. This is here because we cannot assume that\r\n\t// server will always be started with configuration parsing (that could\r\n\t// report issues). Its options can be (incorrectly) set by hand when\r\n\t// server is embedded. If there is an error, return nil.\r\n\tif err := validateOptions(opts); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tinfo := Info{\r\n\t\tID:           pub,\r\n\t\tVersion:      VERSION,\r\n\t\tProto:        PROTO,\r\n\t\tGitCommit:    gitCommit,\r\n\t\tGoVersion:    runtime.Version(),\r\n\t\tName:         serverName,\r\n\t\tHost:         opts.Host,\r\n\t\tPort:         opts.Port,\r\n\t\tAuthRequired: false,\r\n\t\tTLSRequired:  tlsReq,\r\n\t\tTLSVerify:    verify,\r\n\t\tMaxPayload:   opts.MaxPayload,\r\n\t}\r\n\r\n\tnow := time.Now()\r\n\ts := &Server{\r\n\t\tkp:           kp,\r\n\t\tconfigFile:   opts.ConfigFile,\r\n\t\tinfo:         info,\r\n\t\tprand:        rand.New(rand.NewSource(time.Now().UnixNano())),\r\n\t\topts:         opts,\r\n\t\tdone:         make(chan bool, 1),\r\n\t\tstart:        now,\r\n\t\tconfigTime:   now,\r\n\t\tgwLeafSubs:   NewSublistWithCache(),\r\n\t\thttpBasePath: httpBasePath,\r\n\t}\r\n\r\n\t// Trusted root operator keys.\r\n\tif !s.processTrustedKeys() {\r\n\t\treturn nil, fmt.Errorf(\"Error processing trusted operator keys\")\r\n\t}\r\n\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\r\n\t// Ensure that non-exported options (used in tests) are properly set.\r\n\ts.setLeafNodeNonExportedOptions()\r\n\r\n\t// Used internally for quick look-ups.\r\n\ts.clientConnectURLsMap = make(map[string]struct{})\r\n\r\n\t// Call this even if there is no gateway defined. It will\r\n\t// initialize the structure so we don't have to check for\r\n\t// it to be nil or not in various places in the code.\r\n\tif err := s.newGateway(opts); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tif s.gateway.enabled {\r\n\t\ts.info.Cluster = s.getGatewayName()\r\n\t}\r\n\r\n\t// This is normally done in the AcceptLoop, once the\r\n\t// listener has been created (possibly with random port),\r\n\t// but since some tests may expect the INFO to be properly\r\n\t// set after New(), let's do it now.\r\n\ts.setInfoHostPortAndGenerateJSON()\r\n\r\n\t// For tracking clients\r\n\ts.clients = make(map[uint64]*client)\r\n\r\n\t// For tracking closed clients.\r\n\ts.closed = newClosedRingBuffer(opts.MaxClosedClients)\r\n\r\n\t// For tracking connections that are not yet registered\r\n\t// in s.routes, but for which readLoop has started.\r\n\ts.grTmpClients = make(map[uint64]*client)\r\n\r\n\t// For tracking routes and their remote ids\r\n\ts.routes = make(map[uint64]*client)\r\n\ts.remotes = make(map[string]*client)\r\n\r\n\t// For tracking leaf nodes.\r\n\ts.leafs = make(map[uint64]*client)\r\n\r\n\t// Used to kick out all go routines possibly waiting on server\r\n\t// to shutdown.\r\n\ts.quitCh = make(chan struct{})\r\n\t// Closed when Shutdown() is complete. Allows WaitForShutdown() to block\r\n\t// waiting for complete shutdown.\r\n\ts.shutdownComplete = make(chan struct{})\r\n\r\n\t// For tracking accounts\r\n\tif err := s.configureAccounts(); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\t// If there is an URL account resolver, do basic test to see if anyone is home.\r\n\t// Do this after configureAccounts() which calls configureResolver(), which will\r\n\t// set TLSConfig if specified.\r\n\tif ar := opts.AccountResolver; ar != nil {\r\n\t\tif ur, ok := ar.(*URLAccResolver); ok {\r\n\t\t\tif _, err := ur.Fetch(\"\"); err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// In local config mode, check that leafnode configuration\r\n\t// refers to account that exist.\r\n\tif len(opts.TrustedOperators) == 0 {\r\n\t\tcheckAccountExists := func(accName string) error {\r\n\t\t\tif accName == _EMPTY_ {\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\t\tif _, ok := s.accounts.Load(accName); !ok {\r\n\t\t\t\treturn fmt.Errorf(\"cannot find account %q specified in leafnode authorization\", accName)\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\tif err := checkAccountExists(opts.LeafNode.Account); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tfor _, lu := range opts.LeafNode.Users {\r\n\t\t\tif lu.Account == nil {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif err := checkAccountExists(lu.Account.Name); err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t}\r\n\t\tfor _, r := range opts.LeafNode.Remotes {\r\n\t\t\tif r.LocalAccount == _EMPTY_ {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tif _, ok := s.accounts.Load(r.LocalAccount); !ok {\r\n\t\t\t\treturn nil, fmt.Errorf(\"no local account %q for remote leafnode\", r.LocalAccount)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Used to setup Authorization.\r\n\ts.configureAuthorization()\r\n\r\n\t// Start signal handler\r\n\ts.handleSignals()\r\n\r\n\treturn s, nil\r\n}\r\n\r\n// ClientURL returns the URL used to connect clients. Helpful in testing\r\n// when we designate a random client port (-1).\r\nfunc (s *Server) ClientURL() string {\r\n\t// FIXME(dlc) - should we add in user and pass if defined single?\r\n\topts := s.getOpts()\r\n\tscheme := \"nats://\"\r\n\tif opts.TLSConfig != nil {\r\n\t\tscheme = \"tls://\"\r\n\t}\r\n\treturn fmt.Sprintf(\"%s%s:%d\", scheme, opts.Host, opts.Port)\r\n}\r\n\r\nfunc validateOptions(o *Options) error {\r\n\t// Check that the trust configuration is correct.\r\n\tif err := validateTrustedOperators(o); err != nil {\r\n\t\treturn err\r\n\t}\r\n\t// Check on leaf nodes which will require a system\r\n\t// account when gateways are also configured.\r\n\tif err := validateLeafNode(o); err != nil {\r\n\t\treturn err\r\n\t}\r\n\t// Check that authentication is properly configured.\r\n\tif err := validateAuth(o); err != nil {\r\n\t\treturn err\r\n\t}\r\n\t// Check that gateway is properly configured. Returns no error\r\n\t// if there is no gateway defined.\r\n\treturn validateGatewayOptions(o)\r\n}\r\n\r\nfunc (s *Server) getOpts() *Options {\r\n\ts.optsMu.RLock()\r\n\topts := s.opts\r\n\ts.optsMu.RUnlock()\r\n\treturn opts\r\n}\r\n\r\nfunc (s *Server) setOpts(opts *Options) {\r\n\ts.optsMu.Lock()\r\n\ts.opts = opts\r\n\ts.optsMu.Unlock()\r\n}\r\n\r\nfunc (s *Server) globalAccount() *Account {\r\n\ts.mu.Lock()\r\n\tgacc := s.gacc\r\n\ts.mu.Unlock()\r\n\treturn gacc\r\n}\r\n\r\n// Used to setup Accounts.\r\n// Lock is held upon entry.\r\nfunc (s *Server) configureAccounts() error {\r\n\t// Create global account.\r\n\tif s.gacc == nil {\r\n\t\ts.gacc = NewAccount(globalAccountName)\r\n\t\ts.registerAccountNoLock(s.gacc)\r\n\t}\r\n\r\n\topts := s.opts\r\n\r\n\t// Check opts and walk through them. We need to copy them here\r\n\t// so that we do not keep a real one sitting in the options.\r\n\tfor _, acc := range s.opts.Accounts {\r\n\t\ta := acc.shallowCopy()\r\n\t\tacc.sl = nil\r\n\t\tacc.clients = nil\r\n\t\ts.registerAccountNoLock(a)\r\n\t}\r\n\r\n\t// Now that we have this we need to remap any referenced accounts in\r\n\t// import or export maps to the new ones.\r\n\tswapApproved := func(ea *exportAuth) {\r\n\t\tfor sub, a := range ea.approved {\r\n\t\t\tvar acc *Account\r\n\t\t\tif v, ok := s.accounts.Load(a.Name); ok {\r\n\t\t\t\tacc = v.(*Account)\r\n\t\t\t}\r\n\t\t\tea.approved[sub] = acc\r\n\t\t}\r\n\t}\r\n\r\n\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\tacc := v.(*Account)\r\n\t\t// Exports\r\n\t\tfor _, ea := range acc.exports.streams {\r\n\t\t\tif ea != nil {\r\n\t\t\t\tswapApproved(&ea.exportAuth)\r\n\t\t\t}\r\n\t\t}\r\n\t\tfor _, ea := range acc.exports.services {\r\n\t\t\tif ea != nil {\r\n\t\t\t\tswapApproved(&ea.exportAuth)\r\n\t\t\t}\r\n\t\t}\r\n\t\t// Imports\r\n\t\tfor _, si := range acc.imports.streams {\r\n\t\t\tif v, ok := s.accounts.Load(si.acc.Name); ok {\r\n\t\t\t\tsi.acc = v.(*Account)\r\n\t\t\t}\r\n\t\t}\r\n\t\tfor _, si := range acc.imports.services {\r\n\t\t\tif v, ok := s.accounts.Load(si.acc.Name); ok {\r\n\t\t\t\tsi.acc = v.(*Account)\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn true\r\n\t})\r\n\r\n\t// Check for configured account resolvers.\r\n\tif err := s.configureResolver(); err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\t// Set the system account if it was configured.\r\n\tif opts.SystemAccount != _EMPTY_ {\r\n\t\t// Lock may be acquired in lookupAccount, so release to call lookupAccount.\r\n\t\ts.mu.Unlock()\r\n\t\tacc, err := s.lookupAccount(opts.SystemAccount)\r\n\t\ts.mu.Lock()\r\n\t\tif err == nil && s.sys != nil && acc != s.sys.account {\r\n\t\t\t// sys.account.clients (including internal client)/respmap/etc... are transferred separately\r\n\t\t\ts.sys.account = acc\r\n\t\t\ts.mu.Unlock()\r\n\t\t\t// acquires server lock separately\r\n\t\t\ts.addSystemAccountExports(acc)\r\n\t\t\ts.mu.Lock()\r\n\t\t}\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error resolving system account: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// Setup the account resolver. For memory resolver, make sure the JWTs are\r\n// properly formed but do not enforce expiration etc.\r\nfunc (s *Server) configureResolver() error {\r\n\topts := s.getOpts()\r\n\ts.accResolver = opts.AccountResolver\r\n\tif opts.AccountResolver != nil {\r\n\t\t// For URL resolver, set the TLSConfig if specified.\r\n\t\tif opts.AccountResolverTLSConfig != nil {\r\n\t\t\tif ar, ok := opts.AccountResolver.(*URLAccResolver); ok {\r\n\t\t\t\tif t, ok := ar.c.Transport.(*http.Transport); ok {\r\n\t\t\t\t\tt.CloseIdleConnections()\r\n\t\t\t\t\tt.TLSClientConfig = opts.AccountResolverTLSConfig.Clone()\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tif len(opts.resolverPreloads) > 0 {\r\n\t\t\tif _, ok := s.accResolver.(*MemAccResolver); !ok {\r\n\t\t\t\treturn fmt.Errorf(\"resolver preloads only available for resolver type MEM\")\r\n\t\t\t}\r\n\t\t\tfor k, v := range opts.resolverPreloads {\r\n\t\t\t\t_, err := jwt.DecodeAccountClaims(v)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\treturn fmt.Errorf(\"preload account error for %q: %v\", k, err)\r\n\t\t\t\t}\r\n\t\t\t\ts.accResolver.Store(k, v)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// This will check preloads for validation issues.\r\nfunc (s *Server) checkResolvePreloads() {\r\n\topts := s.getOpts()\r\n\t// We can just check the read-only opts versions here, that way we do not need\r\n\t// to grab server lock or access s.accResolver.\r\n\tfor k, v := range opts.resolverPreloads {\r\n\t\tclaims, err := jwt.DecodeAccountClaims(v)\r\n\t\tif err != nil {\r\n\t\t\ts.Errorf(\"Preloaded account [%s] not valid\", k)\r\n\t\t}\r\n\t\t// Check if it is expired.\r\n\t\tvr := jwt.CreateValidationResults()\r\n\t\tclaims.Validate(vr)\r\n\t\tif vr.IsBlocking(true) {\r\n\t\t\ts.Warnf(\"Account [%s] has validation issues:\", k)\r\n\t\t\tfor _, v := range vr.Issues {\r\n\t\t\t\ts.Warnf(\"  - %s\", v.Description)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc (s *Server) generateRouteInfoJSON() {\r\n\t// New proto wants a nonce.\r\n\tvar raw [nonceLen]byte\r\n\tnonce := raw[:]\r\n\ts.generateNonce(nonce)\r\n\ts.routeInfo.Nonce = string(nonce)\r\n\tb, _ := json.Marshal(s.routeInfo)\r\n\tpcs := [][]byte{[]byte(\"INFO\"), b, []byte(CR_LF)}\r\n\ts.routeInfoJSON = bytes.Join(pcs, []byte(\" \"))\r\n}\r\n\r\n// isTrustedIssuer will check that the issuer is a trusted public key.\r\n// This is used to make sure an account was signed by a trusted operator.\r\nfunc (s *Server) isTrustedIssuer(issuer string) bool {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\t// If we are not running in trusted mode and there is no issuer, that is ok.\r\n\tif len(s.trustedKeys) == 0 && issuer == \"\" {\r\n\t\treturn true\r\n\t}\r\n\tfor _, tk := range s.trustedKeys {\r\n\t\tif tk == issuer {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// processTrustedKeys will process binary stamped and\r\n// options-based trusted nkeys. Returns success.\r\nfunc (s *Server) processTrustedKeys() bool {\r\n\tif trustedKeys != \"\" && !s.initStampedTrustedKeys() {\r\n\t\treturn false\r\n\t} else if s.opts.TrustedKeys != nil {\r\n\t\tfor _, key := range s.opts.TrustedKeys {\r\n\t\t\tif !nkeys.IsValidPublicOperatorKey(key) {\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t}\r\n\t\ts.trustedKeys = s.opts.TrustedKeys\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// checkTrustedKeyString will check that the string is a valid array\r\n// of public operator nkeys.\r\nfunc checkTrustedKeyString(keys string) []string {\r\n\ttks := strings.Fields(keys)\r\n\tif len(tks) == 0 {\r\n\t\treturn nil\r\n\t}\r\n\t// Walk all the keys and make sure they are valid.\r\n\tfor _, key := range tks {\r\n\t\tif !nkeys.IsValidPublicOperatorKey(key) {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t}\r\n\treturn tks\r\n}\r\n\r\n// initStampedTrustedKeys will check the stamped trusted keys\r\n// and will set the server field 'trustedKeys'. Returns whether\r\n// it succeeded or not.\r\nfunc (s *Server) initStampedTrustedKeys() bool {\r\n\t// Check to see if we have an override in options, which will cause us to fail.\r\n\tif len(s.opts.TrustedKeys) > 0 {\r\n\t\treturn false\r\n\t}\r\n\ttks := checkTrustedKeyString(trustedKeys)\r\n\tif len(tks) == 0 {\r\n\t\treturn false\r\n\t}\r\n\ts.trustedKeys = tks\r\n\treturn true\r\n}\r\n\r\n// PrintAndDie is exported for access in other packages.\r\nfunc PrintAndDie(msg string) {\r\n\tfmt.Fprintln(os.Stderr, msg)\r\n\tos.Exit(1)\r\n}\r\n\r\n// PrintServerAndExit will print our version and exit.\r\nfunc PrintServerAndExit() {\r\n\tfmt.Printf(\"nats-server: v%s\\n\", VERSION)\r\n\tos.Exit(0)\r\n}\r\n\r\n// ProcessCommandLineArgs takes the command line arguments\r\n// validating and setting flags for handling in case any\r\n// sub command was present.\r\nfunc ProcessCommandLineArgs(cmd *flag.FlagSet) (showVersion bool, showHelp bool, err error) {\r\n\tif len(cmd.Args()) > 0 {\r\n\t\targ := cmd.Args()[0]\r\n\t\tswitch strings.ToLower(arg) {\r\n\t\tcase \"version\":\r\n\t\t\treturn true, false, nil\r\n\t\tcase \"help\":\r\n\t\t\treturn false, true, nil\r\n\t\tdefault:\r\n\t\t\treturn false, false, fmt.Errorf(\"unrecognized command: %q\", arg)\r\n\t\t}\r\n\t}\r\n\r\n\treturn false, false, nil\r\n}\r\n\r\n// Protected check on running state\r\nfunc (s *Server) isRunning() bool {\r\n\ts.mu.Lock()\r\n\trunning := s.running\r\n\ts.mu.Unlock()\r\n\treturn running\r\n}\r\n\r\nfunc (s *Server) logPid() error {\r\n\tpidStr := strconv.Itoa(os.Getpid())\r\n\treturn ioutil.WriteFile(s.getOpts().PidFile, []byte(pidStr), 0660)\r\n}\r\n\r\n// NewAccountsAllowed returns whether or not new accounts can be created on the fly.\r\nfunc (s *Server) NewAccountsAllowed() bool {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.opts.AllowNewAccounts\r\n}\r\n\r\n// numReservedAccounts will return the number of reserved accounts configured in the server.\r\n// Currently this is 1 for the global default service.\r\nfunc (s *Server) numReservedAccounts() int {\r\n\treturn 1\r\n}\r\n\r\n// NumActiveAccounts reports number of active accounts on this server.\r\nfunc (s *Server) NumActiveAccounts() int32 {\r\n\treturn atomic.LoadInt32(&s.activeAccounts)\r\n}\r\n\r\n// incActiveAccounts() just adds one under lock.\r\nfunc (s *Server) incActiveAccounts() {\r\n\tatomic.AddInt32(&s.activeAccounts, 1)\r\n}\r\n\r\n// decActiveAccounts() just subtracts one under lock.\r\nfunc (s *Server) decActiveAccounts() {\r\n\tatomic.AddInt32(&s.activeAccounts, -1)\r\n}\r\n\r\n// This should be used for testing only. Will be slow since we have to\r\n// range over all accounts in the sync.Map to count.\r\nfunc (s *Server) numAccounts() int {\r\n\tcount := 0\r\n\ts.mu.Lock()\r\n\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\tcount++\r\n\t\treturn true\r\n\t})\r\n\ts.mu.Unlock()\r\n\treturn count\r\n}\r\n\r\n// NumLoadedAccounts returns the number of loaded accounts.\r\nfunc (s *Server) NumLoadedAccounts() int {\r\n\treturn s.numAccounts()\r\n}\r\n\r\n// LookupOrRegisterAccount will return the given account if known or create a new entry.\r\nfunc (s *Server) LookupOrRegisterAccount(name string) (account *Account, isNew bool) {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif v, ok := s.accounts.Load(name); ok {\r\n\t\treturn v.(*Account), false\r\n\t}\r\n\tacc := NewAccount(name)\r\n\ts.registerAccountNoLock(acc)\r\n\treturn acc, true\r\n}\r\n\r\n// RegisterAccount will register an account. The account must be new\r\n// or this call will fail.\r\nfunc (s *Server) RegisterAccount(name string) (*Account, error) {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif _, ok := s.accounts.Load(name); ok {\r\n\t\treturn nil, ErrAccountExists\r\n\t}\r\n\tacc := NewAccount(name)\r\n\ts.registerAccountNoLock(acc)\r\n\treturn acc, nil\r\n}\r\n\r\n// SetSystemAccount will set the internal system account.\r\n// If root operators are present it will also check validity.\r\nfunc (s *Server) SetSystemAccount(accName string) error {\r\n\t// Lookup from sync.Map first.\r\n\tif v, ok := s.accounts.Load(accName); ok {\r\n\t\treturn s.setSystemAccount(v.(*Account))\r\n\t}\r\n\r\n\t// If we are here we do not have local knowledge of this account.\r\n\t// Do this one by hand to return more useful error.\r\n\tac, jwt, err := s.fetchAccountClaims(accName)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\tacc := s.buildInternalAccount(ac)\r\n\tacc.claimJWT = jwt\r\n\t// Due to race, we need to make sure that we are not\r\n\t// registering twice.\r\n\tif racc := s.registerAccount(acc); racc != nil {\r\n\t\treturn nil\r\n\t}\r\n\treturn s.setSystemAccount(acc)\r\n}\r\n\r\n// SystemAccount returns the system account if set.\r\nfunc (s *Server) SystemAccount() *Account {\r\n\tvar sacc *Account\r\n\ts.mu.Lock()\r\n\tif s.sys != nil {\r\n\t\tsacc = s.sys.account\r\n\t}\r\n\ts.mu.Unlock()\r\n\treturn sacc\r\n}\r\n\r\n// For internal sends.\r\nconst internalSendQLen = 4096\r\n\r\n// Assign a system account. Should only be called once.\r\n// This sets up a server to send and receive messages from\r\n// inside the server itself.\r\nfunc (s *Server) setSystemAccount(acc *Account) error {\r\n\tif acc == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\t// Don't try to fix this here.\r\n\tif acc.IsExpired() {\r\n\t\treturn ErrAccountExpired\r\n\t}\r\n\t// If we are running with trusted keys for an operator\r\n\t// make sure we check the account is legit.\r\n\tif !s.isTrustedIssuer(acc.Issuer) {\r\n\t\treturn ErrAccountValidation\r\n\t}\r\n\r\n\ts.mu.Lock()\r\n\r\n\tif s.sys != nil {\r\n\t\ts.mu.Unlock()\r\n\t\treturn ErrAccountExists\r\n\t}\r\n\r\n\t// This is here in an attempt to quiet the race detector and not have to place\r\n\t// locks on fast path for inbound messages and checking service imports.\r\n\tacc.mu.Lock()\r\n\tif acc.imports.services == nil {\r\n\t\tacc.imports.services = make(map[string]*serviceImport)\r\n\t}\r\n\tacc.mu.Unlock()\r\n\r\n\tnow := time.Now()\r\n\ts.sys = &internal{\r\n\t\taccount: acc,\r\n\t\tclient:  &client{srv: s, kind: SYSTEM, opts: internalOpts, msubs: -1, mpay: -1, start: now, last: now},\r\n\t\tseq:     1,\r\n\t\tsid:     1,\r\n\t\tservers: make(map[string]*serverUpdate),\r\n\t\tsubs:    make(map[string]msgHandler),\r\n\t\treplies: make(map[string]msgHandler),\r\n\t\tsendq:   make(chan *pubMsg, internalSendQLen),\r\n\t\tresetCh: make(chan struct{}),\r\n\t\tstatsz:  eventsHBInterval,\r\n\t\torphMax: 5 * eventsHBInterval,\r\n\t\tchkOrph: 3 * eventsHBInterval,\r\n\t}\r\n\ts.sys.client.initClient()\r\n\ts.sys.client.echo = false\r\n\ts.sys.wg.Add(1)\r\n\ts.mu.Unlock()\r\n\r\n\t// Register with the account.\r\n\ts.sys.client.registerWithAccount(acc)\r\n\r\n\ts.addSystemAccountExports(acc)\r\n\r\n\t// Start our internal loop to serialize outbound messages.\r\n\t// We do our own wg here since we will stop first during shutdown.\r\n\tgo s.internalSendLoop(&s.sys.wg)\r\n\r\n\t// Start up our general subscriptions\r\n\ts.initEventTracking()\r\n\r\n\t// Track for dead remote servers.\r\n\ts.wrapChk(s.startRemoteServerSweepTimer)()\r\n\r\n\t// Send out statsz updates periodically.\r\n\ts.wrapChk(s.startStatszTimer)()\r\n\r\n\t// If we have existing accounts make sure we enable account tracking.\r\n\ts.mu.Lock()\r\n\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\tacc := v.(*Account)\r\n\t\ts.enableAccountTracking(acc)\r\n\t\treturn true\r\n\t})\r\n\ts.mu.Unlock()\r\n\r\n\treturn nil\r\n}\r\n\r\n// Determine if accounts should track subscriptions for\r\n// efficient propagation.\r\n// Lock should be held on entry.\r\nfunc (s *Server) shouldTrackSubscriptions() bool {\r\n\treturn (s.opts.Cluster.Port != 0 || s.opts.Gateway.Port != 0)\r\n}\r\n\r\n// Invokes registerAccountNoLock under the protection of the server lock.\r\n// That is, server lock is acquired/released in this function.\r\n// See registerAccountNoLock for comment on returned value.\r\nfunc (s *Server) registerAccount(acc *Account) *Account {\r\n\ts.mu.Lock()\r\n\tracc := s.registerAccountNoLock(acc)\r\n\ts.mu.Unlock()\r\n\treturn racc\r\n}\r\n\r\n// Helper to set the sublist based on preferences.\r\nfunc (s *Server) setAccountSublist(acc *Account) {\r\n\tif acc != nil && acc.sl == nil {\r\n\t\topts := s.getOpts()\r\n\t\tif opts != nil && opts.NoSublistCache {\r\n\t\t\tacc.sl = NewSublistNoCache()\r\n\t\t} else {\r\n\t\t\tacc.sl = NewSublistWithCache()\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Registers an account in the server.\r\n// Due to some locking considerations, we may end-up trying\r\n// to register the same account twice. This function will\r\n// then return the already registered account.\r\n// Lock should be held on entry.\r\nfunc (s *Server) registerAccountNoLock(acc *Account) *Account {\r\n\t// We are under the server lock. Lookup from map, if present\r\n\t// return existing account.\r\n\tif a, _ := s.accounts.Load(acc.Name); a != nil {\r\n\t\ts.tmpAccounts.Delete(acc.Name)\r\n\t\treturn a.(*Account)\r\n\t}\r\n\t// Finish account setup and store.\r\n\ts.setAccountSublist(acc)\r\n\tif acc.maxnae == 0 {\r\n\t\tacc.maxnae = DEFAULT_MAX_ACCOUNT_AE_RESPONSE_MAPS\r\n\t}\r\n\tif acc.maxaettl == 0 {\r\n\t\tacc.maxaettl = DEFAULT_TTL_AE_RESPONSE_MAP\r\n\t}\r\n\tif acc.maxnrm == 0 {\r\n\t\tacc.maxnrm = DEFAULT_MAX_ACCOUNT_INTERNAL_RESPONSE_MAPS\r\n\t}\r\n\tif acc.clients == nil {\r\n\t\tacc.clients = make(map[*client]struct{})\r\n\t}\r\n\t// If we are capable of routing we will track subscription\r\n\t// information for efficient interest propagation.\r\n\t// During config reload, it is possible that account was\r\n\t// already created (global account), so use locking and\r\n\t// make sure we create only if needed.\r\n\tacc.mu.Lock()\r\n\t// TODO(dlc)- Double check that we need this for GWs.\r\n\tif acc.rm == nil && s.opts != nil && s.shouldTrackSubscriptions() {\r\n\t\tacc.rm = make(map[string]int32)\r\n\t\tacc.lqws = make(map[string]int32)\r\n\t}\r\n\tacc.srv = s\r\n\tacc.mu.Unlock()\r\n\ts.accounts.Store(acc.Name, acc)\r\n\ts.tmpAccounts.Delete(acc.Name)\r\n\ts.enableAccountTracking(acc)\r\n\treturn nil\r\n}\r\n\r\n// lookupAccount is a function to return the account structure\r\n// associated with an account name.\r\n// Lock MUST NOT be held upon entry.\r\nfunc (s *Server) lookupAccount(name string) (*Account, error) {\r\n\tvar acc *Account\r\n\tif v, ok := s.accounts.Load(name); ok {\r\n\t\tacc = v.(*Account)\r\n\t} else if v, ok := s.tmpAccounts.Load(name); ok {\r\n\t\tacc = v.(*Account)\r\n\t}\r\n\tif acc != nil {\r\n\t\t// If we are expired and we have a resolver, then\r\n\t\t// return the latest information from the resolver.\r\n\t\tif acc.IsExpired() {\r\n\t\t\ts.Debugf(\"Requested account [%s] has expired\", name)\r\n\t\t\tif s.AccountResolver() != nil {\r\n\t\t\t\tif err := s.updateAccount(acc); err != nil {\r\n\t\t\t\t\t// This error could mask expired, so just return expired here.\r\n\t\t\t\t\treturn nil, ErrAccountExpired\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\treturn nil, ErrAccountExpired\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn acc, nil\r\n\t}\r\n\t// If we have a resolver see if it can fetch the account.\r\n\tif s.AccountResolver() == nil {\r\n\t\treturn nil, ErrMissingAccount\r\n\t}\r\n\treturn s.fetchAccount(name)\r\n}\r\n\r\n// LookupAccount is a public function to return the account structure\r\n// associated with name.\r\nfunc (s *Server) LookupAccount(name string) (*Account, error) {\r\n\treturn s.lookupAccount(name)\r\n}\r\n\r\n// This will fetch new claims and if found update the account with new claims.\r\n// Lock MUST NOT be held upon entry.\r\nfunc (s *Server) updateAccount(acc *Account) error {\r\n\t// TODO(dlc) - Make configurable\r\n\tif time.Since(acc.updated) < time.Second {\r\n\t\ts.Debugf(\"Requested account update for [%s] ignored, too soon\", acc.Name)\r\n\t\treturn ErrAccountResolverUpdateTooSoon\r\n\t}\r\n\tclaimJWT, err := s.fetchRawAccountClaims(acc.Name)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\treturn s.updateAccountWithClaimJWT(acc, claimJWT)\r\n}\r\n\r\n// updateAccountWithClaimJWT will check and apply the claim update.\r\n// Lock MUST NOT be held upon entry.\r\nfunc (s *Server) updateAccountWithClaimJWT(acc *Account, claimJWT string) error {\r\n\tif acc == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\tif acc.claimJWT != \"\" && acc.claimJWT == claimJWT && !acc.incomplete {\r\n\t\ts.Debugf(\"Requested account update for [%s], same claims detected\", acc.Name)\r\n\t\treturn ErrAccountResolverSameClaims\r\n\t}\r\n\taccClaims, _, err := s.verifyAccountClaims(claimJWT)\r\n\tif err == nil && accClaims != nil {\r\n\t\tacc.claimJWT = claimJWT\r\n\t\ts.updateAccountClaims(acc, accClaims)\r\n\t\treturn nil\r\n\t}\r\n\treturn err\r\n}\r\n\r\n// fetchRawAccountClaims will grab raw account claims iff we have a resolver.\r\n// Lock is NOT held upon entry.\r\nfunc (s *Server) fetchRawAccountClaims(name string) (string, error) {\r\n\taccResolver := s.AccountResolver()\r\n\tif accResolver == nil {\r\n\t\treturn \"\", ErrNoAccountResolver\r\n\t}\r\n\t// Need to do actual Fetch\r\n\tstart := time.Now()\r\n\tclaimJWT, err := accResolver.Fetch(name)\r\n\tfetchTime := time.Since(start)\r\n\tif fetchTime > time.Second {\r\n\t\ts.Warnf(\"Account [%s] fetch took %v\", name, fetchTime)\r\n\t} else {\r\n\t\ts.Debugf(\"Account [%s] fetch took %v\", name, fetchTime)\r\n\t}\r\n\tif err != nil {\r\n\t\ts.Warnf(\"Account fetch failed: %v\", err)\r\n\t\treturn \"\", err\r\n\t}\r\n\treturn claimJWT, nil\r\n}\r\n\r\n// fetchAccountClaims will attempt to fetch new claims if a resolver is present.\r\n// Lock is NOT held upon entry.\r\nfunc (s *Server) fetchAccountClaims(name string) (*jwt.AccountClaims, string, error) {\r\n\tclaimJWT, err := s.fetchRawAccountClaims(name)\r\n\tif err != nil {\r\n\t\treturn nil, _EMPTY_, err\r\n\t}\r\n\treturn s.verifyAccountClaims(claimJWT)\r\n}\r\n\r\n// verifyAccountClaims will decode and validate any account claims.\r\nfunc (s *Server) verifyAccountClaims(claimJWT string) (*jwt.AccountClaims, string, error) {\r\n\taccClaims, err := jwt.DecodeAccountClaims(claimJWT)\r\n\tif err != nil {\r\n\t\treturn nil, _EMPTY_, err\r\n\t}\r\n\tif !s.isTrustedIssuer(accClaims.Issuer) {\r\n\t\treturn nil, _EMPTY_, ErrAccountValidation\r\n\t}\r\n\tvr := jwt.CreateValidationResults()\r\n\taccClaims.Validate(vr)\r\n\tif vr.IsBlocking(true) {\r\n\t\treturn nil, _EMPTY_, ErrAccountValidation\r\n\t}\r\n\treturn accClaims, claimJWT, nil\r\n}\r\n\r\n// This will fetch an account from a resolver if defined.\r\n// Lock is NOT held upon entry.\r\nfunc (s *Server) fetchAccount(name string) (*Account, error) {\r\n\taccClaims, claimJWT, err := s.fetchAccountClaims(name)\r\n\tif accClaims != nil {\r\n\t\tacc := s.buildInternalAccount(accClaims)\r\n\t\tacc.claimJWT = claimJWT\r\n\t\t// Due to possible race, if registerAccount() returns a non\r\n\t\t// nil account, it means the same account was already\r\n\t\t// registered and we should use this one.\r\n\t\tif racc := s.registerAccount(acc); racc != nil {\r\n\t\t\t// Update with the new claims in case they are new.\r\n\t\t\t// Following call will return ErrAccountResolverSameClaims\r\n\t\t\t// if claims are the same.\r\n\t\t\terr = s.updateAccountWithClaimJWT(racc, claimJWT)\r\n\t\t\tif err != nil && err != ErrAccountResolverSameClaims {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\treturn racc, nil\r\n\t\t}\r\n\t\treturn acc, nil\r\n\t}\r\n\treturn nil, err\r\n}\r\n\r\n// Start up the server, this will block.\r\n// Start via a Go routine if needed.\r\nfunc (s *Server) Start() {\r\n\ts.Noticef(\"Starting nats-server version %s\", VERSION)\r\n\ts.Debugf(\"Go build version %s\", s.info.GoVersion)\r\n\tgc := gitCommit\r\n\tif gc == \"\" {\r\n\t\tgc = \"not set\"\r\n\t}\r\n\ts.Noticef(\"Git commit [%s]\", gc)\r\n\r\n\t// Check for insecure configurations.op\r\n\ts.checkAuthforWarnings()\r\n\r\n\t// Avoid RACE between Start() and Shutdown()\r\n\ts.mu.Lock()\r\n\ts.running = true\r\n\ts.mu.Unlock()\r\n\r\n\ts.grMu.Lock()\r\n\ts.grRunning = true\r\n\ts.grMu.Unlock()\r\n\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\thasOperators := len(opts.TrustedOperators) > 0\r\n\tif hasOperators {\r\n\t\ts.Noticef(\"Trusted Operators\")\r\n\t}\r\n\tfor _, opc := range opts.TrustedOperators {\r\n\t\ts.Noticef(\"  System  : %q\", opc.Audience)\r\n\t\ts.Noticef(\"  Operator: %q\", opc.Name)\r\n\t\ts.Noticef(\"  Issued  : %v\", time.Unix(opc.IssuedAt, 0))\r\n\t\ts.Noticef(\"  Expires : %v\", time.Unix(opc.Expires, 0))\r\n\t}\r\n\tif hasOperators && opts.SystemAccount == _EMPTY_ {\r\n\t\ts.Warnf(\"Trusted Operators should utilize a System Account\")\r\n\t}\r\n\r\n\t// If we have a memory resolver, check the accounts here for validation exceptions.\r\n\t// This allows them to be logged right away vs when they are accessed via a client.\r\n\tif hasOperators && len(opts.resolverPreloads) > 0 {\r\n\t\ts.checkResolvePreloads()\r\n\t}\r\n\r\n\t// Log the pid to a file\r\n\tif opts.PidFile != _EMPTY_ {\r\n\t\tif err := s.logPid(); err != nil {\r\n\t\t\tPrintAndDie(fmt.Sprintf(\"Could not write pidfile: %v\\n\", err))\r\n\t\t}\r\n\t}\r\n\r\n\t// Start monitoring if needed\r\n\tif err := s.StartMonitoring(); err != nil {\r\n\t\ts.Fatalf(\"Can't start monitoring: %v\", err)\r\n\t\treturn\r\n\t}\r\n\r\n\t// Setup system account which will start the eventing stack.\r\n\tif sa := opts.SystemAccount; sa != _EMPTY_ {\r\n\t\tif err := s.SetSystemAccount(sa); err != nil {\r\n\t\t\ts.Fatalf(\"Can't set system account: %v\", err)\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n\r\n\t// Start expiration of mapped GW replies, regardless if\r\n\t// this server is configured with gateway or not.\r\n\ts.startGWReplyMapExpiration()\r\n\r\n\t// Start up gateway if needed. Do this before starting the routes, because\r\n\t// we want to resolve the gateway host:port so that this information can\r\n\t// be sent to other routes.\r\n\tif opts.Gateway.Port != 0 {\r\n\t\ts.startGateways()\r\n\t}\r\n\r\n\t// Start up listen if we want to accept leaf node connections.\r\n\tif opts.LeafNode.Port != 0 {\r\n\t\t// Spin up the accept loop if needed\r\n\t\tch := make(chan struct{})\r\n\t\tgo s.leafNodeAcceptLoop(ch)\r\n\t\t// This ensure that we have resolved or assigned the advertise\r\n\t\t// address for the leafnode listener. We need that in StartRouting().\r\n\t\t<-ch\r\n\t}\r\n\r\n\t// Solicit remote servers for leaf node connections.\r\n\tif len(opts.LeafNode.Remotes) > 0 {\r\n\t\ts.solicitLeafNodeRemotes(opts.LeafNode.Remotes)\r\n\t}\r\n\r\n\t// The Routing routine needs to wait for the client listen\r\n\t// port to be opened and potential ephemeral port selected.\r\n\tclientListenReady := make(chan struct{})\r\n\r\n\t// Start up routing as well if needed.\r\n\tif opts.Cluster.Port != 0 {\r\n\t\ts.startGoRoutine(func() {\r\n\t\t\ts.StartRouting(clientListenReady)\r\n\t\t})\r\n\t}\r\n\r\n\t// Pprof http endpoint for the profiler.\r\n\tif opts.ProfPort != 0 {\r\n\t\ts.StartProfiler()\r\n\t}\r\n\r\n\tif opts.PortsFileDir != _EMPTY_ {\r\n\t\ts.logPorts()\r\n\t}\r\n\r\n\t// Wait for clients.\r\n\ts.AcceptLoop(clientListenReady)\r\n}\r\n\r\n// WaitForShutdown will block until the server has been fully shutdown.\r\nfunc (s *Server) WaitForShutdown() {\r\n\t<-s.shutdownComplete\r\n}\r\n\r\n// This function sets the server's info Host/Port based on server Options.\r\n// Note that this function may be called during config reload, this is why\r\n// Host/Port may be reset to original Options if the ClientAdvertise option\r\n// is not set (since it may have previously been).\r\n// The function then generates the server infoJSON.\r\nfunc (s *Server) setInfoHostPortAndGenerateJSON() error {\r\n\t// When this function is called, opts.Port is set to the actual listen\r\n\t// port (if option was originally set to RANDOM), even during a config\r\n\t// reload. So use of s.opts.Port is safe.\r\n\tif s.opts.ClientAdvertise != \"\" {\r\n\t\th, p, err := parseHostPort(s.opts.ClientAdvertise, s.opts.Port)\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\ts.info.Host = h\r\n\t\ts.info.Port = p\r\n\t} else {\r\n\t\ts.info.Host = s.opts.Host\r\n\t\ts.info.Port = s.opts.Port\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// StartProfiler is called to enable dynamic profiling.\r\nfunc (s *Server) StartProfiler() {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\tport := opts.ProfPort\r\n\r\n\t// Check for Random Port\r\n\tif port == -1 {\r\n\t\tport = 0\r\n\t}\r\n\r\n\thp := net.JoinHostPort(opts.Host, strconv.Itoa(port))\r\n\r\n\tl, err := net.Listen(\"tcp\", hp)\r\n\ts.Noticef(\"profiling port: %d\", l.Addr().(*net.TCPAddr).Port)\r\n\r\n\tif err != nil {\r\n\t\ts.Fatalf(\"error starting profiler: %s\", err)\r\n\t}\r\n\r\n\tsrv := &http.Server{\r\n\t\tAddr:           hp,\r\n\t\tHandler:        http.DefaultServeMux,\r\n\t\tMaxHeaderBytes: 1 << 20,\r\n\t}\r\n\r\n\ts.mu.Lock()\r\n\ts.profiler = l\r\n\ts.profilingServer = srv\r\n\ts.mu.Unlock()\r\n\r\n\t// Enable blocking profile\r\n\truntime.SetBlockProfileRate(1)\r\n\r\n\tgo func() {\r\n\t\t// if this errors out, it's probably because the server is being shutdown\r\n\t\terr := srv.Serve(l)\r\n\t\tif err != nil {\r\n\t\t\ts.mu.Lock()\r\n\t\t\tshutdown := s.shutdown\r\n\t\t\ts.mu.Unlock()\r\n\t\t\tif !shutdown {\r\n\t\t\t\ts.Fatalf(\"error starting profiler: %s\", err)\r\n\t\t\t}\r\n\t\t}\r\n\t\tsrv.Close()\r\n\t\ts.done <- true\r\n\t}()\r\n}\r\n\r\n// StartHTTPMonitoring will enable the HTTP monitoring port.\r\n// DEPRECATED: Should use StartMonitoring.\r\nfunc (s *Server) StartHTTPMonitoring() {\r\n\ts.startMonitoring(false)\r\n}\r\n\r\n// StartHTTPSMonitoring will enable the HTTPS monitoring port.\r\n// DEPRECATED: Should use StartMonitoring.\r\nfunc (s *Server) StartHTTPSMonitoring() {\r\n\ts.startMonitoring(true)\r\n}\r\n\r\n// StartMonitoring starts the HTTP or HTTPs server if needed.\r\nfunc (s *Server) StartMonitoring() error {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\t// Specifying both HTTP and HTTPS ports is a misconfiguration\r\n\tif opts.HTTPPort != 0 && opts.HTTPSPort != 0 {\r\n\t\treturn fmt.Errorf(\"can't specify both HTTP (%v) and HTTPs (%v) ports\", opts.HTTPPort, opts.HTTPSPort)\r\n\t}\r\n\tvar err error\r\n\tif opts.HTTPPort != 0 {\r\n\t\terr = s.startMonitoring(false)\r\n\t} else if opts.HTTPSPort != 0 {\r\n\t\tif opts.TLSConfig == nil {\r\n\t\t\treturn fmt.Errorf(\"TLS cert and key required for HTTPS\")\r\n\t\t}\r\n\t\terr = s.startMonitoring(true)\r\n\t}\r\n\treturn err\r\n}\r\n\r\n// HTTP endpoints\r\nconst (\r\n\tRootPath     = \"/\"\r\n\tVarzPath     = \"/varz\"\r\n\tConnzPath    = \"/connz\"\r\n\tRoutezPath   = \"/routez\"\r\n\tGatewayzPath = \"/gatewayz\"\r\n\tLeafzPath    = \"/leafz\"\r\n\tSubszPath    = \"/subsz\"\r\n\tStackszPath  = \"/stacksz\"\r\n)\r\n\r\nfunc (s *Server) basePath(p string) string {\r\n\treturn path.Join(s.httpBasePath, p)\r\n}\r\n\r\n// Start the monitoring server\r\nfunc (s *Server) startMonitoring(secure bool) error {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\t// Used to track HTTP requests\r\n\ts.httpReqStats = map[string]uint64{\r\n\t\tRootPath:     0,\r\n\t\tVarzPath:     0,\r\n\t\tConnzPath:    0,\r\n\t\tRoutezPath:   0,\r\n\t\tGatewayzPath: 0,\r\n\t\tSubszPath:    0,\r\n\t}\r\n\r\n\tvar (\r\n\t\thp           string\r\n\t\terr          error\r\n\t\thttpListener net.Listener\r\n\t\tport         int\r\n\t)\r\n\r\n\tmonitorProtocol := \"http\"\r\n\r\n\tif secure {\r\n\t\tmonitorProtocol += \"s\"\r\n\t\tport = opts.HTTPSPort\r\n\t\tif port == -1 {\r\n\t\t\tport = 0\r\n\t\t}\r\n\t\thp = net.JoinHostPort(opts.HTTPHost, strconv.Itoa(port))\r\n\t\tconfig := opts.TLSConfig.Clone()\r\n\t\tconfig.ClientAuth = tls.NoClientCert\r\n\t\thttpListener, err = tls.Listen(\"tcp\", hp, config)\r\n\r\n\t} else {\r\n\t\tport = opts.HTTPPort\r\n\t\tif port == -1 {\r\n\t\t\tport = 0\r\n\t\t}\r\n\t\thp = net.JoinHostPort(opts.HTTPHost, strconv.Itoa(port))\r\n\t\thttpListener, err = net.Listen(\"tcp\", hp)\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"can't listen to the monitor port: %v\", err)\r\n\t}\r\n\r\n\ts.Noticef(\"Starting %s monitor on %s\", monitorProtocol,\r\n\t\tnet.JoinHostPort(opts.HTTPHost, strconv.Itoa(httpListener.Addr().(*net.TCPAddr).Port)))\r\n\r\n\tmux := http.NewServeMux()\r\n\r\n\t// Root\r\n\tmux.HandleFunc(s.basePath(RootPath), s.HandleRoot)\r\n\t// Varz\r\n\tmux.HandleFunc(s.basePath(VarzPath), s.HandleVarz)\r\n\t// Connz\r\n\tmux.HandleFunc(s.basePath(ConnzPath), s.HandleConnz)\r\n\t// Routez\r\n\tmux.HandleFunc(s.basePath(RoutezPath), s.HandleRoutez)\r\n\t// Gatewayz\r\n\tmux.HandleFunc(s.basePath(GatewayzPath), s.HandleGatewayz)\r\n\t// Leafz\r\n\tmux.HandleFunc(s.basePath(LeafzPath), s.HandleLeafz)\r\n\t// Subz\r\n\tmux.HandleFunc(s.basePath(SubszPath), s.HandleSubsz)\r\n\t// Subz alias for backwards compatibility\r\n\tmux.HandleFunc(s.basePath(\"/subscriptionsz\"), s.HandleSubsz)\r\n\t// Stacksz\r\n\tmux.HandleFunc(s.basePath(StackszPath), s.HandleStacksz)\r\n\r\n\t// Do not set a WriteTimeout because it could cause cURL/browser\r\n\t// to return empty response or unable to display page if the\r\n\t// server needs more time to build the response.\r\n\tsrv := &http.Server{\r\n\t\tAddr:           hp,\r\n\t\tHandler:        mux,\r\n\t\tMaxHeaderBytes: 1 << 20,\r\n\t}\r\n\ts.mu.Lock()\r\n\ts.http = httpListener\r\n\ts.httpHandler = mux\r\n\ts.monitoringServer = srv\r\n\ts.mu.Unlock()\r\n\r\n\tgo func() {\r\n\t\tif err := srv.Serve(httpListener); err != nil {\r\n\t\t\ts.mu.Lock()\r\n\t\t\tshutdown := s.shutdown\r\n\t\t\ts.mu.Unlock()\r\n\t\t\tif !shutdown {\r\n\t\t\t\ts.Fatalf(\"Error starting monitor on %q: %v\", hp, err)\r\n\t\t\t}\r\n\t\t}\r\n\t\tsrv.Close()\r\n\t\tsrv.Handler = nil\r\n\t\ts.mu.Lock()\r\n\t\ts.httpHandler = nil\r\n\t\ts.mu.Unlock()\r\n\t\ts.done <- true\r\n\t}()\r\n\r\n\treturn nil\r\n}\r\n\r\n// HTTPHandler returns the http.Handler object used to handle monitoring\r\n// endpoints. It will return nil if the server is not configured for\r\n// monitoring, or if the server has not been started yet (Server.Start()).\r\nfunc (s *Server) HTTPHandler() http.Handler {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.httpHandler\r\n}\r\n\r\n// Perform a conditional deep copy due to reference nature of ClientConnectURLs.\r\n// If updates are made to Info, this function should be consulted and updated.\r\n// Assume lock is held.\r\nfunc (s *Server) copyInfo() Info {\r\n\tinfo := s.info\r\n\tif info.ClientConnectURLs != nil {\r\n\t\tinfo.ClientConnectURLs = make([]string, len(s.info.ClientConnectURLs))\r\n\t\tcopy(info.ClientConnectURLs, s.info.ClientConnectURLs)\r\n\t}\r\n\tif s.nonceRequired() {\r\n\t\t// Nonce handling\r\n\t\tvar raw [nonceLen]byte\r\n\t\tnonce := raw[:]\r\n\t\ts.generateNonce(nonce)\r\n\t\tinfo.Nonce = string(nonce)\r\n\t}\r\n\treturn info\r\n}\r\n\r\nfunc (s *Server) createClient(conn net.Conn) *client {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\tmaxPay := int32(opts.MaxPayload)\r\n\tmaxSubs := int32(opts.MaxSubs)\r\n\t// For system, maxSubs of 0 means unlimited, so re-adjust here.\r\n\tif maxSubs == 0 {\r\n\t\tmaxSubs = -1\r\n\t}\r\n\tnow := time.Now()\r\n\r\n\tc := &client{srv: s, nc: conn, opts: defaultOpts, mpay: maxPay, msubs: maxSubs, start: now, last: now}\r\n\r\n\tc.registerWithAccount(s.globalAccount())\r\n\r\n\t// Grab JSON info string\r\n\ts.mu.Lock()\r\n\tinfo := s.copyInfo()\r\n\tc.nonce = []byte(info.Nonce)\r\n\ts.totalClients++\r\n\ts.mu.Unlock()\r\n\r\n\t// Grab lock\r\n\tc.mu.Lock()\r\n\tif info.AuthRequired {\r\n\t\tc.flags.set(expectConnect)\r\n\t}\r\n\r\n\t// Initialize\r\n\tc.initClient()\r\n\r\n\tc.Debugf(\"Client connection created\")\r\n\r\n\t// Send our information.\r\n\t// Need to be sent in place since writeLoop cannot be started until\r\n\t// TLS handshake is done (if applicable).\r\n\tc.sendProtoNow(c.generateClientInfoJSON(info))\r\n\r\n\t// Unlock to register\r\n\tc.mu.Unlock()\r\n\r\n\t// Register with the server.\r\n\ts.mu.Lock()\r\n\t// If server is not running, Shutdown() may have already gathered the\r\n\t// list of connections to close. It won't contain this one, so we need\r\n\t// to bail out now otherwise the readLoop started down there would not\r\n\t// be interrupted. Skip also if in lame duck mode.\r\n\tif !s.running || s.ldm {\r\n\t\ts.mu.Unlock()\r\n\t\treturn c\r\n\t}\r\n\r\n\t// If there is a max connections specified, check that adding\r\n\t// this new client would not push us over the max\r\n\tif opts.MaxConn > 0 && len(s.clients) >= opts.MaxConn {\r\n\t\ts.mu.Unlock()\r\n\t\tc.maxConnExceeded()\r\n\t\treturn nil\r\n\t}\r\n\ts.clients[c.cid] = c\r\n\ts.mu.Unlock()\r\n\r\n\t// Re-Grab lock\r\n\tc.mu.Lock()\r\n\r\n\t// Connection could have been closed while sending the INFO proto,\r\n\t// or during a server shutdown (since already added to s.clients).\r\n\tisClosed := c.isClosed()\r\n\r\n\t// Check for TLS\r\n\tif !isClosed && info.TLSRequired {\r\n\t\tc.Debugf(\"Starting TLS client connection handshake\")\r\n\t\tc.nc = tls.Server(c.nc, opts.TLSConfig)\r\n\t\tconn := c.nc.(*tls.Conn)\r\n\r\n\t\t// Setup the timeout\r\n\t\tttl := secondsToDuration(opts.TLSTimeout)\r\n\t\ttime.AfterFunc(ttl, func() { tlsTimeout(c, conn) })\r\n\t\tconn.SetReadDeadline(time.Now().Add(ttl))\r\n\r\n\t\t// Force handshake\r\n\t\tc.mu.Unlock()\r\n\t\tif err := conn.Handshake(); err != nil {\r\n\t\t\tc.Errorf(\"TLS handshake error: %v\", err)\r\n\t\t\tc.closeConnection(TLSHandshakeError)\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\t// Reset the read deadline\r\n\t\tconn.SetReadDeadline(time.Time{})\r\n\r\n\t\t// Re-Grab lock\r\n\t\tc.mu.Lock()\r\n\r\n\t\t// Indicate that handshake is complete (used in monitoring)\r\n\t\tc.flags.set(handshakeComplete)\r\n\r\n\t\t// The connection may have been closed\r\n\t\tisClosed = c.isClosed()\r\n\t}\r\n\r\n\t// If connection is marked as closed, bail out.\r\n\tif isClosed {\r\n\t\tc.mu.Unlock()\r\n\t\t// If it was due to TLS timeout, teardownConn() has already been called.\r\n\t\t// Otherwise, if connection was marked as closed while sending the INFO,\r\n\t\t// we need to call teardownConn() directly here.\r\n\t\tif !info.TLSRequired {\r\n\t\t\tc.teardownConn()\r\n\t\t}\r\n\t\treturn c\r\n\t}\r\n\r\n\t// Check for Auth. We schedule this timer after the TLS handshake to avoid\r\n\t// the race where the timer fires during the handshake and causes the\r\n\t// server to write bad data to the socket. See issue #432.\r\n\tif info.AuthRequired {\r\n\t\tc.setAuthTimer(secondsToDuration(opts.AuthTimeout))\r\n\t}\r\n\r\n\t// Do final client initialization\r\n\r\n\t// Set the Ping timer. Will be reset once connect was received.\r\n\tc.setPingTimer()\r\n\r\n\t// Spin up the read loop.\r\n\ts.startGoRoutine(func() { c.readLoop() })\r\n\r\n\t// Spin up the write loop.\r\n\ts.startGoRoutine(func() { c.writeLoop() })\r\n\r\n\tif info.TLSRequired {\r\n\t\tc.Debugf(\"TLS handshake complete\")\r\n\t\tcs := c.nc.(*tls.Conn).ConnectionState()\r\n\t\tc.Debugf(\"TLS version %s, cipher suite %s\", tlsVersion(cs.Version), tlsCipher(cs.CipherSuite))\r\n\t}\r\n\r\n\tc.mu.Unlock()\r\n\r\n\treturn c\r\n}\r\n\r\n// This will save off a closed client in a ring buffer such that\r\n// /connz can inspect. Useful for debugging, etc.\r\nfunc (s *Server) saveClosedClient(c *client, nc net.Conn, reason ClosedState) {\r\n\tnow := time.Now()\r\n\r\n\ts.accountDisconnectEvent(c, now, reason.String())\r\n\r\n\tc.mu.Lock()\r\n\r\n\tcc := &closedClient{}\r\n\tcc.fill(c, nc, now)\r\n\tcc.Stop = &now\r\n\tcc.Reason = reason.String()\r\n\r\n\t// Do subs, do not place by default in main ConnInfo\r\n\tif len(c.subs) > 0 {\r\n\t\tcc.subs = make([]SubDetail, 0, len(c.subs))\r\n\t\tfor _, sub := range c.subs {\r\n\t\t\tcc.subs = append(cc.subs, newSubDetail(sub))\r\n\t\t}\r\n\t}\r\n\t// Hold user as well.\r\n\tcc.user = c.opts.Username\r\n\t// Hold account name if not the global account.\r\n\tif c.acc != nil && c.acc.Name != globalAccountName {\r\n\t\tcc.acc = c.acc.Name\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\t// Place in the ring buffer\r\n\ts.mu.Lock()\r\n\tif s.closed != nil {\r\n\t\ts.closed.append(cc)\r\n\t}\r\n\ts.mu.Unlock()\r\n}\r\n\r\n// Adds the given array of urls to the server's INFO.ClientConnectURLs\r\n// array. The server INFO JSON is regenerated.\r\n// Note that a check is made to ensure that given URLs are not\r\n// already present. So the INFO JSON is regenerated only if new ULRs\r\n// were added.\r\n// If there was a change, an INFO protocol is sent to registered clients\r\n// that support async INFO protocols.\r\nfunc (s *Server) addClientConnectURLsAndSendINFOToClients(urls []string) {\r\n\ts.updateServerINFOAndSendINFOToClients(urls, true)\r\n}\r\n\r\n// Removes the given array of urls from the server's INFO.ClientConnectURLs\r\n// array. The server INFO JSON is regenerated if needed.\r\n// If there was a change, an INFO protocol is sent to registered clients\r\n// that support async INFO protocols.\r\nfunc (s *Server) removeClientConnectURLsAndSendINFOToClients(urls []string) {\r\n\ts.updateServerINFOAndSendINFOToClients(urls, false)\r\n}\r\n\r\n// Updates the server's Info object with the given array of URLs and re-generate\r\n// the infoJSON byte array, then send an (async) INFO protocol to clients that\r\n// support it.\r\nfunc (s *Server) updateServerINFOAndSendINFOToClients(urls []string, add bool) {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\r\n\t// Will be set to true if we alter the server's Info object.\r\n\twasUpdated := false\r\n\tremove := !add\r\n\tfor _, url := range urls {\r\n\t\t_, present := s.clientConnectURLsMap[url]\r\n\t\tif add && !present {\r\n\t\t\ts.clientConnectURLsMap[url] = struct{}{}\r\n\t\t\twasUpdated = true\r\n\t\t} else if remove && present {\r\n\t\t\tdelete(s.clientConnectURLsMap, url)\r\n\t\t\twasUpdated = true\r\n\t\t}\r\n\t}\r\n\tif wasUpdated {\r\n\t\t// Recreate the info.ClientConnectURL array from the map\r\n\t\ts.info.ClientConnectURLs = s.info.ClientConnectURLs[:0]\r\n\t\t// Add this server client connect ULRs first...\r\n\t\ts.info.ClientConnectURLs = append(s.info.ClientConnectURLs, s.clientConnectURLs...)\r\n\t\tfor url := range s.clientConnectURLsMap {\r\n\t\t\ts.info.ClientConnectURLs = append(s.info.ClientConnectURLs, url)\r\n\t\t}\r\n\t\t// Update the time of this update\r\n\t\ts.lastCURLsUpdate = time.Now().UnixNano()\r\n\t\t// Send to all registered clients that support async INFO protocols.\r\n\t\ts.sendAsyncInfoToClients()\r\n\t}\r\n}\r\n\r\n// Handle closing down a connection when the handshake has timedout.\r\nfunc tlsTimeout(c *client, conn *tls.Conn) {\r\n\tc.mu.Lock()\r\n\tclosed := c.isClosed()\r\n\tc.mu.Unlock()\r\n\t// Check if already closed\r\n\tif closed {\r\n\t\treturn\r\n\t}\r\n\tcs := conn.ConnectionState()\r\n\tif !cs.HandshakeComplete {\r\n\t\tc.Errorf(\"TLS handshake timeout\")\r\n\t\tc.sendErr(\"Secure Connection - TLS Required\")\r\n\t\tc.closeConnection(TLSHandshakeError)\r\n\t}\r\n}\r\n\r\n// Seems silly we have to write these\r\nfunc tlsVersion(ver uint16) string {\r\n\tswitch ver {\r\n\tcase tls.VersionTLS10:\r\n\t\treturn \"1.0\"\r\n\tcase tls.VersionTLS11:\r\n\t\treturn \"1.1\"\r\n\tcase tls.VersionTLS12:\r\n\t\treturn \"1.2\"\r\n\tcase tls.VersionTLS13:\r\n\t\treturn \"1.3\"\r\n\t}\r\n\treturn fmt.Sprintf(\"Unknown [0x%x]\", ver)\r\n}\r\n\r\n// We use hex here so we don't need multiple versions\r\nfunc tlsCipher(cs uint16) string {\r\n\tname, present := cipherMapByID[cs]\r\n\tif present {\r\n\t\treturn name\r\n\t}\r\n\treturn fmt.Sprintf(\"Unknown [0x%x]\", cs)\r\n}\r\n\r\n// Remove a client or route from our internal accounting.\r\nfunc (s *Server) removeClient(c *client) {\r\n\t// kind is immutable, so can check without lock\r\n\tswitch c.kind {\r\n\tcase CLIENT:\r\n\t\tc.mu.Lock()\r\n\t\tcid := c.cid\r\n\t\tupdateProtoInfoCount := false\r\n\t\tif c.kind == CLIENT && c.opts.Protocol >= ClientProtoInfo {\r\n\t\t\tupdateProtoInfoCount = true\r\n\t\t}\r\n\t\tc.mu.Unlock()\r\n\r\n\t\ts.mu.Lock()\r\n\t\tdelete(s.clients, cid)\r\n\t\tif updateProtoInfoCount {\r\n\t\t\ts.cproto--\r\n\t\t}\r\n\t\ts.mu.Unlock()\r\n\tcase ROUTER:\r\n\t\ts.removeRoute(c)\r\n\tcase GATEWAY:\r\n\t\ts.removeRemoteGatewayConnection(c)\r\n\tcase LEAF:\r\n\t\ts.removeLeafNodeConnection(c)\r\n\t}\r\n}\r\n\r\nfunc (s *Server) removeFromTempClients(cid uint64) {\r\n\ts.grMu.Lock()\r\n\tdelete(s.grTmpClients, cid)\r\n\ts.grMu.Unlock()\r\n}\r\n\r\nfunc (s *Server) addToTempClients(cid uint64, c *client) bool {\r\n\tadded := false\r\n\ts.grMu.Lock()\r\n\tif s.grRunning {\r\n\t\ts.grTmpClients[cid] = c\r\n\t\tadded = true\r\n\t}\r\n\ts.grMu.Unlock()\r\n\treturn added\r\n}\r\n\r\n/////////////////////////////////////////////////////////////////\r\n// These are some helpers for accounting in functional tests.\r\n/////////////////////////////////////////////////////////////////\r\n\r\n// NumRoutes will report the number of registered routes.\r\nfunc (s *Server) NumRoutes() int {\r\n\ts.mu.Lock()\r\n\tnr := len(s.routes)\r\n\ts.mu.Unlock()\r\n\treturn nr\r\n}\r\n\r\n// NumRemotes will report number of registered remotes.\r\nfunc (s *Server) NumRemotes() int {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn len(s.remotes)\r\n}\r\n\r\n// NumLeafNodes will report number of leaf node connections.\r\nfunc (s *Server) NumLeafNodes() int {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn len(s.leafs)\r\n}\r\n\r\n// NumClients will report the number of registered clients.\r\nfunc (s *Server) NumClients() int {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn len(s.clients)\r\n}\r\n\r\n// GetClient will return the client associated with cid.\r\nfunc (s *Server) GetClient(cid uint64) *client {\r\n\treturn s.getClient(cid)\r\n}\r\n\r\n// getClient will return the client associated with cid.\r\nfunc (s *Server) getClient(cid uint64) *client {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.clients[cid]\r\n}\r\n\r\n// GetLeafNode returns the leafnode associated with the cid.\r\nfunc (s *Server) GetLeafNode(cid uint64) *client {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.leafs[cid]\r\n}\r\n\r\n// NumSubscriptions will report how many subscriptions are active.\r\nfunc (s *Server) NumSubscriptions() uint32 {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.numSubscriptions()\r\n}\r\n\r\n// numSubscriptions will report how many subscriptions are active.\r\n// Lock should be held.\r\nfunc (s *Server) numSubscriptions() uint32 {\r\n\tvar subs int\r\n\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\tacc := v.(*Account)\r\n\t\tif acc.sl != nil {\r\n\t\t\tsubs += acc.TotalSubs()\r\n\t\t}\r\n\t\treturn true\r\n\t})\r\n\treturn uint32(subs)\r\n}\r\n\r\n// NumSlowConsumers will report the number of slow consumers.\r\nfunc (s *Server) NumSlowConsumers() int64 {\r\n\treturn atomic.LoadInt64(&s.slowConsumers)\r\n}\r\n\r\n// ConfigTime will report the last time the server configuration was loaded.\r\nfunc (s *Server) ConfigTime() time.Time {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.configTime\r\n}\r\n\r\n// Addr will return the net.Addr object for the current listener.\r\nfunc (s *Server) Addr() net.Addr {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.listener == nil {\r\n\t\treturn nil\r\n\t}\r\n\treturn s.listener.Addr()\r\n}\r\n\r\n// MonitorAddr will return the net.Addr object for the monitoring listener.\r\nfunc (s *Server) MonitorAddr() *net.TCPAddr {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.http == nil {\r\n\t\treturn nil\r\n\t}\r\n\treturn s.http.Addr().(*net.TCPAddr)\r\n}\r\n\r\n// ClusterAddr returns the net.Addr object for the route listener.\r\nfunc (s *Server) ClusterAddr() *net.TCPAddr {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.routeListener == nil {\r\n\t\treturn nil\r\n\t}\r\n\treturn s.routeListener.Addr().(*net.TCPAddr)\r\n}\r\n\r\n// ProfilerAddr returns the net.Addr object for the profiler listener.\r\nfunc (s *Server) ProfilerAddr() *net.TCPAddr {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\tif s.profiler == nil {\r\n\t\treturn nil\r\n\t}\r\n\treturn s.profiler.Addr().(*net.TCPAddr)\r\n}\r\n\r\n// ReadyForConnections returns `true` if the server is ready to accept clients\r\n// and, if routing is enabled, route connections. If after the duration\r\n// `dur` the server is still not ready, returns `false`.\r\nfunc (s *Server) ReadyForConnections(dur time.Duration) bool {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\tend := time.Now().Add(dur)\r\n\tfor time.Now().Before(end) {\r\n\t\ts.mu.Lock()\r\n\t\tok := s.listener != nil && (opts.Cluster.Port == 0 || s.routeListener != nil) && (opts.Gateway.Name == \"\" || s.gatewayListener != nil)\r\n\t\ts.mu.Unlock()\r\n\t\tif ok {\r\n\t\t\treturn true\r\n\t\t}\r\n\t\ttime.Sleep(25 * time.Millisecond)\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// ID returns the server's ID\r\nfunc (s *Server) ID() string {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.info.ID\r\n}\r\n\r\nfunc (s *Server) startGoRoutine(f func()) {\r\n\ts.grMu.Lock()\r\n\tif s.grRunning {\r\n\t\ts.grWG.Add(1)\r\n\t\tgo f()\r\n\t}\r\n\ts.grMu.Unlock()\r\n}\r\n\r\nfunc (s *Server) numClosedConns() int {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.closed.len()\r\n}\r\n\r\nfunc (s *Server) totalClosedConns() uint64 {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.closed.totalConns()\r\n}\r\n\r\nfunc (s *Server) closedClients() []*closedClient {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.closed.closedClients()\r\n}\r\n\r\n// getClientConnectURLs returns suitable URLs for clients to connect to the listen\r\n// port based on the server options' Host and Port. If the Host corresponds to\r\n// \"any\" interfaces, this call returns the list of resolved IP addresses.\r\n// If ClientAdvertise is set, returns the client advertise host and port.\r\n// The server lock is assumed held on entry.\r\nfunc (s *Server) getClientConnectURLs() []string {\r\n\t// Snapshot server options.\r\n\topts := s.getOpts()\r\n\r\n\turls := make([]string, 0, 1)\r\n\r\n\t// short circuit if client advertise is set\r\n\tif opts.ClientAdvertise != \"\" {\r\n\t\t// just use the info host/port. This is updated in s.New()\r\n\t\turls = append(urls, net.JoinHostPort(s.info.Host, strconv.Itoa(s.info.Port)))\r\n\t} else {\r\n\t\tsPort := strconv.Itoa(opts.Port)\r\n\t\t_, ips, err := s.getNonLocalIPsIfHostIsIPAny(opts.Host, true)\r\n\t\tfor _, ip := range ips {\r\n\t\t\turls = append(urls, net.JoinHostPort(ip, sPort))\r\n\t\t}\r\n\t\tif err != nil || len(urls) == 0 {\r\n\t\t\t// We are here if s.opts.Host is not \"0.0.0.0\" nor \"::\", or if for some\r\n\t\t\t// reason we could not add any URL in the loop above.\r\n\t\t\t// We had a case where a Windows VM was hosed and would have err == nil\r\n\t\t\t// and not add any address in the array in the loop above, and we\r\n\t\t\t// ended-up returning 0.0.0.0, which is problematic for Windows clients.\r\n\t\t\t// Check for 0.0.0.0 or :: specifically, and ignore if that's the case.\r\n\t\t\tif opts.Host == \"0.0.0.0\" || opts.Host == \"::\" {\r\n\t\t\t\ts.Errorf(\"Address %q can not be resolved properly\", opts.Host)\r\n\t\t\t} else {\r\n\t\t\t\turls = append(urls, net.JoinHostPort(opts.Host, sPort))\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn urls\r\n}\r\n\r\n// Returns an array of non local IPs if the provided host is\r\n// 0.0.0.0 or ::. It returns the first resolved if `all` is\r\n// false.\r\n// The boolean indicate if the provided host was 0.0.0.0 (or ::)\r\n// so that if the returned array is empty caller can decide\r\n// what to do next.\r\nfunc (s *Server) getNonLocalIPsIfHostIsIPAny(host string, all bool) (bool, []string, error) {\r\n\tip := net.ParseIP(host)\r\n\t// If this is not an IP, we are done\r\n\tif ip == nil {\r\n\t\treturn false, nil, nil\r\n\t}\r\n\t// If this is not 0.0.0.0 or :: we have nothing to do.\r\n\tif !ip.IsUnspecified() {\r\n\t\treturn false, nil, nil\r\n\t}\r\n\ts.Debugf(\"Get non local IPs for %q\", host)\r\n\tvar ips []string\r\n\tifaces, _ := net.Interfaces()\r\n\tfor _, i := range ifaces {\r\n\t\taddrs, _ := i.Addrs()\r\n\t\tfor _, addr := range addrs {\r\n\t\t\tswitch v := addr.(type) {\r\n\t\t\tcase *net.IPNet:\r\n\t\t\t\tip = v.IP\r\n\t\t\tcase *net.IPAddr:\r\n\t\t\t\tip = v.IP\r\n\t\t\t}\r\n\t\t\tipStr := ip.String()\r\n\t\t\t// Skip non global unicast addresses\r\n\t\t\tif !ip.IsGlobalUnicast() || ip.IsUnspecified() {\r\n\t\t\t\tip = nil\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\ts.Debugf(\" ip=%s\", ipStr)\r\n\t\t\tips = append(ips, ipStr)\r\n\t\t\tif !all {\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn true, ips, nil\r\n}\r\n\r\n// if the ip is not specified, attempt to resolve it\r\nfunc resolveHostPorts(addr net.Listener) []string {\r\n\thostPorts := make([]string, 0)\r\n\thp := addr.Addr().(*net.TCPAddr)\r\n\tport := strconv.Itoa(hp.Port)\r\n\tif hp.IP.IsUnspecified() {\r\n\t\tvar ip net.IP\r\n\t\tifaces, _ := net.Interfaces()\r\n\t\tfor _, i := range ifaces {\r\n\t\t\taddrs, _ := i.Addrs()\r\n\t\t\tfor _, addr := range addrs {\r\n\t\t\t\tswitch v := addr.(type) {\r\n\t\t\t\tcase *net.IPNet:\r\n\t\t\t\t\tip = v.IP\r\n\t\t\t\t\thostPorts = append(hostPorts, net.JoinHostPort(ip.String(), port))\r\n\t\t\t\tcase *net.IPAddr:\r\n\t\t\t\t\tip = v.IP\r\n\t\t\t\t\thostPorts = append(hostPorts, net.JoinHostPort(ip.String(), port))\r\n\t\t\t\tdefault:\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t} else {\r\n\t\thostPorts = append(hostPorts, net.JoinHostPort(hp.IP.String(), port))\r\n\t}\r\n\treturn hostPorts\r\n}\r\n\r\n// format the address of a net.Listener with a protocol\r\nfunc formatURL(protocol string, addr net.Listener) []string {\r\n\thostports := resolveHostPorts(addr)\r\n\tfor i, hp := range hostports {\r\n\t\thostports[i] = fmt.Sprintf(\"%s://%s\", protocol, hp)\r\n\t}\r\n\treturn hostports\r\n}\r\n\r\n// Ports describes URLs that the server can be contacted in\r\ntype Ports struct {\r\n\tNats       []string `json:\"nats,omitempty\"`\r\n\tMonitoring []string `json:\"monitoring,omitempty\"`\r\n\tCluster    []string `json:\"cluster,omitempty\"`\r\n\tProfile    []string `json:\"profile,omitempty\"`\r\n}\r\n\r\n// PortsInfo attempts to resolve all the ports. If after maxWait the ports are not\r\n// resolved, it returns nil. Otherwise it returns a Ports struct\r\n// describing ports where the server can be contacted\r\nfunc (s *Server) PortsInfo(maxWait time.Duration) *Ports {\r\n\tif s.readyForListeners(maxWait) {\r\n\t\topts := s.getOpts()\r\n\r\n\t\ts.mu.Lock()\r\n\t\tinfo := s.copyInfo()\r\n\t\tlistener := s.listener\r\n\t\thttpListener := s.http\r\n\t\tclusterListener := s.routeListener\r\n\t\tprofileListener := s.profiler\r\n\t\ts.mu.Unlock()\r\n\r\n\t\tports := Ports{}\r\n\r\n\t\tif listener != nil {\r\n\t\t\tnatsProto := \"nats\"\r\n\t\t\tif info.TLSRequired {\r\n\t\t\t\tnatsProto = \"tls\"\r\n\t\t\t}\r\n\t\t\tports.Nats = formatURL(natsProto, listener)\r\n\t\t}\r\n\r\n\t\tif httpListener != nil {\r\n\t\t\tmonProto := \"http\"\r\n\t\t\tif opts.HTTPSPort != 0 {\r\n\t\t\t\tmonProto = \"https\"\r\n\t\t\t}\r\n\t\t\tports.Monitoring = formatURL(monProto, httpListener)\r\n\t\t}\r\n\r\n\t\tif clusterListener != nil {\r\n\t\t\tclusterProto := \"nats\"\r\n\t\t\tif opts.Cluster.TLSConfig != nil {\r\n\t\t\t\tclusterProto = \"tls\"\r\n\t\t\t}\r\n\t\t\tports.Cluster = formatURL(clusterProto, clusterListener)\r\n\t\t}\r\n\r\n\t\tif profileListener != nil {\r\n\t\t\tports.Profile = formatURL(\"http\", profileListener)\r\n\t\t}\r\n\r\n\t\treturn &ports\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// Returns the portsFile. If a non-empty dirHint is provided, the dirHint\r\n// path is used instead of the server option value\r\nfunc (s *Server) portFile(dirHint string) string {\r\n\tdirname := s.getOpts().PortsFileDir\r\n\tif dirHint != \"\" {\r\n\t\tdirname = dirHint\r\n\t}\r\n\tif dirname == _EMPTY_ {\r\n\t\treturn _EMPTY_\r\n\t}\r\n\treturn filepath.Join(dirname, fmt.Sprintf(\"%s_%d.ports\", filepath.Base(os.Args[0]), os.Getpid()))\r\n}\r\n\r\n// Delete the ports file. If a non-empty dirHint is provided, the dirHint\r\n// path is used instead of the server option value\r\nfunc (s *Server) deletePortsFile(hintDir string) {\r\n\tportsFile := s.portFile(hintDir)\r\n\tif portsFile != \"\" {\r\n\t\tif err := os.Remove(portsFile); err != nil {\r\n\t\t\ts.Errorf(\"Error cleaning up ports file %s: %v\", portsFile, err)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Writes a file with a serialized Ports to the specified ports_file_dir.\r\n// The name of the file is `exename_pid.ports`, typically nats-server_pid.ports.\r\n// if ports file is not set, this function has no effect\r\nfunc (s *Server) logPorts() {\r\n\topts := s.getOpts()\r\n\tportsFile := s.portFile(opts.PortsFileDir)\r\n\tif portsFile != _EMPTY_ {\r\n\t\tgo func() {\r\n\t\t\tinfo := s.PortsInfo(5 * time.Second)\r\n\t\t\tif info == nil {\r\n\t\t\t\ts.Errorf(\"Unable to resolve the ports in the specified time\")\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tdata, err := json.Marshal(info)\r\n\t\t\tif err != nil {\r\n\t\t\t\ts.Errorf(\"Error marshaling ports file: %v\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tif err := ioutil.WriteFile(portsFile, data, 0666); err != nil {\r\n\t\t\t\ts.Errorf(\"Error writing ports file (%s): %v\", portsFile, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\r\n\t\t}()\r\n\t}\r\n}\r\n\r\n// waits until a calculated list of listeners is resolved or a timeout\r\nfunc (s *Server) readyForListeners(dur time.Duration) bool {\r\n\tend := time.Now().Add(dur)\r\n\tfor time.Now().Before(end) {\r\n\t\ts.mu.Lock()\r\n\t\tlisteners := s.serviceListeners()\r\n\t\ts.mu.Unlock()\r\n\t\tif len(listeners) == 0 {\r\n\t\t\treturn false\r\n\t\t}\r\n\r\n\t\tok := true\r\n\t\tfor _, l := range listeners {\r\n\t\t\tif l == nil {\r\n\t\t\t\tok = false\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\t\tif ok {\r\n\t\t\treturn true\r\n\t\t}\r\n\t\tselect {\r\n\t\tcase <-s.quitCh:\r\n\t\t\treturn false\r\n\t\tcase <-time.After(25 * time.Millisecond):\r\n\t\t\t// continue - unable to select from quit - we are still running\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// returns a list of listeners that are intended for the process\r\n// if the entry is nil, the interface is yet to be resolved\r\nfunc (s *Server) serviceListeners() []net.Listener {\r\n\tlisteners := make([]net.Listener, 0)\r\n\topts := s.getOpts()\r\n\tlisteners = append(listeners, s.listener)\r\n\tif opts.Cluster.Port != 0 {\r\n\t\tlisteners = append(listeners, s.routeListener)\r\n\t}\r\n\tif opts.HTTPPort != 0 || opts.HTTPSPort != 0 {\r\n\t\tlisteners = append(listeners, s.http)\r\n\t}\r\n\tif opts.ProfPort != 0 {\r\n\t\tlisteners = append(listeners, s.profiler)\r\n\t}\r\n\treturn listeners\r\n}\r\n\r\n// Returns true if in lame duck mode.\r\nfunc (s *Server) isLameDuckMode() bool {\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\treturn s.ldm\r\n}\r\n\r\n// This function will close the client listener then close the clients\r\n// at some interval to avoid a reconnecting storm.\r\nfunc (s *Server) lameDuckMode() {\r\n\ts.mu.Lock()\r\n\t// Check if there is actually anything to do\r\n\tif s.shutdown || s.ldm || s.listener == nil {\r\n\t\ts.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\ts.Noticef(\"Entering lame duck mode, stop accepting new clients\")\r\n\ts.ldm = true\r\n\ts.ldmCh = make(chan bool, 1)\r\n\ts.listener.Close()\r\n\ts.listener = nil\r\n\ts.mu.Unlock()\r\n\r\n\t// Wait for accept loop to be done to make sure that no new\r\n\t// client can connect\r\n\t<-s.ldmCh\r\n\r\n\ts.mu.Lock()\r\n\t// Need to recheck few things\r\n\tif s.shutdown || len(s.clients) == 0 {\r\n\t\ts.mu.Unlock()\r\n\t\t// If there is no client, we need to call Shutdown() to complete\r\n\t\t// the LDMode. If server has been shutdown while lock was released,\r\n\t\t// calling Shutdown() should be no-op.\r\n\t\ts.Shutdown()\r\n\t\treturn\r\n\t}\r\n\tdur := int64(s.getOpts().LameDuckDuration)\r\n\tdur -= atomic.LoadInt64(&lameDuckModeInitialDelay)\r\n\tif dur <= 0 {\r\n\t\tdur = int64(time.Second)\r\n\t}\r\n\tnumClients := int64(len(s.clients))\r\n\tbatch := 1\r\n\t// Sleep interval between each client connection close.\r\n\tsi := dur / numClients\r\n\tif si < 1 {\r\n\t\t// Should not happen (except in test with very small LD duration), but\r\n\t\t// if there are too many clients, batch the number of close and\r\n\t\t// use a tiny sleep interval that will result in yield likely.\r\n\t\tsi = 1\r\n\t\tbatch = int(numClients / dur)\r\n\t} else if si > int64(time.Second) {\r\n\t\t// Conversely, there is no need to sleep too long between clients\r\n\t\t// and spread say 10 clients for the 2min duration. Sleeping no\r\n\t\t// more than 1sec.\r\n\t\tsi = int64(time.Second)\r\n\t}\r\n\r\n\t// Now capture all clients\r\n\tclients := make([]*client, 0, len(s.clients))\r\n\tfor _, client := range s.clients {\r\n\t\tclients = append(clients, client)\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\tt := time.NewTimer(time.Duration(atomic.LoadInt64(&lameDuckModeInitialDelay)))\r\n\t// Delay start of closing of client connections in case\r\n\t// we have several servers that we want to signal to enter LD mode\r\n\t// and not have their client reconnect to each other.\r\n\tselect {\r\n\tcase <-t.C:\r\n\t\ts.Noticef(\"Closing existing clients\")\r\n\tcase <-s.quitCh:\r\n\t\treturn\r\n\t}\r\n\tfor i, client := range clients {\r\n\t\tclient.closeConnection(ServerShutdown)\r\n\t\tif i == len(clients)-1 {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tif batch == 1 || i%batch == 0 {\r\n\t\t\t// We pick a random interval which will be at least si/2\r\n\t\t\tv := rand.Int63n(si)\r\n\t\t\tif v < si/2 {\r\n\t\t\t\tv = si / 2\r\n\t\t\t}\r\n\t\t\tt.Reset(time.Duration(v))\r\n\t\t\t// Sleep for given interval or bail out if kicked by Shutdown().\r\n\t\t\tselect {\r\n\t\t\tcase <-t.C:\r\n\t\t\tcase <-s.quitCh:\r\n\t\t\t\tt.Stop()\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\ts.Shutdown()\r\n}\r\n\r\n// If given error is a net.Error and is temporary, sleeps for the given\r\n// delay and double it, but cap it to ACCEPT_MAX_SLEEP. The sleep is\r\n// interrupted if the server is shutdown.\r\n// An error message is displayed depending on the type of error.\r\n// Returns the new (or unchanged) delay.\r\nfunc (s *Server) acceptError(acceptName string, err error, tmpDelay time.Duration) time.Duration {\r\n\tif ne, ok := err.(net.Error); ok && ne.Temporary() {\r\n\t\ts.Errorf(\"Temporary %s Accept Error(%v), sleeping %dms\", acceptName, ne, tmpDelay/time.Millisecond)\r\n\t\tselect {\r\n\t\tcase <-time.After(tmpDelay):\r\n\t\tcase <-s.quitCh:\r\n\t\t\treturn tmpDelay\r\n\t\t}\r\n\t\ttmpDelay *= 2\r\n\t\tif tmpDelay > ACCEPT_MAX_SLEEP {\r\n\t\t\ttmpDelay = ACCEPT_MAX_SLEEP\r\n\t\t}\r\n\t} else if s.isRunning() {\r\n\t\ts.Errorf(\"%s Accept error: %v\", acceptName, err)\r\n\t}\r\n\treturn tmpDelay\r\n}\r\n\r\nfunc (s *Server) getRandomIP(resolver netResolver, url string) (string, error) {\r\n\thost, port, err := net.SplitHostPort(url)\r\n\tif err != nil {\r\n\t\treturn \"\", err\r\n\t}\r\n\t// If already an IP, skip.\r\n\tif net.ParseIP(host) != nil {\r\n\t\treturn url, nil\r\n\t}\r\n\tips, err := resolver.LookupHost(context.Background(), host)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"lookup for host %q: %v\", host, err)\r\n\t}\r\n\tvar address string\r\n\tif len(ips) == 0 {\r\n\t\ts.Warnf(\"Unable to get IP for %s, will try with %s: %v\", host, url, err)\r\n\t\taddress = url\r\n\t} else {\r\n\t\tvar ip string\r\n\t\tif len(ips) == 1 {\r\n\t\t\tip = ips[0]\r\n\t\t} else {\r\n\t\t\tip = ips[rand.Int31n(int32(len(ips)))]\r\n\t\t}\r\n\t\t// add the port\r\n\t\taddress = net.JoinHostPort(ip, port)\r\n\t}\r\n\treturn address, nil\r\n}\r\n\r\n// Returns true for the first attempt and depending on the nature\r\n// of the attempt (first connect or a reconnect), when the number\r\n// of attempts is equal to the configured report attempts.\r\nfunc (s *Server) shouldReportConnectErr(firstConnect bool, attempts int) bool {\r\n\topts := s.getOpts()\r\n\tif firstConnect {\r\n\t\tif attempts == 1 || attempts%opts.ConnectErrorReports == 0 {\r\n\t\t\treturn true\r\n\t\t}\r\n\t\treturn false\r\n\t}\r\n\tif attempts == 1 || attempts%opts.ReconnectErrorReports == 0 {\r\n\t\treturn true\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// Invoked for route, leaf and gateway connections. Set the very first\r\n// PING to a lower interval to capture the initial RTT.\r\n// After that the PING interval will be set to the user defined value.\r\n// Client lock should be held.\r\nfunc (s *Server) setFirstPingTimer(c *client) {\r\n\topts := s.getOpts()\r\n\td := opts.PingInterval\r\n\r\n\tif !opts.DisableShortFirstPing {\r\n\t\tif c.kind != CLIENT {\r\n\t\t\tif d > firstPingInterval {\r\n\t\t\t\td = firstPingInterval\r\n\t\t\t}\r\n\t\t} else if d > firstClientPingInterval {\r\n\t\t\td = firstClientPingInterval\r\n\t\t}\r\n\t}\r\n\t// We randomize the first one by an offset up to 20%, e.g. 2m ~= max 24s.\r\n\taddDelay := rand.Int63n(int64(d / 5))\r\n\td += time.Duration(addDelay)\r\n\tc.ping.tmr = time.AfterFunc(d, c.processPingTimer)\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/server.go b/server/gnatsd/server/server.go
--- a/server/gnatsd/server/server.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/server.go	(date 1665399050085)
@@ -38,7 +38,7 @@
 	// Allow dynamic profiling.
 	_ "net/http/pprof"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/test/operator_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2018-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage test\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"strings\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/kubemq-io/broker/client/nats\"\r\n\t\"github.com/kubemq-io/broker/server/gnatsd/server\"\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nconst (\r\n\ttestOpConfig       = \"./configs/operator.conf\"\r\n\ttestOpInlineConfig = \"./configs/operator_inline.conf\"\r\n)\r\n\r\n// This matches ./configs/nkeys_jwts/test.seed\r\n// Test operator seed.\r\nvar oSeed = []byte(\"SOAFYNORQLQFJYBYNUGC5D7SH2MXMUX5BFEWWGHN3EK4VGG5TPT5DZP7QU\")\r\n\r\n// This is a signing key seed.\r\nvar skSeed = []byte(\"SOAEL3NFOTU6YK3DBTEKQYZ2C5IWSVZWWZCQDASBUOHJKBFLVANK27JMMQ\")\r\n\r\nfunc checkKeys(t *testing.T, opts *server.Options, opc *jwt.OperatorClaims, expected int) {\r\n\t// We should have filled in the TrustedKeys here.\r\n\tif len(opts.TrustedKeys) != expected {\r\n\t\tt.Fatalf(\"Should have %d trusted keys, got %d\", expected, len(opts.TrustedKeys))\r\n\t}\r\n\t// Check that we properly placed all keys from the opc into TrustedKeys\r\n\tchkMember := func(s string) {\r\n\t\tfor _, c := range opts.TrustedKeys {\r\n\t\t\tif s == c {\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t}\r\n\t\tt.Fatalf(\"Expected %q to be in TrustedKeys\", s)\r\n\t}\r\n\tchkMember(opc.Issuer)\r\n\tfor _, sk := range opc.SigningKeys {\r\n\t\tchkMember(sk)\r\n\t}\r\n}\r\n\r\n// This will test that we enforce certain restrictions when you use trusted operators.\r\n// Like auth is always true, can't define accounts or users, required to define an account resolver, etc.\r\nfunc TestOperatorRestrictions(t *testing.T) {\r\n\topts, err := server.ProcessConfigFile(testOpConfig)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing config file: %v\", err)\r\n\t}\r\n\topts.NoSigs = true\r\n\tif _, err := server.NewServer(opts); err != nil {\r\n\t\tt.Fatalf(\"Expected to create a server successfully\")\r\n\t}\r\n\t// TrustedKeys get defined when processing from above, trying again with\r\n\t// same opts should not work.\r\n\tif _, err := server.NewServer(opts); err == nil {\r\n\t\tt.Fatalf(\"Expected an error with TrustedKeys defined\")\r\n\t}\r\n\t// Must wipe and rebuild to succeed.\r\n\twipeOpts := func() {\r\n\t\topts.TrustedKeys = nil\r\n\t\topts.Accounts = nil\r\n\t\topts.Users = nil\r\n\t\topts.Nkeys = nil\r\n\t\topts.AllowNewAccounts = false\r\n\t}\r\n\r\n\twipeOpts()\r\n\topts.Accounts = []*server.Account{{Name: \"TEST\"}}\r\n\tif _, err := server.NewServer(opts); err == nil {\r\n\t\tt.Fatalf(\"Expected an error with Accounts defined\")\r\n\t}\r\n\twipeOpts()\r\n\topts.Users = []*server.User{{Username: \"TEST\"}}\r\n\tif _, err := server.NewServer(opts); err == nil {\r\n\t\tt.Fatalf(\"Expected an error with Users defined\")\r\n\t}\r\n\twipeOpts()\r\n\topts.Nkeys = []*server.NkeyUser{{Nkey: \"TEST\"}}\r\n\tif _, err := server.NewServer(opts); err == nil {\r\n\t\tt.Fatalf(\"Expected an error with Nkey Users defined\")\r\n\t}\r\n\twipeOpts()\r\n\topts.AllowNewAccounts = true\r\n\tif _, err := server.NewServer(opts); err == nil {\r\n\t\tt.Fatalf(\"Expected an error with AllowNewAccounts set to true\")\r\n\t}\r\n\r\n\twipeOpts()\r\n\topts.AccountResolver = nil\r\n\tif _, err := server.NewServer(opts); err == nil {\r\n\t\tt.Fatalf(\"Expected an error without an AccountResolver defined\")\r\n\t}\r\n}\r\n\r\nfunc TestOperatorConfig(t *testing.T) {\r\n\topts, err := server.ProcessConfigFile(testOpConfig)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing config file: %v\", err)\r\n\t}\r\n\topts.NoSigs = true\r\n\t// Check we have the TrustedOperators\r\n\tif len(opts.TrustedOperators) != 1 {\r\n\t\tt.Fatalf(\"Expected to load the operator\")\r\n\t}\r\n\t_, err = server.NewServer(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to create a server: %v\", err)\r\n\t}\r\n\t// We should have filled in the public TrustedKeys here.\r\n\t// Our master public key (issuer) plus the signing keys (3).\r\n\tcheckKeys(t, opts, opts.TrustedOperators[0], 4)\r\n}\r\n\r\nfunc TestOperatorConfigInline(t *testing.T) {\r\n\topts, err := server.ProcessConfigFile(testOpInlineConfig)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing config file: %v\", err)\r\n\t}\r\n\topts.NoSigs = true\r\n\t// Check we have the TrustedOperators\r\n\tif len(opts.TrustedOperators) != 1 {\r\n\t\tt.Fatalf(\"Expected to load the operator\")\r\n\t}\r\n\t_, err = server.NewServer(opts)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected to create a server: %v\", err)\r\n\t}\r\n\t// We should have filled in the public TrustedKeys here.\r\n\t// Our master public key (issuer) plus the signing keys (3).\r\n\tcheckKeys(t, opts, opts.TrustedOperators[0], 4)\r\n}\r\n\r\nfunc runOperatorServer(t *testing.T) (*server.Server, *server.Options) {\r\n\treturn RunServerWithConfig(testOpConfig)\r\n}\r\n\r\nfunc createAccountForOperatorKey(t *testing.T, s *server.Server, seed []byte) (*server.Account, nkeys.KeyPair) {\r\n\tt.Helper()\r\n\tokp, _ := nkeys.FromSeed(seed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\tif err := s.AccountResolver().Store(pub, jwt); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\tacc, err := s.LookupAccount(pub)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error looking up account: %v\", err)\r\n\t}\r\n\treturn acc, akp\r\n}\r\n\r\nfunc createAccount(t *testing.T, s *server.Server) (*server.Account, nkeys.KeyPair) {\r\n\tt.Helper()\r\n\treturn createAccountForOperatorKey(t, s, oSeed)\r\n}\r\n\r\nfunc createUserCreds(t *testing.T, s *server.Server, akp nkeys.KeyPair) nats.Option {\r\n\tt.Helper()\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tuserCB := func() (string, error) {\r\n\t\treturn ujwt, nil\r\n\t}\r\n\tsigCB := func(nonce []byte) ([]byte, error) {\r\n\t\tsig, _ := kp.Sign(nonce)\r\n\t\treturn sig, nil\r\n\t}\r\n\treturn nats.UserJWT(userCB, sigCB)\r\n}\r\n\r\nfunc TestOperatorServer(t *testing.T) {\r\n\ts, opts := runOperatorServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tif _, err := nats.Connect(url); err == nil {\r\n\t\tt.Fatalf(\"Expected to fail with no credentials\")\r\n\t}\r\n\r\n\t_, akp := createAccount(t, s)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n\r\n\t// Now create an account from another operator, this should fail.\r\n\tokp, _ := nkeys.CreateOperator()\r\n\tseed, _ := okp.Seed()\r\n\t_, akp = createAccountForOperatorKey(t, s, seed)\r\n\t_, err = nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err == nil {\r\n\t\tt.Fatalf(\"Expected error on connect\")\r\n\t}\r\n}\r\n\r\nfunc TestOperatorSystemAccount(t *testing.T) {\r\n\ts, _ := runOperatorServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create an account from another operator, this should fail if used as a system account.\r\n\tokp, _ := nkeys.CreateOperator()\r\n\tseed, _ := okp.Seed()\r\n\tacc, _ := createAccountForOperatorKey(t, s, seed)\r\n\tif err := s.SetSystemAccount(acc.Name); err == nil {\r\n\t\tt.Fatalf(\"Expected this to fail\")\r\n\t}\r\n\tif acc := s.SystemAccount(); acc != nil {\r\n\t\tt.Fatalf(\"Expected no account to be set for system account\")\r\n\t}\r\n\r\n\tacc, _ = createAccount(t, s)\r\n\tif err := s.SetSystemAccount(acc.Name); err != nil {\r\n\t\tt.Fatalf(\"Expected this succeed, got %v\", err)\r\n\t}\r\n\tif sysAcc := s.SystemAccount(); sysAcc != acc {\r\n\t\tt.Fatalf(\"Did not get matching account for system account\")\r\n\t}\r\n}\r\n\r\nfunc TestOperatorSigningKeys(t *testing.T) {\r\n\ts, opts := runOperatorServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create an account with a signing key, not the master key.\r\n\tacc, akp := createAccountForOperatorKey(t, s, skSeed)\r\n\r\n\t// Make sure we can set system account.\r\n\tif err := s.SetSystemAccount(acc.Name); err != nil {\r\n\t\tt.Fatalf(\"Expected this succeed, got %v\", err)\r\n\t}\r\n\r\n\t// Make sure we can create users with it too.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n}\r\n\r\nfunc TestOperatorMemResolverPreload(t *testing.T) {\r\n\ts, opts := RunServerWithConfig(\"./configs/resolver_preload.conf\")\r\n\tdefer s.Shutdown()\r\n\r\n\t// Make sure we can look up the account.\r\n\tacc, _ := s.LookupAccount(\"ADM2CIIL3RWXBA6T2HW3FODNCQQOUJEHHQD6FKCPVAMHDNTTSMO73ROX\")\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to properly lookup account\")\r\n\t}\r\n\tsacc := s.SystemAccount()\r\n\tif sacc == nil {\r\n\t\tt.Fatalf(\"Expected to have system account registered\")\r\n\t}\r\n\tif sacc.Name != opts.SystemAccount {\r\n\t\tt.Fatalf(\"System account does not match, wanted %q, got %q\", opts.SystemAccount, sacc.Name)\r\n\t}\r\n}\r\n\r\nfunc TestOperatorConfigReloadDoesntKillNonce(t *testing.T) {\r\n\ts, _ := runOperatorServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tif !s.NonceRequired() {\r\n\t\tt.Fatalf(\"Error nonce should be required\")\r\n\t}\r\n\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error on reload: %v\", err)\r\n\t}\r\n\r\n\tif !s.NonceRequired() {\r\n\t\tt.Fatalf(\"Error nonce should still be required after reload\")\r\n\t}\r\n}\r\n\r\nfunc createAccountForConfig(t *testing.T) (string, nkeys.KeyPair) {\r\n\tt.Helper()\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\treturn jwt, akp\r\n}\r\n\r\nfunc TestReloadDoesNotWipeAccountsWithOperatorMode(t *testing.T) {\r\n\t// We will run an operator mode server that forms a cluster. We will\r\n\t// make sure that a reload does not wipe account information.\r\n\t// We will force reload of auth by changing cluster auth timeout.\r\n\r\n\t// Create two accounts, system and normal account.\r\n\tsysJWT, sysKP := createAccountForConfig(t)\r\n\tsysPub, _ := sysKP.PublicKey()\r\n\r\n\taccJWT, accKP := createAccountForConfig(t)\r\n\taccPub, _ := accKP.PublicKey()\r\n\r\n\tcf := `\r\n\tlisten: 127.0.0.1:-1\r\n\tcluster {\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tauthorization {\r\n\t\t\ttimeout: 2.2\r\n\t\t} %s\r\n\t}\r\n\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tsystem_account = \"%s\"\r\n\r\n\tresolver = MEMORY\r\n\tresolver_preload = {\r\n\t\t%s : \"%s\"\r\n\t\t%s : \"%s\"\r\n\t}\r\n\t`\r\n\tcontents := strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\tconf := createConfFile(t, []byte(contents))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create a new server and route to main one.\r\n\trouteStr := fmt.Sprintf(\"\\n\\t\\troutes = [nats-route://%s:%d]\", opts.Cluster.Host, opts.Cluster.Port)\r\n\tcontents2 := strings.Replace(fmt.Sprintf(cf, routeStr, sysPub, sysPub, sysJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\r\n\tconf2 := createConfFile(t, []byte(contents2))\r\n\tdefer os.Remove(conf2)\r\n\r\n\ts2, opts2 := RunServerWithConfig(conf2)\r\n\tdefer s2.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, s2)\r\n\r\n\t// Create a client on the first server and subscribe.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, accKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tch := make(chan bool)\r\n\tnc.Subscribe(\"foo\", func(m *nats.Msg) { ch <- true })\r\n\tnc.Flush()\r\n\r\n\t// Use this to check for message.\r\n\tcheckForMsg := func() {\r\n\t\tselect {\r\n\t\tcase <-ch:\r\n\t\tcase <-time.After(2 * time.Second):\r\n\t\t\tt.Fatal(\"Timeout waiting for message across route\")\r\n\t\t}\r\n\t}\r\n\r\n\t// Create second client and send message from this one. Interest should be here.\r\n\turl2 := fmt.Sprintf(\"nats://%s:%d/\", opts2.Host, opts2.Port)\r\n\tnc2, err := nats.Connect(url2, createUserCreds(t, s2, accKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Check that we can send messages.\r\n\tnc2.Publish(\"foo\", nil)\r\n\tcheckForMsg()\r\n\r\n\t// Now shutdown nc2 and srvA.\r\n\tnc2.Close()\r\n\ts2.Shutdown()\r\n\r\n\t// Now change config and do reload which will do an auth change.\r\n\tb, err := ioutil.ReadFile(conf)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tnewConf := bytes.Replace(b, []byte(\"2.2\"), []byte(\"3.3\"), 1)\r\n\terr = ioutil.WriteFile(conf, newConf, 0644)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\t// This will cause reloadAuthorization to kick in and reprocess accounts.\r\n\ts.Reload()\r\n\r\n\ts2, opts2 = RunServerWithConfig(conf2)\r\n\tdefer s2.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, s2)\r\n\r\n\t// Reconnect and make sure this works. If accounts blown away this will fail.\r\n\turl2 = fmt.Sprintf(\"nats://%s:%d/\", opts2.Host, opts2.Port)\r\n\tnc2, err = nats.Connect(url2, createUserCreds(t, s2, accKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Check that we can send messages.\r\n\tnc2.Publish(\"foo\", nil)\r\n\tcheckForMsg()\r\n}\r\n\r\nfunc TestReloadDoesUpdateAccountsWithMemoryResolver(t *testing.T) {\r\n\t// We will run an operator mode server with a memory resolver.\r\n\t// Reloading should behave similar to configured accounts.\r\n\r\n\t// Create two accounts, system and normal account.\r\n\tsysJWT, sysKP := createAccountForConfig(t)\r\n\tsysPub, _ := sysKP.PublicKey()\r\n\r\n\taccJWT, accKP := createAccountForConfig(t)\r\n\taccPub, _ := accKP.PublicKey()\r\n\r\n\tcf := `\r\n\tlisten: 127.0.0.1:-1\r\n\tcluster {\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tauthorization {\r\n\t\t\ttimeout: 2.2\r\n\t\t} %s\r\n\t}\r\n\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tsystem_account = \"%s\"\r\n\r\n\tresolver = MEMORY\r\n\tresolver_preload = {\r\n\t\t%s : \"%s\"\r\n\t\t%s : \"%s\"\r\n\t}\r\n\t`\r\n\tcontents := strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\tconf := createConfFile(t, []byte(contents))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create a client on the first server and subscribe.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, accKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\tdefer nc.Close()\r\n\r\n\tnc.Subscribe(\"foo\", func(m *nats.Msg) {})\r\n\tnc.Flush()\r\n\r\n\t// Now update and remove normal account and make sure we get disconnected.\r\n\taccJWT2, accKP2 := createAccountForConfig(t)\r\n\taccPub2, _ := accKP2.PublicKey()\r\n\tcontents = strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, accPub2, accJWT2), \"\\n\\t\", \"\\n\", -1)\r\n\terr = ioutil.WriteFile(conf, []byte(contents), 0644)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\t// This will cause reloadAuthorization to kick in and reprocess accounts.\r\n\ts.Reload()\r\n\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(2 * time.Second):\r\n\t\t// Give it up to 2 sec.\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n\r\n\t// Make sure we can lool up new account and not old one.\r\n\tif _, err := s.LookupAccount(accPub2); err != nil {\r\n\t\tt.Fatalf(\"Error looking up account: %v\", err)\r\n\t}\r\n\r\n\tif _, err := s.LookupAccount(accPub); err == nil {\r\n\t\tt.Fatalf(\"Expected error looking up old account\")\r\n\t}\r\n}\r\n\r\nfunc TestReloadFailsWithBadAccountsWithMemoryResolver(t *testing.T) {\r\n\t// Create two accounts, system and normal account.\r\n\tsysJWT, sysKP := createAccountForConfig(t)\r\n\tsysPub, _ := sysKP.PublicKey()\r\n\r\n\t// Create an expired account by hand here. We want to make sure we start up correctly\r\n\t// with expired or otherwise accounts with validation issues.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tnac.IssuedAt = time.Now().Add(-10 * time.Second).Unix()\r\n\tnac.Expires = time.Now().Add(-2 * time.Second).Unix()\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\r\n\tcf := `\r\n\tlisten: 127.0.0.1:-1\r\n\tcluster {\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tauthorization {\r\n\t\t\ttimeout: 2.2\r\n\t\t} %s\r\n\t}\r\n\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tsystem_account = \"%s\"\r\n\r\n\tresolver = MEMORY\r\n\tresolver_preload = {\r\n\t\t%s : \"%s\"\r\n\t\t%s : \"%s\"\r\n\t}\r\n\t`\r\n\tcontents := strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, apub, ajwt), \"\\n\\t\", \"\\n\", -1)\r\n\tconf := createConfFile(t, []byte(contents))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Now add in bogus account for second item and make sure reload fails.\r\n\tcontents = strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, \"foo\", \"bar\"), \"\\n\\t\", \"\\n\", -1)\r\n\terr = ioutil.WriteFile(conf, []byte(contents), 0644)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tif err := s.Reload(); err == nil {\r\n\t\tt.Fatalf(\"Expected fatal error with bad account on reload\")\r\n\t}\r\n\r\n\t// Put it back with a normal account and reload should succeed.\r\n\taccJWT, accKP := createAccountForConfig(t)\r\n\taccPub, _ := accKP.PublicKey()\r\n\r\n\tcontents = strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\terr = ioutil.WriteFile(conf, []byte(contents), 0644)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Got unexpected error on reload: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestConnsRequestDoesNotLoadAccountCheckingConnLimits(t *testing.T) {\r\n\t// Create two accounts, system and normal account.\r\n\tsysJWT, sysKP := createAccountForConfig(t)\r\n\tsysPub, _ := sysKP.PublicKey()\r\n\r\n\t// Do this account by nad to add in connection limits\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\taccKP, _ := nkeys.CreateAccount()\r\n\taccPub, _ := accKP.PublicKey()\r\n\tnac := jwt.NewAccountClaims(accPub)\r\n\tnac.Limits.Conn = 10\r\n\taccJWT, _ := nac.Encode(okp)\r\n\r\n\tcf := `\r\n\tlisten: 127.0.0.1:-1\r\n\tcluster {\r\n\t\tlisten: 127.0.0.1:-1\r\n\t\tauthorization {\r\n\t\t\ttimeout: 2.2\r\n\t\t} %s\r\n\t}\r\n\r\n\toperator = \"./configs/nkeys/op.jwt\"\r\n\tsystem_account = \"%s\"\r\n\r\n\tresolver = MEMORY\r\n\tresolver_preload = {\r\n\t\t%s : \"%s\"\r\n\t\t%s : \"%s\"\r\n\t}\r\n\t`\r\n\tcontents := strings.Replace(fmt.Sprintf(cf, \"\", sysPub, sysPub, sysJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\tconf := createConfFile(t, []byte(contents))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create a new server and route to main one.\r\n\trouteStr := fmt.Sprintf(\"\\n\\t\\troutes = [nats-route://%s:%d]\", opts.Cluster.Host, opts.Cluster.Port)\r\n\tcontents2 := strings.Replace(fmt.Sprintf(cf, routeStr, sysPub, sysPub, sysJWT, accPub, accJWT), \"\\n\\t\", \"\\n\", -1)\r\n\r\n\tconf2 := createConfFile(t, []byte(contents2))\r\n\tdefer os.Remove(conf2)\r\n\r\n\ts2, _ := RunServerWithConfig(conf2)\r\n\tdefer s2.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, s2)\r\n\r\n\t// Make sure that we do not have the account loaded.\r\n\t// Just SYS and $G\r\n\tif nla := s.NumLoadedAccounts(); nla != 2 {\r\n\t\tt.Fatalf(\"Expected only 2 loaded accounts, got %d\", nla)\r\n\t}\r\n\tif nla := s2.NumLoadedAccounts(); nla != 2 {\r\n\t\tt.Fatalf(\"Expected only 2 loaded accounts, got %d\", nla)\r\n\t}\r\n\r\n\t// Now connect to first server on accPub.\r\n\tnc, err := nats.Connect(s.ClientURL(), createUserCreds(t, s, accKP))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Just wait for the request for connections to move to S2 and cause a fetch.\r\n\t// This is what we want to fix.\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\r\n\t// We should have 3 here for sure.\r\n\tif nla := s.NumLoadedAccounts(); nla != 3 {\r\n\t\tt.Fatalf(\"Expected 3 loaded accounts, got %d\", nla)\r\n\t}\r\n\r\n\t// Now make sure that we still only have 2 loaded accounts on server 2.\r\n\tif nla := s2.NumLoadedAccounts(); nla != 2 {\r\n\t\tt.Fatalf(\"Expected only 2 loaded accounts, got %d\", nla)\r\n\t}\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/test/operator_test.go b/server/gnatsd/test/operator_test.go
--- a/server/gnatsd/test/operator_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/test/operator_test.go	(date 1665399050215)
@@ -24,7 +24,7 @@
 
 	"github.com/kubemq-io/broker/client/nats"
 	"github.com/kubemq-io/broker/server/gnatsd/server"
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: go.sum
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>github.com/DataDog/datadog-go v2.2.0+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\r\ngithub.com/armon/go-metrics v0.0.0-20190430140413-ec5e00d3c878 h1:EFSB7Zo9Eg91v7MJPVsifUysc/wPdN+NOnVe6bWbdBM=\r\ngithub.com/armon/go-metrics v0.0.0-20190430140413-ec5e00d3c878/go.mod h1:3AMJUQhVx52RsWOnlkpikZr01T/yAVN2gn0861vByNg=\r\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\r\ngithub.com/boltdb/bolt v1.3.1/go.mod h1:clJnj/oiGkjum5o1McbSZDSLxVThjynRyGBgiAx27Ps=\r\ngithub.com/circonus-labs/circonus-gometrics v2.3.1+incompatible/go.mod h1:nmEj6Dob7S7YxXgwXpfOuvO54S+tGdZdw9fuRZt25Ag=\r\ngithub.com/circonus-labs/circonusllhist v0.1.3/go.mod h1:kMXHVDlOchFAehlya5ePtbp5jckzBHf4XRpQvBOLI+I=\r\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\r\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\r\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\r\ngithub.com/fatih/color v1.7.0 h1:DkWD4oS2D8LGGgTQ6IvwJJXSL5Vp2ffcQg58nFV38Ys=\r\ngithub.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=\r\ngithub.com/go-sql-driver/mysql v1.5.0 h1:ozyZYNQW3x3HtqT1jira07DN2PArx2v7/mN66gGcHOs=\r\ngithub.com/go-sql-driver/mysql v1.5.0/go.mod h1:DCzpHaOWr8IXmIStZouvnhqoel9Qv2LBy8hT2VhHyBg=\r\ngithub.com/gogo/protobuf v1.3.1 h1:DqDEcV5aeaTmdFBePNpYsp3FlcVH/2ISVVM9Qf8PSls=\r\ngithub.com/gogo/protobuf v1.3.1/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXPKa29o=\r\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\r\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\r\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\r\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\r\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\r\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\r\ngithub.com/golang/protobuf v1.4.3 h1:JjCZWpVbqXDqFVmTfYWEVTMIYrL/NPdPSCHPJ0T/raM=\r\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\r\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\r\ngithub.com/google/go-cmp v0.3.1 h1:Xye71clBPdm5HgqGwUkwhbynsUJZhDbS20FvLhQ2izg=\r\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\r\ngithub.com/google/go-cmp v0.4.0 h1:xsAVV57WRhGj6kEIi8ReJzQlHHqcBYCElAvkovg3B/4=\r\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\r\ngithub.com/hashicorp/go-cleanhttp v0.5.0/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\r\ngithub.com/hashicorp/go-hclog v0.9.1 h1:9PZfAcVEvez4yhLH2TBU64/h/z4xlFI80cWXRrxuKuM=\r\ngithub.com/hashicorp/go-hclog v0.9.1/go.mod h1:5CU+agLiy3J7N7QjHK5d05KxGsuXiQLrjA0H7acj2lQ=\r\ngithub.com/hashicorp/go-hclog v0.15.0 h1:qMuK0wxsoW4D0ddCCYwPSTm4KQv1X1ke3WmPWZ0Mvsk=\r\ngithub.com/hashicorp/go-hclog v0.15.0/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\r\ngithub.com/hashicorp/go-immutable-radix v1.0.0 h1:AKDB1HM5PWEA7i4nhcpwOrO2byshxBjXVn/J/3+z5/0=\r\ngithub.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\r\ngithub.com/hashicorp/go-msgpack v0.5.5 h1:i9R9JSrqIz0QVLz3sz+i3YJdT7TTSLcfLLzJi9aZTuI=\r\ngithub.com/hashicorp/go-msgpack v0.5.5/go.mod h1:ahLV/dePpqEmjfWmKiqvPkv/twdG7iPBM1vqhUKIvfM=\r\ngithub.com/hashicorp/go-msgpack v1.1.5 h1:9byZdVjKTe5mce63pRVNP1L7UAmdHOTEMGehn6KvJWs=\r\ngithub.com/hashicorp/go-msgpack v1.1.5/go.mod h1:gWVc3sv/wbDmR3rQsj1CAktEZzoz1YNK9NfGLXJ69/4=\r\ngithub.com/hashicorp/go-retryablehttp v0.5.3/go.mod h1:9B5zBasrRhHXnJnui7y6sL7es7NDiJgTc6Er0maI1Xs=\r\ngithub.com/hashicorp/go-uuid v1.0.0 h1:RS8zrF7PhGwyNPOtxSClXXj9HA8feRnJzgnI1RJCSnM=\r\ngithub.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\r\ngithub.com/hashicorp/golang-lru v0.5.0 h1:CL2msUPvZTLb5O648aiLNJw3hnBxN2+1Jq8rCOH9wdo=\r\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\r\ngithub.com/hashicorp/raft v1.2.0 h1:mHzHIrF0S91d3A7RPBvuqkgB4d/7oFJZyvf1Q4m7GA0=\r\ngithub.com/hashicorp/raft v1.2.0/go.mod h1:vPAJM8Asw6u8LxC3eJCUZmRP/E4QmUGE1R7g7k8sG/8=\r\ngithub.com/hashicorp/raft-boltdb v0.0.0-20171010151810-6e5ba93211ea/go.mod h1:pNv7Wc3ycL6F5oOWn+tPGo2gWD4a5X+yp/ntwdKLjRk=\r\ngithub.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=\r\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\r\ngithub.com/lib/pq v1.9.0 h1:L8nSXQQzAYByakOFMTwpjRoHsMJklur4Gi59b6VivR8=\r\ngithub.com/lib/pq v1.9.0/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\r\ngithub.com/mattn/go-colorable v0.1.4 h1:snbPLB8fVfU9iwbbo30TPtbLRzwWu6aJS6Xh4eaaviA=\r\ngithub.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\r\ngithub.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\r\ngithub.com/mattn/go-isatty v0.0.10 h1:qxFzApOv4WsAL965uUPIsXzAKCZxN2p9UqdhFS4ZW10=\r\ngithub.com/mattn/go-isatty v0.0.10/go.mod h1:qgIWMr58cqv1PHHyhnkY9lrL7etaEgOFcMEpPG5Rm84=\r\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\r\ngithub.com/nats-io/jwt v1.2.2 h1:w3GMTO969dFg+UOKTmmyuu7IGdusK+7Ytlt//OYH/uU=\r\ngithub.com/nats-io/jwt v1.2.2/go.mod h1:/xX356yQA6LuXI9xWW7mZNpxgF2mBmGecH+Fj34sP5Q=\r\ngithub.com/nats-io/nkeys v0.2.0 h1:WXKF7diOaPU9cJdLD7nuzwasQy9vT1tBqzXZZf3AMJM=\r\ngithub.com/nats-io/nkeys v0.2.0/go.mod h1:XdZpAbhgyyODYqjTawOnIOI7VlbKSarI9Gfy1tqEu/s=\r\ngithub.com/nats-io/nuid v1.0.1 h1:5iA8DT8V7q8WK2EScv2padNa/rTESc1KdnPw4TC2paw=\r\ngithub.com/nats-io/nuid v1.0.1/go.mod h1:19wcPz3Ph3q0Jbyiqsd0kePYG7A95tJPxeL+1OSON2c=\r\ngithub.com/pascaldekloe/goe v0.1.0 h1:cBOtyMzM9HTpWjXfbbunk26uA6nG3a8n06Wieeh0MwY=\r\ngithub.com/pascaldekloe/goe v0.1.0/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\r\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\r\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\r\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\r\ngithub.com/prometheus/client_golang v0.9.2/go.mod h1:OsXs2jCmiKlQ1lTBmv21f2mNfw4xf/QclQDMrYNZzcM=\r\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\r\ngithub.com/prometheus/common v0.0.0-20181126121408-4724e9255275/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=\r\ngithub.com/prometheus/procfs v0.0.0-20181204211112-1dc9a6cbc91a/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\r\ngithub.com/prometheus/procfs v0.2.0 h1:wH4vA7pcjKuZzjF7lM8awk4fnuJO6idemZXoKnULUx4=\r\ngithub.com/prometheus/procfs v0.2.0/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\r\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\r\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\r\ngithub.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=\r\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\r\ngithub.com/tv42/httpunix v0.0.0-20150427012821-b75d8614f926/go.mod h1:9ESjWnEqriFuLhtthL60Sar/7RFoluCcXsuvEwTV5KM=\r\ngo.etcd.io/bbolt v1.3.5 h1:XAzx9gjCb0Rxj7EoqcClPD1d5ZBxZJk0jbuoPHenBt0=\r\ngo.etcd.io/bbolt v1.3.5/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=\r\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\r\ngolang.org/x/crypto v0.0.0-20200323165209-0ec3e9974c59 h1:3zb4D3T4G8jdExgVU/95+vQXfpEPiMdCaZgmGVxjNHM=\r\ngolang.org/x/crypto v0.0.0-20200323165209-0ec3e9974c59/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\r\ngolang.org/x/crypto v0.0.0-20201221181555-eec23a3978ad h1:DN0cp81fZ3njFcrLCytUHRSUkqBjfTo4Tx9RJTWs0EY=\r\ngolang.org/x/crypto v0.0.0-20201221181555-eec23a3978ad/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=\r\ngolang.org/x/net v0.0.0-20181201002055-351d144fa1fc/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\r\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\r\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\r\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\r\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\r\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\r\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\r\ngolang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\r\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/sys v0.0.0-20190523142557-0e01d883c5c5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/sys v0.0.0-20191008105621-543471e840be/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/sys v0.0.0-20200202164722-d101bd2416d5 h1:LfCXLvNmTYH9kEmVgqbnsWfruoXZIrh4YBgqVHtDvw0=\r\ngolang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/sys v0.0.0-20201223074533-0d417f636930 h1:vRgIt+nup/B/BwIS0g2oC0haq0iqbV3ZA+u6+0TlNCo=\r\ngolang.org/x/sys v0.0.0-20201223074533-0d417f636930/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\r\ngolang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=\r\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\r\ngolang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\r\ngolang.org/x/tools v0.0.0-20190424220101-1e8e1cfdf96b/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\r\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543 h1:E7g+9GITq07hpfrRu66IVDexMakfv52eLZ2CXBWiKr4=\r\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\r\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\r\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\r\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\r\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\r\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\r\ngoogle.golang.org/protobuf v1.23.0 h1:4MY060fB1DLGMB/7MBTLnwQUY6+F09GEiz6SsrNqyzM=\r\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/go.sum b/go.sum
--- a/go.sum	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/go.sum	(date 1665399350414)
@@ -2,7 +2,6 @@
 github.com/armon/go-metrics v0.0.0-20190430140413-ec5e00d3c878 h1:EFSB7Zo9Eg91v7MJPVsifUysc/wPdN+NOnVe6bWbdBM=
 github.com/armon/go-metrics v0.0.0-20190430140413-ec5e00d3c878/go.mod h1:3AMJUQhVx52RsWOnlkpikZr01T/yAVN2gn0861vByNg=
 github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=
-github.com/boltdb/bolt v1.3.1/go.mod h1:clJnj/oiGkjum5o1McbSZDSLxVThjynRyGBgiAx27Ps=
 github.com/circonus-labs/circonus-gometrics v2.3.1+incompatible/go.mod h1:nmEj6Dob7S7YxXgwXpfOuvO54S+tGdZdw9fuRZt25Ag=
 github.com/circonus-labs/circonusllhist v0.1.3/go.mod h1:kMXHVDlOchFAehlya5ePtbp5jckzBHf4XRpQvBOLI+I=
 github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
@@ -10,31 +9,23 @@
 github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
 github.com/fatih/color v1.7.0 h1:DkWD4oS2D8LGGgTQ6IvwJJXSL5Vp2ffcQg58nFV38Ys=
 github.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=
-github.com/go-sql-driver/mysql v1.5.0 h1:ozyZYNQW3x3HtqT1jira07DN2PArx2v7/mN66gGcHOs=
-github.com/go-sql-driver/mysql v1.5.0/go.mod h1:DCzpHaOWr8IXmIStZouvnhqoel9Qv2LBy8hT2VhHyBg=
-github.com/gogo/protobuf v1.3.1 h1:DqDEcV5aeaTmdFBePNpYsp3FlcVH/2ISVVM9Qf8PSls=
-github.com/gogo/protobuf v1.3.1/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXPKa29o=
+github.com/go-sql-driver/mysql v1.6.0 h1:BCTh4TKNUYmOmMUcQ3IipzF5prigylS7XXjEkfCHuOE=
+github.com/go-sql-driver/mysql v1.6.0/go.mod h1:DCzpHaOWr8IXmIStZouvnhqoel9Qv2LBy8hT2VhHyBg=
+github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
+github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
 github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
-github.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=
-github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=
-github.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=
-github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=
-github.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=
-github.com/golang/protobuf v1.4.3 h1:JjCZWpVbqXDqFVmTfYWEVTMIYrL/NPdPSCHPJ0T/raM=
-github.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=
-github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
-github.com/google/go-cmp v0.3.1 h1:Xye71clBPdm5HgqGwUkwhbynsUJZhDbS20FvLhQ2izg=
+github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
+github.com/golang/protobuf v1.5.2 h1:ROPKBNFfQgOUMifHyP+KYbvpjbdoFNs+aK7DXlji0Tw=
+github.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
 github.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
-github.com/google/go-cmp v0.4.0 h1:xsAVV57WRhGj6kEIi8ReJzQlHHqcBYCElAvkovg3B/4=
-github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.5 h1:Khx7svrCpmxxtHBq5j2mp/xVjsi8hQMfNLvJFAlrGgU=
+github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
 github.com/hashicorp/go-cleanhttp v0.5.0/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=
-github.com/hashicorp/go-hclog v0.9.1 h1:9PZfAcVEvez4yhLH2TBU64/h/z4xlFI80cWXRrxuKuM=
 github.com/hashicorp/go-hclog v0.9.1/go.mod h1:5CU+agLiy3J7N7QjHK5d05KxGsuXiQLrjA0H7acj2lQ=
 github.com/hashicorp/go-hclog v0.15.0 h1:qMuK0wxsoW4D0ddCCYwPSTm4KQv1X1ke3WmPWZ0Mvsk=
 github.com/hashicorp/go-hclog v0.15.0/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=
 github.com/hashicorp/go-immutable-radix v1.0.0 h1:AKDB1HM5PWEA7i4nhcpwOrO2byshxBjXVn/J/3+z5/0=
 github.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=
-github.com/hashicorp/go-msgpack v0.5.5 h1:i9R9JSrqIz0QVLz3sz+i3YJdT7TTSLcfLLzJi9aZTuI=
 github.com/hashicorp/go-msgpack v0.5.5/go.mod h1:ahLV/dePpqEmjfWmKiqvPkv/twdG7iPBM1vqhUKIvfM=
 github.com/hashicorp/go-msgpack v1.1.5 h1:9byZdVjKTe5mce63pRVNP1L7UAmdHOTEMGehn6KvJWs=
 github.com/hashicorp/go-msgpack v1.1.5/go.mod h1:gWVc3sv/wbDmR3rQsj1CAktEZzoz1YNK9NfGLXJ69/4=
@@ -43,23 +34,28 @@
 github.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=
 github.com/hashicorp/golang-lru v0.5.0 h1:CL2msUPvZTLb5O648aiLNJw3hnBxN2+1Jq8rCOH9wdo=
 github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=
-github.com/hashicorp/raft v1.2.0 h1:mHzHIrF0S91d3A7RPBvuqkgB4d/7oFJZyvf1Q4m7GA0=
-github.com/hashicorp/raft v1.2.0/go.mod h1:vPAJM8Asw6u8LxC3eJCUZmRP/E4QmUGE1R7g7k8sG/8=
-github.com/hashicorp/raft-boltdb v0.0.0-20171010151810-6e5ba93211ea/go.mod h1:pNv7Wc3ycL6F5oOWn+tPGo2gWD4a5X+yp/ntwdKLjRk=
-github.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=
+github.com/hashicorp/raft v1.3.11 h1:p3v6gf6l3S797NnK5av3HcczOC1T5CLoaRvg0g9ys4A=
+github.com/hashicorp/raft v1.3.11/go.mod h1:J8naEwc6XaaCfts7+28whSeRvCqTd6e20BlCU3LtEO4=
+github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
 github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
-github.com/lib/pq v1.9.0 h1:L8nSXQQzAYByakOFMTwpjRoHsMJklur4Gi59b6VivR8=
-github.com/lib/pq v1.9.0/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
+github.com/klauspost/compress v1.15.11 h1:Lcadnb3RKGin4FYM/orgq0qde+nc15E5Cbqg4B9Sx9c=
+github.com/lib/pq v1.10.7 h1:p7ZhMD+KsSRozJr34udlUrhboJwWAgCg34+/ZZNvZZw=
+github.com/lib/pq v1.10.7/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
 github.com/mattn/go-colorable v0.1.4 h1:snbPLB8fVfU9iwbbo30TPtbLRzwWu6aJS6Xh4eaaviA=
 github.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=
 github.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=
 github.com/mattn/go-isatty v0.0.10 h1:qxFzApOv4WsAL965uUPIsXzAKCZxN2p9UqdhFS4ZW10=
 github.com/mattn/go-isatty v0.0.10/go.mod h1:qgIWMr58cqv1PHHyhnkY9lrL7etaEgOFcMEpPG5Rm84=
 github.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=
-github.com/nats-io/jwt v1.2.2 h1:w3GMTO969dFg+UOKTmmyuu7IGdusK+7Ytlt//OYH/uU=
-github.com/nats-io/jwt v1.2.2/go.mod h1:/xX356yQA6LuXI9xWW7mZNpxgF2mBmGecH+Fj34sP5Q=
-github.com/nats-io/nkeys v0.2.0 h1:WXKF7diOaPU9cJdLD7nuzwasQy9vT1tBqzXZZf3AMJM=
-github.com/nats-io/nkeys v0.2.0/go.mod h1:XdZpAbhgyyODYqjTawOnIOI7VlbKSarI9Gfy1tqEu/s=
+github.com/minio/highwayhash v1.0.2 h1:Aak5U0nElisjDCfPSG79Tgzkn2gl66NxOMspRrKnA/g=
+github.com/nats-io/jwt/v2 v2.3.0 h1:z2mA1a7tIf5ShggOFlR1oBPgd6hGqcDYsISxZByUzdI=
+github.com/nats-io/jwt/v2 v2.3.0/go.mod h1:0tqz9Hlu6bCBFLWAASKhE5vUA4c24L9KPUUgvwumE/k=
+github.com/nats-io/nats-server/v2 v2.9.2 h1:XNDgJgOYYaYlquLdbSHI3xssLipfKUOq3EmYIMNCOsE=
+github.com/nats-io/nats-server/v2 v2.9.2/go.mod h1:4sq8wvrpbvSzL1n3ZfEYnH4qeUuIl5W990j3kw13rRk=
+github.com/nats-io/nats.go v1.17.0 h1:1jp5BThsdGlN91hW0k3YEfJbfACjiOYtUiLXG0RL4IE=
+github.com/nats-io/nats.go v1.17.0/go.mod h1:BPko4oXsySz4aSWeFgOHLZs3G4Jq4ZAyE6/zMCxRT6w=
+github.com/nats-io/nkeys v0.3.0 h1:cgM5tL53EvYRU+2YLXIK0G2mJtK12Ft9oeooSZMA2G8=
+github.com/nats-io/nkeys v0.3.0/go.mod h1:gvUNGjVcM2IPr5rCsRsC6Wb3Hr2CQAm08dsxtV6A5y4=
 github.com/nats-io/nuid v1.0.1 h1:5iA8DT8V7q8WK2EScv2padNa/rTESc1KdnPw4TC2paw=
 github.com/nats-io/nuid v1.0.1/go.mod h1:19wcPz3Ph3q0Jbyiqsd0kePYG7A95tJPxeL+1OSON2c=
 github.com/pascaldekloe/goe v0.1.0 h1:cBOtyMzM9HTpWjXfbbunk26uA6nG3a8n06Wieeh0MwY=
@@ -78,40 +74,53 @@
 github.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=
 github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
 github.com/tv42/httpunix v0.0.0-20150427012821-b75d8614f926/go.mod h1:9ESjWnEqriFuLhtthL60Sar/7RFoluCcXsuvEwTV5KM=
+github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
 go.etcd.io/bbolt v1.3.5 h1:XAzx9gjCb0Rxj7EoqcClPD1d5ZBxZJk0jbuoPHenBt0=
 go.etcd.io/bbolt v1.3.5/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=
 golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
-golang.org/x/crypto v0.0.0-20200323165209-0ec3e9974c59 h1:3zb4D3T4G8jdExgVU/95+vQXfpEPiMdCaZgmGVxjNHM=
-golang.org/x/crypto v0.0.0-20200323165209-0ec3e9974c59/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
-golang.org/x/crypto v0.0.0-20201221181555-eec23a3978ad h1:DN0cp81fZ3njFcrLCytUHRSUkqBjfTo4Tx9RJTWs0EY=
-golang.org/x/crypto v0.0.0-20201221181555-eec23a3978ad/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=
+golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
+golang.org/x/crypto v0.0.0-20210314154223-e6e6c4f2bb5b/go.mod h1:T9bdIzuCu7OtxOm1hfPfRQxPLYneinmdGuTeoZ9dtd4=
+golang.org/x/crypto v0.0.0-20220926161630-eccd6366d1be h1:fmw3UbQh+nxngCAHrDCCztao/kbYFnWjoqop8dHx05A=
+golang.org/x/crypto v0.0.0-20220926161630-eccd6366d1be/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=
+golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
 golang.org/x/net v0.0.0-20181201002055-351d144fa1fc/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
 golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
 golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
+golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
 golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
 golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
 golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
 golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
 golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
 golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/sys v0.0.0-20190523142557-0e01d883c5c5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20191008105621-543471e840be/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5 h1:LfCXLvNmTYH9kEmVgqbnsWfruoXZIrh4YBgqVHtDvw0=
 golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/sys v0.0.0-20201223074533-0d417f636930 h1:vRgIt+nup/B/BwIS0g2oC0haq0iqbV3ZA+u6+0TlNCo=
-golang.org/x/sys v0.0.0-20201223074533-0d417f636930/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=
+golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20220928140112-f11e5e49a4ec h1:BkDtF2Ih9xZ7le9ndzTA7KJow28VbQW3odyk/8drmuI=
+golang.org/x/sys v0.0.0-20220928140112-f11e5e49a4ec/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
 golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
-golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/time v0.0.0-20220922220347-f3bd1da661af h1:Yx9k8YCG3dvF87UAn2tu2HQLf2dt/eR1bXxpLMWeH+Y=
+golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
 golang.org/x/tools v0.0.0-20190424220101-1e8e1cfdf96b/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
-golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543 h1:E7g+9GITq07hpfrRu66IVDexMakfv52eLZ2CXBWiKr4=
+golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
+golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
 golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
-google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=
-google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=
-google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=
-google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=
-google.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=
-google.golang.org/protobuf v1.23.0 h1:4MY060fB1DLGMB/7MBTLnwQUY6+F09GEiz6SsrNqyzM=
-google.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=
+golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1 h1:go1bK/D/BFZV2I8cIQd1NKEZ+0owSTG1fDTci4IqFcE=
+golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
+google.golang.org/protobuf v1.26.0 h1:bxAC2xTBsZGibn2RTntX0oH50xLsqy1OxA9tTL3p/lk=
+google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
Index: server/gnatsd/server/jwt.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2018-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"regexp\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nvar nscDecoratedRe = regexp.MustCompile(`\\s*(?:(?:[-]{3,}[^\\n]*[-]{3,}\\n)(.+)(?:\\n\\s*[-]{3,}[^\\n]*[-]{3,}[\\n]*))`)\r\n\r\n// All JWTs once encoded start with this\r\nconst jwtPrefix = \"eyJ\"\r\n\r\n// ReadOperatorJWT will read a jwt file for an operator claim. This can be a decorated file.\r\nfunc ReadOperatorJWT(jwtfile string) (*jwt.OperatorClaims, error) {\r\n\tcontents, err := ioutil.ReadFile(jwtfile)\r\n\tif err != nil {\r\n\t\t// Check to see if the JWT has been inlined.\r\n\t\tif !strings.HasPrefix(jwtfile, jwtPrefix) {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\t// We may have an inline jwt here.\r\n\t\tcontents = []byte(jwtfile)\r\n\t}\r\n\tdefer wipeSlice(contents)\r\n\r\n\tvar claim string\r\n\titems := nscDecoratedRe.FindAllSubmatch(contents, -1)\r\n\tif len(items) == 0 {\r\n\t\tclaim = string(contents)\r\n\t} else {\r\n\t\t// First result should be the JWT.\r\n\t\t// We copy here so that if the file contained a seed file too we wipe appropriately.\r\n\t\traw := items[0][1]\r\n\t\ttmp := make([]byte, len(raw))\r\n\t\tcopy(tmp, raw)\r\n\t\tclaim = string(tmp)\r\n\t}\r\n\topc, err := jwt.DecodeOperatorClaims(claim)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\treturn opc, nil\r\n}\r\n\r\n// Just wipe slice with 'x', for clearing contents of nkey seed file.\r\nfunc wipeSlice(buf []byte) {\r\n\tfor i := range buf {\r\n\t\tbuf[i] = 'x'\r\n\t}\r\n}\r\n\r\n// validateTrustedOperators will check that we do not have conflicts with\r\n// assigned trusted keys and trusted operators. If operators are defined we\r\n// will expand the trusted keys in options.\r\nfunc validateTrustedOperators(o *Options) error {\r\n\tif len(o.TrustedOperators) == 0 {\r\n\t\treturn nil\r\n\t}\r\n\tif o.AllowNewAccounts {\r\n\t\treturn fmt.Errorf(\"operators do not allow dynamic creation of new accounts\")\r\n\t}\r\n\tif o.AccountResolver == nil {\r\n\t\treturn fmt.Errorf(\"operators require an account resolver to be configured\")\r\n\t}\r\n\tif len(o.Accounts) > 0 {\r\n\t\treturn fmt.Errorf(\"operators do not allow Accounts to be configured directly\")\r\n\t}\r\n\tif len(o.Users) > 0 || len(o.Nkeys) > 0 {\r\n\t\treturn fmt.Errorf(\"operators do not allow users to be configured directly\")\r\n\t}\r\n\tif len(o.TrustedOperators) > 0 && len(o.TrustedKeys) > 0 {\r\n\t\treturn fmt.Errorf(\"conflicting options for 'TrustedKeys' and 'TrustedOperators'\")\r\n\t}\r\n\t// If we have operators, fill in the trusted keys.\r\n\t// FIXME(dlc) - We had TrustedKeys before TrustedOperators. The jwt.OperatorClaims\r\n\t// has a DidSign(). Use that longer term. For now we can expand in place.\r\n\tfor _, opc := range o.TrustedOperators {\r\n\t\tif o.TrustedKeys == nil {\r\n\t\t\to.TrustedKeys = make([]string, 0, 4)\r\n\t\t}\r\n\t\to.TrustedKeys = append(o.TrustedKeys, opc.Issuer)\r\n\t\to.TrustedKeys = append(o.TrustedKeys, opc.SigningKeys...)\r\n\t}\r\n\tfor _, key := range o.TrustedKeys {\r\n\t\tif !nkeys.IsValidPublicOperatorKey(key) {\r\n\t\t\treturn fmt.Errorf(\"trusted Keys %q are required to be a valid public operator nkey\", key)\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/jwt.go b/server/gnatsd/server/jwt.go
--- a/server/gnatsd/server/jwt.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/jwt.go	(date 1665399049607)
@@ -19,7 +19,7 @@
 	"regexp"
 	"strings"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/test/maxpayload_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2015-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage test\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"net\"\r\n\t\"runtime\"\r\n\t\"strings\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/kubemq-io/broker/client/nats\"\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nfunc TestMaxPayload(t *testing.T) {\r\n\tsrv, opts := RunServerWithConfig(\"./configs/override.conf\")\r\n\tdefer srv.Shutdown()\r\n\r\n\tendpoint := fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(fmt.Sprintf(\"nats://%s/\", endpoint))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not connect to server: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tsize := 4 * 1024 * 1024\r\n\tbig := sizedBytes(size)\r\n\terr = nc.Publish(\"foo\", big)\r\n\r\n\tif err != nats.ErrMaxPayload {\r\n\t\tt.Fatalf(\"Expected a Max Payload error\")\r\n\t}\r\n\r\n\tconn, err := net.DialTimeout(\"tcp\", endpoint, nc.Opts.Timeout)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not make a raw connection to the server: %v\", err)\r\n\t}\r\n\tdefer conn.Close()\r\n\tinfo := make([]byte, 512)\r\n\t_, err = conn.Read(info)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected an info message to be sent by the server: %s\", err)\r\n\t}\r\n\tpub := fmt.Sprintf(\"PUB bar %d\\r\\n\", size)\r\n\t_, err = conn.Write([]byte(pub))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not publish event to the server: %s\", err)\r\n\t}\r\n\r\n\terrMsg := make([]byte, 35)\r\n\t_, err = conn.Read(errMsg)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Expected an error message to be sent by the server: %s\", err)\r\n\t}\r\n\r\n\tif !strings.Contains(string(errMsg), \"Maximum Payload Violation\") {\r\n\t\tt.Errorf(\"Received wrong error message (%v)\\n\", string(errMsg))\r\n\t}\r\n\r\n\t// Client proactively omits sending the message so server\r\n\t// does not close the connection.\r\n\tif nc.IsClosed() {\r\n\t\tt.Errorf(\"Expected connection to not be closed.\")\r\n\t}\r\n\r\n\t// On the other hand client which did not proactively omitted\r\n\t// publishing the bytes following what is suggested by server\r\n\t// in the info message has its connection closed.\r\n\t_, err = conn.Write(big)\r\n\tif err == nil && runtime.GOOS != \"windows\" {\r\n\t\tt.Errorf(\"Expected error due to maximum payload transgression.\")\r\n\t}\r\n\r\n\t// On windows, the previous write will not fail because the connection\r\n\t// is not fully closed at this stage.\r\n\tif runtime.GOOS == \"windows\" {\r\n\t\t// Issuing a PING and not expecting the PONG.\r\n\t\t_, err = conn.Write([]byte(\"PING\\r\\n\"))\r\n\t\tif err == nil {\r\n\t\t\tconn.SetReadDeadline(time.Now().Add(500 * time.Millisecond))\r\n\t\t\t_, err = conn.Read(big)\r\n\t\t\tif err == nil {\r\n\t\t\t\tt.Errorf(\"Expected closed connection due to maximum payload transgression.\")\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc TestMaxPayloadOverrun(t *testing.T) {\r\n\topts := DefaultTestOptions\r\n\topts.Port = -1\r\n\topts.MaxPayload = 10000\r\n\ts := RunServer(&opts)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Overrun a int32\r\n\tc := createClientConn(t, \"127.0.0.1\", opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, expect := setupConn(t, c)\r\n\tsend(\"PUB foo 380571791000988\\r\\n\")\r\n\texpect(errRe)\r\n\r\n\t// Now overrun an int64, parseSize will have returned -1,\r\n\t// so we get disconnected.\r\n\tc = createClientConn(t, \"127.0.0.1\", opts.Port)\r\n\tdefer c.Close()\r\n\r\n\tsend, _ = setupConn(t, c)\r\n\tsend(\"PUB foo 18446744073709551615123\\r\\n\")\r\n\texpectDisconnect(t, c)\r\n}\r\n\r\nfunc TestAsyncInfoWithSmallerMaxPayload(t *testing.T) {\r\n\ts, opts := runOperatorServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tconst testMaxPayload = 522\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tnac.Limits.Payload = testMaxPayload\r\n\tajwt, err := nac.Encode(okp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\tif err := s.AccountResolver().Store(apub, ajwt); err != nil {\r\n\t\tt.Fatalf(\"Account Resolver returned an error: %v\", err)\r\n\t}\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\tdefer nc.Close()\r\n\r\n\tif mp := nc.MaxPayload(); mp != testMaxPayload {\r\n\t\tt.Fatalf(\"Expected MaxPayload of %d, got %d\", testMaxPayload, mp)\r\n\t}\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/test/maxpayload_test.go b/server/gnatsd/test/maxpayload_test.go
--- a/server/gnatsd/test/maxpayload_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/test/maxpayload_test.go	(date 1665399050194)
@@ -22,7 +22,7 @@
 	"time"
 
 	"github.com/kubemq-io/broker/client/nats"
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nkeys"
 )
 
Index: server/gnatsd/server/client.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2012-2020 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"crypto/tls\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"math/rand\"\r\n\t\"net\"\r\n\t\"regexp\"\r\n\t\"runtime\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n)\r\n\r\n// Type of client connection.\r\nconst (\r\n\t// CLIENT is an end user.\r\n\tCLIENT = iota\r\n\t// ROUTER represents another server in the cluster.\r\n\tROUTER\r\n\t// GATEWAY is a link between 2 clusters.\r\n\tGATEWAY\r\n\t// SYSTEM is an internal system client.\r\n\tSYSTEM\r\n\t// LEAF is for leaf node connections.\r\n\tLEAF\r\n)\r\n\r\nconst (\r\n\t// ClientProtoZero is the original Client protocol from 2009.\r\n\t// http://nats.io/documentation/internals/nats-protocol/\r\n\tClientProtoZero = iota\r\n\t// ClientProtoInfo signals a client can receive more then the original INFO block.\r\n\t// This can be used to update clients on other cluster members, etc.\r\n\tClientProtoInfo\r\n)\r\n\r\nconst (\r\n\tpingProto = \"PING\" + _CRLF_\r\n\tpongProto = \"PONG\" + _CRLF_\r\n\terrProto  = \"-ERR '%s'\" + _CRLF_\r\n\tokProto   = \"+OK\" + _CRLF_\r\n)\r\n\r\nfunc init() {\r\n\trand.Seed(time.Now().UnixNano())\r\n}\r\n\r\nconst (\r\n\t// Scratch buffer size for the processMsg() calls.\r\n\tmsgScratchSize  = 1024\r\n\tmsgHeadProto    = \"RMSG \"\r\n\tmsgHeadProtoLen = len(msgHeadProto)\r\n\r\n\t// For controlling dynamic buffer sizes.\r\n\tstartBufSize    = 512   // For INFO/CONNECT block\r\n\tminBufSize      = 64    // Smallest to shrink to for PING/PONG\r\n\tmaxBufSize      = 65536 // 64k\r\n\tshortsToShrink  = 2     // Trigger to shrink dynamic buffers\r\n\tmaxFlushPending = 10    // Max fsps to have in order to wait for writeLoop\r\n\treadLoopReport  = 2 * time.Second\r\n\r\n\t// Server should not send a PING (for RTT) before the first PONG has\r\n\t// been sent to the client. However, in case some client libs don't\r\n\t// send CONNECT+PING, cap the maximum time before server can send\r\n\t// the RTT PING.\r\n\tmaxNoRTTPingBeforeFirstPong = 2 * time.Second\r\n\r\n\t// For stalling fast producers\r\n\tstallClientMinDuration = 100 * time.Millisecond\r\n\tstallClientMaxDuration = time.Second\r\n)\r\n\r\nvar readLoopReportThreshold = readLoopReport\r\n\r\n// Represent client booleans with a bitmask\r\ntype clientFlag uint16\r\n\r\n// Some client state represented as flags\r\nconst (\r\n\tconnectReceived   clientFlag = 1 << iota // The CONNECT proto has been received\r\n\tinfoReceived                             // The INFO protocol has been received\r\n\tfirstPongSent                            // The first PONG has been sent\r\n\thandshakeComplete                        // For TLS clients, indicate that the handshake is complete\r\n\tflushOutbound                            // Marks client as having a flushOutbound call in progress.\r\n\tnoReconnect                              // Indicate that on close, this connection should not attempt a reconnect\r\n\tcloseConnection                          // Marks that closeConnection has already been called.\r\n\twriteLoopStarted                         // Marks that the writeLoop has been started.\r\n\tskipFlushOnClose                         // Marks that flushOutbound() should not be called on connection close.\r\n\texpectConnect                            // Marks if this connection is expected to send a CONNECT\r\n)\r\n\r\n// set the flag (would be equivalent to set the boolean to true)\r\nfunc (cf *clientFlag) set(c clientFlag) {\r\n\t*cf |= c\r\n}\r\n\r\n// clear the flag (would be equivalent to set the boolean to false)\r\nfunc (cf *clientFlag) clear(c clientFlag) {\r\n\t*cf &= ^c\r\n}\r\n\r\n// isSet returns true if the flag is set, false otherwise\r\nfunc (cf clientFlag) isSet(c clientFlag) bool {\r\n\treturn cf&c != 0\r\n}\r\n\r\n// setIfNotSet will set the flag `c` only if that flag was not already\r\n// set and return true to indicate that the flag has been set. Returns\r\n// false otherwise.\r\nfunc (cf *clientFlag) setIfNotSet(c clientFlag) bool {\r\n\tif *cf&c == 0 {\r\n\t\t*cf |= c\r\n\t\treturn true\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// ClosedState is the reason client was closed. This will\r\n// be passed into calls to clearConnection, but will only\r\n// be stored in ConnInfo for monitoring.\r\ntype ClosedState int\r\n\r\nconst (\r\n\tClientClosed = ClosedState(iota + 1)\r\n\tAuthenticationTimeout\r\n\tAuthenticationViolation\r\n\tTLSHandshakeError\r\n\tSlowConsumerPendingBytes\r\n\tSlowConsumerWriteDeadline\r\n\tWriteError\r\n\tReadError\r\n\tParseError\r\n\tStaleConnection\r\n\tProtocolViolation\r\n\tBadClientProtocolVersion\r\n\tWrongPort\r\n\tMaxAccountConnectionsExceeded\r\n\tMaxConnectionsExceeded\r\n\tMaxPayloadExceeded\r\n\tMaxControlLineExceeded\r\n\tMaxSubscriptionsExceeded\r\n\tDuplicateRoute\r\n\tRouteRemoved\r\n\tServerShutdown\r\n\tAuthenticationExpired\r\n\tWrongGateway\r\n\tMissingAccount\r\n\tRevocation\r\n)\r\n\r\n// Some flags passed to processMsgResultsEx\r\nconst pmrNoFlag int = 0\r\nconst (\r\n\tpmrCollectQueueNames int = 1 << iota\r\n\tpmrIgnoreEmptyQueueFilter\r\n\tpmrAllowSendFromRouteToRoute\r\n)\r\n\r\ntype client struct {\r\n\t// Here first because of use of atomics, and memory alignment.\r\n\tstats\r\n\t// Indicate if we should check gwrm or not. Since checking gwrm is done\r\n\t// when processing inbound messages and requires the lock we want to\r\n\t// check only when needed. This is set/get using atomic, so needs to\r\n\t// be memory aligned.\r\n\tcgwrt   int32\r\n\tmpay    int32\r\n\tmsubs   int32\r\n\tmcl     int32\r\n\tmu      sync.Mutex\r\n\tkind    int\r\n\tcid     uint64\r\n\topts    clientOpts\r\n\tstart   time.Time\r\n\tnonce   []byte\r\n\tnc      net.Conn\r\n\tncs     string\r\n\tout     outbound\r\n\tsrv     *Server\r\n\tacc     *Account\r\n\tuser    *NkeyUser\r\n\thost    string\r\n\tport    uint16\r\n\tsubs    map[string]*subscription\r\n\tperms   *permissions\r\n\treplies map[string]*resp\r\n\tmperms  *msgDeny\r\n\tdarray  []string\r\n\tin      readCache\r\n\tpcd     map[*client]struct{}\r\n\tatmr    *time.Timer\r\n\tping    pinfo\r\n\tmsgb    [msgScratchSize]byte\r\n\tlast    time.Time\r\n\tparseState\r\n\r\n\trtt        time.Duration\r\n\trttStart   time.Time\r\n\trrTracking map[string]*remoteLatency\r\n\trrMax      int\r\n\r\n\troute *route\r\n\tgw    *gateway\r\n\tleaf  *leaf\r\n\r\n\t// To keep track of gateway replies mapping\r\n\tgwrm map[string]*gwReplyMap\r\n\r\n\tflags clientFlag // Compact booleans into a single field. Size will be increased when needed.\r\n\r\n\ttrace bool\r\n\techo  bool\r\n}\r\n\r\n// Struct for PING initiation from the server.\r\ntype pinfo struct {\r\n\ttmr  *time.Timer\r\n\tlast time.Time\r\n\tout  int\r\n}\r\n\r\n// outbound holds pending data for a socket.\r\ntype outbound struct {\r\n\tp   []byte        // Primary write buffer\r\n\ts   []byte        // Secondary for use post flush\r\n\tnb  net.Buffers   // net.Buffers for writev IO\r\n\tsz  int32         // limit size per []byte, uses variable BufSize constants, start, min, max.\r\n\tsws int32         // Number of short writes, used for dynamic resizing.\r\n\tpb  int64         // Total pending/queued bytes.\r\n\tpm  int32         // Total pending/queued messages.\r\n\tfsp int32         // Flush signals that are pending per producer from readLoop's pcd.\r\n\tsch chan struct{} // To signal writeLoop that there is data to flush.\r\n\twdl time.Duration // Snapshot of write deadline.\r\n\tmp  int64         // Snapshot of max pending for client.\r\n\tlft time.Duration // Last flush time for Write.\r\n\tstc chan struct{} // Stall chan we create to slow down producers on overrun, e.g. fan-in.\r\n\tlwb int32         // Last byte size of Write.\r\n}\r\n\r\ntype perm struct {\r\n\tallow *Sublist\r\n\tdeny  *Sublist\r\n}\r\n\r\ntype permissions struct {\r\n\tsub    perm\r\n\tpub    perm\r\n\tresp   *ResponsePermission\r\n\tpcache map[string]bool\r\n}\r\n\r\n// This is used to dynamically track responses and reply subjects\r\n// for dynamic permissioning.\r\ntype resp struct {\r\n\tt time.Time\r\n\tn int\r\n}\r\n\r\n// msgDeny is used when a user permission for subscriptions has a deny\r\n// clause but a subscription could be made that is of broader scope.\r\n// e.g. deny = \"foo\", but user subscribes to \"*\". That subscription should\r\n// succeed but no message sent on foo should be delivered.\r\ntype msgDeny struct {\r\n\tdeny   *Sublist\r\n\tdcache map[string]bool\r\n}\r\n\r\n// routeTarget collects information regarding routes and queue groups for\r\n// sending information to a remote.\r\ntype routeTarget struct {\r\n\tsub *subscription\r\n\tqs  []byte\r\n\t_qs [32]byte\r\n}\r\n\r\nconst (\r\n\tmaxResultCacheSize   = 512\r\n\tmaxDenyPermCacheSize = 256\r\n\tmaxPermCacheSize     = 128\r\n\tpruneSize            = 32\r\n\trouteTargetInit      = 8\r\n\treplyPermLimit       = 4096\r\n)\r\n\r\n// Used in readloop to cache hot subject lookups and group statistics.\r\ntype readCache struct {\r\n\t// These are for clients who are bound to a single account.\r\n\tgenid   uint64\r\n\tresults map[string]*SublistResult\r\n\r\n\t// This is for routes and gateways to have their own L1 as well that is account aware.\r\n\tpacache map[string]*perAccountCache\r\n\r\n\t// This is for when we deliver messages across a route. We use this structure\r\n\t// to make sure to only send one message and properly scope to queues as needed.\r\n\trts []routeTarget\r\n\r\n\tprand *rand.Rand\r\n\r\n\t// These are all temporary totals for an invocation of a read in readloop.\r\n\tmsgs  int32\r\n\tbytes int32\r\n\tsubs  int32\r\n\r\n\trsz int32 // Read buffer size\r\n\tsrs int32 // Short reads, used for dynamic buffer resizing.\r\n}\r\n\r\nconst (\r\n\tdefaultMaxPerAccountCacheSize   = 4096\r\n\tdefaultPrunePerAccountCacheSize = 256\r\n\tdefaultClosedSubsCheckInterval  = 5 * time.Minute\r\n)\r\n\r\nvar (\r\n\tmaxPerAccountCacheSize   = defaultMaxPerAccountCacheSize\r\n\tprunePerAccountCacheSize = defaultPrunePerAccountCacheSize\r\n\tclosedSubsCheckInterval  = defaultClosedSubsCheckInterval\r\n)\r\n\r\n// perAccountCache is for L1 semantics for inbound messages from a route or gateway to mimic the performance of clients.\r\ntype perAccountCache struct {\r\n\tacc     *Account\r\n\tresults *SublistResult\r\n\tgenid   uint64\r\n}\r\n\r\nfunc (c *client) String() (id string) {\r\n\treturn c.ncs\r\n}\r\n\r\n// GetName returns the application supplied name for the connection.\r\nfunc (c *client) GetName() string {\r\n\tc.mu.Lock()\r\n\tname := c.opts.Name\r\n\tc.mu.Unlock()\r\n\treturn name\r\n}\r\n\r\n// GetOpts returns the client options provided by the application.\r\nfunc (c *client) GetOpts() *clientOpts {\r\n\treturn &c.opts\r\n}\r\n\r\n// GetTLSConnectionState returns the TLS ConnectionState if TLS is enabled, nil\r\n// otherwise. Implements the ClientAuth interface.\r\nfunc (c *client) GetTLSConnectionState() *tls.ConnectionState {\r\n\ttc, ok := c.nc.(*tls.Conn)\r\n\tif !ok {\r\n\t\treturn nil\r\n\t}\r\n\tstate := tc.ConnectionState()\r\n\treturn &state\r\n}\r\n\r\n// This is the main subscription struct that indicates\r\n// interest in published messages.\r\n// FIXME(dlc) - This is getting bloated for normal subs, need\r\n// to optionally have an opts section for non-normal stuff.\r\ntype subscription struct {\r\n\tclient  *client\r\n\tim      *streamImport   // This is for import stream support.\r\n\tshadow  []*subscription // This is to track shadowed accounts.\r\n\tsubject []byte\r\n\tqueue   []byte\r\n\tsid     []byte\r\n\tnm      int64\r\n\tmax     int64\r\n\tqw      int32\r\n\tclosed  int32\r\n}\r\n\r\n// Indicate that this subscription is closed.\r\n// This is used in pruning of route and gateway cache items.\r\nfunc (s *subscription) close() {\r\n\tatomic.StoreInt32(&s.closed, 1)\r\n}\r\n\r\n// Return true if this subscription was unsubscribed\r\n// or its connection has been closed.\r\nfunc (s *subscription) isClosed() bool {\r\n\treturn atomic.LoadInt32(&s.closed) == 1\r\n}\r\n\r\ntype clientOpts struct {\r\n\tEcho          bool   `json:\"echo\"`\r\n\tVerbose       bool   `json:\"verbose\"`\r\n\tPedantic      bool   `json:\"pedantic\"`\r\n\tTLSRequired   bool   `json:\"tls_required\"`\r\n\tNkey          string `json:\"nkey,omitempty\"`\r\n\tJWT           string `json:\"jwt,omitempty\"`\r\n\tSig           string `json:\"sig,omitempty\"`\r\n\tAuthorization string `json:\"auth_token,omitempty\"`\r\n\tUsername      string `json:\"user,omitempty\"`\r\n\tPassword      string `json:\"pass,omitempty\"`\r\n\tName          string `json:\"name\"`\r\n\tLang          string `json:\"lang\"`\r\n\tVersion       string `json:\"version\"`\r\n\tProtocol      int    `json:\"protocol\"`\r\n\tAccount       string `json:\"account,omitempty\"`\r\n\tAccountNew    bool   `json:\"new_account,omitempty\"`\r\n\r\n\t// Routes only\r\n\tImport *SubjectPermission `json:\"import,omitempty\"`\r\n\tExport *SubjectPermission `json:\"export,omitempty\"`\r\n}\r\n\r\nvar defaultOpts = clientOpts{Verbose: true, Pedantic: true, Echo: true}\r\nvar internalOpts = clientOpts{Verbose: false, Pedantic: false, Echo: false}\r\n\r\nfunc init() {\r\n\trand.Seed(time.Now().UnixNano())\r\n}\r\n\r\nfunc (c *client) setTraceLevel() {\r\n\tif c.kind == SYSTEM && !(atomic.LoadInt32(&c.srv.logging.traceSysAcc) != 0) {\r\n\t\tc.trace = false\r\n\t} else {\r\n\t\tc.trace = (atomic.LoadInt32(&c.srv.logging.trace) != 0)\r\n\t}\r\n}\r\n\r\n// Lock should be held\r\nfunc (c *client) initClient() {\r\n\ts := c.srv\r\n\tc.cid = atomic.AddUint64(&s.gcid, 1)\r\n\r\n\t// Outbound data structure setup\r\n\tc.out.sz = startBufSize\r\n\tc.out.sch = make(chan struct{}, 1)\r\n\topts := s.getOpts()\r\n\t// Snapshots to avoid mutex access in fast paths.\r\n\tc.out.wdl = opts.WriteDeadline\r\n\tc.out.mp = opts.MaxPending\r\n\r\n\tc.subs = make(map[string]*subscription)\r\n\tc.echo = true\r\n\r\n\tc.setTraceLevel()\r\n\r\n\t// This is a scratch buffer used for processMsg()\r\n\t// The msg header starts with \"RMSG \", which can be used\r\n\t// for both local and routes.\r\n\t// in bytes that is [82 77 83 71 32].\r\n\tc.msgb = [msgScratchSize]byte{82, 77, 83, 71, 32}\r\n\r\n\t// This is to track pending clients that have data to be flushed\r\n\t// after we process inbound msgs from our own connection.\r\n\tc.pcd = make(map[*client]struct{})\r\n\r\n\t// snapshot the string version of the connection\r\n\tvar conn string\r\n\tif c.nc != nil {\r\n\t\tif addr := c.nc.RemoteAddr(); addr != nil {\r\n\t\t\tif conn = addr.String(); conn != _EMPTY_ {\r\n\t\t\t\thost, port, _ := net.SplitHostPort(conn)\r\n\t\t\t\tiPort, _ := strconv.Atoi(port)\r\n\t\t\t\tc.host, c.port = host, uint16(iPort)\r\n\t\t\t\t// Now that we have extracted host and port, escape\r\n\t\t\t\t// the string because it is going to be used in Sprintf\r\n\t\t\t\tconn = strings.ReplaceAll(conn, \"%\", \"%%\")\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tswitch c.kind {\r\n\tcase CLIENT:\r\n\t\tc.ncs = fmt.Sprintf(\"%s - cid:%d\", conn, c.cid)\r\n\tcase ROUTER:\r\n\t\tc.ncs = fmt.Sprintf(\"%s - rid:%d\", conn, c.cid)\r\n\tcase GATEWAY:\r\n\t\tc.ncs = fmt.Sprintf(\"%s - gid:%d\", conn, c.cid)\r\n\tcase LEAF:\r\n\t\tc.ncs = fmt.Sprintf(\"%s - lid:%d\", conn, c.cid)\r\n\tcase SYSTEM:\r\n\t\tc.ncs = \"SYSTEM\"\r\n\t}\r\n}\r\n\r\n// RemoteAddress expose the Address of the client connection,\r\n// nil when not connected or unknown\r\nfunc (c *client) RemoteAddress() net.Addr {\r\n\tc.mu.Lock()\r\n\tdefer c.mu.Unlock()\r\n\r\n\tif c.nc == nil {\r\n\t\treturn nil\r\n\t}\r\n\r\n\treturn c.nc.RemoteAddr()\r\n}\r\n\r\n// Helper function to report errors.\r\nfunc (c *client) reportErrRegisterAccount(acc *Account, err error) {\r\n\tif err == ErrTooManyAccountConnections {\r\n\t\tc.maxAccountConnExceeded()\r\n\t\treturn\r\n\t}\r\n\tc.Errorf(\"Problem registering with account [%s]\", acc.Name)\r\n\tc.sendErr(\"Failed Account Registration\")\r\n}\r\n\r\n// registerWithAccount will register the given user with a specific\r\n// account. This will change the subject namespace.\r\nfunc (c *client) registerWithAccount(acc *Account) error {\r\n\tif acc == nil || acc.sl == nil {\r\n\t\treturn ErrBadAccount\r\n\t}\r\n\t// If we were previously registered, usually to $G, do accounting here to remove.\r\n\tif c.acc != nil {\r\n\t\tif prev := c.acc.removeClient(c); prev == 1 && c.srv != nil {\r\n\t\t\tc.srv.decActiveAccounts()\r\n\t\t}\r\n\t}\r\n\r\n\tc.mu.Lock()\r\n\tkind := c.kind\r\n\tsrv := c.srv\r\n\tc.acc = acc\r\n\tc.applyAccountLimits()\r\n\tc.mu.Unlock()\r\n\r\n\t// Check if we have a max connections violation\r\n\tif kind == CLIENT && acc.MaxTotalConnectionsReached() {\r\n\t\treturn ErrTooManyAccountConnections\r\n\t} else if kind == LEAF && acc.MaxTotalLeafNodesReached() {\r\n\t\treturn ErrTooManyAccountConnections\r\n\t}\r\n\r\n\t// Add in new one.\r\n\tif prev := acc.addClient(c); prev == 0 && srv != nil {\r\n\t\tsrv.incActiveAccounts()\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// Helper to determine if we have met or exceeded max subs.\r\nfunc (c *client) subsAtLimit() bool {\r\n\treturn c.msubs != jwt.NoLimit && len(c.subs) >= int(c.msubs)\r\n}\r\n\r\n// Apply account limits\r\n// Lock is held on entry.\r\n// FIXME(dlc) - Should server be able to override here?\r\nfunc (c *client) applyAccountLimits() {\r\n\tif c.acc == nil || (c.kind != CLIENT && c.kind != LEAF) {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Set here, will need to fo checks for NoLimit.\r\n\tif c.acc.msubs != jwt.NoLimit {\r\n\t\tc.msubs = c.acc.msubs\r\n\t}\r\n\tif c.acc.mpay != jwt.NoLimit {\r\n\t\tc.mpay = c.acc.mpay\r\n\t}\r\n\r\n\ts := c.srv\r\n\topts := s.getOpts()\r\n\r\n\t// We check here if the server has an option set that is lower than the account limit.\r\n\tif c.mpay != jwt.NoLimit && opts.MaxPayload != 0 && int32(opts.MaxPayload) < c.acc.mpay {\r\n\t\tc.Errorf(\"Max Payload set to %d from server config which overrides %d from account claims\", opts.MaxPayload, c.acc.mpay)\r\n\t\tc.mpay = int32(opts.MaxPayload)\r\n\t}\r\n\r\n\t// We check here if the server has an option set that is lower than the account limit.\r\n\tif c.msubs != jwt.NoLimit && opts.MaxSubs != 0 && opts.MaxSubs < int(c.acc.msubs) {\r\n\t\tc.Errorf(\"Max Subscriptions set to %d from server config which overrides %d from account claims\", opts.MaxSubs, c.acc.msubs)\r\n\t\tc.msubs = int32(opts.MaxSubs)\r\n\t}\r\n\r\n\tif c.subsAtLimit() {\r\n\t\tgo func() {\r\n\t\t\tc.maxSubsExceeded()\r\n\t\t\ttime.Sleep(20 * time.Millisecond)\r\n\t\t\tc.closeConnection(MaxSubscriptionsExceeded)\r\n\t\t}()\r\n\t}\r\n}\r\n\r\n// RegisterUser allows auth to call back into a new client\r\n// with the authenticated user. This is used to map\r\n// any permissions into the client and setup accounts.\r\nfunc (c *client) RegisterUser(user *User) {\r\n\t// Register with proper account and sublist.\r\n\tif user.Account != nil {\r\n\t\tif err := c.registerWithAccount(user.Account); err != nil {\r\n\t\t\tc.reportErrRegisterAccount(user.Account, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n\r\n\tc.mu.Lock()\r\n\r\n\t// Assign permissions.\r\n\tif user.Permissions == nil {\r\n\t\t// Reset perms to nil in case client previously had them.\r\n\t\tc.perms = nil\r\n\t\tc.mperms = nil\r\n\t} else {\r\n\t\tc.setPermissions(user.Permissions)\r\n\t}\r\n\tc.mu.Unlock()\r\n}\r\n\r\n// RegisterNkey allows auth to call back into a new nkey\r\n// client with the authenticated user. This is used to map\r\n// any permissions into the client and setup accounts.\r\nfunc (c *client) RegisterNkeyUser(user *NkeyUser) error {\r\n\t// Register with proper account and sublist.\r\n\tif user.Account != nil {\r\n\t\tif err := c.registerWithAccount(user.Account); err != nil {\r\n\t\t\tc.reportErrRegisterAccount(user.Account, err)\r\n\t\t\treturn err\r\n\t\t}\r\n\t}\r\n\r\n\tc.mu.Lock()\r\n\tc.user = user\r\n\t// Assign permissions.\r\n\tif user.Permissions == nil {\r\n\t\t// Reset perms to nil in case client previously had them.\r\n\t\tc.perms = nil\r\n\t\tc.mperms = nil\r\n\t} else {\r\n\t\tc.setPermissions(user.Permissions)\r\n\t}\r\n\tc.mu.Unlock()\r\n\treturn nil\r\n}\r\n\r\nfunc splitSubjectQueue(sq string) ([]byte, []byte, error) {\r\n\tvals := strings.Fields(strings.TrimSpace(sq))\r\n\ts := []byte(vals[0])\r\n\tvar q []byte\r\n\tif len(vals) == 2 {\r\n\t\tq = []byte(vals[1])\r\n\t} else if len(vals) > 2 {\r\n\t\treturn nil, nil, fmt.Errorf(\"invalid subject-queue %q\", sq)\r\n\t}\r\n\treturn s, q, nil\r\n}\r\n\r\n// Initializes client.perms structure.\r\n// Lock is held on entry.\r\nfunc (c *client) setPermissions(perms *Permissions) {\r\n\tif perms == nil {\r\n\t\treturn\r\n\t}\r\n\tc.perms = &permissions{}\r\n\tc.perms.pcache = make(map[string]bool)\r\n\r\n\t// Loop over publish permissions\r\n\tif perms.Publish != nil {\r\n\t\tif perms.Publish.Allow != nil {\r\n\t\t\tc.perms.pub.allow = NewSublistWithCache()\r\n\t\t}\r\n\t\tfor _, pubSubject := range perms.Publish.Allow {\r\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\r\n\t\t\tc.perms.pub.allow.Insert(sub)\r\n\t\t}\r\n\t\tif len(perms.Publish.Deny) > 0 {\r\n\t\t\tc.perms.pub.deny = NewSublistWithCache()\r\n\t\t}\r\n\t\tfor _, pubSubject := range perms.Publish.Deny {\r\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\r\n\t\t\tc.perms.pub.deny.Insert(sub)\r\n\t\t}\r\n\t}\r\n\r\n\t// Check if we are allowed to send responses.\r\n\tif perms.Response != nil {\r\n\t\trp := *perms.Response\r\n\t\tc.perms.resp = &rp\r\n\t\tc.replies = make(map[string]*resp)\r\n\t}\r\n\r\n\t// Loop over subscribe permissions\r\n\tif perms.Subscribe != nil {\r\n\t\tvar err error\r\n\t\tif len(perms.Subscribe.Allow) > 0 {\r\n\t\t\tc.perms.sub.allow = NewSublistWithCache()\r\n\t\t}\r\n\t\tfor _, subSubject := range perms.Subscribe.Allow {\r\n\t\t\tsub := &subscription{}\r\n\t\t\tsub.subject, sub.queue, err = splitSubjectQueue(subSubject)\r\n\t\t\tif err != nil {\r\n\t\t\t\tc.Errorf(\"%s\", err.Error())\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tc.perms.sub.allow.Insert(sub)\r\n\t\t}\r\n\t\tif len(perms.Subscribe.Deny) > 0 {\r\n\t\t\tc.perms.sub.deny = NewSublistWithCache()\r\n\t\t\t// Also hold onto this array for later.\r\n\t\t\tc.darray = perms.Subscribe.Deny\r\n\t\t}\r\n\t\tfor _, subSubject := range perms.Subscribe.Deny {\r\n\t\t\tsub := &subscription{}\r\n\t\t\tsub.subject, sub.queue, err = splitSubjectQueue(subSubject)\r\n\t\t\tif err != nil {\r\n\t\t\t\tc.Errorf(\"%s\", err.Error())\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tc.perms.sub.deny.Insert(sub)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Check to see if we have an expiration for the user JWT via base claims.\r\n// FIXME(dlc) - Clear on connect with new JWT.\r\nfunc (c *client) checkExpiration(claims *jwt.ClaimsData) {\r\n\tif claims.Expires == 0 {\r\n\t\treturn\r\n\t}\r\n\ttn := time.Now().Unix()\r\n\tif claims.Expires < tn {\r\n\t\treturn\r\n\t}\r\n\texpiresAt := time.Duration(claims.Expires - tn)\r\n\tc.setExpirationTimer(expiresAt * time.Second)\r\n}\r\n\r\n// This will load up the deny structure used for filtering delivered\r\n// messages based on a deny clause for subscriptions.\r\n// Lock should be held.\r\nfunc (c *client) loadMsgDenyFilter() {\r\n\tc.mperms = &msgDeny{NewSublistWithCache(), make(map[string]bool)}\r\n\tfor _, sub := range c.darray {\r\n\t\tc.mperms.deny.Insert(&subscription{subject: []byte(sub)})\r\n\t}\r\n}\r\n\r\n// writeLoop is the main socket write functionality.\r\n// Runs in its own Go routine.\r\nfunc (c *client) writeLoop() {\r\n\tdefer c.srv.grWG.Done()\r\n\tc.mu.Lock()\r\n\tif c.isClosed() {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\tc.flags.set(writeLoopStarted)\r\n\tch := c.out.sch\r\n\tc.mu.Unlock()\r\n\r\n\t// This will clear connection state and remove it from the server.\r\n\tdefer c.teardownConn()\r\n\r\n\t// Used to check that we did flush from last wake up.\r\n\twaitOk := true\r\n\r\n\t// Used to limit the wait for a signal\r\n\tconst maxWait = time.Second\r\n\tt := time.NewTimer(maxWait)\r\n\r\n\tvar close bool\r\n\r\n\t// Main loop. Will wait to be signaled and then will use\r\n\t// buffered outbound structure for efficient writev to the underlying socket.\r\n\tfor {\r\n\t\tc.mu.Lock()\r\n\t\tif close = c.flags.isSet(closeConnection); !close {\r\n\t\t\towtf := c.out.fsp > 0 && c.out.pb < maxBufSize && c.out.fsp < maxFlushPending\r\n\t\t\tif waitOk && (c.out.pb == 0 || owtf) {\r\n\t\t\t\tc.mu.Unlock()\r\n\r\n\t\t\t\t// Reset our timer\r\n\t\t\t\tt.Reset(maxWait)\r\n\r\n\t\t\t\t// Wait on pending data.\r\n\t\t\t\tselect {\r\n\t\t\t\tcase <-ch:\r\n\t\t\t\tcase <-t.C:\r\n\t\t\t\t}\r\n\r\n\t\t\t\tc.mu.Lock()\r\n\t\t\t\tclose = c.flags.isSet(closeConnection)\r\n\t\t\t}\r\n\t\t}\r\n\t\tif close {\r\n\t\t\tc.flushAndClose(false)\r\n\t\t\tc.mu.Unlock()\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// Flush data\r\n\t\twaitOk = c.flushOutbound()\r\n\t\tc.mu.Unlock()\r\n\t}\r\n}\r\n\r\n// flushClients will make sure to flush any clients we may have\r\n// sent to during processing. We pass in a budget as a time.Duration\r\n// for how much time to spend in place flushing for this client. This\r\n// will normally be called in the readLoop of the client who sent the\r\n// message that now is being delivered.\r\nfunc (c *client) flushClients(budget time.Duration) time.Time {\r\n\tlast := time.Now()\r\n\t// Check pending clients for flush.\r\n\tfor cp := range c.pcd {\r\n\t\t// TODO(dlc) - Wonder if it makes more sense to create a new map?\r\n\t\tdelete(c.pcd, cp)\r\n\r\n\t\t// Queue up a flush for those in the set\r\n\t\tcp.mu.Lock()\r\n\t\t// Update last activity for message delivery\r\n\t\tcp.last = last\r\n\t\t// Remove ourselves from the pending list.\r\n\t\tcp.out.fsp--\r\n\r\n\t\t// Just ignore if this was closed.\r\n\t\tif cp.flags.isSet(closeConnection) {\r\n\t\t\tcp.mu.Unlock()\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tif budget > 0 && cp.flushOutbound() {\r\n\t\t\tbudget -= cp.out.lft\r\n\t\t} else {\r\n\t\t\tcp.flushSignal()\r\n\t\t}\r\n\r\n\t\tcp.mu.Unlock()\r\n\t}\r\n\treturn last\r\n}\r\n\r\n// readLoop is the main socket read functionality.\r\n// Runs in its own Go routine.\r\nfunc (c *client) readLoop() {\r\n\t// Grab the connection off the client, it will be cleared on a close.\r\n\t// We check for that after the loop, but want to avoid a nil dereference\r\n\tc.mu.Lock()\r\n\ts := c.srv\r\n\tdefer s.grWG.Done()\r\n\tif c.isClosed() {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\tnc := c.nc\r\n\tc.in.rsz = startBufSize\r\n\t// Snapshot max control line since currently can not be changed on reload and we\r\n\t// were checking it on each call to parse. If this changes and we allow MaxControlLine\r\n\t// to be reloaded without restart, this code will need to change.\r\n\tc.mcl = MAX_CONTROL_LINE_SIZE\r\n\tif s != nil {\r\n\t\tif opts := s.getOpts(); opts != nil {\r\n\t\t\tc.mcl = int32(opts.MaxControlLine)\r\n\t\t}\r\n\t}\r\n\t// Check the per-account-cache for closed subscriptions\r\n\tcpacc := c.kind == ROUTER || c.kind == GATEWAY\r\n\t// Last per-account-cache check for closed subscriptions\r\n\tlpacc := time.Now()\r\n\tc.mu.Unlock()\r\n\r\n\tdefer func() {\r\n\t\t// These are used only in the readloop, so we can set them to nil\r\n\t\t// on exit of the readLoop.\r\n\t\tc.in.results, c.in.pacache = nil, nil\r\n\t}()\r\n\r\n\t// Start read buffer.\r\n\tb := make([]byte, c.in.rsz)\r\n\r\n\tfor {\r\n\t\tn, err := nc.Read(b)\r\n\t\t// If we have any data we will try to parse and exit at the end.\r\n\t\tif n == 0 && err != nil {\r\n\t\t\tc.closeConnection(closedStateForErr(err))\r\n\t\t\treturn\r\n\t\t}\r\n\t\tstart := time.Now()\r\n\r\n\t\t// Clear inbound stats cache\r\n\t\tc.in.msgs = 0\r\n\t\tc.in.bytes = 0\r\n\t\tc.in.subs = 0\r\n\r\n\t\t// Main call into parser for inbound data. This will generate callouts\r\n\t\t// to process messages, etc.\r\n\t\tif err := c.parse(b[:n]); err != nil {\r\n\t\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\r\n\t\t\t\tc.Warnf(\"Readloop processing time: %v\", dur)\r\n\t\t\t}\r\n\t\t\t// Need to call flushClients because some of the clients have been\r\n\t\t\t// assigned messages and their \"fsp\" incremented, and need now to be\r\n\t\t\t// decremented and their writeLoop signaled.\r\n\t\t\tc.flushClients(0)\r\n\t\t\t// handled inline\r\n\t\t\tif err != ErrMaxPayload && err != ErrAuthentication {\r\n\t\t\t\tc.Error(err)\r\n\t\t\t\tc.closeConnection(ProtocolViolation)\r\n\t\t\t}\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\t// Updates stats for client and server that were collected\r\n\t\t// from parsing through the buffer.\r\n\t\tif c.in.msgs > 0 {\r\n\t\t\tatomic.AddInt64(&c.inMsgs, int64(c.in.msgs))\r\n\t\t\tatomic.AddInt64(&c.inBytes, int64(c.in.bytes))\r\n\t\t\tatomic.AddInt64(&s.inMsgs, int64(c.in.msgs))\r\n\t\t\tatomic.AddInt64(&s.inBytes, int64(c.in.bytes))\r\n\t\t}\r\n\r\n\t\t// Budget to spend in place flushing outbound data.\r\n\t\t// Client will be checked on several fronts to see\r\n\t\t// if applicable. Routes and Gateways will never\r\n\t\t// spend time flushing outbound in place.\r\n\t\tvar budget time.Duration\r\n\t\tif c.kind == CLIENT {\r\n\t\t\tbudget = time.Millisecond\r\n\t\t}\r\n\r\n\t\t// Flush, or signal to writeLoop to flush to socket.\r\n\t\tlast := c.flushClients(budget)\r\n\r\n\t\t// Update activity, check read buffer size.\r\n\t\tc.mu.Lock()\r\n\t\tclosed := c.isClosed()\r\n\r\n\t\t// Activity based on interest changes or data/msgs.\r\n\t\tif c.in.msgs > 0 || c.in.subs > 0 {\r\n\t\t\tc.last = last\r\n\t\t}\r\n\r\n\t\tif n >= cap(b) {\r\n\t\t\tc.in.srs = 0\r\n\t\t} else if n < cap(b)/2 { // divide by 2 b/c we want less than what we would shrink to.\r\n\t\t\tc.in.srs++\r\n\t\t}\r\n\r\n\t\t// Update read buffer size as/if needed.\r\n\t\tif n >= cap(b) && cap(b) < maxBufSize {\r\n\t\t\t// Grow\r\n\t\t\tc.in.rsz = int32(cap(b) * 2)\r\n\t\t\tb = make([]byte, c.in.rsz)\r\n\t\t} else if n < cap(b) && cap(b) > minBufSize && c.in.srs > shortsToShrink {\r\n\t\t\t// Shrink, for now don't accelerate, ping/pong will eventually sort it out.\r\n\t\t\tc.in.rsz = int32(cap(b) / 2)\r\n\t\t\tb = make([]byte, c.in.rsz)\r\n\t\t}\r\n\t\tc.mu.Unlock()\r\n\r\n\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\r\n\t\t\tc.Warnf(\"Readloop processing time: %v\", dur)\r\n\t\t}\r\n\r\n\t\t// Check to see if we got closed, e.g. slow consumer\r\n\t\tif closed {\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\t// We could have had a read error from above but still read some data.\r\n\t\t// If so do the close here unconditionally.\r\n\t\tif err != nil {\r\n\t\t\tc.closeConnection(closedStateForErr(err))\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\tif cpacc && start.Sub(lpacc) >= closedSubsCheckInterval {\r\n\t\t\tc.pruneClosedSubFromPerAccountCache()\r\n\t\t\tlpacc = time.Now()\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Returns the appropriate closed state for a given read error.\r\nfunc closedStateForErr(err error) ClosedState {\r\n\tif err == io.EOF {\r\n\t\treturn ClientClosed\r\n\t}\r\n\treturn ReadError\r\n}\r\n\r\n// collapsePtoNB will place primary onto nb buffer as needed in prep for WriteTo.\r\n// This will return a copy on purpose.\r\nfunc (c *client) collapsePtoNB() net.Buffers {\r\n\tif c.out.p != nil {\r\n\t\tp := c.out.p\r\n\t\tc.out.p = nil\r\n\t\treturn append(c.out.nb, p)\r\n\t}\r\n\treturn c.out.nb\r\n}\r\n\r\n// This will handle the fixup needed on a partial write.\r\n// Assume pending has been already calculated correctly.\r\nfunc (c *client) handlePartialWrite(pnb net.Buffers) {\r\n\tnb := c.collapsePtoNB()\r\n\t// The partial needs to be first, so append nb to pnb\r\n\tc.out.nb = append(pnb, nb...)\r\n}\r\n\r\n// flushOutbound will flush outbound buffer to a client.\r\n// Will return true if data was attempted to be written.\r\n// Lock must be held\r\nfunc (c *client) flushOutbound() bool {\r\n\tif c.flags.isSet(flushOutbound) {\r\n\t\t// For CLIENT connections, it is possible that the readLoop calls\r\n\t\t// flushOutbound(). If writeLoop and readLoop compete and we are\r\n\t\t// here we should release the lock to reduce the risk of spinning.\r\n\t\tc.mu.Unlock()\r\n\t\truntime.Gosched()\r\n\t\tc.mu.Lock()\r\n\t\treturn false\r\n\t}\r\n\tc.flags.set(flushOutbound)\r\n\tdefer c.flags.clear(flushOutbound)\r\n\r\n\t// Check for nothing to do.\r\n\tif c.nc == nil || c.srv == nil || c.out.pb == 0 {\r\n\t\treturn true // true because no need to queue a signal.\r\n\t}\r\n\r\n\t// Place primary on nb, assign primary to secondary, nil out nb and secondary.\r\n\tnb := c.collapsePtoNB()\r\n\tc.out.p, c.out.nb, c.out.s = c.out.s, nil, nil\r\n\r\n\t// For selecting primary replacement.\r\n\tcnb := nb\r\n\tvar lfs int\r\n\tif len(cnb) > 0 {\r\n\t\tlfs = len(cnb[0])\r\n\t}\r\n\r\n\t// In case it goes away after releasing the lock.\r\n\tnc := c.nc\r\n\tattempted := c.out.pb\r\n\tapm := c.out.pm\r\n\r\n\t// Capture this (we change the value in some tests)\r\n\twdl := c.out.wdl\r\n\t// Do NOT hold lock during actual IO.\r\n\tc.mu.Unlock()\r\n\r\n\t// flush here\r\n\tnow := time.Now()\r\n\t// FIXME(dlc) - writev will do multiple IOs past 1024 on\r\n\t// most platforms, need to account for that with deadline?\r\n\tnc.SetWriteDeadline(now.Add(wdl))\r\n\r\n\t// Actual write to the socket.\r\n\tn, err := nb.WriteTo(nc)\r\n\tnc.SetWriteDeadline(time.Time{})\r\n\tlft := time.Since(now)\r\n\r\n\t// Re-acquire client lock.\r\n\tc.mu.Lock()\r\n\r\n\tif err != nil {\r\n\t\t// Handle timeout error (slow consumer) differently\r\n\t\tif ne, ok := err.(net.Error); ok && ne.Timeout() {\r\n\t\t\tif closed := c.handleWriteTimeout(n, attempted, len(cnb)); closed {\r\n\t\t\t\treturn true\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\t// Other errors will cause connection to be closed.\r\n\t\t\t// For clients, report as debug but for others report as error.\r\n\t\t\treport := c.Debugf\r\n\t\t\tif c.kind != CLIENT {\r\n\t\t\t\treport = c.Errorf\r\n\t\t\t}\r\n\t\t\treport(\"Error flushing: %v\", err)\r\n\t\t\tc.markConnAsClosed(WriteError, true)\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\r\n\t// Update flush time statistics.\r\n\tc.out.lft = lft\r\n\tc.out.lwb = int32(n)\r\n\r\n\t// Subtract from pending bytes and messages.\r\n\tc.out.pb -= int64(c.out.lwb)\r\n\tc.out.pm -= apm // FIXME(dlc) - this will not be totally accurate on partials.\r\n\r\n\t// Check for partial writes\r\n\t// TODO(dlc) - zero write with no error will cause lost message and the writeloop to spin.\r\n\tif int64(c.out.lwb) != attempted && n > 0 {\r\n\t\tc.handlePartialWrite(nb)\r\n\t} else if c.out.lwb >= c.out.sz {\r\n\t\tc.out.sws = 0\r\n\t}\r\n\r\n\t// Adjust based on what we wrote plus any pending.\r\n\tpt := int64(c.out.lwb) + c.out.pb\r\n\r\n\t// Adjust sz as needed downward, keeping power of 2.\r\n\t// We do this at a slower rate.\r\n\tif pt < int64(c.out.sz) && c.out.sz > minBufSize {\r\n\t\tc.out.sws++\r\n\t\tif c.out.sws > shortsToShrink {\r\n\t\t\tc.out.sz >>= 1\r\n\t\t}\r\n\t}\r\n\t// Adjust sz as needed upward, keeping power of 2.\r\n\tif pt > int64(c.out.sz) && c.out.sz < maxBufSize {\r\n\t\tc.out.sz <<= 1\r\n\t}\r\n\r\n\t// Check to see if we can reuse buffers.\r\n\tif lfs != 0 && n >= int64(lfs) {\r\n\t\toldp := cnb[0][:0]\r\n\t\tif cap(oldp) >= int(c.out.sz) {\r\n\t\t\t// Replace primary or secondary if they are nil, reusing same buffer.\r\n\t\t\tif c.out.p == nil {\r\n\t\t\t\tc.out.p = oldp\r\n\t\t\t} else if c.out.s == nil || cap(c.out.s) < int(c.out.sz) {\r\n\t\t\t\tc.out.s = oldp\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Check that if there is still data to send and writeLoop is in wait,\r\n\t// then we need to signal.\r\n\tif c.out.pb > 0 {\r\n\t\tc.flushSignal()\r\n\t}\r\n\r\n\t// Check if we have a stalled gate and if so and we are recovering release\r\n\t// any stalled producers. Only kind==CLIENT will stall.\r\n\tif c.out.stc != nil && (int64(c.out.lwb) == attempted || c.out.pb < c.out.mp/2) {\r\n\t\tclose(c.out.stc)\r\n\t\tc.out.stc = nil\r\n\t}\r\n\r\n\treturn true\r\n}\r\n\r\n// This is invoked from flushOutbound() for io/timeout error (slow consumer).\r\n// Returns a boolean to indicate if the connection has been closed or not.\r\n// Lock is held on entry.\r\nfunc (c *client) handleWriteTimeout(written, attempted int64, numChunks int) bool {\r\n\tif tlsConn, ok := c.nc.(*tls.Conn); ok {\r\n\t\tif !tlsConn.ConnectionState().HandshakeComplete {\r\n\t\t\t// Likely a TLSTimeout error instead...\r\n\t\t\tc.markConnAsClosed(TLSHandshakeError, true)\r\n\t\t\t// Would need to coordinate with tlstimeout()\r\n\t\t\t// to avoid double logging, so skip logging\r\n\t\t\t// here, and don't report a slow consumer error.\r\n\t\t\treturn true\r\n\t\t}\r\n\t} else if c.flags.isSet(expectConnect) && !c.flags.isSet(connectReceived) {\r\n\t\t// Under some conditions, a connection may hit a slow consumer write deadline\r\n\t\t// before the authorization timeout. If that is the case, then we handle\r\n\t\t// as slow consumer though we do not increase the counter as that can be\r\n\t\t// misleading.\r\n\t\tc.markConnAsClosed(SlowConsumerWriteDeadline, true)\r\n\t\treturn true\r\n\t}\r\n\r\n\t// Slow consumer here..\r\n\tatomic.AddInt64(&c.srv.slowConsumers, 1)\r\n\tc.Noticef(\"Slow Consumer Detected: WriteDeadline of %v exceeded with %d chunks of %d total bytes.\",\r\n\t\tc.out.wdl, numChunks, attempted)\r\n\r\n\t// We always close CLIENT connections, or when nothing was written at all...\r\n\tif c.kind == CLIENT || written == 0 {\r\n\t\tc.markConnAsClosed(SlowConsumerWriteDeadline, true)\r\n\t\treturn true\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// Marks this connection has closed with the given reason.\r\n// Sets the closeConnection flag and skipFlushOnClose flag if asked.\r\n// Depending on the kind of connection, the connection will be saved.\r\n// If a writeLoop has been started, the final flush/close/teardown will\r\n// be done there, otherwise flush and close of TCP connection is done here in place.\r\n// Returns true if closed in place, flase otherwise.\r\n// Lock is held on entry.\r\nfunc (c *client) markConnAsClosed(reason ClosedState, skipFlush bool) bool {\r\n\tif c.flags.isSet(closeConnection) {\r\n\t\treturn false\r\n\t}\r\n\tc.flags.set(closeConnection)\r\n\tif skipFlush {\r\n\t\tc.flags.set(skipFlushOnClose)\r\n\t}\r\n\t// Be consistent with the creation: for routes and gateways,\r\n\t// we use Noticef on create, so use that too for delete.\r\n\tif c.kind == ROUTER || c.kind == GATEWAY {\r\n\t\tc.Noticef(\"%s connection closed: %s\", c.typeString(), reason)\r\n\t} else { // Client and Leaf Node connections.\r\n\t\tc.Debugf(\"%s connection closed: %s\", c.typeString(), reason)\r\n\t}\r\n\r\n\t// Save off the connection if its a client or leafnode.\r\n\tif c.kind == CLIENT || c.kind == LEAF {\r\n\t\tif nc := c.nc; nc != nil && c.srv != nil {\r\n\t\t\t// TODO: May want to send events to single go routine instead\r\n\t\t\t// of creating a new go routine for each save.\r\n\t\t\tgo c.srv.saveClosedClient(c, nc, reason)\r\n\t\t}\r\n\t}\r\n\t// If writeLoop exists, let it do the final flush, close and teardown.\r\n\tif c.flags.isSet(writeLoopStarted) {\r\n\t\tc.flushSignal()\r\n\t\treturn false\r\n\t}\r\n\t// Flush (if skipFlushOnClose is not set) and close in place. If flushing,\r\n\t// use a small WriteDeadline.\r\n\tc.flushAndClose(true)\r\n\treturn true\r\n}\r\n\r\n// flushSignal will use server to queue the flush IO operation to a pool of flushers.\r\n// Lock must be held.\r\nfunc (c *client) flushSignal() bool {\r\n\tselect {\r\n\tcase c.out.sch <- struct{}{}:\r\n\t\treturn true\r\n\tdefault:\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// Traces a message.\r\n// Will NOT check if tracing is enabled, does NOT need the client lock.\r\nfunc (c *client) traceMsg(msg []byte) {\r\n\tmaxTrace := c.srv.getOpts().MaxTracedMsgLen\r\n\tif maxTrace > 0 && (len(msg)-LEN_CR_LF) > maxTrace {\r\n\t\tc.Tracef(\"<<- MSG_PAYLOAD: [\\\"%s...\\\"]\", msg[:maxTrace])\r\n\t} else {\r\n\t\tc.Tracef(\"<<- MSG_PAYLOAD: [%q]\", msg[:len(msg)-LEN_CR_LF])\r\n\t}\r\n}\r\n\r\n// Traces an incoming operation.\r\n// Will NOT check if tracing is enabled, does NOT need the client lock.\r\nfunc (c *client) traceInOp(op string, arg []byte) {\r\n\tc.traceOp(\"<<- %s\", op, arg)\r\n}\r\n\r\n// Traces an outgoing operation.\r\n// Will NOT check if tracing is enabled, does NOT need the client lock.\r\nfunc (c *client) traceOutOp(op string, arg []byte) {\r\n\tc.traceOp(\"->> %s\", op, arg)\r\n}\r\n\r\nfunc (c *client) traceOp(format, op string, arg []byte) {\r\n\topa := []interface{}{}\r\n\tif op != \"\" {\r\n\t\topa = append(opa, op)\r\n\t}\r\n\tif arg != nil {\r\n\t\topa = append(opa, string(arg))\r\n\t}\r\n\tc.Tracef(format, opa)\r\n}\r\n\r\n// Process the information messages from Clients and other Routes.\r\nfunc (c *client) processInfo(arg []byte) error {\r\n\tinfo := Info{}\r\n\tif err := json.Unmarshal(arg, &info); err != nil {\r\n\t\treturn err\r\n\t}\r\n\tswitch c.kind {\r\n\tcase ROUTER:\r\n\t\tc.processRouteInfo(&info)\r\n\tcase GATEWAY:\r\n\t\tc.processGatewayInfo(&info)\r\n\tcase LEAF:\r\n\t\treturn c.processLeafnodeInfo(&info)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc (c *client) processErr(errStr string) {\r\n\tclose := true\r\n\tswitch c.kind {\r\n\tcase CLIENT:\r\n\t\tc.Errorf(\"Client Error %s\", errStr)\r\n\tcase ROUTER:\r\n\t\tc.Errorf(\"Route Error %s\", errStr)\r\n\tcase GATEWAY:\r\n\t\tc.Errorf(\"Gateway Error %s\", errStr)\r\n\tcase LEAF:\r\n\t\tc.Errorf(\"Leafnode Error %s\", errStr)\r\n\t\tc.leafProcessErr(errStr)\r\n\t\tclose = false\r\n\t}\r\n\tif close {\r\n\t\tc.closeConnection(ParseError)\r\n\t}\r\n}\r\n\r\n// Password pattern matcher.\r\nvar passPat = regexp.MustCompile(`\"?\\s*pass\\S*?\"?\\s*[:=]\\s*\"?(([^\",\\r\\n}])*)`)\r\n\r\n// removePassFromTrace removes any notion of passwords from trace\r\n// messages for logging.\r\nfunc removePassFromTrace(arg []byte) []byte {\r\n\tif !bytes.Contains(arg, []byte(`pass`)) {\r\n\t\treturn arg\r\n\t}\r\n\t// Take a copy of the connect proto just for the trace message.\r\n\tvar _arg [4096]byte\r\n\tbuf := append(_arg[:0], arg...)\r\n\r\n\tm := passPat.FindAllSubmatchIndex(buf, -1)\r\n\tif len(m) == 0 {\r\n\t\treturn arg\r\n\t}\r\n\r\n\tredactedPass := []byte(\"[REDACTED]\")\r\n\tfor _, i := range m {\r\n\t\tif len(i) < 4 {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tstart := i[2]\r\n\t\tend := i[3]\r\n\r\n\t\t// Replace password substring.\r\n\t\tbuf = append(buf[:start], append(redactedPass, buf[end:]...)...)\r\n\t\tbreak\r\n\t}\r\n\treturn buf\r\n}\r\n\r\n// Returns the RTT by computing the elapsed time since now and `start`.\r\n// On Windows VM where I (IK) run tests, time.Since() will return 0\r\n// (I suspect some time granularity issues). So return at minimum 1ns.\r\nfunc computeRTT(start time.Time) time.Duration {\r\n\trtt := time.Since(start)\r\n\tif rtt <= 0 {\r\n\t\trtt = time.Nanosecond\r\n\t}\r\n\treturn rtt\r\n}\r\n\r\nfunc (c *client) processConnect(arg []byte) error {\r\n\tc.mu.Lock()\r\n\t// If we can't stop the timer because the callback is in progress...\r\n\tif !c.clearAuthTimer() {\r\n\t\t// wait for it to finish and handle sending the failure back to\r\n\t\t// the client.\r\n\t\tfor !c.isClosed() {\r\n\t\t\tc.mu.Unlock()\r\n\t\t\ttime.Sleep(25 * time.Millisecond)\r\n\t\t\tc.mu.Lock()\r\n\t\t}\r\n\t\tc.mu.Unlock()\r\n\t\treturn nil\r\n\t}\r\n\tc.last = time.Now()\r\n\t// Estimate RTT to start.\r\n\tif c.kind == CLIENT {\r\n\t\tc.rtt = computeRTT(c.start)\r\n\r\n\t\tif c.srv != nil {\r\n\t\t\tc.clearPingTimer()\r\n\t\t\tc.srv.setFirstPingTimer(c)\r\n\t\t}\r\n\t}\r\n\tkind := c.kind\r\n\tsrv := c.srv\r\n\r\n\t// Moved unmarshalling of clients' Options under the lock.\r\n\t// The client has already been added to the server map, so it is possible\r\n\t// that other routines lookup the client, and access its options under\r\n\t// the client's lock, so unmarshalling the options outside of the lock\r\n\t// would cause data RACEs.\r\n\tif err := json.Unmarshal(arg, &c.opts); err != nil {\r\n\t\tc.mu.Unlock()\r\n\t\treturn err\r\n\t}\r\n\t// Indicate that the CONNECT protocol has been received, and that the\r\n\t// server now knows which protocol this client supports.\r\n\tc.flags.set(connectReceived)\r\n\t// Capture these under lock\r\n\tc.echo = c.opts.Echo\r\n\tproto := c.opts.Protocol\r\n\tverbose := c.opts.Verbose\r\n\tlang := c.opts.Lang\r\n\taccount := c.opts.Account\r\n\taccountNew := c.opts.AccountNew\r\n\tujwt := c.opts.JWT\r\n\tc.mu.Unlock()\r\n\r\n\tif srv != nil {\r\n\t\t// Applicable to clients only:\r\n\t\t// As soon as c.opts is unmarshalled and if the proto is at\r\n\t\t// least ClientProtoInfo, we need to increment the following counter.\r\n\t\t// This is decremented when client is removed from the server's\r\n\t\t// clients map.\r\n\t\tif kind == CLIENT && proto >= ClientProtoInfo {\r\n\t\t\tsrv.mu.Lock()\r\n\t\t\tsrv.cproto++\r\n\t\t\tsrv.mu.Unlock()\r\n\t\t}\r\n\r\n\t\t// Check for Auth\r\n\t\tif ok := srv.checkAuthentication(c); !ok {\r\n\t\t\t// We may fail here because we reached max limits on an account.\r\n\t\t\tif ujwt != \"\" {\r\n\t\t\t\tc.mu.Lock()\r\n\t\t\t\tacc := c.acc\r\n\t\t\t\tc.mu.Unlock()\r\n\t\t\t\tsrv.mu.Lock()\r\n\t\t\t\ttooManyAccCons := acc != nil && acc != srv.gacc\r\n\t\t\t\tsrv.mu.Unlock()\r\n\t\t\t\tif tooManyAccCons {\r\n\t\t\t\t\treturn ErrTooManyAccountConnections\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tc.authViolation()\r\n\t\t\treturn ErrAuthentication\r\n\t\t}\r\n\r\n\t\t// Check for Account designation, this section should be only used when there is not a jwt.\r\n\t\tif account != \"\" {\r\n\t\t\tvar acc *Account\r\n\t\t\tvar wasNew bool\r\n\t\t\tvar err error\r\n\t\t\tif !srv.NewAccountsAllowed() {\r\n\t\t\t\tacc, err = srv.LookupAccount(account)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tc.Errorf(err.Error())\r\n\t\t\t\t\tc.sendErr(ErrMissingAccount.Error())\r\n\t\t\t\t\treturn err\r\n\t\t\t\t} else if accountNew && acc != nil {\r\n\t\t\t\t\tc.sendErrAndErr(ErrAccountExists.Error())\r\n\t\t\t\t\treturn ErrAccountExists\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\t// We can create this one on the fly.\r\n\t\t\t\tacc, wasNew = srv.LookupOrRegisterAccount(account)\r\n\t\t\t\tif accountNew && !wasNew {\r\n\t\t\t\t\tc.sendErrAndErr(ErrAccountExists.Error())\r\n\t\t\t\t\treturn ErrAccountExists\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\t// If we are here we can register ourselves with the new account.\r\n\t\t\tif err := c.registerWithAccount(acc); err != nil {\r\n\t\t\t\tc.reportErrRegisterAccount(acc, err)\r\n\t\t\t\treturn ErrBadAccount\r\n\t\t\t}\r\n\t\t} else if c.acc == nil {\r\n\t\t\t// By default register with the global account.\r\n\t\t\tc.registerWithAccount(srv.gacc)\r\n\t\t}\r\n\r\n\t}\r\n\r\n\tswitch kind {\r\n\tcase CLIENT:\r\n\t\t// Check client protocol request if it exists.\r\n\t\tif proto < ClientProtoZero || proto > ClientProtoInfo {\r\n\t\t\tc.sendErr(ErrBadClientProtocol.Error())\r\n\t\t\tc.closeConnection(BadClientProtocolVersion)\r\n\t\t\treturn ErrBadClientProtocol\r\n\t\t}\r\n\t\tif verbose {\r\n\t\t\tc.sendOK()\r\n\t\t}\r\n\tcase ROUTER:\r\n\t\t// Delegate the rest of processing to the route\r\n\t\treturn c.processRouteConnect(srv, arg, lang)\r\n\tcase GATEWAY:\r\n\t\t// Delegate the rest of processing to the gateway\r\n\t\treturn c.processGatewayConnect(arg)\r\n\tcase LEAF:\r\n\t\t// Delegate the rest of processing to the leaf node\r\n\t\treturn c.processLeafNodeConnect(srv, arg, lang)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc (c *client) sendErrAndErr(err string) {\r\n\tc.sendErr(err)\r\n\tc.Errorf(err)\r\n}\r\n\r\nfunc (c *client) sendErrAndDebug(err string) {\r\n\tc.sendErr(err)\r\n\tc.Debugf(err)\r\n}\r\n\r\nfunc (c *client) authTimeout() {\r\n\tc.sendErrAndDebug(\"Authentication Timeout\")\r\n\tc.closeConnection(AuthenticationTimeout)\r\n}\r\n\r\nfunc (c *client) authExpired() {\r\n\tc.sendErrAndDebug(\"User Authentication Expired\")\r\n\tc.closeConnection(AuthenticationExpired)\r\n}\r\n\r\nfunc (c *client) accountAuthExpired() {\r\n\tc.sendErrAndDebug(\"Account Authentication Expired\")\r\n\tc.closeConnection(AuthenticationExpired)\r\n}\r\n\r\nfunc (c *client) authViolation() {\r\n\tvar s *Server\r\n\tvar hasTrustedNkeys, hasNkeys, hasUsers bool\r\n\tif s = c.srv; s != nil {\r\n\t\ts.mu.Lock()\r\n\t\thasTrustedNkeys = len(s.trustedKeys) > 0\r\n\t\thasNkeys = s.nkeys != nil\r\n\t\thasUsers = s.users != nil\r\n\t\ts.mu.Unlock()\r\n\t\tdefer s.sendAuthErrorEvent(c)\r\n\r\n\t}\r\n\tif hasTrustedNkeys {\r\n\t\tc.Errorf(\"%v\", ErrAuthentication)\r\n\t} else if hasNkeys {\r\n\t\tc.Errorf(\"%s - Nkey %q\",\r\n\t\t\tErrAuthentication.Error(),\r\n\t\t\tc.opts.Nkey)\r\n\t} else if hasUsers {\r\n\t\tc.Errorf(\"%s - User %q\",\r\n\t\t\tErrAuthentication.Error(),\r\n\t\t\tc.opts.Username)\r\n\t} else {\r\n\t\tc.Errorf(ErrAuthentication.Error())\r\n\t}\r\n\tc.sendErr(\"Authorization Violation\")\r\n\tc.closeConnection(AuthenticationViolation)\r\n}\r\n\r\nfunc (c *client) maxAccountConnExceeded() {\r\n\tc.sendErrAndErr(ErrTooManyAccountConnections.Error())\r\n\tc.closeConnection(MaxAccountConnectionsExceeded)\r\n}\r\n\r\nfunc (c *client) maxConnExceeded() {\r\n\tc.sendErrAndErr(ErrTooManyConnections.Error())\r\n\tc.closeConnection(MaxConnectionsExceeded)\r\n}\r\n\r\nfunc (c *client) maxSubsExceeded() {\r\n\tc.sendErrAndErr(ErrTooManySubs.Error())\r\n}\r\n\r\nfunc (c *client) maxPayloadViolation(sz int, max int32) {\r\n\tc.Errorf(\"%s: %d vs %d\", ErrMaxPayload.Error(), sz, max)\r\n\tc.sendErr(\"Maximum Payload Violation\")\r\n\tc.closeConnection(MaxPayloadExceeded)\r\n}\r\n\r\n// queueOutbound queues data for a clientconnection.\r\n// Return if the data is referenced or not. If referenced, the caller\r\n// should not reuse the `data` array.\r\n// Lock should be held.\r\nfunc (c *client) queueOutbound(data []byte) bool {\r\n\t// Do not keep going if closed\r\n\tif c.flags.isSet(closeConnection) {\r\n\t\treturn false\r\n\t}\r\n\r\n\t// Assume data will not be referenced\r\n\treferenced := false\r\n\t// Add to pending bytes total.\r\n\tc.out.pb += int64(len(data))\r\n\r\n\t// Check for slow consumer via pending bytes limit.\r\n\t// ok to return here, client is going away.\r\n\tif c.kind == CLIENT && c.out.pb > c.out.mp {\r\n\t\t// Perf wise, it looks like it is faster to optimistically add than\r\n\t\t// checking current pb+len(data) and then add to pb.\r\n\t\tc.out.pb -= int64(len(data))\r\n\t\tatomic.AddInt64(&c.srv.slowConsumers, 1)\r\n\t\tc.Noticef(\"Slow Consumer Detected: MaxPending of %d Exceeded\", c.out.mp)\r\n\t\tc.markConnAsClosed(SlowConsumerPendingBytes, true)\r\n\t\treturn referenced\r\n\t}\r\n\r\n\tif c.out.p == nil && len(data) < maxBufSize {\r\n\t\tif c.out.sz == 0 {\r\n\t\t\tc.out.sz = startBufSize\r\n\t\t}\r\n\t\tif c.out.s != nil && cap(c.out.s) >= int(c.out.sz) {\r\n\t\t\tc.out.p = c.out.s\r\n\t\t\tc.out.s = nil\r\n\t\t} else {\r\n\t\t\t// FIXME(dlc) - make power of 2 if less than maxBufSize?\r\n\t\t\tc.out.p = make([]byte, 0, c.out.sz)\r\n\t\t}\r\n\t}\r\n\t// Determine if we copy or reference\r\n\tavailable := cap(c.out.p) - len(c.out.p)\r\n\tif len(data) > available {\r\n\t\t// We can't fit everything into existing primary, but message will\r\n\t\t// fit in next one we allocate or utilize from the secondary.\r\n\t\t// So copy what we can.\r\n\t\tif available > 0 && len(data) < int(c.out.sz) {\r\n\t\t\tc.out.p = append(c.out.p, data[:available]...)\r\n\t\t\tdata = data[available:]\r\n\t\t}\r\n\t\t// Put the primary on the nb if it has a payload\r\n\t\tif len(c.out.p) > 0 {\r\n\t\t\tc.out.nb = append(c.out.nb, c.out.p)\r\n\t\t\tc.out.p = nil\r\n\t\t}\r\n\t\t// Check for a big message, and if found place directly on nb\r\n\t\t// FIXME(dlc) - do we need signaling of ownership here if we want len(data) < maxBufSize\r\n\t\tif len(data) > maxBufSize {\r\n\t\t\tc.out.nb = append(c.out.nb, data)\r\n\t\t\treferenced = true\r\n\t\t} else {\r\n\t\t\t// We will copy to primary.\r\n\t\t\tif c.out.p == nil {\r\n\t\t\t\t// Grow here\r\n\t\t\t\tif (c.out.sz << 1) <= maxBufSize {\r\n\t\t\t\t\tc.out.sz <<= 1\r\n\t\t\t\t}\r\n\t\t\t\tif len(data) > int(c.out.sz) {\r\n\t\t\t\t\tc.out.p = make([]byte, 0, len(data))\r\n\t\t\t\t} else {\r\n\t\t\t\t\tif c.out.s != nil && cap(c.out.s) >= int(c.out.sz) { // TODO(dlc) - Size mismatch?\r\n\t\t\t\t\t\tc.out.p = c.out.s\r\n\t\t\t\t\t\tc.out.s = nil\r\n\t\t\t\t\t} else {\r\n\t\t\t\t\t\tc.out.p = make([]byte, 0, c.out.sz)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tc.out.p = append(c.out.p, data...)\r\n\t\t}\r\n\t} else {\r\n\t\tc.out.p = append(c.out.p, data...)\r\n\t}\r\n\r\n\t// Check here if we should create a stall channel if we are falling behind.\r\n\t// We do this here since if we wait for consumer's writeLoop it could be\r\n\t// too late with large number of fan in producers.\r\n\tif c.out.pb > c.out.mp/2 && c.out.stc == nil {\r\n\t\tc.out.stc = make(chan struct{})\r\n\t}\r\n\r\n\treturn referenced\r\n}\r\n\r\n// Assume the lock is held upon entry.\r\nfunc (c *client) enqueueProtoAndFlush(proto []byte, doFlush bool) {\r\n\tif c.isClosed() {\r\n\t\treturn\r\n\t}\r\n\tc.queueOutbound(proto)\r\n\tif !(doFlush && c.flushOutbound()) {\r\n\t\tc.flushSignal()\r\n\t}\r\n}\r\n\r\n// Queues and then flushes the connection. This should only be called when\r\n// the writeLoop cannot be started yet. Use enqueueProto() otherwise.\r\n// Lock is held on entry.\r\nfunc (c *client) sendProtoNow(proto []byte) {\r\n\tc.enqueueProtoAndFlush(proto, true)\r\n}\r\n\r\n// Enqueues the given protocol and signal the writeLoop if necessary.\r\n// Lock is held on entry.\r\nfunc (c *client) enqueueProto(proto []byte) {\r\n\tc.enqueueProtoAndFlush(proto, false)\r\n}\r\n\r\n// Assume the lock is held upon entry.\r\nfunc (c *client) sendPong() {\r\n\tif c.trace {\r\n\t\tc.traceOutOp(\"PONG\", nil)\r\n\t}\r\n\tc.enqueueProto([]byte(pongProto))\r\n}\r\n\r\n// Used to kick off a RTT measurement for latency tracking.\r\nfunc (c *client) sendRTTPing() bool {\r\n\tc.mu.Lock()\r\n\tsent := c.sendRTTPingLocked()\r\n\tc.mu.Unlock()\r\n\treturn sent\r\n}\r\n\r\n// Used to kick off a RTT measurement for latency tracking.\r\n// This is normally called only when the caller has checked that\r\n// the c.rtt is 0 and wants to force an update by sending a PING.\r\n// Client lock held on entry.\r\nfunc (c *client) sendRTTPingLocked() bool {\r\n\t// Most client libs send a CONNECT+PING and wait for a PONG from the\r\n\t// server. So if firstPongSent flag is set, it is ok for server to\r\n\t// send the PING. But in case we have client libs that don't do that,\r\n\t// allow the send of the PING if more than 2 secs have elapsed since\r\n\t// the client TCP connection was accepted.\r\n\tif !c.flags.isSet(closeConnection) &&\r\n\t\t(c.flags.isSet(firstPongSent) || time.Since(c.start) > maxNoRTTPingBeforeFirstPong) {\r\n\t\tc.sendPing()\r\n\t\treturn true\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// Assume the lock is held upon entry.\r\nfunc (c *client) sendPing() {\r\n\tc.rttStart = time.Now()\r\n\tc.ping.out++\r\n\tif c.trace {\r\n\t\tc.traceOutOp(\"PING\", nil)\r\n\t}\r\n\tc.enqueueProto([]byte(pingProto))\r\n}\r\n\r\n// Generates the INFO to be sent to the client with the client ID included.\r\n// info arg will be copied since passed by value.\r\n// Assume lock is held.\r\nfunc (c *client) generateClientInfoJSON(info Info) []byte {\r\n\tinfo.CID = c.cid\r\n\tinfo.ClientIP = c.host\r\n\tinfo.MaxPayload = c.mpay\r\n\t// Generate the info json\r\n\tb, _ := json.Marshal(info)\r\n\tpcs := [][]byte{[]byte(\"INFO\"), b, []byte(CR_LF)}\r\n\treturn bytes.Join(pcs, []byte(\" \"))\r\n}\r\n\r\nfunc (c *client) sendErr(err string) {\r\n\tc.mu.Lock()\r\n\tif c.trace {\r\n\t\tc.traceOutOp(\"-ERR\", []byte(err))\r\n\t}\r\n\tc.enqueueProto([]byte(fmt.Sprintf(errProto, err)))\r\n\tc.mu.Unlock()\r\n}\r\n\r\nfunc (c *client) sendOK() {\r\n\tc.mu.Lock()\r\n\tif c.trace {\r\n\t\tc.traceOutOp(\"OK\", nil)\r\n\t}\r\n\tc.enqueueProto([]byte(okProto))\r\n\tc.pcd[c] = needFlush\r\n\tc.mu.Unlock()\r\n}\r\n\r\nfunc (c *client) processPing() {\r\n\tc.mu.Lock()\r\n\r\n\tif c.isClosed() {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\tc.sendPong()\r\n\r\n\t// Record this to suppress us sending one if this\r\n\t// is within a given time interval for activity.\r\n\tc.ping.last = time.Now()\r\n\r\n\t// If not a CLIENT, we are done. Also the CONNECT should\r\n\t// have been received, but make sure it is so before proceeding\r\n\tif c.kind != CLIENT || !c.flags.isSet(connectReceived) {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\t// If we are here, the CONNECT has been received so we know\r\n\t// if this client supports async INFO or not.\r\n\tvar (\r\n\t\tcheckInfoChange bool\r\n\t\tsrv             = c.srv\r\n\t)\r\n\t// For older clients, just flip the firstPongSent flag if not already\r\n\t// set and we are done.\r\n\tif c.opts.Protocol < ClientProtoInfo || srv == nil {\r\n\t\tc.flags.setIfNotSet(firstPongSent)\r\n\t} else {\r\n\t\t// This is a client that supports async INFO protocols.\r\n\t\t// If this is the first PING (so firstPongSent is not set yet),\r\n\t\t// we will need to check if there was a change in cluster topology\r\n\t\t// or we have a different max payload. We will send this first before\r\n\t\t// pong since most clients do flush after connect call.\r\n\t\tcheckInfoChange = !c.flags.isSet(firstPongSent)\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\tif checkInfoChange {\r\n\t\topts := srv.getOpts()\r\n\t\tsrv.mu.Lock()\r\n\t\tc.mu.Lock()\r\n\t\t// Now that we are under both locks, we can flip the flag.\r\n\t\t// This prevents sendAsyncInfoToClients() and code here to\r\n\t\t// send a double INFO protocol.\r\n\t\tc.flags.set(firstPongSent)\r\n\t\t// If there was a cluster update since this client was created,\r\n\t\t// send an updated INFO protocol now.\r\n\t\tif srv.lastCURLsUpdate >= c.start.UnixNano() || c.mpay != int32(opts.MaxPayload) {\r\n\t\t\tc.enqueueProto(c.generateClientInfoJSON(srv.copyInfo()))\r\n\t\t}\r\n\t\tc.mu.Unlock()\r\n\t\tsrv.mu.Unlock()\r\n\t}\r\n}\r\n\r\nfunc (c *client) processPong() {\r\n\tc.mu.Lock()\r\n\tc.ping.out = 0\r\n\tc.rtt = computeRTT(c.rttStart)\r\n\tsrv := c.srv\r\n\treorderGWs := c.kind == GATEWAY && c.gw.outbound\r\n\tc.mu.Unlock()\r\n\tif reorderGWs {\r\n\t\tsrv.gateway.orderOutboundConnections()\r\n\t}\r\n}\r\n\r\nfunc (c *client) processPub(arg []byte) error {\r\n\t// Unroll splitArgs to avoid runtime/heap issues\r\n\ta := [MAX_PUB_ARGS][]byte{}\r\n\targs := a[:0]\r\n\tstart := -1\r\n\tfor i, b := range arg {\r\n\t\tswitch b {\r\n\t\tcase ' ', '\\t':\r\n\t\t\tif start >= 0 {\r\n\t\t\t\targs = append(args, arg[start:i])\r\n\t\t\t\tstart = -1\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif start < 0 {\r\n\t\t\t\tstart = i\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tif start >= 0 {\r\n\t\targs = append(args, arg[start:])\r\n\t}\r\n\r\n\tc.pa.arg = arg\r\n\tswitch len(args) {\r\n\tcase 2:\r\n\t\tc.pa.subject = args[0]\r\n\t\tc.pa.reply = nil\r\n\t\tc.pa.size = parseSize(args[1])\r\n\t\tc.pa.szb = args[1]\r\n\tcase 3:\r\n\t\tc.pa.subject = args[0]\r\n\t\tc.pa.reply = args[1]\r\n\t\tc.pa.size = parseSize(args[2])\r\n\t\tc.pa.szb = args[2]\r\n\tdefault:\r\n\t\treturn fmt.Errorf(\"processPub Parse Error: '%s'\", arg)\r\n\t}\r\n\t// If number overruns an int64, parseSize() will have returned a negative value\r\n\tif c.pa.size < 0 {\r\n\t\treturn fmt.Errorf(\"processPub Bad or Missing Size: '%s'\", arg)\r\n\t}\r\n\tmaxPayload := atomic.LoadInt32(&c.mpay)\r\n\t// Use int64() to avoid int32 overrun...\r\n\tif maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\r\n\t\tc.maxPayloadViolation(c.pa.size, maxPayload)\r\n\t\treturn ErrMaxPayload\r\n\t}\r\n\r\n\tif c.opts.Pedantic && !IsValidLiteralSubject(string(c.pa.subject)) {\r\n\t\tc.sendErr(\"Invalid Publish Subject\")\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc splitArg(arg []byte) [][]byte {\r\n\ta := [MAX_MSG_ARGS][]byte{}\r\n\targs := a[:0]\r\n\tstart := -1\r\n\tfor i, b := range arg {\r\n\t\tswitch b {\r\n\t\tcase ' ', '\\t', '\\r', '\\n':\r\n\t\t\tif start >= 0 {\r\n\t\t\t\targs = append(args, arg[start:i])\r\n\t\t\t\tstart = -1\r\n\t\t\t}\r\n\t\tdefault:\r\n\t\t\tif start < 0 {\r\n\t\t\t\tstart = i\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tif start >= 0 {\r\n\t\targs = append(args, arg[start:])\r\n\t}\r\n\treturn args\r\n}\r\n\r\nfunc (c *client) processSub(argo []byte, noForward bool) (*subscription, error) {\r\n\t// Indicate activity.\r\n\tc.in.subs++\r\n\r\n\t// Copy so we do not reference a potentially large buffer\r\n\t// FIXME(dlc) - make more efficient.\r\n\targ := make([]byte, len(argo))\r\n\tcopy(arg, argo)\r\n\targs := splitArg(arg)\r\n\tsub := &subscription{client: c}\r\n\tswitch len(args) {\r\n\tcase 2:\r\n\t\tsub.subject = args[0]\r\n\t\tsub.queue = nil\r\n\t\tsub.sid = args[1]\r\n\tcase 3:\r\n\t\tsub.subject = args[0]\r\n\t\tsub.queue = args[1]\r\n\t\tsub.sid = args[2]\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"processSub Parse Error: '%s'\", arg)\r\n\t}\r\n\r\n\tc.mu.Lock()\r\n\r\n\t// Grab connection type, account and server info.\r\n\tkind := c.kind\r\n\tacc := c.acc\r\n\tsrv := c.srv\r\n\r\n\tsid := string(sub.sid)\r\n\r\n\t// This check does not apply to SYSTEM clients (because they don't have a `nc`...)\r\n\tif kind != SYSTEM && c.isClosed() {\r\n\t\tc.mu.Unlock()\r\n\t\treturn sub, nil\r\n\t}\r\n\r\n\t// Check permissions if applicable.\r\n\tif kind == CLIENT {\r\n\t\t// First do a pass whether queue subscription is valid. This does not necessarily\r\n\t\t// mean that it will not be able to plain subscribe.\r\n\t\t//\r\n\t\t// allow = [\"foo\"]            -> can subscribe or queue subscribe to foo using any queue\r\n\t\t// allow = [\"foo v1\"]         -> can only queue subscribe to 'foo v1', no plain subs allowed.\r\n\t\t// allow = [\"foo\", \"foo v1\"]  -> can subscribe to 'foo' but can only queue subscribe to 'foo v1'\r\n\t\t//\r\n\t\tif sub.queue != nil {\r\n\t\t\tif !c.canQueueSubscribe(string(sub.subject), string(sub.queue)) {\r\n\t\t\t\tc.mu.Unlock()\r\n\t\t\t\tc.subPermissionViolation(sub)\r\n\t\t\t\treturn nil, nil\r\n\t\t\t}\r\n\t\t} else if !c.canSubscribe(string(sub.subject)) {\r\n\t\t\tc.mu.Unlock()\r\n\t\t\tc.subPermissionViolation(sub)\r\n\t\t\treturn nil, nil\r\n\t\t}\r\n\t}\r\n\t// Check if we have a maximum on the number of subscriptions.\r\n\tif c.subsAtLimit() {\r\n\t\tc.mu.Unlock()\r\n\t\tc.maxSubsExceeded()\r\n\t\treturn nil, nil\r\n\t}\r\n\r\n\tvar updateGWs bool\r\n\tvar err error\r\n\r\n\t// Subscribe here.\r\n\tes := c.subs[sid]\r\n\tif es == nil {\r\n\t\tc.subs[sid] = sub\r\n\t\tif acc != nil && acc.sl != nil {\r\n\t\t\terr = acc.sl.Insert(sub)\r\n\t\t\tif err != nil {\r\n\t\t\t\tdelete(c.subs, sid)\r\n\t\t\t} else {\r\n\t\t\t\tupdateGWs = c.srv.gateway.enabled\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t// Unlocked from here onward\r\n\tc.mu.Unlock()\r\n\r\n\tif err != nil {\r\n\t\tc.sendErr(\"Invalid Subject\")\r\n\t\treturn nil, nil\r\n\t} else if c.opts.Verbose && kind != SYSTEM {\r\n\t\tc.sendOK()\r\n\t}\r\n\r\n\t// If it was already registered, return it.\r\n\tif es != nil {\r\n\t\treturn es, nil\r\n\t}\r\n\r\n\t// No account just return.\r\n\tif acc == nil {\r\n\t\treturn sub, nil\r\n\t}\r\n\r\n\tif err := c.addShadowSubscriptions(acc, sub); err != nil {\r\n\t\tc.Errorf(err.Error())\r\n\t}\r\n\r\n\tif noForward {\r\n\t\treturn sub, nil\r\n\t}\r\n\r\n\t// If we are routing and this is a local sub, add to the route map for the associated account.\r\n\tif kind == CLIENT || kind == SYSTEM {\r\n\t\tsrv.updateRouteSubscriptionMap(acc, sub, 1)\r\n\t\tif updateGWs {\r\n\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, 1)\r\n\t\t}\r\n\t}\r\n\t// Now check on leafnode updates.\r\n\tsrv.updateLeafNodes(acc, sub, 1)\r\n\treturn sub, nil\r\n}\r\n\r\n// If the client's account has stream imports and there are matches for\r\n// this subscription's subject, then add shadow subscriptions in the\r\n// other accounts that export this subject.\r\nfunc (c *client) addShadowSubscriptions(acc *Account, sub *subscription) error {\r\n\tif acc == nil {\r\n\t\treturn ErrMissingAccount\r\n\t}\r\n\r\n\tvar (\r\n\t\trims   [32]*streamImport\r\n\t\tims    = rims[:0]\r\n\t\trfroms [32]*streamImport\r\n\t\tfroms  = rfroms[:0]\r\n\t\ttokens []string\r\n\t\ttsa    [32]string\r\n\t\thasWC  bool\r\n\t)\r\n\r\n\tacc.mu.RLock()\r\n\t// Loop over the import subjects. We have 3 scenarios. If we exact\r\n\t// match or we know the proposed subject is a strict subset of the\r\n\t// import we can subscribe to the subscription's subject directly.\r\n\t// The third scenario is where the proposed subject has a wildcard\r\n\t// and may not be an exact subset, but is a match. Therefore we have to\r\n\t// subscribe to the import subject, not the subscription's subject.\r\n\tfor _, im := range acc.imports.streams {\r\n\t\tif im.invalid {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tsubj := string(sub.subject)\r\n\t\tif subj == im.prefix+im.from {\r\n\t\t\tims = append(ims, im)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tif tokens == nil {\r\n\t\t\ttokens = tsa[:0]\r\n\t\t\tstart := 0\r\n\t\t\tfor i := 0; i < len(subj); i++ {\r\n\t\t\t\t// This is not perfect, but the test below will\r\n\t\t\t\t// be more exact, this is just to trigger the\r\n\t\t\t\t// additional test.\r\n\t\t\t\tif subj[i] == pwc || subj[i] == fwc {\r\n\t\t\t\t\thasWC = true\r\n\t\t\t\t} else if subj[i] == btsep {\r\n\t\t\t\t\ttokens = append(tokens, subj[start:i])\r\n\t\t\t\t\tstart = i + 1\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\ttokens = append(tokens, subj[start:])\r\n\t\t}\r\n\t\tif isSubsetMatch(tokens, im.prefix+im.from) {\r\n\t\t\tims = append(ims, im)\r\n\t\t} else if hasWC {\r\n\t\t\tif subjectIsSubsetMatch(im.prefix+im.from, subj) {\r\n\t\t\t\tfroms = append(froms, im)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tacc.mu.RUnlock()\r\n\r\n\tvar shadow []*subscription\r\n\r\n\tif len(ims) > 0 || len(froms) > 0 {\r\n\t\tshadow = make([]*subscription, 0, len(ims)+len(froms))\r\n\t}\r\n\r\n\t// Now walk through collected importMaps\r\n\tfor _, im := range ims {\r\n\t\t// We will create a shadow subscription.\r\n\t\tnsub, err := c.addShadowSub(sub, im, false)\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tshadow = append(shadow, nsub)\r\n\t}\r\n\t// Now walk through importMaps that we need to subscribe\r\n\t// exactly to the \"from\" property.\r\n\tfor _, im := range froms {\r\n\t\t// We will create a shadow subscription.\r\n\t\tnsub, err := c.addShadowSub(sub, im, true)\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tshadow = append(shadow, nsub)\r\n\t}\r\n\r\n\tif shadow != nil {\r\n\t\tc.mu.Lock()\r\n\t\tsub.shadow = shadow\r\n\t\tc.mu.Unlock()\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// Add in the shadow subscription.\r\nfunc (c *client) addShadowSub(sub *subscription, im *streamImport, useFrom bool) (*subscription, error) {\r\n\tnsub := *sub // copy\r\n\tnsub.im = im\r\n\tif useFrom {\r\n\t\tnsub.subject = []byte(im.from)\r\n\t} else if im.prefix != \"\" {\r\n\t\t// redo subject here to match subject in the publisher account space.\r\n\t\t// Just remove prefix from what they gave us. That maps into other space.\r\n\t\tnsub.subject = sub.subject[len(im.prefix):]\r\n\t}\r\n\r\n\tc.Debugf(\"Creating import subscription on %q from account %q\", nsub.subject, im.acc.Name)\r\n\r\n\tif err := im.acc.sl.Insert(&nsub); err != nil {\r\n\t\terrs := fmt.Sprintf(\"Could not add shadow import subscription for account %q\", im.acc.Name)\r\n\t\tc.Debugf(errs)\r\n\t\treturn nil, fmt.Errorf(errs)\r\n\t}\r\n\r\n\t// Update our route map here.\r\n\tc.srv.updateRouteSubscriptionMap(im.acc, &nsub, 1)\r\n\tc.srv.updateLeafNodes(im.acc, &nsub, 1)\r\n\r\n\treturn &nsub, nil\r\n}\r\n\r\n// canSubscribe determines if the client is authorized to subscribe to the\r\n// given subject. Assumes caller is holding lock.\r\nfunc (c *client) canSubscribe(subject string) bool {\r\n\tif c.perms == nil {\r\n\t\treturn true\r\n\t}\r\n\r\n\tallowed := true\r\n\r\n\t// Check allow list. If no allow list that means all are allowed. Deny can overrule.\r\n\tif c.perms.sub.allow != nil {\r\n\t\tr := c.perms.sub.allow.Match(subject)\r\n\t\tallowed = len(r.psubs) != 0\r\n\t}\r\n\t// If we have a deny list and we think we are allowed, check that as well.\r\n\tif allowed && c.perms.sub.deny != nil {\r\n\t\tr := c.perms.sub.deny.Match(subject)\r\n\t\tallowed = len(r.psubs) == 0\r\n\r\n\t\t// We use the actual subscription to signal us to spin up the deny mperms\r\n\t\t// and cache. We check if the subject is a wildcard that contains any of\r\n\t\t// the deny clauses.\r\n\t\t// FIXME(dlc) - We could be smarter and track when these go away and remove.\r\n\t\tif allowed && c.mperms == nil && subjectHasWildcard(subject) {\r\n\t\t\t// Whip through the deny array and check if this wildcard subject is within scope.\r\n\t\t\tfor _, sub := range c.darray {\r\n\t\t\t\ttokens := strings.Split(sub, tsep)\r\n\t\t\t\tif isSubsetMatch(tokens, sub) {\r\n\t\t\t\t\tc.loadMsgDenyFilter()\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn allowed\r\n}\r\n\r\nfunc queueMatches(queue string, qsubs [][]*subscription) bool {\r\n\tif len(qsubs) == 0 {\r\n\t\treturn true\r\n\t}\r\n\tfor _, qsub := range qsubs {\r\n\t\tqs := qsub[0]\r\n\t\tqname := string(qs.queue)\r\n\r\n\t\t// NOTE: '*' and '>' tokens can also be valid\r\n\t\t// queue names so we first check against the\r\n\t\t// literal name.  e.g. v1.* == v1.*\r\n\t\tif queue == qname || (subjectHasWildcard(qname) && subjectIsSubsetMatch(queue, qname)) {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\nfunc (c *client) canQueueSubscribe(subject, queue string) bool {\r\n\tif c.perms == nil {\r\n\t\treturn true\r\n\t}\r\n\r\n\tallowed := true\r\n\r\n\tif c.perms.sub.allow != nil {\r\n\t\tr := c.perms.sub.allow.Match(subject)\r\n\r\n\t\t// If perms DO NOT have queue name, then psubs will be greater than\r\n\t\t// zero. If perms DO have queue name, then qsubs will be greater than\r\n\t\t// zero.\r\n\t\tallowed = len(r.psubs) > 0\r\n\t\tif len(r.qsubs) > 0 {\r\n\t\t\t// If the queue appears in the allow list, then DO allow.\r\n\t\t\tallowed = queueMatches(queue, r.qsubs)\r\n\t\t}\r\n\t}\r\n\r\n\tif allowed && c.perms.sub.deny != nil {\r\n\t\tr := c.perms.sub.deny.Match(subject)\r\n\r\n\t\t// If perms DO NOT have queue name, then psubs will be greater than\r\n\t\t// zero. If perms DO have queue name, then qsubs will be greater than\r\n\t\t// zero.\r\n\t\tallowed = len(r.psubs) == 0\r\n\t\tif len(r.qsubs) > 0 {\r\n\t\t\t// If the queue appears in the deny list, then DO NOT allow.\r\n\t\t\tallowed = !queueMatches(queue, r.qsubs)\r\n\t\t}\r\n\t}\r\n\r\n\treturn allowed\r\n}\r\n\r\n// Low level unsubscribe for a given client.\r\nfunc (c *client) unsubscribe(acc *Account, sub *subscription, force, remove bool) {\r\n\tc.mu.Lock()\r\n\tif !force && sub.max > 0 && sub.nm < sub.max {\r\n\t\tc.Debugf(\r\n\t\t\t\"Deferring actual UNSUB(%s): %d max, %d received\",\r\n\t\t\tstring(sub.subject), sub.max, sub.nm)\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\tif c.trace {\r\n\t\tc.traceOp(\"<-> %s\", \"DELSUB\", sub.sid)\r\n\t}\r\n\r\n\tif c.kind != CLIENT && c.kind != SYSTEM {\r\n\t\tc.removeReplySubTimeout(sub)\r\n\t}\r\n\r\n\t// Remove accounting if requested. This will be false when we close a connection\r\n\t// with open subscriptions.\r\n\tif remove {\r\n\t\tdelete(c.subs, string(sub.sid))\r\n\t\tif acc != nil {\r\n\t\t\tacc.sl.Remove(sub)\r\n\t\t}\r\n\t}\r\n\r\n\t// Check to see if we have shadow subscriptions.\r\n\tvar updateRoute bool\r\n\tshadowSubs := sub.shadow\r\n\tsub.shadow = nil\r\n\tif len(shadowSubs) > 0 {\r\n\t\tupdateRoute = (c.kind == CLIENT || c.kind == SYSTEM || c.kind == LEAF) && c.srv != nil\r\n\t}\r\n\tsub.close()\r\n\tc.mu.Unlock()\r\n\r\n\t// Process shadow subs if we have them.\r\n\tfor _, nsub := range shadowSubs {\r\n\t\tif err := nsub.im.acc.sl.Remove(nsub); err != nil {\r\n\t\t\tc.Debugf(\"Could not remove shadow import subscription for account %q\", nsub.im.acc.Name)\r\n\t\t} else if updateRoute {\r\n\t\t\tc.srv.updateRouteSubscriptionMap(nsub.im.acc, nsub, -1)\r\n\t\t}\r\n\t\t// Now check on leafnode updates.\r\n\t\tc.srv.updateLeafNodes(nsub.im.acc, nsub, -1)\r\n\t}\r\n\r\n\t// Now check to see if this was part of a respMap entry for service imports.\r\n\tif acc != nil {\r\n\t\tacc.checkForRespEntry(string(sub.subject))\r\n\t}\r\n}\r\n\r\nfunc (c *client) processUnsub(arg []byte) error {\r\n\targs := splitArg(arg)\r\n\tvar sid []byte\r\n\tmax := -1\r\n\r\n\tswitch len(args) {\r\n\tcase 1:\r\n\t\tsid = args[0]\r\n\tcase 2:\r\n\t\tsid = args[0]\r\n\t\tmax = parseSize(args[1])\r\n\tdefault:\r\n\t\treturn fmt.Errorf(\"processUnsub Parse Error: '%s'\", arg)\r\n\t}\r\n\t// Indicate activity.\r\n\tc.in.subs++\r\n\r\n\tvar sub *subscription\r\n\tvar ok, unsub bool\r\n\r\n\tc.mu.Lock()\r\n\r\n\t// Grab connection type.\r\n\tkind := c.kind\r\n\tsrv := c.srv\r\n\tvar acc *Account\r\n\r\n\tupdateGWs := false\r\n\tif sub, ok = c.subs[string(sid)]; ok {\r\n\t\tacc = c.acc\r\n\t\tif max > 0 {\r\n\t\t\tsub.max = int64(max)\r\n\t\t} else {\r\n\t\t\t// Clear it here to override\r\n\t\t\tsub.max = 0\r\n\t\t\tunsub = true\r\n\t\t}\r\n\t\tupdateGWs = srv.gateway.enabled\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\tif c.opts.Verbose {\r\n\t\tc.sendOK()\r\n\t}\r\n\r\n\tif unsub {\r\n\t\tc.unsubscribe(acc, sub, false, true)\r\n\t\tif acc != nil && kind == CLIENT || kind == SYSTEM {\r\n\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\r\n\t\t\tif updateGWs {\r\n\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -1)\r\n\t\t\t}\r\n\t\t}\r\n\t\t// Now check on leafnode updates.\r\n\t\tsrv.updateLeafNodes(acc, sub, -1)\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\n// checkDenySub will check if we are allowed to deliver this message in the\r\n// presence of deny clauses for subscriptions. Deny clauses will not prevent\r\n// larger scoped wildcard subscriptions, so we need to check at delivery time.\r\n// Lock should be held.\r\nfunc (c *client) checkDenySub(subject string) bool {\r\n\tif denied, ok := c.mperms.dcache[subject]; ok {\r\n\t\treturn denied\r\n\t} else if r := c.mperms.deny.Match(subject); len(r.psubs) != 0 {\r\n\t\tc.mperms.dcache[subject] = true\r\n\t\treturn true\r\n\t} else {\r\n\t\tc.mperms.dcache[subject] = false\r\n\t}\r\n\tif len(c.mperms.dcache) > maxDenyPermCacheSize {\r\n\t\tc.pruneDenyCache()\r\n\t}\r\n\treturn false\r\n}\r\n\r\nfunc (c *client) msgHeader(mh []byte, sub *subscription, reply []byte) []byte {\r\n\tif len(sub.sid) > 0 {\r\n\t\tmh = append(mh, sub.sid...)\r\n\t\tmh = append(mh, ' ')\r\n\t}\r\n\tif reply != nil {\r\n\t\tmh = append(mh, reply...)\r\n\t\tmh = append(mh, ' ')\r\n\t}\r\n\tmh = append(mh, c.pa.szb...)\r\n\tmh = append(mh, _CRLF_...)\r\n\treturn mh\r\n}\r\n\r\nfunc (c *client) stalledWait(producer *client) {\r\n\tstall := c.out.stc\r\n\tttl := stallDuration(c.out.pb, c.out.mp)\r\n\tc.mu.Unlock()\r\n\tdefer c.mu.Lock()\r\n\r\n\tselect {\r\n\tcase <-stall:\r\n\tcase <-time.After(ttl):\r\n\t\tproducer.Debugf(\"Timed out of fast producer stall (%v)\", ttl)\r\n\t}\r\n}\r\n\r\nfunc stallDuration(pb, mp int64) time.Duration {\r\n\tttl := stallClientMinDuration\r\n\tif pb >= mp {\r\n\t\tttl = stallClientMaxDuration\r\n\t} else if hmp := mp / 2; pb > hmp {\r\n\t\tbsz := hmp / 10\r\n\t\tadditional := int64(ttl) * ((pb - hmp) / bsz)\r\n\t\tttl += time.Duration(additional)\r\n\t}\r\n\treturn ttl\r\n}\r\n\r\n// Used to treat maps as efficient set\r\nvar needFlush = struct{}{}\r\n\r\n// deliverMsg will deliver a message to a matching subscription and its underlying client.\r\n// We process all connection/client types. mh is the part that will be protocol/client specific.\r\nfunc (c *client) deliverMsg(sub *subscription, subject, reply, mh, msg []byte, gwrply bool) bool {\r\n\tif sub.client == nil {\r\n\t\treturn false\r\n\t}\r\n\tclient := sub.client\r\n\tclient.mu.Lock()\r\n\r\n\t// Check echo\r\n\tif c == client && !client.echo {\r\n\t\tclient.mu.Unlock()\r\n\t\treturn false\r\n\t}\r\n\r\n\t// Check if we have a subscribe deny clause. This will trigger us to check the subject\r\n\t// for a match against the denied subjects.\r\n\tif client.mperms != nil && client.checkDenySub(string(subject)) {\r\n\t\tclient.mu.Unlock()\r\n\t\treturn false\r\n\t}\r\n\r\n\t// This is set under the client lock using atomic because it can be\r\n\t// checked with atomic without the client lock. Here, we don't need\r\n\t// the atomic operation since we are under the lock.\r\n\tif sub.closed == 1 {\r\n\t\tclient.mu.Unlock()\r\n\t\treturn false\r\n\t}\r\n\r\n\tsrv := client.srv\r\n\r\n\tsub.nm++\r\n\t// Check if we should auto-unsubscribe.\r\n\tif sub.max > 0 {\r\n\t\tif client.kind == ROUTER && sub.nm >= sub.max {\r\n\t\t\t// The only router based messages that we will see here are remoteReplies.\r\n\t\t\t// We handle these slightly differently.\r\n\t\t\tdefer client.removeReplySub(sub)\r\n\t\t} else {\r\n\t\t\t// For routing..\r\n\t\t\tshouldForward := client.kind == CLIENT || client.kind == SYSTEM && client.srv != nil\r\n\t\t\t// If we are at the exact number, unsubscribe but\r\n\t\t\t// still process the message in hand, otherwise\r\n\t\t\t// unsubscribe and drop message on the floor.\r\n\t\t\tif sub.nm == sub.max {\r\n\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit of %d reached for sid '%s'\", sub.max, string(sub.sid))\r\n\t\t\t\t// Due to defer, reverse the code order so that execution\r\n\t\t\t\t// is consistent with other cases where we unsubscribe.\r\n\t\t\t\tif shouldForward {\r\n\t\t\t\t\tif srv.gateway.enabled {\r\n\t\t\t\t\t\tdefer srv.gatewayUpdateSubInterest(client.acc.Name, sub, -1)\r\n\t\t\t\t\t}\r\n\t\t\t\t\tdefer srv.updateRouteSubscriptionMap(client.acc, sub, -1)\r\n\t\t\t\t}\r\n\t\t\t\tdefer client.unsubscribe(client.acc, sub, true, true)\r\n\t\t\t} else if sub.nm > sub.max {\r\n\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit [%d] exceeded\", sub.max)\r\n\t\t\t\tclient.mu.Unlock()\r\n\t\t\t\tclient.unsubscribe(client.acc, sub, true, true)\r\n\t\t\t\tif shouldForward {\r\n\t\t\t\t\tsrv.updateRouteSubscriptionMap(client.acc, sub, -1)\r\n\t\t\t\t\tif srv.gateway.enabled {\r\n\t\t\t\t\t\tsrv.gatewayUpdateSubInterest(client.acc.Name, sub, -1)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\treturn false\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Update statistics\r\n\r\n\t// The msg includes the CR_LF, so pull back out for accounting.\r\n\tmsgSize := int64(len(msg) - LEN_CR_LF)\r\n\r\n\t// No atomic needed since accessed under client lock.\r\n\t// Monitor is reading those also under client's lock.\r\n\tclient.outMsgs++\r\n\tclient.outBytes += msgSize\r\n\r\n\tatomic.AddInt64(&srv.outMsgs, 1)\r\n\tatomic.AddInt64(&srv.outBytes, msgSize)\r\n\r\n\t// Check for internal subscription.\r\n\tif client.kind == SYSTEM {\r\n\t\ts := client.srv\r\n\t\tclient.mu.Unlock()\r\n\t\ts.deliverInternalMsg(sub, c, subject, c.pa.reply, msg[:msgSize])\r\n\t\treturn true\r\n\t}\r\n\r\n\t// If we are a client and we detect that the consumer we are\r\n\t// sending to is in a stalled state, go ahead and wait here\r\n\t// with a limit.\r\n\tif c.kind == CLIENT && client.out.stc != nil {\r\n\t\tclient.stalledWait(c)\r\n\t}\r\n\r\n\t// Check for closed connection\r\n\tif client.isClosed() {\r\n\t\tclient.mu.Unlock()\r\n\t\treturn false\r\n\t}\r\n\r\n\t// Do a fast check here to see if we should be tracking this from a latency\r\n\t// perspective. This will be for a request being received for an exported service.\r\n\t// This needs to be from a non-client (otherwise tracking happens at requestor).\r\n\t//\r\n\t// Also this check captures if the original reply (c.pa.reply) is a GW routed\r\n\t// reply (since it is known to be > minReplyLen). If that is the case, we need to\r\n\t// track the binding between the routed reply and the reply set in the message\r\n\t// header (which is c.pa.reply without the GNR routing prefix).\r\n\tif client.kind == CLIENT && len(c.pa.reply) > minReplyLen {\r\n\r\n\t\tif gwrply {\r\n\t\t\t// Note we keep track \"in\" the destination client (`client`) but the\r\n\t\t\t// routed reply subject is in `c.pa.reply`. Should that change, we\r\n\t\t\t// would have to pass the \"reply\" in deliverMsg().\r\n\t\t\tsrv.trackGWReply(client, c.pa.reply)\r\n\t\t}\r\n\r\n\t\t// If we do not have a registered RTT queue that up now.\r\n\t\tif client.rtt == 0 {\r\n\t\t\tclient.sendRTTPingLocked()\r\n\t\t}\r\n\t\t// FIXME(dlc) - We may need to optimize this.\r\n\t\t// We will have tagged this with a suffix ('.T') if we are tracking. This is\r\n\t\t// needed from sampling. Not all will be tracked.\r\n\t\tif c.kind != CLIENT && client.acc.IsExportServiceTracking(string(subject)) && isTrackedReply(c.pa.reply) {\r\n\t\t\tclient.trackRemoteReply(string(c.pa.reply))\r\n\t\t}\r\n\t}\r\n\r\n\t// Queue to outbound buffer\r\n\tclient.queueOutbound(mh)\r\n\tclient.queueOutbound(msg)\r\n\r\n\tclient.out.pm++\r\n\r\n\t// If we are tracking dynamic publish permissions that track reply subjects,\r\n\t// do that accounting here. We only look at client.replies which will be non-nil.\r\n\tif client.replies != nil && len(reply) > 0 {\r\n\t\tclient.replies[string(reply)] = &resp{time.Now(), 0}\r\n\t\tif len(client.replies) > replyPermLimit {\r\n\t\t\tclient.pruneReplyPerms()\r\n\t\t}\r\n\t}\r\n\r\n\t// Check outbound threshold and queue IO flush if needed.\r\n\t// This is specifically looking at situations where we are getting behind and may want\r\n\t// to intervene before this producer goes back to top of readloop. We are in the producer's\r\n\t// readloop go routine at this point.\r\n\t// FIXME(dlc) - We may call this alot, maybe suppress after first call?\r\n\tif client.out.pm > 1 && client.out.pb > maxBufSize*2 {\r\n\t\tclient.flushSignal()\r\n\t}\r\n\r\n\t// Add the data size we are responsible for here. This will be processed when we\r\n\t// return to the top of the readLoop.\r\n\tif _, ok := c.pcd[client]; !ok {\r\n\t\tclient.out.fsp++\r\n\t\tc.pcd[client] = needFlush\r\n\t}\r\n\r\n\tif client.trace {\r\n\t\tclient.traceOutOp(string(mh[:len(mh)-LEN_CR_LF]), nil)\r\n\t}\r\n\r\n\tclient.mu.Unlock()\r\n\r\n\treturn true\r\n}\r\n\r\n// This will track a remote reply for an exported service that has requested\r\n// latency tracking.\r\n// Lock assumed to be held.\r\nfunc (c *client) trackRemoteReply(reply string) {\r\n\tif c.rrTracking == nil {\r\n\t\tc.rrTracking = make(map[string]*remoteLatency)\r\n\t\tc.rrMax = c.acc.MaxAutoExpireResponseMaps()\r\n\t}\r\n\trl := remoteLatency{\r\n\t\tAccount: c.acc.Name,\r\n\t\tReqId:   reply,\r\n\t}\r\n\trl.M2.RequestStart = time.Now()\r\n\tc.rrTracking[reply] = &rl\r\n\tif len(c.rrTracking) >= c.rrMax {\r\n\t\tc.pruneRemoteTracking()\r\n\t}\r\n}\r\n\r\n// pruneReplyPerms will remove any stale or expired entries\r\n// in our reply cache. We make sure to not check too often.\r\nfunc (c *client) pruneReplyPerms() {\r\n\t// Make sure we do not check too often.\r\n\tif c.perms.resp == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\tmm := c.perms.resp.MaxMsgs\r\n\tttl := c.perms.resp.Expires\r\n\tnow := time.Now()\r\n\r\n\tfor k, resp := range c.replies {\r\n\t\tif mm > 0 && resp.n >= mm {\r\n\t\t\tdelete(c.replies, k)\r\n\t\t} else if ttl > 0 && now.Sub(resp.t) > ttl {\r\n\t\t\tdelete(c.replies, k)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// pruneDenyCache will prune the deny cache via randomly\r\n// deleting items. Doing so pruneSize items at a time.\r\n// Lock must be held for this one since it is shared under\r\n// deliverMsg.\r\nfunc (c *client) pruneDenyCache() {\r\n\tr := 0\r\n\tfor subject := range c.mperms.dcache {\r\n\t\tdelete(c.mperms.dcache, subject)\r\n\t\tif r++; r > pruneSize {\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// prunePubPermsCache will prune the cache via randomly\r\n// deleting items. Doing so pruneSize items at a time.\r\nfunc (c *client) prunePubPermsCache() {\r\n\tr := 0\r\n\tfor subject := range c.perms.pcache {\r\n\t\tdelete(c.perms.pcache, subject)\r\n\t\tif r++; r > pruneSize {\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// pruneRemoteTracking will prune any remote tracking objects\r\n// that are too old. These are orphaned when a service is not\r\n// sending reponses etc.\r\n// Lock should be held upon entry.\r\nfunc (c *client) pruneRemoteTracking() {\r\n\tttl := c.acc.AutoExpireTTL()\r\n\tnow := time.Now()\r\n\tfor reply, rl := range c.rrTracking {\r\n\t\tif now.Sub(rl.M2.RequestStart) > ttl {\r\n\t\t\tdelete(c.rrTracking, reply)\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// pubAllowed checks on publish permissioning.\r\n// Lock should not be held.\r\nfunc (c *client) pubAllowed(subject string) bool {\r\n\treturn c.pubAllowedFullCheck(subject, true)\r\n}\r\n\r\n// pubAllowedFullCheck checks on all publish permissioning depending\r\n// on the flag for dynamic reply permissions.\r\nfunc (c *client) pubAllowedFullCheck(subject string, fullCheck bool) bool {\r\n\tif c.perms == nil || (c.perms.pub.allow == nil && c.perms.pub.deny == nil) {\r\n\t\treturn true\r\n\t}\r\n\t// Check if published subject is allowed if we have permissions in place.\r\n\tallowed, ok := c.perms.pcache[subject]\r\n\tif ok {\r\n\t\treturn allowed\r\n\t}\r\n\t// Cache miss, check allow then deny as needed.\r\n\tif c.perms.pub.allow != nil {\r\n\t\tr := c.perms.pub.allow.Match(subject)\r\n\t\tallowed = len(r.psubs) != 0\r\n\t} else {\r\n\t\t// No entries means all are allowed. Deny will overrule as needed.\r\n\t\tallowed = true\r\n\t}\r\n\t// If we have a deny list and are currently allowed, check that as well.\r\n\tif allowed && c.perms.pub.deny != nil {\r\n\t\tr := c.perms.pub.deny.Match(subject)\r\n\t\tallowed = len(r.psubs) == 0\r\n\t}\r\n\r\n\t// If we are currently not allowed but we are tracking reply subjects\r\n\t// dynamically, check to see if we are allowed here but avoid pcache.\r\n\t// We need to acquire the lock though.\r\n\tif !allowed && fullCheck && c.perms.resp != nil {\r\n\t\tc.mu.Lock()\r\n\t\tif resp := c.replies[subject]; resp != nil {\r\n\t\t\tresp.n++\r\n\t\t\t// Check if we have sent too many responses.\r\n\t\t\tif c.perms.resp.MaxMsgs > 0 && resp.n > c.perms.resp.MaxMsgs {\r\n\t\t\t\tdelete(c.replies, subject)\r\n\t\t\t} else if c.perms.resp.Expires > 0 && time.Since(resp.t) > c.perms.resp.Expires {\r\n\t\t\t\tdelete(c.replies, subject)\r\n\t\t\t} else {\r\n\t\t\t\tallowed = true\r\n\t\t\t}\r\n\t\t}\r\n\t\tc.mu.Unlock()\r\n\t} else {\r\n\t\t// Update our cache here.\r\n\t\tc.perms.pcache[string(subject)] = allowed\r\n\t\t// Prune if needed.\r\n\t\tif len(c.perms.pcache) > maxPermCacheSize {\r\n\t\t\tc.prunePubPermsCache()\r\n\t\t}\r\n\t}\r\n\treturn allowed\r\n}\r\n\r\n// Test whether a reply subject is a service import reply.\r\nfunc isServiceReply(reply []byte) bool {\r\n\t// This function is inlined and checking this way is actually faster\r\n\t// than byte-by-byte comparison.\r\n\treturn len(reply) > 3 && string(reply[:4]) == replyPrefix\r\n}\r\n\r\n// Test whether a reply subject is a service import or a gateway routed reply.\r\nfunc isReservedReply(reply []byte) bool {\r\n\tif isServiceReply(reply) {\r\n\t\treturn true\r\n\t}\r\n\t// Faster to check with string([:]) than byte-by-byte\r\n\tif len(reply) > gwReplyPrefixLen && string(reply[:gwReplyPrefixLen]) == gwReplyPrefix {\r\n\t\treturn true\r\n\t}\r\n\treturn false\r\n}\r\n\r\n// This will decide to call the client code or router code.\r\nfunc (c *client) processInboundMsg(msg []byte) {\r\n\tswitch c.kind {\r\n\tcase CLIENT:\r\n\t\tc.processInboundClientMsg(msg)\r\n\tcase ROUTER:\r\n\t\tc.processInboundRoutedMsg(msg)\r\n\tcase GATEWAY:\r\n\t\tc.processInboundGatewayMsg(msg)\r\n\tcase LEAF:\r\n\t\tc.processInboundLeafMsg(msg)\r\n\t}\r\n}\r\n\r\n// processInboundClientMsg is called to process an inbound msg from a client.\r\nfunc (c *client) processInboundClientMsg(msg []byte) {\r\n\t// Update statistics\r\n\t// The msg includes the CR_LF, so pull back out for accounting.\r\n\tc.in.msgs++\r\n\tc.in.bytes += int32(len(msg) - LEN_CR_LF)\r\n\r\n\t// Check that client (could be here with SYSTEM) is not publishing on reserved \"$GNR\" prefix.\r\n\tif c.kind == CLIENT && hasGWRoutedReplyPrefix(c.pa.subject) {\r\n\t\tc.pubPermissionViolation(c.pa.subject)\r\n\t\treturn\r\n\t}\r\n\r\n\t// Check pub permissions\r\n\tif c.perms != nil && (c.perms.pub.allow != nil || c.perms.pub.deny != nil) && !c.pubAllowed(string(c.pa.subject)) {\r\n\t\tc.pubPermissionViolation(c.pa.subject)\r\n\t\treturn\r\n\t}\r\n\r\n\t// Now check for reserved replies. These are used for service imports.\r\n\tif len(c.pa.reply) > 0 && isReservedReply(c.pa.reply) {\r\n\t\tc.replySubjectViolation(c.pa.reply)\r\n\t\treturn\r\n\t}\r\n\r\n\tif c.opts.Verbose {\r\n\t\tc.sendOK()\r\n\t}\r\n\r\n\t// Mostly under testing scenarios.\r\n\tif c.srv == nil || c.acc == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Check if this client's gateway replies map is not empty\r\n\tif atomic.LoadInt32(&c.cgwrt) > 0 && c.handleGWReplyMap(msg) {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Check to see if we need to map/route to another account.\r\n\tif c.acc.imports.services != nil {\r\n\t\tc.checkForImportServices(c.acc, msg)\r\n\t}\r\n\r\n\t// If we have an exported service and we are doing remote tracking, check this subject\r\n\t// to see if we need to report the latency.\r\n\tif c.rrTracking != nil {\r\n\t\tc.mu.Lock()\r\n\t\trl := c.rrTracking[string(c.pa.subject)]\r\n\t\tif rl != nil {\r\n\t\t\tdelete(c.rrTracking, string(c.pa.subject))\r\n\t\t}\r\n\t\trtt := c.rtt\r\n\t\tc.mu.Unlock()\r\n\t\tif rl != nil {\r\n\t\t\tsl := &rl.M2\r\n\t\t\t// Fill this in and send it off to the other side.\r\n\t\t\tsl.AppName = c.opts.Name\r\n\t\t\tsl.ServiceLatency = time.Since(sl.RequestStart) - rtt\r\n\t\t\tsl.NATSLatency.Responder = rtt\r\n\t\t\tsl.TotalLatency = sl.ServiceLatency + rtt\r\n\t\t\tsanitizeLatencyMetric(sl)\r\n\r\n\t\t\tlsub := remoteLatencySubjectForResponse(c.pa.subject)\r\n\t\t\tc.srv.sendInternalAccountMsg(nil, lsub, &rl) // Send to SYS account\r\n\t\t}\r\n\t}\r\n\r\n\t// Match the subscriptions. We will use our own L1 map if\r\n\t// it's still valid, avoiding contention on the shared sublist.\r\n\tvar r *SublistResult\r\n\tvar ok bool\r\n\r\n\tgenid := atomic.LoadUint64(&c.acc.sl.genid)\r\n\tif genid == c.in.genid && c.in.results != nil {\r\n\t\tr, ok = c.in.results[string(c.pa.subject)]\r\n\t} else {\r\n\t\t// Reset our L1 completely.\r\n\t\tc.in.results = make(map[string]*SublistResult)\r\n\t\tc.in.genid = genid\r\n\t}\r\n\r\n\t// Go back to the sublist data structure.\r\n\tif !ok {\r\n\t\tr = c.acc.sl.Match(string(c.pa.subject))\r\n\t\tc.in.results[string(c.pa.subject)] = r\r\n\t\t// Prune the results cache. Keeps us from unbounded growth. Random delete.\r\n\t\tif len(c.in.results) > maxResultCacheSize {\r\n\t\t\tn := 0\r\n\t\t\tfor subject := range c.in.results {\r\n\t\t\t\tdelete(c.in.results, subject)\r\n\t\t\t\tif n++; n > pruneSize {\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tvar qnames [][]byte\r\n\r\n\t// Check for no interest, short circuit if so.\r\n\t// This is the fanout scale.\r\n\tif len(r.psubs)+len(r.qsubs) > 0 {\r\n\t\tflag := pmrNoFlag\r\n\t\t// If there are matching queue subs and we are in gateway mode,\r\n\t\t// we need to keep track of the queue names the messages are\r\n\t\t// delivered to. When sending to the GWs, the RMSG will include\r\n\t\t// those names so that the remote clusters do not deliver messages\r\n\t\t// to their queue subs of the same names.\r\n\t\tif len(r.qsubs) > 0 && c.srv.gateway.enabled &&\r\n\t\t\tatomic.LoadInt64(&c.srv.gateway.totalQSubs) > 0 {\r\n\t\t\tflag |= pmrCollectQueueNames\r\n\t\t}\r\n\t\tqnames = c.processMsgResults(c.acc, r, msg, c.pa.subject, c.pa.reply, flag)\r\n\t}\r\n\r\n\t// Now deal with gateways\r\n\tif c.srv.gateway.enabled {\r\n\t\tc.sendMsgToGateways(c.acc, msg, c.pa.subject, c.pa.reply, qnames)\r\n\t}\r\n}\r\n\r\n// This is invoked knowing that this client has some GW replies\r\n// in its map. It will check if one is find for the c.pa.subject\r\n// and if so will process it directly (send to GWs and LEAF) and\r\n// return true to notify the caller that the message was handled.\r\n// If there is no mapping for the subject, false is returned.\r\nfunc (c *client) handleGWReplyMap(msg []byte) bool {\r\n\tc.mu.Lock()\r\n\trm, ok := c.gwrm[string(c.pa.subject)]\r\n\tif !ok {\r\n\t\tc.mu.Unlock()\r\n\t\treturn false\r\n\t}\r\n\t// Set subject to the mapped reply subject\r\n\tc.pa.subject = []byte(rm.ms)\r\n\r\n\tvar rl *remoteLatency\r\n\tvar rtt time.Duration\r\n\r\n\tif c.rrTracking != nil {\r\n\t\trl = c.rrTracking[string(c.pa.subject)]\r\n\t\tif rl != nil {\r\n\t\t\tdelete(c.rrTracking, string(c.pa.subject))\r\n\t\t}\r\n\t\trtt = c.rtt\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\tif rl != nil {\r\n\t\tsl := &rl.M2\r\n\t\t// Fill this in and send it off to the other side.\r\n\t\tsl.AppName = c.opts.Name\r\n\t\tsl.ServiceLatency = time.Since(sl.RequestStart) - rtt\r\n\t\tsl.NATSLatency.Responder = rtt\r\n\t\tsl.TotalLatency = sl.ServiceLatency + rtt\r\n\t\tsanitizeLatencyMetric(sl)\r\n\r\n\t\tlsub := remoteLatencySubjectForResponse(c.pa.subject)\r\n\t\tc.srv.sendInternalAccountMsg(nil, lsub, &rl) // Send to SYS account\r\n\t}\r\n\r\n\t// Check for leaf nodes\r\n\tif c.srv.gwLeafSubs.Count() > 0 {\r\n\t\tif r := c.srv.gwLeafSubs.Match(string(c.pa.subject)); len(r.psubs) > 0 {\r\n\t\t\tc.processMsgResults(c.acc, r, msg, c.pa.subject, c.pa.reply, pmrNoFlag)\r\n\t\t}\r\n\t}\r\n\tif c.srv.gateway.enabled {\r\n\t\tc.sendMsgToGateways(c.acc, msg, c.pa.subject, c.pa.reply, nil)\r\n\t}\r\n\r\n\treturn true\r\n}\r\n\r\n// This checks and process import services by doing the mapping and sending the\r\n// message onward if applicable.\r\nfunc (c *client) checkForImportServices(acc *Account, msg []byte) {\r\n\tif acc == nil || acc.imports.services == nil {\r\n\t\treturn\r\n\t}\r\n\r\n\tacc.mu.RLock()\r\n\tsi := acc.imports.services[string(c.pa.subject)]\r\n\tinvalid := si != nil && si.invalid\r\n\tacc.mu.RUnlock()\r\n\r\n\t// Get the results from the other account for the mapped \"to\" subject.\r\n\t// If we have been marked invalid simply return here.\r\n\tif si != nil && !invalid && si.acc != nil && si.acc.sl != nil {\r\n\t\tvar nrr []byte\r\n\t\tif c.pa.reply != nil {\r\n\t\t\tvar latency *serviceLatency\r\n\t\t\tvar tracking bool\r\n\t\t\tif tracking = shouldSample(si.latency); tracking {\r\n\t\t\t\tlatency = si.latency\r\n\t\t\t}\r\n\t\t\t// We want to remap this to provide anonymity.\r\n\t\t\tnrr = si.acc.newServiceReply(tracking)\r\n\t\t\tsi.acc.addRespServiceImport(acc, string(nrr), string(c.pa.reply), si.rt, latency)\r\n\r\n\t\t\t// Track our responses for cleanup if not auto-expire.\r\n\t\t\tif si.rt != Singleton {\r\n\t\t\t\tacc.addRespMapEntry(si.acc, string(c.pa.reply), string(nrr))\r\n\t\t\t} else if si.latency != nil && c.rtt == 0 {\r\n\t\t\t\t// We have a service import that we are tracking but have not established RTT.\r\n\t\t\t\tc.sendRTTPing()\r\n\t\t\t}\r\n\t\t}\r\n\t\t// FIXME(dlc) - Do L1 cache trick from above.\r\n\t\trr := si.acc.sl.Match(si.to)\r\n\r\n\t\t// Check to see if we have no results and this is an internal serviceImport. If so we\r\n\t\t// need to clean that up.\r\n\t\tif len(rr.psubs)+len(rr.qsubs) == 0 && si.internal {\r\n\t\t\t// We may also have a response entry, so go through that way.\r\n\t\t\tsi.acc.checkForRespEntry(si.to)\r\n\t\t}\r\n\r\n\t\tflags := pmrNoFlag\r\n\t\t// If we are a route or gateway or leafnode and this message is flipped to a queue subscriber we\r\n\t\t// need to handle that since the processMsgResults will want a queue filter.\r\n\t\tif c.kind == GATEWAY || c.kind == ROUTER || c.kind == LEAF {\r\n\t\t\tflags |= pmrIgnoreEmptyQueueFilter\r\n\t\t}\r\n\t\tif c.srv.gateway.enabled {\r\n\t\t\tflags |= pmrCollectQueueNames\r\n\t\t\tqueues := c.processMsgResults(si.acc, rr, msg, []byte(si.to), nrr, flags)\r\n\t\t\tc.sendMsgToGateways(si.acc, msg, []byte(si.to), nrr, queues)\r\n\t\t} else {\r\n\t\t\tc.processMsgResults(si.acc, rr, msg, []byte(si.to), nrr, flags)\r\n\t\t}\r\n\r\n\t\tshouldRemove := si.ae\r\n\r\n\t\t// Calculate tracking info here if we are tracking this request/response.\r\n\t\tif si.tracking {\r\n\t\t\tif requesting := firstSubFromResult(rr); requesting != nil {\r\n\t\t\t\tshouldRemove = acc.sendTrackingLatency(si, requesting.client, c)\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tif shouldRemove {\r\n\t\t\tacc.removeServiceImport(si.from)\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc (c *client) addSubToRouteTargets(sub *subscription) {\r\n\tif c.in.rts == nil {\r\n\t\tc.in.rts = make([]routeTarget, 0, routeTargetInit)\r\n\t}\r\n\r\n\tfor i := range c.in.rts {\r\n\t\trt := &c.in.rts[i]\r\n\t\tif rt.sub.client == sub.client {\r\n\t\t\tif sub.queue != nil {\r\n\t\t\t\trt.qs = append(rt.qs, sub.queue...)\r\n\t\t\t\trt.qs = append(rt.qs, ' ')\r\n\t\t\t}\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n\r\n\tvar rt *routeTarget\r\n\tlrts := len(c.in.rts)\r\n\r\n\t// If we are here we do not have the sub yet in our list\r\n\t// If we have to grow do so here.\r\n\tif lrts == cap(c.in.rts) {\r\n\t\tc.in.rts = append(c.in.rts, routeTarget{})\r\n\t}\r\n\r\n\tc.in.rts = c.in.rts[:lrts+1]\r\n\trt = &c.in.rts[lrts]\r\n\trt.sub = sub\r\n\trt.qs = rt._qs[:0]\r\n\tif sub.queue != nil {\r\n\t\trt.qs = append(rt.qs, sub.queue...)\r\n\t\trt.qs = append(rt.qs, ' ')\r\n\t}\r\n}\r\n\r\n// This processes the sublist results for a given message.\r\nfunc (c *client) processMsgResults(acc *Account, r *SublistResult, msg, subject, reply []byte, flags int) [][]byte {\r\n\tvar queues [][]byte\r\n\t// msg header for clients.\r\n\tmsgh := c.msgb[1:msgHeadProtoLen]\r\n\tmsgh = append(msgh, subject...)\r\n\tmsgh = append(msgh, ' ')\r\n\tsi := len(msgh)\r\n\r\n\t// For sending messages across routes and leafnodes.\r\n\t// Reset if we have one since we reuse this data structure.\r\n\tif c.in.rts != nil {\r\n\t\tc.in.rts = c.in.rts[:0]\r\n\t}\r\n\r\n\tvar rplyHasGWPrefix bool\r\n\tvar creply = reply\r\n\r\n\t// If the reply subject is a GW routed reply, we will perform some\r\n\t// tracking in deliverMsg(). We also want to send to the user the\r\n\t// reply without the prefix. `creply` will be set to that and be\r\n\t// used to create the message header for client connections.\r\n\tif rplyHasGWPrefix = isGWRoutedReply(reply); rplyHasGWPrefix {\r\n\t\tcreply = reply[gwSubjectOffset:]\r\n\t}\r\n\r\n\t// Loop over all normal subscriptions that match.\r\n\tfor _, sub := range r.psubs {\r\n\t\t// Check if this is a send to a ROUTER. We now process\r\n\t\t// these after everything else.\r\n\t\tswitch sub.client.kind {\r\n\t\tcase ROUTER:\r\n\t\t\tif (c.kind != ROUTER && !c.isSpokeLeafNode()) || (flags&pmrAllowSendFromRouteToRoute != 0) {\r\n\t\t\t\tc.addSubToRouteTargets(sub)\r\n\t\t\t}\r\n\t\t\tcontinue\r\n\t\tcase GATEWAY:\r\n\t\t\t// Never send to gateway from here.\r\n\t\t\tcontinue\r\n\t\tcase LEAF:\r\n\t\t\t// We handle similarly to routes and use the same data structures.\r\n\t\t\t// Leaf node delivery audience is different however.\r\n\t\t\t// Also leaf nodes are always no echo, so we make sure we are not\r\n\t\t\t// going to send back to ourselves here.\r\n\t\t\tif c != sub.client && (c.kind != ROUTER || !c.isSpokeLeafNode()) {\r\n\t\t\t\tc.addSubToRouteTargets(sub)\r\n\t\t\t}\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\t// Check for stream import mapped subs. These apply to local subs only.\r\n\t\tif sub.im != nil && sub.im.prefix != \"\" {\r\n\t\t\t// Redo the subject here on the fly.\r\n\t\t\tmsgh = c.msgb[1:msgHeadProtoLen]\r\n\t\t\tmsgh = append(msgh, sub.im.prefix...)\r\n\t\t\tmsgh = append(msgh, subject...)\r\n\t\t\tmsgh = append(msgh, ' ')\r\n\t\t\tsi = len(msgh)\r\n\t\t}\r\n\t\t// Normal delivery\r\n\t\tmh := c.msgHeader(msgh[:si], sub, creply)\r\n\t\tc.deliverMsg(sub, subject, creply, mh, msg, rplyHasGWPrefix)\r\n\t}\r\n\r\n\t// Set these up to optionally filter based on the queue lists.\r\n\t// This is for messages received from routes which will have directed\r\n\t// guidance on which queue groups we should deliver to.\r\n\tqf := c.pa.queues\r\n\r\n\t// For all non-client connections, we may still want to send messages to\r\n\t// leaf nodes or routes even if there are no queue filters since we collect\r\n\t// them above and do not process inline like normal clients.\r\n\t// However, do select queue subs if asked to ignore empty queue filter.\r\n\tif c.kind != CLIENT && qf == nil && flags&pmrIgnoreEmptyQueueFilter == 0 {\r\n\t\tgoto sendToRoutesOrLeafs\r\n\t}\r\n\r\n\t// Check to see if we have our own rand yet. Global rand\r\n\t// has contention with lots of clients, etc.\r\n\tif c.in.prand == nil {\r\n\t\tc.in.prand = rand.New(rand.NewSource(time.Now().UnixNano()))\r\n\t}\r\n\r\n\t// Process queue subs\r\n\tfor i := 0; i < len(r.qsubs); i++ {\r\n\t\tqsubs := r.qsubs[i]\r\n\t\t// If we have a filter check that here. We could make this a map or someting more\r\n\t\t// complex but linear search since we expect queues to be small. Should be faster\r\n\t\t// and more cache friendly.\r\n\t\tif qf != nil && len(qsubs) > 0 {\r\n\t\t\ttqn := qsubs[0].queue\r\n\t\t\tfor _, qn := range qf {\r\n\t\t\t\tif bytes.Equal(qn, tqn) {\r\n\t\t\t\t\tgoto selectQSub\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\tselectQSub:\r\n\t\t// We will hold onto remote or lead qsubs when we are coming from\r\n\t\t// a route or a leaf node just in case we can no longer do local delivery.\r\n\t\tvar rsub, sub *subscription\r\n\t\tvar _ql [32]*subscription\r\n\r\n\t\tsrc := c.kind\r\n\t\t// If we just came from a route we want to prefer local subs.\r\n\t\t// So only select from local subs but remember the first rsub\r\n\t\t// in case all else fails.\r\n\t\tif src == ROUTER {\r\n\t\t\tql := _ql[:0]\r\n\t\t\tfor i := 0; i < len(qsubs); i++ {\r\n\t\t\t\tsub = qsubs[i]\r\n\t\t\t\tif sub.client.kind == CLIENT {\r\n\t\t\t\t\tql = append(ql, sub)\r\n\t\t\t\t} else if rsub == nil {\r\n\t\t\t\t\trsub = sub\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tqsubs = ql\r\n\t\t}\r\n\r\n\t\tsindex := 0\r\n\t\tlqs := len(qsubs)\r\n\t\tif lqs > 1 {\r\n\t\t\tsindex = c.in.prand.Int() % lqs\r\n\t\t}\r\n\r\n\t\t// Find a subscription that is able to deliver this message starting at a random index.\r\n\t\tfor i := 0; i < lqs; i++ {\r\n\t\t\tif sindex+i < lqs {\r\n\t\t\t\tsub = qsubs[sindex+i]\r\n\t\t\t} else {\r\n\t\t\t\tsub = qsubs[(sindex+i)%lqs]\r\n\t\t\t}\r\n\t\t\tif sub == nil {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\t// We have taken care of preferring local subs for a message from a route above.\r\n\t\t\t// Here we just care about a client or leaf and skipping a leaf and preferring locals.\r\n\t\t\tif dst := sub.client.kind; dst == ROUTER || dst == LEAF {\r\n\t\t\t\tif (src == LEAF || src == CLIENT) && dst == LEAF {\r\n\t\t\t\t\tif rsub == nil {\r\n\t\t\t\t\t\trsub = sub\r\n\t\t\t\t\t}\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t} else {\r\n\t\t\t\t\tc.addSubToRouteTargets(sub)\r\n\t\t\t\t\tif flags&pmrCollectQueueNames != 0 {\r\n\t\t\t\t\t\tqueues = append(queues, sub.queue)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\r\n\t\t\t// Check for mapped subs\r\n\t\t\tif sub.im != nil && sub.im.prefix != \"\" {\r\n\t\t\t\t// Redo the subject here on the fly.\r\n\t\t\t\tmsgh = c.msgb[1:msgHeadProtoLen]\r\n\t\t\t\tmsgh = append(msgh, sub.im.prefix...)\r\n\t\t\t\tmsgh = append(msgh, subject...)\r\n\t\t\t\tmsgh = append(msgh, ' ')\r\n\t\t\t\tsi = len(msgh)\r\n\t\t\t}\r\n\r\n\t\t\tvar rreply = reply\r\n\t\t\tif rplyHasGWPrefix && sub.client.kind == CLIENT {\r\n\t\t\t\trreply = creply\r\n\t\t\t}\r\n\t\t\t// \"rreply\" will be stripped of the $GNR prefix (if present)\r\n\t\t\t// for client connections only.\r\n\t\t\tmh := c.msgHeader(msgh[:si], sub, rreply)\r\n\t\t\tif c.deliverMsg(sub, subject, rreply, mh, msg, rplyHasGWPrefix) {\r\n\t\t\t\t// Clear rsub\r\n\t\t\t\trsub = nil\r\n\t\t\t\tif flags&pmrCollectQueueNames != 0 {\r\n\t\t\t\t\tqueues = append(queues, sub.queue)\r\n\t\t\t\t}\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tif rsub != nil {\r\n\t\t\t// If we are here we tried to deliver to a local qsub\r\n\t\t\t// but failed. So we will send it to a remote or leaf node.\r\n\t\t\tc.addSubToRouteTargets(rsub)\r\n\t\t\tif flags&pmrCollectQueueNames != 0 {\r\n\t\t\t\tqueues = append(queues, rsub.queue)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\nsendToRoutesOrLeafs:\r\n\r\n\t// If no messages for routes or leafnodes return here.\r\n\tif len(c.in.rts) == 0 {\r\n\t\treturn queues\r\n\t}\r\n\r\n\t// We address by index to avoid struct copy.\r\n\t// We have inline structs for memory layout and cache coherency.\r\n\tfor i := range c.in.rts {\r\n\t\trt := &c.in.rts[i]\r\n\t\tkind := rt.sub.client.kind\r\n\t\tmh := c.msgb[:msgHeadProtoLen]\r\n\t\tif kind == ROUTER {\r\n\t\t\t// Router (and Gateway) nodes are RMSG. Set here since leafnodes may rewrite.\r\n\t\t\tmh[0] = 'R'\r\n\t\t\tmh = append(mh, acc.Name...)\r\n\t\t\tmh = append(mh, ' ')\r\n\t\t} else {\r\n\t\t\t// Leaf nodes are LMSG\r\n\t\t\tmh[0] = 'L'\r\n\t\t\t// Remap subject if its a shadow subscription, treat like a normal client.\r\n\t\t\tif rt.sub.im != nil && rt.sub.im.prefix != \"\" {\r\n\t\t\t\tmh = append(mh, rt.sub.im.prefix...)\r\n\t\t\t}\r\n\t\t}\r\n\t\tmh = append(mh, subject...)\r\n\t\tmh = append(mh, ' ')\r\n\r\n\t\tif len(rt.qs) > 0 {\r\n\t\t\tif reply != nil {\r\n\t\t\t\tmh = append(mh, \"+ \"...) // Signal that there is a reply.\r\n\t\t\t\tmh = append(mh, reply...)\r\n\t\t\t\tmh = append(mh, ' ')\r\n\t\t\t} else {\r\n\t\t\t\tmh = append(mh, \"| \"...) // Only queues\r\n\t\t\t}\r\n\t\t\tmh = append(mh, rt.qs...)\r\n\t\t} else if reply != nil {\r\n\t\t\tmh = append(mh, reply...)\r\n\t\t\tmh = append(mh, ' ')\r\n\t\t}\r\n\t\tmh = append(mh, c.pa.szb...)\r\n\t\tmh = append(mh, _CRLF_...)\r\n\t\tc.deliverMsg(rt.sub, subject, reply, mh, msg, false)\r\n\t}\r\n\treturn queues\r\n}\r\n\r\nfunc (c *client) pubPermissionViolation(subject []byte) {\r\n\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish to %q\", subject))\r\n\tc.Errorf(\"Publish Violation - %s, Subject %q\", c.getAuthUser(), subject)\r\n}\r\n\r\nfunc (c *client) subPermissionViolation(sub *subscription) {\r\n\terrTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q\", sub.subject)\r\n\tlogTxt := fmt.Sprintf(\"Subscription Violation - %s, Subject %q, SID %s\",\r\n\t\tc.getAuthUser(), sub.subject, sub.sid)\r\n\r\n\tif sub.queue != nil {\r\n\t\terrTxt = fmt.Sprintf(\"Permissions Violation for Subscription to %q using queue %q\", sub.subject, sub.queue)\r\n\t\tlogTxt = fmt.Sprintf(\"Subscription Violation - %s, Subject %q, Queue: %q, SID %s\",\r\n\t\t\tc.getAuthUser(), sub.subject, sub.queue, sub.sid)\r\n\t}\r\n\r\n\tc.sendErr(errTxt)\r\n\tc.Errorf(logTxt)\r\n}\r\n\r\nfunc (c *client) replySubjectViolation(reply []byte) {\r\n\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish with Reply of %q\", reply))\r\n\tc.Errorf(\"Publish Violation - %s, Reply %q\", c.getAuthUser(), reply)\r\n}\r\n\r\nfunc (c *client) processPingTimer() {\r\n\tc.mu.Lock()\r\n\tc.ping.tmr = nil\r\n\t// Check if connection is still opened\r\n\tif c.isClosed() {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\r\n\tc.Debugf(\"%s Ping Timer\", c.typeString())\r\n\r\n\t// If we have had activity within the PingInterval then\r\n\t// there is no need to send a ping. This can be client data\r\n\t// or if we received a ping from the other side.\r\n\tpingInterval := c.srv.getOpts().PingInterval\r\n\tnow := time.Now()\r\n\tneedRTT := c.rtt == 0 || now.Sub(c.rttStart) > DEFAULT_RTT_MEASUREMENT_INTERVAL\r\n\r\n\tif delta := now.Sub(c.last); delta < pingInterval && !needRTT {\r\n\t\tc.Debugf(\"Delaying PING due to client activity %v ago\", delta.Round(time.Second))\r\n\t} else if delta := now.Sub(c.ping.last); delta < pingInterval && !needRTT {\r\n\t\tc.Debugf(\"Delaying PING due to remote ping %v ago\", delta.Round(time.Second))\r\n\t} else {\r\n\t\t// Check for violation\r\n\t\tif c.ping.out+1 > c.srv.getOpts().MaxPingsOut {\r\n\t\t\tc.Debugf(\"Stale Client Connection - Closing\")\r\n\t\t\tc.enqueueProto([]byte(fmt.Sprintf(errProto, \"Stale Connection\")))\r\n\t\t\tc.mu.Unlock()\r\n\t\t\tc.closeConnection(StaleConnection)\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// Send PING\r\n\t\tc.sendPing()\r\n\t}\r\n\r\n\t// Reset to fire again.\r\n\tc.setPingTimer()\r\n\tc.mu.Unlock()\r\n}\r\n\r\n// Lock should be held\r\nfunc (c *client) setPingTimer() {\r\n\tif c.srv == nil {\r\n\t\treturn\r\n\t}\r\n\td := c.srv.getOpts().PingInterval\r\n\tc.ping.tmr = time.AfterFunc(d, c.processPingTimer)\r\n}\r\n\r\n// Lock should be held\r\nfunc (c *client) clearPingTimer() {\r\n\tif c.ping.tmr == nil {\r\n\t\treturn\r\n\t}\r\n\tc.ping.tmr.Stop()\r\n\tc.ping.tmr = nil\r\n}\r\n\r\n// Lock should be held\r\nfunc (c *client) setAuthTimer(d time.Duration) {\r\n\tc.atmr = time.AfterFunc(d, c.authTimeout)\r\n}\r\n\r\n// Lock should be held\r\nfunc (c *client) clearAuthTimer() bool {\r\n\tif c.atmr == nil {\r\n\t\treturn true\r\n\t}\r\n\tstopped := c.atmr.Stop()\r\n\tc.atmr = nil\r\n\treturn stopped\r\n}\r\n\r\n// We may reuse atmr for expiring user jwts,\r\n// so check connectReceived.\r\n// Lock assume held on entry.\r\nfunc (c *client) awaitingAuth() bool {\r\n\treturn !c.flags.isSet(connectReceived) && c.atmr != nil\r\n}\r\n\r\n// This will set the atmr for the JWT expiration time.\r\n// We will lock on entry.\r\nfunc (c *client) setExpirationTimer(d time.Duration) {\r\n\tc.mu.Lock()\r\n\tc.atmr = time.AfterFunc(d, c.authExpired)\r\n\tc.mu.Unlock()\r\n}\r\n\r\n// Possibly flush the connection and then close the low level connection.\r\n// The boolean `minimalFlush` indicates if the flush operation should have a\r\n// minimal write deadline.\r\n// Lock is held on entry.\r\nfunc (c *client) flushAndClose(minimalFlush bool) {\r\n\tif !c.flags.isSet(skipFlushOnClose) && c.out.pb > 0 {\r\n\t\tif minimalFlush {\r\n\t\t\tconst lowWriteDeadline = 100 * time.Millisecond\r\n\r\n\t\t\t// Reduce the write deadline if needed.\r\n\t\t\tif c.out.wdl > lowWriteDeadline {\r\n\t\t\t\tc.out.wdl = lowWriteDeadline\r\n\t\t\t}\r\n\t\t}\r\n\t\tc.flushOutbound()\r\n\t}\r\n\tc.out.p, c.out.s = nil, nil\r\n\r\n\t// Close the low level connection. WriteDeadline need to be set\r\n\t// in case this is a TLS connection.\r\n\tif c.nc != nil {\r\n\t\tc.nc.SetWriteDeadline(time.Now().Add(100 * time.Millisecond))\r\n\t\tc.nc.Close()\r\n\t}\r\n}\r\n\r\nfunc (c *client) typeString() string {\r\n\tswitch c.kind {\r\n\tcase CLIENT:\r\n\t\treturn \"Client\"\r\n\tcase ROUTER:\r\n\t\treturn \"Router\"\r\n\tcase GATEWAY:\r\n\t\treturn \"Gateway\"\r\n\tcase LEAF:\r\n\t\treturn \"LeafNode\"\r\n\t}\r\n\treturn \"Unknown Type\"\r\n}\r\n\r\n// processSubsOnConfigReload removes any subscriptions the client has that are no\r\n// longer authorized, and check for imports (accounts) due to a config reload.\r\nfunc (c *client) processSubsOnConfigReload(awcsti map[string]struct{}) {\r\n\tc.mu.Lock()\r\n\tvar (\r\n\t\tcheckPerms = c.perms != nil\r\n\t\tcheckAcc   = c.acc != nil\r\n\t\tacc        = c.acc\r\n\t)\r\n\tif !checkPerms && !checkAcc {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\tvar (\r\n\t\t_subs    [32]*subscription\r\n\t\tsubs     = _subs[:0]\r\n\t\t_removed [32]*subscription\r\n\t\tremoved  = _removed[:0]\r\n\t\tsrv      = c.srv\r\n\t)\r\n\tif checkAcc {\r\n\t\t// We actually only want to check if stream imports have changed.\r\n\t\tif _, ok := awcsti[acc.Name]; !ok {\r\n\t\t\tcheckAcc = false\r\n\t\t}\r\n\t}\r\n\t// We will clear any mperms we have here. It will rebuild on the fly with canSubscribe,\r\n\t// so we do that here as we collect them. We will check result down below.\r\n\tc.mperms = nil\r\n\t// Collect client's subs under the lock\r\n\tfor _, sub := range c.subs {\r\n\t\t// Just checking to rebuild mperms under the lock, will collect removed though here.\r\n\t\t// Only collect under subs array of canSubscribe and checkAcc true.\r\n\t\tcanSub := c.canSubscribe(string(sub.subject))\r\n\t\tcanQSub := sub.queue != nil && c.canQueueSubscribe(string(sub.subject), string(sub.queue))\r\n\r\n\t\tif !canSub && !canQSub {\r\n\t\t\tremoved = append(removed, sub)\r\n\t\t} else if checkAcc {\r\n\t\t\tsubs = append(subs, sub)\r\n\t\t}\r\n\t}\r\n\tc.mu.Unlock()\r\n\r\n\t// This list is all subs who are allowed and we need to check accounts.\r\n\tfor _, sub := range subs {\r\n\t\tc.mu.Lock()\r\n\t\toldShadows := sub.shadow\r\n\t\tsub.shadow = nil\r\n\t\tc.mu.Unlock()\r\n\t\tc.addShadowSubscriptions(acc, sub)\r\n\t\tfor _, nsub := range oldShadows {\r\n\t\t\tnsub.im.acc.sl.Remove(nsub)\r\n\t\t}\r\n\t}\r\n\r\n\t// Unsubscribe all that need to be removed and report back to client and logs.\r\n\tfor _, sub := range removed {\r\n\t\tc.unsubscribe(acc, sub, true, true)\r\n\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q (sid %q)\",\r\n\t\t\tsub.subject, sub.sid))\r\n\t\tsrv.Noticef(\"Removed sub %q (sid %q) for %s - not authorized\",\r\n\t\t\tsub.subject, sub.sid, c.getAuthUser())\r\n\t}\r\n}\r\n\r\n// Allows us to count up all the queue subscribers during close.\r\ntype qsub struct {\r\n\tsub *subscription\r\n\tn   int32\r\n}\r\n\r\nfunc (c *client) closeConnection(reason ClosedState) {\r\n\tc.mu.Lock()\r\n\tif c.nc == nil || c.flags.isSet(closeConnection) {\r\n\t\tc.mu.Unlock()\r\n\t\treturn\r\n\t}\r\n\t// This will set the closeConnection flag and save the connection, etc..\r\n\t// Will return true if no writeLoop was started and TCP connection was\r\n\t// closed in place, in which case we need to do the teardown.\r\n\tteardownNow := c.markConnAsClosed(reason, false)\r\n\tc.mu.Unlock()\r\n\r\n\tif teardownNow {\r\n\t\tc.teardownConn()\r\n\t}\r\n}\r\n\r\n// Clear the state of this connection and remove it from the server.\r\n// If the connection was initiated (such as ROUTE, GATEWAY, etc..) this may trigger\r\n// a reconnect. This function MUST be called only once per connection. It normally\r\n// happens when the writeLoop returns, or in closeConnection() if no writeLoop has\r\n// been started.\r\nfunc (c *client) teardownConn() {\r\n\tc.mu.Lock()\r\n\r\n\tc.clearAuthTimer()\r\n\tc.clearPingTimer()\r\n\t// Unblock anyone who is potentially stalled waiting on us.\r\n\tif c.out.stc != nil {\r\n\t\tclose(c.out.stc)\r\n\t\tc.out.stc = nil\r\n\t}\r\n\tc.nc = nil\r\n\r\n\tvar (\r\n\t\tretryImplicit bool\r\n\t\tconnectURLs   []string\r\n\t\tgwName        string\r\n\t\tgwIsOutbound  bool\r\n\t\tgwCfg         *gatewayCfg\r\n\t\tkind          = c.kind\r\n\t\tsrv           = c.srv\r\n\t\tnoReconnect   = c.flags.isSet(noReconnect)\r\n\t\tacc           = c.acc\r\n\t)\r\n\r\n\t// Snapshot for use if we are a client connection.\r\n\t// FIXME(dlc) - we can just stub in a new one for client\r\n\t// and reference existing one.\r\n\tvar subs []*subscription\r\n\tif kind == CLIENT || kind == LEAF {\r\n\t\tvar _subs [32]*subscription\r\n\t\tsubs = _subs[:0]\r\n\t\tfor _, sub := range c.subs {\r\n\t\t\t// Auto-unsubscribe subscriptions must be unsubscribed forcibly.\r\n\t\t\tsub.max = 0\r\n\t\t\tsub.close()\r\n\t\t\tsubs = append(subs, sub)\r\n\t\t}\r\n\t}\r\n\r\n\tif c.route != nil {\r\n\t\tif !noReconnect {\r\n\t\t\tretryImplicit = c.route.retry\r\n\t\t}\r\n\t\tconnectURLs = c.route.connectURLs\r\n\t}\r\n\tif kind == GATEWAY {\r\n\t\tgwName = c.gw.name\r\n\t\tgwIsOutbound = c.gw.outbound\r\n\t\tgwCfg = c.gw.cfg\r\n\t}\r\n\r\n\tc.mu.Unlock()\r\n\r\n\t// Remove client's or leaf node subscriptions.\r\n\tif (kind == CLIENT || kind == LEAF) && acc != nil {\r\n\t\tacc.sl.RemoveBatch(subs)\r\n\t} else if kind == ROUTER {\r\n\t\tgo c.removeRemoteSubs()\r\n\t}\r\n\r\n\tif srv != nil {\r\n\t\t// This is a route that disconnected, but we are not in lame duck mode...\r\n\t\tif len(connectURLs) > 0 && !srv.isLameDuckMode() {\r\n\t\t\t// Unless disabled, possibly update the server's INFO protocol\r\n\t\t\t// and send to clients that know how to handle async INFOs.\r\n\t\t\tif !srv.getOpts().Cluster.NoAdvertise {\r\n\t\t\t\tsrv.removeClientConnectURLsAndSendINFOToClients(connectURLs)\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\t// Unregister\r\n\t\tsrv.removeClient(c)\r\n\r\n\t\t// Update remote subscriptions.\r\n\t\tif acc != nil && (kind == CLIENT || kind == LEAF) {\r\n\t\t\tqsubs := map[string]*qsub{}\r\n\t\t\tfor _, sub := range subs {\r\n\t\t\t\t// Call unsubscribe here to cleanup shadow subscriptions and such.\r\n\t\t\t\tc.unsubscribe(acc, sub, true, false)\r\n\t\t\t\t// Update route as normal for a normal subscriber.\r\n\t\t\t\tif sub.queue == nil {\r\n\t\t\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\r\n\t\t\t\t\tsrv.updateLeafNodes(acc, sub, -1)\r\n\t\t\t\t} else {\r\n\t\t\t\t\t// We handle queue subscribers special in case we\r\n\t\t\t\t\t// have a bunch we can just send one update to the\r\n\t\t\t\t\t// connected routes.\r\n\t\t\t\t\tkey := string(sub.subject) + \" \" + string(sub.queue)\r\n\t\t\t\t\tif esub, ok := qsubs[key]; ok {\r\n\t\t\t\t\t\tesub.n++\r\n\t\t\t\t\t} else {\r\n\t\t\t\t\t\tqsubs[key] = &qsub{sub, 1}\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tif srv.gateway.enabled {\r\n\t\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -1)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\t// Process any qsubs here.\r\n\t\t\tfor _, esub := range qsubs {\r\n\t\t\t\tsrv.updateRouteSubscriptionMap(acc, esub.sub, -(esub.n))\r\n\t\t\t\tsrv.updateLeafNodes(acc, esub.sub, -(esub.n))\r\n\t\t\t}\r\n\t\t\tif prev := acc.removeClient(c); prev == 1 && srv != nil {\r\n\t\t\t\tsrv.decActiveAccounts()\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Don't reconnect connections that have been marked with\r\n\t// the no reconnect flag.\r\n\tif noReconnect {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Check for a solicited route. If it was, start up a reconnect unless\r\n\t// we are already connected to the other end.\r\n\tif c.isSolicitedRoute() || retryImplicit {\r\n\t\t// Capture these under lock\r\n\t\tc.mu.Lock()\r\n\t\trid := c.route.remoteID\r\n\t\trtype := c.route.routeType\r\n\t\trurl := c.route.url\r\n\t\tc.mu.Unlock()\r\n\r\n\t\tsrv.mu.Lock()\r\n\t\tdefer srv.mu.Unlock()\r\n\r\n\t\t// It is possible that the server is being shutdown.\r\n\t\t// If so, don't try to reconnect\r\n\t\tif !srv.running {\r\n\t\t\treturn\r\n\t\t}\r\n\r\n\t\tif rid != \"\" && srv.remotes[rid] != nil {\r\n\t\t\tsrv.Debugf(\"Not attempting reconnect for solicited route, already connected to \\\"%s\\\"\", rid)\r\n\t\t\treturn\r\n\t\t} else if rid == srv.info.ID {\r\n\t\t\tsrv.Debugf(\"Detected route to self, ignoring \\\"%s\\\"\", rurl)\r\n\t\t\treturn\r\n\t\t} else if rtype != Implicit || retryImplicit {\r\n\t\t\tsrv.Debugf(\"Attempting reconnect for solicited route \\\"%s\\\"\", rurl)\r\n\t\t\t// Keep track of this go-routine so we can wait for it on\r\n\t\t\t// server shutdown.\r\n\t\t\tsrv.startGoRoutine(func() { srv.reConnectToRoute(rurl, rtype) })\r\n\t\t}\r\n\t} else if srv != nil && kind == GATEWAY && gwIsOutbound {\r\n\t\tif gwCfg != nil {\r\n\t\t\tsrv.Debugf(\"Attempting reconnect for gateway %q\", gwName)\r\n\t\t\t// Run this as a go routine since we may be called within\r\n\t\t\t// the solicitGateway itself if there was an error during\r\n\t\t\t// the creation of the gateway connection.\r\n\t\t\tsrv.startGoRoutine(func() { srv.reconnectGateway(gwCfg) })\r\n\t\t} else {\r\n\t\t\tsrv.Debugf(\"Gateway %q not in configuration, not attempting reconnect\", gwName)\r\n\t\t}\r\n\t} else if c.isSolicitedLeafNode() {\r\n\t\t// Check if this is a solicited leaf node. Start up a reconnect.\r\n\t\tsrv.startGoRoutine(func() { srv.reConnectToRemoteLeafNode(c.leaf.remote) })\r\n\t}\r\n}\r\n\r\n// Set the noReconnect flag. This is used before a call to closeConnection()\r\n// to prevent the connection to reconnect (routes, gateways).\r\nfunc (c *client) setNoReconnect() {\r\n\tc.mu.Lock()\r\n\tc.flags.set(noReconnect)\r\n\tc.mu.Unlock()\r\n}\r\n\r\n// Returns the client's RTT value with the protection of the client's lock.\r\nfunc (c *client) getRTTValue() time.Duration {\r\n\tc.mu.Lock()\r\n\trtt := c.rtt\r\n\tc.mu.Unlock()\r\n\treturn rtt\r\n}\r\n\r\n// This function is used by ROUTER and GATEWAY connections to\r\n// look for a subject on a given account (since these type of\r\n// connections are not bound to a specific account).\r\n// If the c.pa.subject is found in the cache, the cached result\r\n// is returned, otherwse, we match the account's sublist and update\r\n// the cache. The cache is pruned if reaching a certain size.\r\nfunc (c *client) getAccAndResultFromCache() (*Account, *SublistResult) {\r\n\tvar (\r\n\t\tacc *Account\r\n\t\tpac *perAccountCache\r\n\t\tr   *SublistResult\r\n\t\tok  bool\r\n\t)\r\n\t// Check our cache.\r\n\tif pac, ok = c.in.pacache[string(c.pa.pacache)]; ok {\r\n\t\t// Check the genid to see if it's still valid.\r\n\t\tif genid := atomic.LoadUint64(&pac.acc.sl.genid); genid != pac.genid {\r\n\t\t\tok = false\r\n\t\t\tdelete(c.in.pacache, string(c.pa.pacache))\r\n\t\t} else {\r\n\t\t\tacc = pac.acc\r\n\t\t\tr = pac.results\r\n\t\t}\r\n\t}\r\n\r\n\tif !ok {\r\n\t\t// Match correct account and sublist.\r\n\t\tif acc, _ = c.srv.LookupAccount(string(c.pa.account)); acc == nil {\r\n\t\t\treturn nil, nil\r\n\t\t}\r\n\r\n\t\t// Match against the account sublist.\r\n\t\tr = acc.sl.Match(string(c.pa.subject))\r\n\r\n\t\t// Store in our cache\r\n\t\tc.in.pacache[string(c.pa.pacache)] = &perAccountCache{acc, r, atomic.LoadUint64(&acc.sl.genid)}\r\n\r\n\t\t// Check if we need to prune.\r\n\t\tif len(c.in.pacache) > maxPerAccountCacheSize {\r\n\t\t\tc.prunePerAccountCache()\r\n\t\t}\r\n\t}\r\n\treturn acc, r\r\n}\r\n\r\n// Account will return the associated account for this client.\r\nfunc (c *client) Account() *Account {\r\n\tif c == nil {\r\n\t\treturn nil\r\n\t}\r\n\tc.mu.Lock()\r\n\tdefer c.mu.Unlock()\r\n\treturn c.acc\r\n}\r\n\r\n// prunePerAccountCache will prune off a random number of cache entries.\r\nfunc (c *client) prunePerAccountCache() {\r\n\tn := 0\r\n\tfor cacheKey := range c.in.pacache {\r\n\t\tdelete(c.in.pacache, cacheKey)\r\n\t\tif n++; n > prunePerAccountCacheSize {\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// pruneClosedSubFromPerAccountCache remove entries that contain subscriptions\r\n// that have been closed.\r\nfunc (c *client) pruneClosedSubFromPerAccountCache() {\r\n\tfor cacheKey, pac := range c.in.pacache {\r\n\t\tfor _, sub := range pac.results.psubs {\r\n\t\t\tif sub.isClosed() {\r\n\t\t\t\tgoto REMOVE\r\n\t\t\t}\r\n\t\t}\r\n\t\tfor _, qsub := range pac.results.qsubs {\r\n\t\t\tfor _, sub := range qsub {\r\n\t\t\t\tif sub.isClosed() {\r\n\t\t\t\t\tgoto REMOVE\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tcontinue\r\n\tREMOVE:\r\n\t\tdelete(c.in.pacache, cacheKey)\r\n\t}\r\n}\r\n\r\n// getAuthUser returns the auth user for the client.\r\nfunc (c *client) getAuthUser() string {\r\n\tswitch {\r\n\tcase c.opts.Nkey != \"\":\r\n\t\treturn fmt.Sprintf(\"Nkey %q\", c.opts.Nkey)\r\n\tcase c.opts.Username != \"\":\r\n\t\treturn fmt.Sprintf(\"User %q\", c.opts.Username)\r\n\tdefault:\r\n\t\treturn `User \"N/A\"`\r\n\t}\r\n}\r\n\r\n// isClosed returns true if either closeConnection or clearConnection\r\n// flag have been set, or if `nc` is nil, which may happen in tests.\r\nfunc (c *client) isClosed() bool {\r\n\treturn c.flags.isSet(closeConnection) || c.nc == nil\r\n}\r\n\r\n// Logging functionality scoped to a client or route.\r\nfunc (c *client) Error(err error) {\r\n\tc.srv.Errors(c, err)\r\n}\r\n\r\nfunc (c *client) Errorf(format string, v ...interface{}) {\r\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\r\n\tc.srv.Errorf(format, v...)\r\n}\r\n\r\nfunc (c *client) Debugf(format string, v ...interface{}) {\r\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\r\n\tc.srv.Debugf(format, v...)\r\n}\r\n\r\nfunc (c *client) Noticef(format string, v ...interface{}) {\r\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\r\n\tc.srv.Noticef(format, v...)\r\n}\r\n\r\nfunc (c *client) Tracef(format string, v ...interface{}) {\r\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\r\n\tc.srv.Tracef(format, v...)\r\n}\r\n\r\nfunc (c *client) Warnf(format string, v ...interface{}) {\r\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\r\n\tc.srv.Warnf(format, v...)\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/client.go b/server/gnatsd/server/client.go
--- a/server/gnatsd/server/client.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/client.go	(date 1665399049953)
@@ -29,7 +29,7 @@
 	"sync/atomic"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 )
 
 // Type of client connection.
Index: server/gnatsd/server/events_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2018-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"net/http\"\r\n\t\"net/http/httptest\"\r\n\t\"os\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nats.go\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nfunc createAccount(s *Server) (*Account, nkeys.KeyPair) {\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\taddAccountToMemResolver(s, pub, jwt)\r\n\tacc, err := s.LookupAccount(pub)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\treturn acc, akp\r\n}\r\n\r\nfunc createUserCreds(t *testing.T, s *Server, akp nkeys.KeyPair) nats.Option {\r\n\tt.Helper()\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(akp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tuserCB := func() (string, error) {\r\n\t\treturn ujwt, nil\r\n\t}\r\n\tsigCB := func(nonce []byte) ([]byte, error) {\r\n\t\tsig, _ := kp.Sign(nonce)\r\n\t\treturn sig, nil\r\n\t}\r\n\treturn nats.UserJWT(userCB, sigCB)\r\n}\r\n\r\nfunc runTrustedServer(t *testing.T) (*Server, *Options) {\r\n\tt.Helper()\r\n\topts := DefaultOptions()\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\topts.TrustedKeys = []string{pub}\r\n\topts.AccountResolver = &MemAccResolver{}\r\n\ts := RunServer(opts)\r\n\treturn s, opts\r\n}\r\n\r\nfunc runTrustedCluster(t *testing.T) (*Server, *Options, *Server, *Options, nkeys.KeyPair) {\r\n\tt.Helper()\r\n\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\r\n\tmr := &MemAccResolver{}\r\n\r\n\t// Now create a system account.\r\n\t// NOTE: This can NOT be shared directly between servers.\r\n\t// Set via server options.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\tmr.Store(apub, jwt)\r\n\r\n\toptsA := DefaultOptions()\r\n\toptsA.Cluster.Host = \"127.0.0.1\"\r\n\toptsA.TrustedKeys = []string{pub}\r\n\toptsA.AccountResolver = mr\r\n\toptsA.SystemAccount = apub\r\n\toptsA.ServerName = \"A\"\r\n\t// Add in dummy gateway\r\n\toptsA.Gateway.Name = \"TEST CLUSTER 22\"\r\n\toptsA.Gateway.Host = \"127.0.0.1\"\r\n\toptsA.Gateway.Port = -1\r\n\toptsA.gatewaysSolicitDelay = 30 * time.Second\r\n\r\n\tsa := RunServer(optsA)\r\n\r\n\toptsB := nextServerOpts(optsA)\r\n\toptsB.ServerName = \"B\"\r\n\toptsB.Routes = RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", optsA.Cluster.Host, optsA.Cluster.Port))\r\n\tsb := RunServer(optsB)\r\n\r\n\tcheckClusterFormed(t, sa, sb)\r\n\r\n\treturn sa, optsA, sb, optsB, akp\r\n}\r\n\r\nfunc runTrustedGateways(t *testing.T) (*Server, *Options, *Server, *Options, nkeys.KeyPair) {\r\n\tt.Helper()\r\n\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\r\n\tmr := &MemAccResolver{}\r\n\r\n\t// Now create a system account.\r\n\t// NOTE: This can NOT be shared directly between servers.\r\n\t// Set via server options.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\tmr.Store(apub, jwt)\r\n\r\n\toptsA := testDefaultOptionsForGateway(\"A\")\r\n\toptsA.Cluster.Host = \"127.0.0.1\"\r\n\toptsA.TrustedKeys = []string{pub}\r\n\toptsA.AccountResolver = mr\r\n\toptsA.SystemAccount = apub\r\n\r\n\tsa := RunServer(optsA)\r\n\r\n\toptsB := testGatewayOptionsFromToWithServers(t, \"B\", \"A\", sa)\r\n\toptsB.TrustedKeys = []string{pub}\r\n\toptsB.AccountResolver = mr\r\n\toptsB.SystemAccount = apub\r\n\r\n\tsb := RunServer(optsB)\r\n\r\n\twaitForOutboundGateways(t, sa, 1, time.Second)\r\n\twaitForOutboundGateways(t, sb, 1, time.Second)\r\n\r\n\treturn sa, optsA, sb, optsB, akp\r\n}\r\n\r\nfunc TestSystemAccount(t *testing.T) {\r\n\ts, _ := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tacc, _ := createAccount(s)\r\n\ts.setSystemAccount(acc)\r\n\r\n\ts.mu.Lock()\r\n\tdefer s.mu.Unlock()\r\n\r\n\tif s.sys == nil || s.sys.account == nil {\r\n\t\tt.Fatalf(\"Expected sys.account to be non-nil\")\r\n\t}\r\n\tif s.sys.client == nil {\r\n\t\tt.Fatalf(\"Expected sys.client to be non-nil\")\r\n\t}\r\n\tif s.sys.client.echo {\r\n\t\tt.Fatalf(\"Internal clients should always have echo false\")\r\n\t}\r\n}\r\n\r\nfunc TestSystemAccountNewConnection(t *testing.T) {\r\n\ts, opts := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tacc, akp := createAccount(s)\r\n\ts.setSystemAccount(acc)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tncs, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncs.Close()\r\n\r\n\t// We may not be able to hear ourselves (if the event is processed\r\n\t// before we create the sub), so we need to create a second client to\r\n\t// trigger the connect/disconnect events.\r\n\tacc2, akp2 := createAccount(s)\r\n\r\n\t// Be explicit to only receive the event for acc2.\r\n\tsub, _ := ncs.SubscribeSync(fmt.Sprintf(\"$SYS.ACCOUNT.%s.>\", acc2.Name))\r\n\tdefer sub.Unsubscribe()\r\n\tncs.Flush()\r\n\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp2), nats.Name(\"TEST EVENTS\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tmsg, err := sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\r\n\tif !strings.HasPrefix(msg.Subject, fmt.Sprintf(\"$SYS.ACCOUNT.%s.CONNECT\", acc2.Name)) {\r\n\t\tt.Fatalf(\"Expected subject to start with %q, got %q\", \"$SYS.ACCOUNT.<account>.CONNECT\", msg.Subject)\r\n\t}\r\n\ttokens := strings.Split(msg.Subject, \".\")\r\n\tif len(tokens) < 4 {\r\n\t\tt.Fatalf(\"Expected 4 tokens, got %d\", len(tokens))\r\n\t}\r\n\taccount := tokens[2]\r\n\tif account != acc2.Name {\r\n\t\tt.Fatalf(\"Expected %q for account, got %q\", acc2.Name, account)\r\n\t}\r\n\r\n\tcem := ConnectEventMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &cem); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling connect event message: %v\", err)\r\n\t}\r\n\tif cem.Server.ID != s.ID() {\r\n\t\tt.Fatalf(\"Expected server to be %q, got %q\", s.ID(), cem.Server)\r\n\t}\r\n\tif cem.Server.Seq == 0 {\r\n\t\tt.Fatalf(\"Expected sequence to be non-zero\")\r\n\t}\r\n\tif cem.Client.Name != \"TEST EVENTS\" {\r\n\t\tt.Fatalf(\"Expected client name to be %q, got %q\", \"TEST EVENTS\", cem.Client.Name)\r\n\t}\r\n\tif cem.Client.Lang != \"go\" {\r\n\t\tt.Fatalf(\"Expected client lang to be \\\"go\\\", got %q\", cem.Client.Lang)\r\n\t}\r\n\r\n\t// Now close the other client. Should fire a disconnect event.\r\n\t// First send and receive some messages.\r\n\tsub2, _ := nc.SubscribeSync(\"foo\")\r\n\tdefer sub2.Unsubscribe()\r\n\tsub3, _ := nc.SubscribeSync(\"*\")\r\n\tdefer sub3.Unsubscribe()\r\n\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnc.Publish(\"foo\", []byte(\"HELLO WORLD\"))\r\n\t}\r\n\tnc.Flush()\r\n\tnc.Close()\r\n\r\n\tmsg, err = sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\r\n\tif !strings.HasPrefix(msg.Subject, fmt.Sprintf(\"$SYS.ACCOUNT.%s.DISCONNECT\", acc2.Name)) {\r\n\t\tt.Fatalf(\"Expected subject to start with %q, got %q\", \"$SYS.ACCOUNT.<account>.DISCONNECT\", msg.Subject)\r\n\t}\r\n\ttokens = strings.Split(msg.Subject, \".\")\r\n\tif len(tokens) < 4 {\r\n\t\tt.Fatalf(\"Expected 4 tokens, got %d\", len(tokens))\r\n\t}\r\n\taccount = tokens[2]\r\n\tif account != acc2.Name {\r\n\t\tt.Fatalf(\"Expected %q for account, got %q\", acc2.Name, account)\r\n\t}\r\n\r\n\tdem := DisconnectEventMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &dem); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling disconnect event message: %v\", err)\r\n\t}\r\n\r\n\tif dem.Server.ID != s.ID() {\r\n\t\tt.Fatalf(\"Expected server to be %q, got %q\", s.ID(), dem.Server)\r\n\t}\r\n\tif dem.Server.Seq == 0 {\r\n\t\tt.Fatalf(\"Expected sequence to be non-zero\")\r\n\t}\r\n\tif dem.Server.Seq <= cem.Server.Seq {\r\n\t\tt.Fatalf(\"Expected sequence to be increasing\")\r\n\t}\r\n\r\n\tif cem.Client.Name != \"TEST EVENTS\" {\r\n\t\tt.Fatalf(\"Expected client name to be %q, got %q\", \"TEST EVENTS\", dem.Client.Name)\r\n\t}\r\n\tif dem.Client.Lang != \"go\" {\r\n\t\tt.Fatalf(\"Expected client lang to be \\\"go\\\", got %q\", dem.Client.Lang)\r\n\t}\r\n\r\n\tif dem.Sent.Msgs != 10 {\r\n\t\tt.Fatalf(\"Expected 10 msgs sent, got %d\", dem.Sent.Msgs)\r\n\t}\r\n\tif dem.Sent.Bytes != 110 {\r\n\t\tt.Fatalf(\"Expected 110 bytes sent, got %d\", dem.Sent.Bytes)\r\n\t}\r\n\tif dem.Received.Msgs != 20 {\r\n\t\tt.Fatalf(\"Expected 20 msgs received, got %d\", dem.Sent.Msgs)\r\n\t}\r\n\tif dem.Received.Bytes != 220 {\r\n\t\tt.Fatalf(\"Expected 220 bytes sent, got %d\", dem.Sent.Bytes)\r\n\t}\r\n}\r\n\r\nfunc runTrustedLeafServer(t *testing.T) (*Server, *Options) {\r\n\tt.Helper()\r\n\topts := DefaultOptions()\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\tpub, _ := kp.PublicKey()\r\n\topts.TrustedKeys = []string{pub}\r\n\topts.AccountResolver = &MemAccResolver{}\r\n\topts.LeafNode.Port = -1\r\n\ts := RunServer(opts)\r\n\treturn s, opts\r\n}\r\n\r\nfunc genCredsFile(t *testing.T, jwt string, seed []byte) string {\r\n\tcreds := `\r\n\t\t-----BEGIN NATS USER JWT-----\r\n\t\t%s\r\n\t\t------END NATS USER JWT------\r\n\r\n\t\t************************* IMPORTANT *************************\r\n\t\tNKEY Seed printed below can be used to sign and prove identity.\r\n\t\tNKEYs are sensitive and should be treated as secrets.\r\n\r\n\t\t-----BEGIN USER NKEY SEED-----\r\n\t\t%s\r\n\t\t------END USER NKEY SEED------\r\n\r\n\t\t*************************************************************\r\n\t\t`\r\n\treturn createConfFile(t, []byte(strings.Replace(fmt.Sprintf(creds, jwt, seed), \"\\t\\t\", \"\", -1)))\r\n}\r\n\r\nfunc runSolicitWithCredentials(t *testing.T, opts *Options, creds string) (*Server, *Options, string) {\r\n\tcontent := `\r\n\t\tport: -1\r\n\t\tleafnodes {\r\n\t\t\tremotes = [\r\n\t\t\t\t{\r\n\t\t\t\t\turl: nats-leaf://127.0.0.1:%d\r\n\t\t\t\t\tcredentials: '%s'\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t\t`\r\n\tconfig := fmt.Sprintf(content, opts.LeafNode.Port, creds)\r\n\tconf := createConfFile(t, []byte(config))\r\n\ts, opts := RunServerWithConfig(conf)\r\n\treturn s, opts, conf\r\n}\r\n\r\n// Helper function to check that a leaf node has connected to our server.\r\nfunc checkLeafNodeConnected(t *testing.T, s *Server) {\r\n\tcheckLeafNodeConnectedCount(t, s, 1)\r\n}\r\n\r\n// Helper function to check that a leaf node has connected to n server.\r\nfunc checkLeafNodeConnectedCount(t *testing.T, s *Server, lnCons int) {\r\n\tt.Helper()\r\n\tcheckFor(t, 5*time.Second, 15*time.Millisecond, func() error {\r\n\t\tif nln := s.NumLeafNodes(); nln != lnCons {\r\n\t\t\treturn fmt.Errorf(\"Expected %d connected leafnode(s) for server %q, got %d\",\r\n\t\t\t\tlnCons, s.ID(), nln)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestSystemAccountingWithLeafNodes(t *testing.T) {\r\n\ts, opts := runTrustedLeafServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tacc, akp := createAccount(s)\r\n\ts.setSystemAccount(acc)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tncs, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncs.Close()\r\n\r\n\tacc2, akp2 := createAccount(s)\r\n\r\n\t// Be explicit to only receive the event for acc2 account.\r\n\tsub, _ := ncs.SubscribeSync(fmt.Sprintf(\"$SYS.ACCOUNT.%s.DISCONNECT\", acc2.Name))\r\n\tdefer sub.Unsubscribe()\r\n\tncs.Flush()\r\n\r\n\tkp, _ := nkeys.CreateUser()\r\n\tpub, _ := kp.PublicKey()\r\n\tnuc := jwt.NewUserClaims(pub)\r\n\tujwt, err := nuc.Encode(akp2)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating user JWT: %v\", err)\r\n\t}\r\n\tseed, _ := kp.Seed()\r\n\tmycreds := genCredsFile(t, ujwt, seed)\r\n\tdefer os.Remove(mycreds)\r\n\r\n\t// Create a server that solicits a leafnode connection.\r\n\tsl, slopts, lnconf := runSolicitWithCredentials(t, opts, mycreds)\r\n\tdefer os.Remove(lnconf)\r\n\tdefer sl.Shutdown()\r\n\r\n\tcheckLeafNodeConnected(t, s)\r\n\r\n\t// Compute the expected number of subs on \"sl\" based on number\r\n\t// of existing subs before creating the sub on \"s\".\r\n\texpected := int(sl.NumSubscriptions() + 1)\r\n\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp2), nats.Name(\"TEST LEAFNODE EVENTS\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tfooSub := natsSubSync(t, nc, \"foo\")\r\n\tnatsFlush(t, nc)\r\n\r\n\tcheckExpectedSubs(t, expected, sl)\r\n\r\n\tsurl := fmt.Sprintf(\"nats://%s:%d\", slopts.Host, slopts.Port)\r\n\tnc2, err := nats.Connect(surl, nats.Name(\"TEST LEAFNODE EVENTS\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Compute the expected number of subs on \"s\" based on number\r\n\t// of existing subs before creating the sub on \"sl\".\r\n\texpected = int(s.NumSubscriptions() + 1)\r\n\r\n\tm := []byte(\"HELLO WORLD\")\r\n\r\n\t// Now generate some traffic\r\n\tstarSub := natsSubSync(t, nc2, \"*\")\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnc2.Publish(\"foo\", m)\r\n\t\tnc2.Publish(\"bar\", m)\r\n\t}\r\n\tnatsFlush(t, nc2)\r\n\r\n\tcheckExpectedSubs(t, expected, s)\r\n\r\n\t// Now send some from the cluster side too.\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnc.Publish(\"foo\", m)\r\n\t\tnc.Publish(\"bar\", m)\r\n\t}\r\n\tnc.Flush()\r\n\r\n\t// Make sure all messages are received\r\n\tfor i := 0; i < 20; i++ {\r\n\t\tif _, err := fooSub.NextMsg(time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Did not get message: %v\", err)\r\n\t\t}\r\n\t}\r\n\tfor i := 0; i < 40; i++ {\r\n\t\tif _, err := starSub.NextMsg(time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"Did not get message: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\t// Now shutdown the leafnode server since this is where the event tracking should\r\n\t// happen. Right now we do not track local clients to the leafnode server that\r\n\t// solicited to the cluster, but we should track usage once the leafnode connection stops.\r\n\tsl.Shutdown()\r\n\r\n\t// Make sure we get disconnect event and that tracking is correct.\r\n\tmsg, err := sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\r\n\tdem := DisconnectEventMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &dem); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling disconnect event message: %v\", err)\r\n\t}\r\n\tif dem.Sent.Msgs != 10 {\r\n\t\tt.Fatalf(\"Expected 10 msgs sent, got %d\", dem.Sent.Msgs)\r\n\t}\r\n\tif dem.Sent.Bytes != 110 {\r\n\t\tt.Fatalf(\"Expected 110 bytes sent, got %d\", dem.Sent.Bytes)\r\n\t}\r\n\tif dem.Received.Msgs != 20 {\r\n\t\tt.Fatalf(\"Expected 20 msgs received, got %d\", dem.Received.Msgs)\r\n\t}\r\n\tif dem.Received.Bytes != 220 {\r\n\t\tt.Fatalf(\"Expected 220 bytes sent, got %d\", dem.Received.Bytes)\r\n\t}\r\n}\r\n\r\nfunc TestSystemAccountDisconnectBadLogin(t *testing.T) {\r\n\ts, opts := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tacc, akp := createAccount(s)\r\n\ts.setSystemAccount(acc)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tncs, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncs.Close()\r\n\r\n\t// We should never hear $G account events for bad logins.\r\n\tsub, _ := ncs.SubscribeSync(\"$SYS.ACCOUNT.$G.*\")\r\n\tdefer sub.Unsubscribe()\r\n\r\n\t// Listen for auth error events though.\r\n\tasub, _ := ncs.SubscribeSync(\"$SYS.SERVER.*.CLIENT.AUTH.ERR\")\r\n\tdefer asub.Unsubscribe()\r\n\r\n\tncs.Flush()\r\n\r\n\tnats.Connect(url, nats.Name(\"TEST BAD LOGIN\"))\r\n\r\n\t// Should not hear these.\r\n\tif _, err := sub.NextMsg(100 * time.Millisecond); err == nil {\r\n\t\tt.Fatalf(\"Received a disconnect message from bad login, expected none\")\r\n\t}\r\n\r\n\tm, err := asub.NextMsg(100 * time.Millisecond)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Should have heard an auth error event\")\r\n\t}\r\n\tdem := DisconnectEventMsg{}\r\n\tif err := json.Unmarshal(m.Data, &dem); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling disconnect event message: %v\", err)\r\n\t}\r\n\tif dem.Reason != \"Authentication Failure\" {\r\n\t\tt.Fatalf(\"Expected auth error, got %q\", dem.Reason)\r\n\t}\r\n}\r\n\r\nfunc TestSystemAccountInternalSubscriptions(t *testing.T) {\r\n\ts, opts := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tsub, err := s.sysSubscribe(\"foo\", nil)\r\n\tif sub != nil || err != ErrNoSysAccount {\r\n\t\tt.Fatalf(\"Expected to get proper error, got %v\", err)\r\n\t}\r\n\r\n\tacc, akp := createAccount(s)\r\n\ts.setSystemAccount(acc)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tsub, err = s.sysSubscribe(\"foo\", nil)\r\n\tif sub != nil || err == nil {\r\n\t\tt.Fatalf(\"Expected to get error for no handler, got %v\", err)\r\n\t}\r\n\r\n\treceived := make(chan *nats.Msg)\r\n\t// Create message callback handler.\r\n\tcb := func(sub *subscription, _ *client, subject, reply string, msg []byte) {\r\n\t\tcopy := append([]byte(nil), msg...)\r\n\t\treceived <- &nats.Msg{Subject: subject, Reply: reply, Data: copy}\r\n\t}\r\n\r\n\t// Now create an internal subscription\r\n\tsub, err = s.sysSubscribe(\"foo\", cb)\r\n\tif sub == nil || err != nil {\r\n\t\tt.Fatalf(\"Expected to subscribe, got %v\", err)\r\n\t}\r\n\t// Now send out a message from our normal client.\r\n\tnc.Publish(\"foo\", []byte(\"HELLO WORLD\"))\r\n\r\n\tvar msg *nats.Msg\r\n\r\n\tselect {\r\n\tcase msg = <-received:\r\n\t\tif msg.Subject != \"foo\" {\r\n\t\t\tt.Fatalf(\"Expected \\\"foo\\\" as subject, got %q\", msg.Subject)\r\n\t\t}\r\n\t\tif msg.Reply != \"\" {\r\n\t\t\tt.Fatalf(\"Expected no reply, got %q\", msg.Reply)\r\n\t\t}\r\n\t\tif !bytes.Equal(msg.Data, []byte(\"HELLO WORLD\")) {\r\n\t\t\tt.Fatalf(\"Got the wrong msg payload: %q\", msg.Data)\r\n\t\t}\r\n\t\tbreak\r\n\tcase <-time.After(time.Second):\r\n\t\tt.Fatalf(\"Did not receive the message\")\r\n\t}\r\n\ts.sysUnsubscribe(sub)\r\n\r\n\t// Now send out a message from our normal client.\r\n\t// We should not see this one.\r\n\tnc.Publish(\"foo\", []byte(\"You There?\"))\r\n\r\n\tselect {\r\n\tcase <-received:\r\n\t\tt.Fatalf(\"Received a message when we should not have\")\r\n\tcase <-time.After(100 * time.Millisecond):\r\n\t\tbreak\r\n\t}\r\n\r\n\t// Now make sure we do not hear ourselves. We optimize this for internally\r\n\t// generated messages.\r\n\ts.mu.Lock()\r\n\ts.sendInternalMsg(\"foo\", \"\", nil, msg.Data)\r\n\ts.mu.Unlock()\r\n\r\n\tselect {\r\n\tcase <-received:\r\n\t\tt.Fatalf(\"Received a message when we should not have\")\r\n\tcase <-time.After(100 * time.Millisecond):\r\n\t\tbreak\r\n\t}\r\n}\r\n\r\nfunc TestSystemAccountConnectionUpdatesStopAfterNoLocal(t *testing.T) {\r\n\tsa, _, sb, optsB, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\t// Normal Account\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 4 // Limit to 4 connections.\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, jwt)\r\n\r\n\t// Listen for updates to the new account connection activity.\r\n\treceived := make(chan *nats.Msg, 10)\r\n\tcb := func(sub *subscription, _ *client, subject, reply string, msg []byte) {\r\n\t\tcopy := append([]byte(nil), msg...)\r\n\t\treceived <- &nats.Msg{Subject: subject, Reply: reply, Data: copy}\r\n\t}\r\n\tsubj := fmt.Sprintf(accConnsEventSubj, pub)\r\n\tsub, err := sa.sysSubscribe(subj, cb)\r\n\tif sub == nil || err != nil {\r\n\t\tt.Fatalf(\"Expected to subscribe, got %v\", err)\r\n\t}\r\n\tdefer sa.sysUnsubscribe(sub)\r\n\r\n\t// Create a few users on the new account.\r\n\tclients := []*nats.Conn{}\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tnc, err := nats.Connect(url, createUserCreds(t, sb, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tclients = append(clients, nc)\r\n\t}\r\n\r\n\t// Wait for all 4 notifications.\r\n\tcheckFor(t, time.Second, 50*time.Millisecond, func() error {\r\n\t\tif len(received) == 4 {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\treturn fmt.Errorf(\"Not enough messages, %d vs 4\", len(received))\r\n\t})\r\n\r\n\t// Now lookup the account doing the events on sb.\r\n\tacc, _ := sb.LookupAccount(pub)\r\n\t// Make sure we have the timer running.\r\n\tacc.mu.RLock()\r\n\tctmr := acc.ctmr\r\n\tacc.mu.RUnlock()\r\n\tif ctmr == nil {\r\n\t\tt.Fatalf(\"Expected event timer for acc conns to be running\")\r\n\t}\r\n\r\n\t// Now close all of the connections.\r\n\tfor _, nc := range clients {\r\n\t\tnc.Close()\r\n\t}\r\n\r\n\t// Wait for the 4 new notifications, 8 total (4 for connect, 4 for disconnect)\r\n\tcheckFor(t, time.Second, 50*time.Millisecond, func() error {\r\n\t\tif len(received) == 8 {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\treturn fmt.Errorf(\"Not enough messages, %d vs 4\", len(received))\r\n\t})\r\n\t// Drain the messages.\r\n\tfor i := 0; i < 7; i++ {\r\n\t\t<-received\r\n\t}\r\n\t// Check last one.\r\n\tmsg := <-received\r\n\tm := AccountNumConns{}\r\n\tif err := json.Unmarshal(msg.Data, &m); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling account connections request message: %v\", err)\r\n\t}\r\n\tif m.Conns != 0 {\r\n\t\tt.Fatalf(\"Expected Conns to be 0, got %d\", m.Conns)\r\n\t}\r\n\r\n\t// Should not receive any more messages..\r\n\tselect {\r\n\tcase <-received:\r\n\t\tt.Fatalf(\"Did not expect a message here\")\r\n\tcase <-time.After(50 * time.Millisecond):\r\n\t\tbreak\r\n\t}\r\n\r\n\t// Make sure we have the timer is NOT running.\r\n\tacc.mu.RLock()\r\n\tctmr = acc.ctmr\r\n\tacc.mu.RUnlock()\r\n\tif ctmr != nil {\r\n\t\tt.Fatalf(\"Expected event timer for acc conns to NOT be running after reaching zero local clients\")\r\n\t}\r\n}\r\n\r\nfunc TestSystemAccountConnectionLimits(t *testing.T) {\r\n\tsa, optsA, sb, optsB, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\t// We want to test that we are limited to a certain number of active connections\r\n\t// across multiple servers.\r\n\r\n\t// Let's create a user account.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 4 // Limit to 4 connections.\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, jwt)\r\n\r\n\turlA := fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port)\r\n\turlB := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\r\n\t// Create a user on each server. Break on first failure.\r\n\tfor {\r\n\t\tnca1, err := nats.Connect(urlA, createUserCreds(t, sa, akp))\r\n\t\tif err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tdefer nca1.Close()\r\n\t\tncb1, err := nats.Connect(urlB, createUserCreds(t, sb, akp))\r\n\t\tif err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tdefer ncb1.Close()\r\n\t}\r\n\r\n\tcheckFor(t, 1*time.Second, 50*time.Millisecond, func() error {\r\n\t\ttotal := sa.NumClients() + sb.NumClients()\r\n\t\tif total > int(nac.Limits.Conn) {\r\n\t\t\treturn fmt.Errorf(\"Expected only %d connections, was allowed to connect %d\", nac.Limits.Conn, total)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\n// Make sure connection limits apply to the system account itself.\r\nfunc TestSystemAccountSystemConnectionLimitsHonored(t *testing.T) {\r\n\tsa, optsA, sb, optsB, sakp := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\t// Update system account to have 10 connections\r\n\tpub, _ := sakp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 10\r\n\tajwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, ajwt)\r\n\taddAccountToMemResolver(sb, pub, ajwt)\r\n\r\n\t// Update the accounts on each server with new claims to force update.\r\n\tsysAccA := sa.SystemAccount()\r\n\tsa.updateAccountWithClaimJWT(sysAccA, ajwt)\r\n\tsysAccB := sb.SystemAccount()\r\n\tsb.updateAccountWithClaimJWT(sysAccB, ajwt)\r\n\r\n\t// Check system here first, with no external it should be zero.\r\n\tsacc := sa.SystemAccount()\r\n\tif nlc := sacc.NumLocalConnections(); nlc != 0 {\r\n\t\tt.Fatalf(\"Expected no local connections, got %d\", nlc)\r\n\t}\r\n\r\n\turlA := fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port)\r\n\turlB := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\r\n\t// Create a user on each server. Break on first failure.\r\n\ttc := 0\r\n\tfor {\r\n\t\tnca1, err := nats.Connect(urlA, createUserCreds(t, sa, sakp))\r\n\t\tif err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tdefer nca1.Close()\r\n\t\ttc++\r\n\r\n\t\tncb1, err := nats.Connect(urlB, createUserCreds(t, sb, sakp))\r\n\t\tif err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tdefer ncb1.Close()\r\n\t\ttc++\r\n\r\n\t\t// The account's connection count is exchanged between servers\r\n\t\t// so that the local count on each server reflects the total count.\r\n\t\t// Pause a bit to give a chance to each server to process the update.\r\n\t\ttime.Sleep(15 * time.Millisecond)\r\n\t}\r\n\tif tc != 10 {\r\n\t\tt.Fatalf(\"Expected to get 10 external connections, got %d\", tc)\r\n\t}\r\n\r\n\tcheckFor(t, 1*time.Second, 50*time.Millisecond, func() error {\r\n\t\ttotal := sa.NumClients() + sb.NumClients()\r\n\t\tif total > int(nac.Limits.Conn) {\r\n\t\t\treturn fmt.Errorf(\"Expected only %d connections, was allowed to connect %d\", nac.Limits.Conn, total)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\n// Test that the remote accounting works when a server is started some time later.\r\nfunc TestSystemAccountConnectionLimitsServersStaggered(t *testing.T) {\r\n\tsa, optsA, sb, optsB, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tsb.Shutdown()\r\n\r\n\t// Let's create a user account.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 4 // Limit to 4 connections.\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, jwt)\r\n\r\n\turlA := fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port)\r\n\t// Create max connections on sa.\r\n\tfor i := 0; i < int(nac.Limits.Conn); i++ {\r\n\t\tnc, err := nats.Connect(urlA, createUserCreds(t, sa, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Unexpected error on #%d try: %v\", i+1, err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\t// Restart server B.\r\n\toptsB.AccountResolver = sa.AccountResolver()\r\n\toptsB.SystemAccount = sa.SystemAccount().Name\r\n\tsb = RunServer(optsB)\r\n\tdefer sb.Shutdown()\r\n\tcheckClusterFormed(t, sa, sb)\r\n\r\n\t// Trigger a load of the user account on the new server\r\n\t// NOTE: If we do not load the user, the user can be the first\r\n\t// to request this account, hence the connection will succeed.\r\n\tcheckFor(t, time.Second, 15*time.Millisecond, func() error {\r\n\t\tif acc, err := sb.LookupAccount(pub); acc == nil || err != nil {\r\n\t\t\treturn fmt.Errorf(\"LookupAccount did not return account or failed, err=%v\", err)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Expect this to fail.\r\n\turlB := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\tif _, err := nats.Connect(urlB, createUserCreds(t, sb, akp)); err == nil {\r\n\t\tt.Fatalf(\"Expected connection to fail due to max limit\")\r\n\t}\r\n}\r\n\r\n// Test that the remote accounting works when a server is shutdown.\r\nfunc TestSystemAccountConnectionLimitsServerShutdownGraceful(t *testing.T) {\r\n\tsa, optsA, sb, optsB, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\t// Let's create a user account.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 10 // Limit to 10 connections.\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, jwt)\r\n\taddAccountToMemResolver(sb, pub, jwt)\r\n\r\n\turlA := fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port)\r\n\turlB := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tnc, err := nats.Connect(urlA, nats.NoReconnect(), createUserCreds(t, sa, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t\tnc, err = nats.Connect(urlB, nats.NoReconnect(), createUserCreds(t, sb, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\t// We are at capacity so both of these should fail.\r\n\tif _, err := nats.Connect(urlA, createUserCreds(t, sa, akp)); err == nil {\r\n\t\tt.Fatalf(\"Expected connection to fail due to max limit\")\r\n\t}\r\n\tif _, err := nats.Connect(urlB, createUserCreds(t, sb, akp)); err == nil {\r\n\t\tt.Fatalf(\"Expected connection to fail due to max limit\")\r\n\t}\r\n\r\n\t// Now shutdown Server B.\r\n\tsb.Shutdown()\r\n\r\n\t// Now we should be able to create more on A now.\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tnc, err := nats.Connect(urlA, createUserCreds(t, sa, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to connect on %d, got %v\", i, err)\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t}\r\n}\r\n\r\n// Test that the remote accounting works when a server goes away.\r\nfunc TestSystemAccountConnectionLimitsServerShutdownForced(t *testing.T) {\r\n\tsa, optsA, sb, optsB, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\r\n\t// Let's create a user account.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 20 // Limit to 20 connections.\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, jwt)\r\n\taddAccountToMemResolver(sb, pub, jwt)\r\n\r\n\turlA := fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port)\r\n\turlB := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tc, err := nats.Connect(urlA, nats.NoReconnect(), createUserCreds(t, sa, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t\t}\r\n\t\tdefer c.Close()\r\n\t\tc, err = nats.Connect(urlB, nats.NoReconnect(), createUserCreds(t, sb, akp))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Expected to connect, got %v\", err)\r\n\t\t}\r\n\t\tdefer c.Close()\r\n\t}\r\n\r\n\t// Now shutdown Server B. Do so such that no communications go out.\r\n\tsb.mu.Lock()\r\n\tsb.sys = nil\r\n\tsb.mu.Unlock()\r\n\tsb.Shutdown()\r\n\r\n\tif _, err := nats.Connect(urlA, createUserCreds(t, sa, akp)); err == nil {\r\n\t\tt.Fatalf(\"Expected connection to fail due to max limit\")\r\n\t}\r\n\r\n\t// Let's speed up the checking process.\r\n\tsa.mu.Lock()\r\n\tsa.sys.chkOrph = 10 * time.Millisecond\r\n\tsa.sys.orphMax = 30 * time.Millisecond\r\n\tsa.sys.sweeper.Reset(sa.sys.chkOrph)\r\n\tsa.mu.Unlock()\r\n\r\n\t// We should eventually be able to connect.\r\n\tcheckFor(t, 2*time.Second, 50*time.Millisecond, func() error {\r\n\t\tif c, err := nats.Connect(urlA, createUserCreds(t, sa, akp)); err != nil {\r\n\t\t\treturn err\r\n\t\t} else {\r\n\t\t\tc.Close()\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestSystemAccountFromConfig(t *testing.T) {\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\topub, _ := kp.PublicKey()\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(kp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\tts := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tw.Write([]byte(ajwt))\r\n\t}))\r\n\tdefer ts.Close()\r\n\r\n\tconfTemplate := `\r\n\t\tlisten: -1\r\n\t\ttrusted: %s\r\n\t\tsystem_account: %s\r\n\t\tresolver: URL(\"%s/jwt/v1/accounts/\")\r\n    `\r\n\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(confTemplate, opub, apub, ts.URL)))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tif acc := s.SystemAccount(); acc == nil || acc.Name != apub {\r\n\t\tt.Fatalf(\"System Account not properly set\")\r\n\t}\r\n}\r\n\r\nfunc TestAccountClaimsUpdates(t *testing.T) {\r\n\ts, opts := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tsacc, sakp := createAccount(s)\r\n\ts.setSystemAccount(sacc)\r\n\r\n\t// Let's create a normal  account with limits we can update.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 4\r\n\tajwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(s, pub, ajwt)\r\n\r\n\tacc, _ := s.LookupAccount(pub)\r\n\tif acc.MaxActiveConnections() != 4 {\r\n\t\tt.Fatalf(\"Expected to see a limit of 4 connections\")\r\n\t}\r\n\r\n\t// Simulate a systems publisher so we can do an account claims update.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, s, sakp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Update the account\r\n\tnac = jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 8\r\n\tissAt := time.Now().Add(-30 * time.Second).Unix()\r\n\tnac.IssuedAt = issAt\r\n\texpires := time.Now().Add(2 * time.Second).Unix()\r\n\tnac.Expires = expires\r\n\tajwt, _ = nac.Encode(okp)\r\n\r\n\t// Publish to the system update subject.\r\n\tclaimUpdateSubj := fmt.Sprintf(accUpdateEventSubj, pub)\r\n\tnc.Publish(claimUpdateSubj, []byte(ajwt))\r\n\tnc.Flush()\r\n\r\n\tacc, _ = s.LookupAccount(pub)\r\n\tif acc.MaxActiveConnections() != 8 {\r\n\t\tt.Fatalf(\"Account was not updated\")\r\n\t}\r\n}\r\n\r\nfunc TestAccountConnsLimitExceededAfterUpdate(t *testing.T) {\r\n\ts, opts := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tsacc, _ := createAccount(s)\r\n\ts.setSystemAccount(sacc)\r\n\r\n\t// Let's create a normal  account with limits we can update.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 10\r\n\tajwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(s, pub, ajwt)\r\n\tacc, _ := s.LookupAccount(pub)\r\n\r\n\t// Now create the max connections.\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tfor {\r\n\t\tnc, err := nats.Connect(url, createUserCreds(t, s, akp))\r\n\t\tif err != nil {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tdefer nc.Close()\r\n\t}\r\n\r\n\t// We should have max here.\r\n\tcheckFor(t, 2*time.Second, 50*time.Millisecond, func() error {\r\n\t\tif total := s.NumClients(); total != acc.MaxActiveConnections() {\r\n\t\t\treturn fmt.Errorf(\"Expected %d connections, got %d\", acc.MaxActiveConnections(), total)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// Now change limits to make current connections over the limit.\r\n\tnac = jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 2\r\n\tajwt, _ = nac.Encode(okp)\r\n\r\n\ts.updateAccountWithClaimJWT(acc, ajwt)\r\n\tif acc.MaxActiveConnections() != 2 {\r\n\t\tt.Fatalf(\"Expected max connections to be set to 2, got %d\", acc.MaxActiveConnections())\r\n\t}\r\n\t// We should have closed the excess connections.\r\n\tcheckClientsCount(t, s, acc.MaxActiveConnections())\r\n}\r\n\r\nfunc TestAccountConnsLimitExceededAfterUpdateDisconnectNewOnly(t *testing.T) {\r\n\ts, opts := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tsacc, _ := createAccount(s)\r\n\ts.setSystemAccount(sacc)\r\n\r\n\t// Let's create a normal  account with limits we can update.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 10\r\n\tajwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(s, pub, ajwt)\r\n\tacc, _ := s.LookupAccount(pub)\r\n\r\n\t// Now create the max connections.\r\n\t// We create half then we will wait and then create the rest.\r\n\t// Will test that we disconnect the newest ones.\r\n\tnewConns := make([]*nats.Conn, 0, 5)\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tnats.Connect(url, nats.NoReconnect(), createUserCreds(t, s, akp))\r\n\t}\r\n\ttime.Sleep(500 * time.Millisecond)\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tnc, _ := nats.Connect(url, nats.NoReconnect(), createUserCreds(t, s, akp))\r\n\t\tnewConns = append(newConns, nc)\r\n\t}\r\n\r\n\t// We should have max here.\r\n\tcheckClientsCount(t, s, acc.MaxActiveConnections())\r\n\r\n\t// Now change limits to make current connections over the limit.\r\n\tnac = jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 5\r\n\tajwt, _ = nac.Encode(okp)\r\n\r\n\ts.updateAccountWithClaimJWT(acc, ajwt)\r\n\tif acc.MaxActiveConnections() != 5 {\r\n\t\tt.Fatalf(\"Expected max connections to be set to 2, got %d\", acc.MaxActiveConnections())\r\n\t}\r\n\t// We should have closed the excess connections.\r\n\tcheckClientsCount(t, s, acc.MaxActiveConnections())\r\n\r\n\t// Now make sure that only the new ones were closed.\r\n\tvar closed int\r\n\tfor _, nc := range newConns {\r\n\t\tif !nc.IsClosed() {\r\n\t\t\tclosed++\r\n\t\t}\r\n\t}\r\n\tif closed != 5 {\r\n\t\tt.Fatalf(\"Expected all new clients to be closed, only got %d of 5\", closed)\r\n\t}\r\n}\r\n\r\nfunc TestSystemAccountWithBadRemoteLatencyUpdate(t *testing.T) {\r\n\ts, _ := runTrustedServer(t)\r\n\tdefer s.Shutdown()\r\n\r\n\tacc, _ := createAccount(s)\r\n\ts.setSystemAccount(acc)\r\n\r\n\trl := remoteLatency{\r\n\t\tAccount: \"NONSENSE\",\r\n\t\tReqId:   \"_INBOX.22\",\r\n\t}\r\n\tb, _ := json.Marshal(&rl)\r\n\ts.remoteLatencyUpdate(nil, nil, \"foo\", \"\", b)\r\n}\r\n\r\nfunc TestSystemAccountWithGateways(t *testing.T) {\r\n\tsa, oa, sb, ob, akp := runTrustedGateways(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\t// Create a client on A that will subscribe on $SYS.ACCOUNT.>\r\n\turla := fmt.Sprintf(\"nats://%s:%d\", oa.Host, oa.Port)\r\n\tnca := natsConnect(t, urla, createUserCreds(t, sa, akp))\r\n\tdefer nca.Close()\r\n\r\n\tsub, _ := nca.SubscribeSync(\"$SYS.ACCOUNT.>\")\r\n\tdefer sub.Unsubscribe()\r\n\tnca.Flush()\r\n\r\n\t// If this tests fails with wrong number after 10 seconds we may have\r\n\t// added a new inititial subscription for the eventing system.\r\n\tcheckExpectedSubs(t, 25, sa)\r\n\r\n\t// Create a client on B and see if we receive the event\r\n\turlb := fmt.Sprintf(\"nats://%s:%d\", ob.Host, ob.Port)\r\n\tncb := natsConnect(t, urlb, createUserCreds(t, sb, akp), nats.Name(\"TEST EVENTS\"))\r\n\tdefer ncb.Close()\r\n\r\n\tmsg, err := sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\t// Basic checks, could expand on that...\r\n\taccName := sa.SystemAccount().Name\r\n\tif !strings.HasPrefix(msg.Subject, fmt.Sprintf(\"$SYS.ACCOUNT.%s.CONNECT\", accName)) {\r\n\t\tt.Fatalf(\"Expected subject to start with %q, got %q\", \"$SYS.ACCOUNT.<account>.CONNECT\", msg.Subject)\r\n\t}\r\n\ttokens := strings.Split(msg.Subject, \".\")\r\n\tif len(tokens) < 4 {\r\n\t\tt.Fatalf(\"Expected 4 tokens, got %d\", len(tokens))\r\n\t}\r\n\taccount := tokens[2]\r\n\tif account != accName {\r\n\t\tt.Fatalf(\"Expected %q for account, got %q\", accName, account)\r\n\t}\r\n}\r\nfunc TestServerEventsStatsZ(t *testing.T) {\r\n\tpreStart := time.Now()\r\n\t// Add little bit of delay to make sure that time check\r\n\t// between pre-start and actual start does not fail.\r\n\ttime.Sleep(5 * time.Millisecond)\r\n\tsa, optsA, sb, _, akp := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\t// Same between actual start and post start.\r\n\ttime.Sleep(5 * time.Millisecond)\r\n\tpostStart := time.Now()\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port)\r\n\tncs, err := nats.Connect(url, createUserCreds(t, sa, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncs.Close()\r\n\r\n\tsubj := fmt.Sprintf(serverStatsSubj, sa.ID())\r\n\tsub, _ := ncs.SubscribeSync(subj)\r\n\tdefer sub.Unsubscribe()\r\n\tncs.Publish(\"foo\", []byte(\"HELLO WORLD\"))\r\n\tncs.Flush()\r\n\r\n\t// Let's speed up the checking process.\r\n\tsa.mu.Lock()\r\n\tsa.sys.statsz = 10 * time.Millisecond\r\n\tsa.sys.stmr.Reset(sa.sys.statsz)\r\n\tsa.mu.Unlock()\r\n\r\n\t_, err = sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\t// Get it the second time so we can check some stats\r\n\tmsg, err := sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tm := ServerStatsMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &m); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling the statz json: %v\", err)\r\n\t}\r\n\tif m.Server.ID != sa.ID() {\r\n\t\tt.Fatalf(\"Did not match IDs\")\r\n\t}\r\n\tif m.Server.Cluster != \"TEST CLUSTER 22\" {\r\n\t\tt.Fatalf(\"Did not match cluster name\")\r\n\t}\r\n\tif m.Server.Version != VERSION {\r\n\t\tt.Fatalf(\"Did not match server version\")\r\n\t}\r\n\tif !m.Stats.Start.After(preStart) && m.Stats.Start.Before(postStart) {\r\n\t\tt.Fatalf(\"Got a wrong start time for the server %v\", m.Stats.Start)\r\n\t}\r\n\tif m.Stats.Connections != 1 {\r\n\t\tt.Fatalf(\"Did not match connections of 1, got %d\", m.Stats.Connections)\r\n\t}\r\n\tif m.Stats.ActiveAccounts != 2 {\r\n\t\tt.Fatalf(\"Did not match active accounts of 2, got %d\", m.Stats.ActiveAccounts)\r\n\t}\r\n\tif m.Stats.Sent.Msgs < 1 {\r\n\t\tt.Fatalf(\"Did not match sent msgs of >=1, got %d\", m.Stats.Sent.Msgs)\r\n\t}\r\n\tif m.Stats.Received.Msgs < 1 {\r\n\t\tt.Fatalf(\"Did not match received msgs of >=1, got %d\", m.Stats.Received.Msgs)\r\n\t}\r\n\tif lr := len(m.Stats.Routes); lr != 1 {\r\n\t\tt.Fatalf(\"Expected a route, but got %d\", lr)\r\n\t}\r\n\r\n\t// Now let's prompt this server to send us the statsz\r\n\tsubj = fmt.Sprintf(serverStatsReqSubj, sa.ID())\r\n\tmsg, err = ncs.Request(subj, nil, time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error trying to request statsz: %v\", err)\r\n\t}\r\n\tm2 := ServerStatsMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &m2); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling the statz json: %v\", err)\r\n\t}\r\n\tif m2.Server.ID != sa.ID() {\r\n\t\tt.Fatalf(\"Did not match IDs\")\r\n\t}\r\n\tif m2.Stats.Connections != 1 {\r\n\t\tt.Fatalf(\"Did not match connections of 1, got %d\", m2.Stats.Connections)\r\n\t}\r\n\tif m2.Stats.ActiveAccounts != 2 {\r\n\t\tt.Fatalf(\"Did not match active accounts of 2, got %d\", m2.Stats.ActiveAccounts)\r\n\t}\r\n\tif m2.Stats.Sent.Msgs < 3 {\r\n\t\tt.Fatalf(\"Did not match sent msgs of >= 3, got %d\", m2.Stats.Sent.Msgs)\r\n\t}\r\n\tif m2.Stats.Received.Msgs < 1 {\r\n\t\tt.Fatalf(\"Did not match received msgs of >= 1, got %d\", m2.Stats.Received.Msgs)\r\n\t}\r\n\tif lr := len(m2.Stats.Routes); lr != 1 {\r\n\t\tt.Fatalf(\"Expected a route, but got %d\", lr)\r\n\t}\r\n\r\n\tmsg, err = ncs.Request(subj, nil, time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error trying to request statsz: %v\", err)\r\n\t}\r\n\tm3 := ServerStatsMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &m3); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling the statz json: %v\", err)\r\n\t}\r\n\tif m3.Server.ID != sa.ID() {\r\n\t\tt.Fatalf(\"Did not match IDs\")\r\n\t}\r\n\tif m3.Stats.Connections != 1 {\r\n\t\tt.Fatalf(\"Did not match connections of 1, got %d\", m3.Stats.Connections)\r\n\t}\r\n\tif m3.Stats.ActiveAccounts != 2 {\r\n\t\tt.Fatalf(\"Did not match active accounts of 2, got %d\", m3.Stats.ActiveAccounts)\r\n\t}\r\n\tif m3.Stats.Sent.Msgs < 5 {\r\n\t\tt.Fatalf(\"Did not match sent msgs of >= 5, got %d\", m3.Stats.Sent.Msgs)\r\n\t}\r\n\tif m3.Stats.Received.Msgs < 2 {\r\n\t\tt.Fatalf(\"Did not match received msgs of >= 2, got %d\", m3.Stats.Received.Msgs)\r\n\t}\r\n\tif lr := len(m3.Stats.Routes); lr != 1 {\r\n\t\tt.Fatalf(\"Expected a route, but got %d\", lr)\r\n\t}\r\n\tif sr := m3.Stats.Routes[0]; sr.Name != \"B\" {\r\n\t\tt.Fatalf(\"Expected server A's route to B to have Name set to %q, got %q\", \"B\", sr.Name)\r\n\t}\r\n\r\n\t// Now query B and check that route's name is \"A\"\r\n\tsubj = fmt.Sprintf(serverStatsReqSubj, sb.ID())\r\n\tncs.SubscribeSync(subj)\r\n\tmsg, err = ncs.Request(subj, nil, time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error trying to request statsz: %v\", err)\r\n\t}\r\n\tm = ServerStatsMsg{}\r\n\tif err := json.Unmarshal(msg.Data, &m); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling the statz json: %v\", err)\r\n\t}\r\n\tif lr := len(m.Stats.Routes); lr != 1 {\r\n\t\tt.Fatalf(\"Expected a route, but got %d\", lr)\r\n\t}\r\n\tif sr := m.Stats.Routes[0]; sr.Name != \"A\" {\r\n\t\tt.Fatalf(\"Expected server B's route to A to have Name set to %q, got %q\", \"A\", sr.Name)\r\n\t}\r\n}\r\n\r\nfunc TestServerEventsPingStatsZ(t *testing.T) {\r\n\tsa, _, sb, optsB, akp := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, sb, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\treply := nc.NewRespInbox()\r\n\tsub, _ := nc.SubscribeSync(reply)\r\n\r\n\tnc.PublishRequest(serverStatsPingReqSubj, reply, nil)\r\n\r\n\t// Make sure its a statsz\r\n\tm := ServerStatsMsg{}\r\n\r\n\t// Receive both manually.\r\n\tmsg, err := sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif err := json.Unmarshal(msg.Data, &m); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling the statz json: %v\", err)\r\n\t}\r\n\tmsg, err = sub.NextMsg(time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif err := json.Unmarshal(msg.Data, &m); err != nil {\r\n\t\tt.Fatalf(\"Error unmarshalling the statz json: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestServerEventsPingMonitorz(t *testing.T) {\r\n\tsa, _, sb, optsB, akp := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\tnc, err := nats.Connect(url, createUserCreds(t, sb, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tnc.Flush()\r\n\r\n\ttests := []struct {\r\n\t\tendpoint  string\r\n\t\topt       interface{}\r\n\t\tresp      interface{}\r\n\t\trespField []string\r\n\t}{\r\n\t\t{\"VARZ\", nil, &Varz{},\r\n\t\t\t[]string{\"now\", \"cpu\"}},\r\n\t\t{\"SUBSZ\", nil, &Subsz{},\r\n\t\t\t[]string{\"num_subscriptions\", \"num_cache\"}},\r\n\t\t{\"CONNZ\", nil, &Connz{},\r\n\t\t\t[]string{\"now\", \"connections\"}},\r\n\t\t{\"ROUTEZ\", nil, &Routez{},\r\n\t\t\t[]string{\"now\", \"routes\"}},\r\n\t\t{\"GATEWAYZ\", nil, &Gatewayz{},\r\n\t\t\t[]string{\"now\", \"outbound_gateways\", \"inbound_gateways\"}},\r\n\t\t{\"LEAFZ\", nil, &Leafz{},\r\n\t\t\t[]string{\"now\", \"leafs\"}},\r\n\r\n\t\t{\"SUBSZ\", &SubszOptions{}, &Subsz{},\r\n\t\t\t[]string{\"num_subscriptions\", \"num_cache\"}},\r\n\t\t{\"CONNZ\", &ConnzOptions{}, &Connz{},\r\n\t\t\t[]string{\"now\", \"connections\"}},\r\n\t\t{\"ROUTEZ\", &RoutezOptions{}, &Routez{},\r\n\t\t\t[]string{\"now\", \"routes\"}},\r\n\t\t{\"GATEWAYZ\", &GatewayzOptions{}, &Gatewayz{},\r\n\t\t\t[]string{\"now\", \"outbound_gateways\", \"inbound_gateways\"}},\r\n\t\t{\"LEAFZ\", &LeafzOptions{}, &Leafz{},\r\n\t\t\t[]string{\"now\", \"leafs\"}},\r\n\r\n\t\t{\"SUBSZ\", &SubszOptions{Limit: 5}, &Subsz{},\r\n\t\t\t[]string{\"num_subscriptions\", \"num_cache\"}},\r\n\t\t{\"CONNZ\", &ConnzOptions{Limit: 5}, &Connz{},\r\n\t\t\t[]string{\"now\", \"connections\"}},\r\n\t\t{\"ROUTEZ\", &RoutezOptions{SubscriptionsDetail: true}, &Routez{},\r\n\t\t\t[]string{\"now\", \"routes\"}},\r\n\t\t{\"GATEWAYZ\", &GatewayzOptions{Accounts: true}, &Gatewayz{},\r\n\t\t\t[]string{\"now\", \"outbound_gateways\", \"inbound_gateways\"}},\r\n\t\t{\"LEAFZ\", &LeafzOptions{Subscriptions: true}, &Leafz{},\r\n\t\t\t[]string{\"now\", \"leafs\"}},\r\n\t}\r\n\r\n\tfor i, test := range tests {\r\n\t\tt.Run(fmt.Sprintf(\"%s-%d\", test.endpoint, i), func(t *testing.T) {\r\n\t\t\tvar opt []byte\r\n\t\t\tif test.opt != nil {\r\n\t\t\t\topt, err = json.Marshal(test.opt)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tt.Fatalf(\"Error marshaling opts: %v\", err)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\treply := nc.NewRespInbox()\r\n\t\t\treplySubj, _ := nc.SubscribeSync(reply)\r\n\r\n\t\t\tdestSubj := fmt.Sprintf(\"%s.%s\", serverStatsPingReqSubj, test.endpoint)\r\n\t\t\tnc.PublishRequest(destSubj, reply, opt)\r\n\r\n\t\t\t// Receive both manually.\r\n\t\t\tmsg, err := replySubj.NextMsg(time.Second)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t\t\t}\r\n\t\t\tresponse1 := make(map[string]map[string]interface{})\r\n\r\n\t\t\tif err := json.Unmarshal(msg.Data, &response1); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error unmarshalling response1 json: %v\", err)\r\n\t\t\t}\r\n\r\n\t\t\tserverName := \"\"\r\n\t\t\tif response1[\"server\"][\"name\"] == \"A\" {\r\n\t\t\t\tserverName = \"B\"\r\n\t\t\t} else if response1[\"server\"][\"name\"] == \"B\" {\r\n\t\t\t\tserverName = \"A\"\r\n\t\t\t} else {\r\n\t\t\t\tt.Fatalf(\"Error finding server in %s\", string(msg.Data))\r\n\t\t\t}\r\n\t\t\tif resp, ok := response1[\"data\"]; !ok {\r\n\t\t\t\tt.Fatalf(\"Error finding: %s in %s\",\r\n\t\t\t\t\tstrings.ToLower(test.endpoint), string(msg.Data))\r\n\t\t\t} else {\r\n\t\t\t\tfor _, respField := range test.respField {\r\n\t\t\t\t\tif _, ok := resp[respField]; !ok {\r\n\t\t\t\t\t\tt.Fatalf(\"Error finding: %s in %s\", respField, resp)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\tmsg, err = replySubj.NextMsg(time.Second)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t\t\t}\r\n\t\t\tresponse2 := make(map[string]map[string]interface{})\r\n\t\t\tif err := json.Unmarshal(msg.Data, &response2); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error unmarshalling the response2 json: %v\", err)\r\n\t\t\t}\r\n\t\t\tif response2[\"server\"][\"name\"] != serverName {\r\n\t\t\t\tt.Fatalf(\"Error finding server %s in %s\", serverName, string(msg.Data))\r\n\t\t\t}\r\n\t\t\tif resp, ok := response2[\"data\"]; !ok {\r\n\t\t\t\tt.Fatalf(\"Error finding: %s in %s\",\r\n\t\t\t\t\tstrings.ToLower(test.endpoint), string(msg.Data))\r\n\t\t\t} else {\r\n\t\t\t\tfor _, respField := range test.respField {\r\n\t\t\t\t\tif _, ok := resp[respField]; !ok {\r\n\t\t\t\t\t\tt.Fatalf(\"Error finding: %s in %s\", respField, resp)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestGatewayNameClientInfo(t *testing.T) {\r\n\tsa, _, sb, _, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\tc, _, l := newClientForServer(sa)\r\n\tdefer c.close()\r\n\r\n\tvar info Info\r\n\terr := json.Unmarshal([]byte(l[5:]), &info)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not parse INFO json: %v\\n\", err)\r\n\t}\r\n\tif info.Cluster != \"TEST CLUSTER 22\" {\r\n\t\tt.Fatalf(\"Expected a cluster name of 'TEST CLUSTER 22', got %q\", info.Cluster)\r\n\t}\r\n}\r\n\r\ntype slowAccResolver struct {\r\n\tsync.Mutex\r\n\tAccountResolver\r\n\tacc string\r\n}\r\n\r\nfunc (sr *slowAccResolver) Fetch(name string) (string, error) {\r\n\tsr.Lock()\r\n\tdelay := sr.acc == name\r\n\tsr.Unlock()\r\n\tif delay {\r\n\t\ttime.Sleep(200 * time.Millisecond)\r\n\t}\r\n\treturn sr.AccountResolver.Fetch(name)\r\n}\r\n\r\nfunc TestConnectionUpdatesTimerProperlySet(t *testing.T) {\r\n\torigEventsHBInterval := eventsHBInterval\r\n\teventsHBInterval = 50 * time.Millisecond\r\n\tdefer func() { eventsHBInterval = origEventsHBInterval }()\r\n\r\n\tsa, _, sb, optsB, _ := runTrustedCluster(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\t// Normal Account\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tnac.Limits.Conn = 10 // set any limit...\r\n\tjwt, _ := nac.Encode(okp)\r\n\r\n\taddAccountToMemResolver(sa, pub, jwt)\r\n\r\n\t// Listen for HB updates...\r\n\tcount := int32(0)\r\n\tcb := func(sub *subscription, _ *client, subject, reply string, msg []byte) {\r\n\t\tatomic.AddInt32(&count, 1)\r\n\t}\r\n\tsubj := fmt.Sprintf(accConnsEventSubj, pub)\r\n\tsub, err := sa.sysSubscribe(subj, cb)\r\n\tif sub == nil || err != nil {\r\n\t\tt.Fatalf(\"Expected to subscribe, got %v\", err)\r\n\t}\r\n\tdefer sa.sysUnsubscribe(sub)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port)\r\n\tnc := natsConnect(t, url, createUserCreds(t, sb, akp))\r\n\tdefer nc.Close()\r\n\r\n\ttime.Sleep(500 * time.Millisecond)\r\n\t// After waiting 500ms with HB interval of 50ms, we should get\r\n\t// about 10 updates, no much more\r\n\tif n := atomic.LoadInt32(&count); n > 15 {\r\n\t\tt.Fatalf(\"Expected about 10 updates, got %v\", n)\r\n\t}\r\n\r\n\t// Now lookup the account doing the events on sb.\r\n\tacc, _ := sb.LookupAccount(pub)\r\n\t// Make sure we have the timer running.\r\n\tacc.mu.RLock()\r\n\tctmr := acc.ctmr\r\n\tacc.mu.RUnlock()\r\n\tif ctmr == nil {\r\n\t\tt.Fatalf(\"Expected event timer for acc conns to be running\")\r\n\t}\r\n\r\n\tnc.Close()\r\n\r\n\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\t// Make sure we have the timer is NOT running.\r\n\t\tacc.mu.RLock()\r\n\t\tctmr = acc.ctmr\r\n\t\tacc.mu.RUnlock()\r\n\t\tif ctmr != nil {\r\n\t\t\treturn fmt.Errorf(\"Expected event timer for acc conns to NOT be running after reaching zero local clients\")\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/events_test.go b/server/gnatsd/server/events_test.go
--- a/server/gnatsd/server/events_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/events_test.go	(date 1665399049649)
@@ -26,7 +26,7 @@
 	"testing"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nats.go"
 	"github.com/nats-io/nkeys"
 )
Index: server/gnatsd/server/reload_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2017-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"crypto/tls\"\r\n\t\"encoding/base64\"\r\n\t\"encoding/json\"\r\n\t\"flag\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"log\"\r\n\t\"net\"\r\n\t\"net/http\"\r\n\t\"net/http/httptest\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"reflect\"\r\n\t\"runtime\"\r\n\t\"strings\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\r\n\t\"github.com/nats-io/nats.go\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\nfunc newServerWithConfig(t *testing.T, configFile string) (*Server, *Options, string) {\r\n\tt.Helper()\r\n\tcontent, err := ioutil.ReadFile(configFile)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error loading file: %v\", err)\r\n\t}\r\n\treturn newServerWithContent(t, content)\r\n}\r\n\r\nfunc newServerWithContent(t *testing.T, content []byte) (*Server, *Options, string) {\r\n\tt.Helper()\r\n\topts, tmpFile := newOptionsFromContent(t, content)\r\n\treturn New(opts), opts, tmpFile\r\n}\r\n\r\nfunc newOptionsFromContent(t *testing.T, content []byte) (*Options, string) {\r\n\tt.Helper()\r\n\ttmpFile := createConfFile(t, content)\r\n\topts, err := ProcessConfigFile(tmpFile)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing config file: %v\", err)\r\n\t}\r\n\topts.NoSigs = true\r\n\treturn opts, tmpFile\r\n}\r\n\r\nfunc createConfFile(t *testing.T, content []byte) string {\r\n\tt.Helper()\r\n\tconf, err := ioutil.TempFile(\"\", \"\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating conf file: %v\", err)\r\n\t}\r\n\tfName := conf.Name()\r\n\tconf.Close()\r\n\tif err := ioutil.WriteFile(fName, content, 0666); err != nil {\r\n\t\tos.Remove(fName)\r\n\t\tt.Fatalf(\"Error writing conf file: %v\", err)\r\n\t}\r\n\treturn fName\r\n}\r\n\r\nfunc runReloadServerWithConfig(t *testing.T, configFile string) (*Server, *Options, string) {\r\n\tt.Helper()\r\n\tcontent, err := ioutil.ReadFile(configFile)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error loading file: %v\", err)\r\n\t}\r\n\treturn runReloadServerWithContent(t, content)\r\n}\r\n\r\nfunc runReloadServerWithContent(t *testing.T, content []byte) (*Server, *Options, string) {\r\n\tt.Helper()\r\n\topts, tmpFile := newOptionsFromContent(t, content)\r\n\topts.NoLog = true\r\n\topts.NoSigs = true\r\n\ts := RunServer(opts)\r\n\treturn s, opts, tmpFile\r\n}\r\n\r\nfunc changeCurrentConfigContent(t *testing.T, curConfig, newConfig string) {\r\n\tt.Helper()\r\n\tcontent, err := ioutil.ReadFile(newConfig)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error loading file: %v\", err)\r\n\t}\r\n\tchangeCurrentConfigContentWithNewContent(t, curConfig, content)\r\n}\r\n\r\nfunc changeCurrentConfigContentWithNewContent(t *testing.T, curConfig string, content []byte) {\r\n\tt.Helper()\r\n\tif err := ioutil.WriteFile(curConfig, content, 0666); err != nil {\r\n\t\tt.Fatalf(\"Error writing config: %v\", err)\r\n\t}\r\n}\r\n\r\n// Ensure Reload returns an error when attempting to reload a server that did\r\n// not start with a config file.\r\nfunc TestConfigReloadNoConfigFile(t *testing.T) {\r\n\tserver := New(&Options{NoSigs: true})\r\n\tloaded := server.ConfigTime()\r\n\tif server.Reload() == nil {\r\n\t\tt.Fatal(\"Expected Reload to return an error\")\r\n\t}\r\n\tif reloaded := server.ConfigTime(); reloaded != loaded {\r\n\t\tt.Fatalf(\"ConfigTime is incorrect.\\nexpected: %s\\ngot: %s\", loaded, reloaded)\r\n\t}\r\n}\r\n\r\n// Ensure Reload returns an error when attempting to change an option which\r\n// does not support reloading.\r\nfunc TestConfigReloadUnsupported(t *testing.T) {\r\n\tserver, _, config := newServerWithConfig(t, \"./configs/reload/test.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\tloaded := server.ConfigTime()\r\n\r\n\tgolden := &Options{\r\n\t\tConfigFile:     config,\r\n\t\tHost:           \"0.0.0.0\",\r\n\t\tPort:           2233,\r\n\t\tAuthTimeout:    1.0,\r\n\t\tDebug:          false,\r\n\t\tTrace:          false,\r\n\t\tLogtime:        false,\r\n\t\tMaxControlLine: 4096,\r\n\t\tMaxPayload:     1048576,\r\n\t\tMaxConn:        65536,\r\n\t\tPingInterval:   2 * time.Minute,\r\n\t\tMaxPingsOut:    2,\r\n\t\tWriteDeadline:  2 * time.Second,\r\n\t\tCluster: ClusterOpts{\r\n\t\t\tHost: \"127.0.0.1\",\r\n\t\t\tPort: -1,\r\n\t\t},\r\n\t\tNoSigs: true,\r\n\t}\r\n\tsetBaselineOptions(golden)\r\n\r\n\tcheckOptionsEqual(t, golden, server.getOpts())\r\n\r\n\t// Change config file to bad config.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/reload_unsupported.conf\")\r\n\r\n\t// This should fail because `cluster` host cannot be changed.\r\n\tif err := server.Reload(); err == nil {\r\n\t\tt.Fatal(\"Expected Reload to return an error\")\r\n\t}\r\n\r\n\t// Ensure config didn't change.\r\n\tcheckOptionsEqual(t, golden, server.getOpts())\r\n\r\n\tif reloaded := server.ConfigTime(); reloaded != loaded {\r\n\t\tt.Fatalf(\"ConfigTime is incorrect.\\nexpected: %s\\ngot: %s\", loaded, reloaded)\r\n\t}\r\n}\r\n\r\n// This checks that if we change an option that does not support hot-swapping\r\n// we get an error. Using `listen` for now (test may need to be updated if\r\n// server is changed to support change of listen spec).\r\nfunc TestConfigReloadUnsupportedHotSwapping(t *testing.T) {\r\n\tserver, _, config := newServerWithContent(t, []byte(\"listen: 127.0.0.1:-1\"))\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\tloaded := server.ConfigTime()\r\n\r\n\ttime.Sleep(time.Millisecond)\r\n\r\n\t// Change config file with unsupported option hot-swap\r\n\tchangeCurrentConfigContentWithNewContent(t, config, []byte(\"listen: 127.0.0.1:9999\"))\r\n\r\n\t// This should fail because `listen` host cannot be changed.\r\n\tif err := server.Reload(); err == nil || !strings.Contains(err.Error(), \"not supported\") {\r\n\t\tt.Fatalf(\"Expected Reload to return a not supported error, got %v\", err)\r\n\t}\r\n\r\n\tif reloaded := server.ConfigTime(); reloaded != loaded {\r\n\t\tt.Fatalf(\"ConfigTime is incorrect.\\nexpected: %s\\ngot: %s\", loaded, reloaded)\r\n\t}\r\n}\r\n\r\n// Ensure Reload returns an error when reloading from a bad config file.\r\nfunc TestConfigReloadInvalidConfig(t *testing.T) {\r\n\tserver, _, config := newServerWithConfig(t, \"./configs/reload/test.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\tloaded := server.ConfigTime()\r\n\r\n\tgolden := &Options{\r\n\t\tConfigFile:     config,\r\n\t\tHost:           \"0.0.0.0\",\r\n\t\tPort:           2233,\r\n\t\tAuthTimeout:    1.0,\r\n\t\tDebug:          false,\r\n\t\tTrace:          false,\r\n\t\tLogtime:        false,\r\n\t\tMaxControlLine: 4096,\r\n\t\tMaxPayload:     1048576,\r\n\t\tMaxConn:        65536,\r\n\t\tPingInterval:   2 * time.Minute,\r\n\t\tMaxPingsOut:    2,\r\n\t\tWriteDeadline:  2 * time.Second,\r\n\t\tCluster: ClusterOpts{\r\n\t\t\tHost: \"127.0.0.1\",\r\n\t\t\tPort: -1,\r\n\t\t},\r\n\t\tNoSigs: true,\r\n\t}\r\n\tsetBaselineOptions(golden)\r\n\r\n\tcheckOptionsEqual(t, golden, server.getOpts())\r\n\r\n\t// Change config file to bad config.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/invalid.conf\")\r\n\r\n\t// This should fail because the new config should not parse.\r\n\tif err := server.Reload(); err == nil {\r\n\t\tt.Fatal(\"Expected Reload to return an error\")\r\n\t}\r\n\r\n\t// Ensure config didn't change.\r\n\tcheckOptionsEqual(t, golden, server.getOpts())\r\n\r\n\tif reloaded := server.ConfigTime(); reloaded != loaded {\r\n\t\tt.Fatalf(\"ConfigTime is incorrect.\\nexpected: %s\\ngot: %s\", loaded, reloaded)\r\n\t}\r\n}\r\n\r\n// Ensure Reload returns nil and the config is changed on success.\r\nfunc TestConfigReload(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/test.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer os.Remove(\"nats-server.pid\")\r\n\tdefer os.Remove(\"nats-server.log\")\r\n\tdefer server.Shutdown()\r\n\r\n\tdir := filepath.Dir(config)\r\n\tvar content []byte\r\n\tif runtime.GOOS != \"windows\" {\r\n\t\tcontent = []byte(`\r\n\t\t\tremote_syslog: \"udp://127.0.0.1:514\" # change on reload\r\n\t\t\tsyslog:        true # enable on reload\r\n\t\t`)\r\n\t}\r\n\tplatformConf := filepath.Join(dir, \"platform.conf\")\r\n\tdefer os.Remove(platformConf)\r\n\tif err := ioutil.WriteFile(platformConf, content, 0666); err != nil {\r\n\t\tt.Fatalf(\"Unable to write config file: %v\", err)\r\n\t}\r\n\r\n\tloaded := server.ConfigTime()\r\n\r\n\tgolden := &Options{\r\n\t\tConfigFile:     config,\r\n\t\tHost:           \"0.0.0.0\",\r\n\t\tPort:           2233,\r\n\t\tAuthTimeout:    1.0,\r\n\t\tDebug:          false,\r\n\t\tTrace:          false,\r\n\t\tNoLog:          true,\r\n\t\tLogtime:        false,\r\n\t\tMaxControlLine: 4096,\r\n\t\tMaxPayload:     1048576,\r\n\t\tMaxConn:        65536,\r\n\t\tPingInterval:   2 * time.Minute,\r\n\t\tMaxPingsOut:    2,\r\n\t\tWriteDeadline:  2 * time.Second,\r\n\t\tCluster: ClusterOpts{\r\n\t\t\tHost: \"127.0.0.1\",\r\n\t\t\tPort: server.ClusterAddr().Port,\r\n\t\t},\r\n\t\tNoSigs: true,\r\n\t}\r\n\tsetBaselineOptions(golden)\r\n\r\n\tcheckOptionsEqual(t, golden, opts)\r\n\r\n\t// Change config file to new config.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/reload.conf\")\r\n\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure config changed.\r\n\tupdated := server.getOpts()\r\n\tif !updated.Trace {\r\n\t\tt.Fatal(\"Expected Trace to be true\")\r\n\t}\r\n\tif !updated.Debug {\r\n\t\tt.Fatal(\"Expected Debug to be true\")\r\n\t}\r\n\tif !updated.Logtime {\r\n\t\tt.Fatal(\"Expected Logtime to be true\")\r\n\t}\r\n\tif runtime.GOOS != \"windows\" {\r\n\t\tif !updated.Syslog {\r\n\t\t\tt.Fatal(\"Expected Syslog to be true\")\r\n\t\t}\r\n\t\tif updated.RemoteSyslog != \"udp://127.0.0.1:514\" {\r\n\t\t\tt.Fatalf(\"RemoteSyslog is incorrect.\\nexpected: udp://127.0.0.1:514\\ngot: %s\", updated.RemoteSyslog)\r\n\t\t}\r\n\t}\r\n\tif updated.LogFile != \"nats-server.log\" {\r\n\t\tt.Fatalf(\"LogFile is incorrect.\\nexpected: nats-server.log\\ngot: %s\", updated.LogFile)\r\n\t}\r\n\tif updated.TLSConfig == nil {\r\n\t\tt.Fatal(\"Expected TLSConfig to be non-nil\")\r\n\t}\r\n\tif !server.info.TLSRequired {\r\n\t\tt.Fatal(\"Expected TLSRequired to be true\")\r\n\t}\r\n\tif !server.info.TLSVerify {\r\n\t\tt.Fatal(\"Expected TLSVerify to be true\")\r\n\t}\r\n\tif updated.Username != \"tyler\" {\r\n\t\tt.Fatalf(\"Username is incorrect.\\nexpected: tyler\\ngot: %s\", updated.Username)\r\n\t}\r\n\tif updated.Password != \"T0pS3cr3t\" {\r\n\t\tt.Fatalf(\"Password is incorrect.\\nexpected: T0pS3cr3t\\ngot: %s\", updated.Password)\r\n\t}\r\n\tif updated.AuthTimeout != 2 {\r\n\t\tt.Fatalf(\"AuthTimeout is incorrect.\\nexpected: 2\\ngot: %f\", updated.AuthTimeout)\r\n\t}\r\n\tif !server.info.AuthRequired {\r\n\t\tt.Fatal(\"Expected AuthRequired to be true\")\r\n\t}\r\n\tif !updated.Cluster.NoAdvertise {\r\n\t\tt.Fatal(\"Expected NoAdvertise to be true\")\r\n\t}\r\n\tif updated.PidFile != \"nats-server.pid\" {\r\n\t\tt.Fatalf(\"PidFile is incorrect.\\nexpected: nats-server.pid\\ngot: %s\", updated.PidFile)\r\n\t}\r\n\tif updated.MaxControlLine != 512 {\r\n\t\tt.Fatalf(\"MaxControlLine is incorrect.\\nexpected: 512\\ngot: %d\", updated.MaxControlLine)\r\n\t}\r\n\tif updated.PingInterval != 5*time.Second {\r\n\t\tt.Fatalf(\"PingInterval is incorrect.\\nexpected 5s\\ngot: %s\", updated.PingInterval)\r\n\t}\r\n\tif updated.MaxPingsOut != 1 {\r\n\t\tt.Fatalf(\"MaxPingsOut is incorrect.\\nexpected 1\\ngot: %d\", updated.MaxPingsOut)\r\n\t}\r\n\tif updated.WriteDeadline != 3*time.Second {\r\n\t\tt.Fatalf(\"WriteDeadline is incorrect.\\nexpected 3s\\ngot: %s\", updated.WriteDeadline)\r\n\t}\r\n\tif updated.MaxPayload != 1024 {\r\n\t\tt.Fatalf(\"MaxPayload is incorrect.\\nexpected 1024\\ngot: %d\", updated.MaxPayload)\r\n\t}\r\n\r\n\tif reloaded := server.ConfigTime(); !reloaded.After(loaded) {\r\n\t\tt.Fatalf(\"ConfigTime is incorrect.\\nexpected greater than: %s\\ngot: %s\", loaded, reloaded)\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports TLS config changes. Test this by starting a server\r\n// with TLS enabled, connect to it to verify, reload config using a different\r\n// key pair and client verification enabled, ensure reconnect fails, then\r\n// ensure reconnect succeeds when the client provides a cert.\r\nfunc TestConfigReloadRotateTLS(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/tls_test.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, server.Addr().(*net.TCPAddr).Port)\r\n\r\n\tnc, err := nats.Connect(addr, nats.Secure(&tls.Config{InsecureSkipVerify: true}))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tsub, err := nc.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tdefer sub.Unsubscribe()\r\n\r\n\t// Rotate cert and enable client verification.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/tls_verify_test.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr, nats.Secure(&tls.Config{InsecureSkipVerify: true})); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when client presents cert.\r\n\tcert := nats.ClientCert(\"./configs/certs/cert.new.pem\", \"./configs/certs/key.new.pem\")\r\n\tconn, err := nats.Connect(addr, cert, nats.RootCAs(\"./configs/certs/cert.new.pem\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the original connection can still publish/receive.\r\n\tif err := nc.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\tmsg, err := sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports enabling TLS. Test this by starting a server without\r\n// TLS enabled, connect to it to verify, reload config with TLS enabled, ensure\r\n// reconnect fails, then ensure reconnect succeeds when using secure.\r\nfunc TestConfigReloadEnableTLS(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, server.Addr().(*net.TCPAddr).Port)\r\n\tnc, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n\r\n\t// Enable TLS.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/tls_test.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting is OK (we need to skip server cert verification since\r\n\t// the library is not doing that by default now).\r\n\tnc, err = nats.Connect(addr, nats.Secure(&tls.Config{InsecureSkipVerify: true}))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n}\r\n\r\n// Ensure Reload supports disabling TLS. Test this by starting a server with\r\n// TLS enabled, connect to it to verify, reload config with TLS disabled,\r\n// ensure reconnect fails, then ensure reconnect succeeds when connecting\r\n// without secure.\r\nfunc TestConfigReloadDisableTLS(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/tls_test.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, server.Addr().(*net.TCPAddr).Port)\r\n\tnc, err := nats.Connect(addr, nats.Secure(&tls.Config{InsecureSkipVerify: true}))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n\r\n\t// Disable TLS.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/basic.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr, nats.Secure(&tls.Config{InsecureSkipVerify: true})); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when not using secure.\r\n\tnc, err = nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tnc.Close()\r\n}\r\n\r\n// Ensure Reload supports single user authentication config changes. Test this\r\n// by starting a server with authentication enabled, connect to it to verify,\r\n// reload config using a different username/password, ensure reconnect fails,\r\n// then ensure reconnect succeeds when using the correct credentials.\r\nfunc TestConfigReloadRotateUserAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/single_user_authentication_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.UserInfo(\"tyler\", \"T0pS3cr3t\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tdisconnected := make(chan struct{}, 1)\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\tnc.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tdisconnected <- struct{}{}\r\n\t})\r\n\r\n\t// Change user credentials.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/single_user_authentication_2.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr, nats.UserInfo(\"tyler\", \"T0pS3cr3t\")); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when using new credentials.\r\n\tconn, err := nats.Connect(addr, nats.UserInfo(\"derek\", \"passw0rd\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the previous connection received an authorization error.\r\n\t// Note that it is possible that client gets EOF and not able to\r\n\t// process async error, so don't fail if we don't get it.\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(time.Second):\r\n\t\t// Give it up to 1 sec.\r\n\t}\r\n\r\n\t// Ensure the previous connection was disconnected.\r\n\tselect {\r\n\tcase <-disconnected:\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports enabling single user authentication. Test this by\r\n// starting a server with authentication disabled, connect to it to verify,\r\n// reload config using with a username/password, ensure reconnect fails, then\r\n// ensure reconnect succeeds when using the correct credentials.\r\nfunc TestConfigReloadEnableUserAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tdisconnected := make(chan struct{}, 1)\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\tnc.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tdisconnected <- struct{}{}\r\n\t})\r\n\r\n\t// Enable authentication.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/single_user_authentication_1.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when using new credentials.\r\n\tconn, err := nats.Connect(addr, nats.UserInfo(\"tyler\", \"T0pS3cr3t\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the previous connection received an authorization error.\r\n\t// Note that it is possible that client gets EOF and not able to\r\n\t// process async error, so don't fail if we don't get it.\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(time.Second):\r\n\t}\r\n\r\n\t// Ensure the previous connection was disconnected.\r\n\tselect {\r\n\tcase <-disconnected:\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports disabling single user authentication. Test this by\r\n// starting a server with authentication enabled, connect to it to verify,\r\n// reload config using with authentication disabled, then ensure connecting\r\n// with no credentials succeeds.\r\nfunc TestConfigReloadDisableUserAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/single_user_authentication_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.UserInfo(\"tyler\", \"T0pS3cr3t\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tt.Fatalf(\"Client received an unexpected error: %v\", err)\r\n\t})\r\n\r\n\t// Disable authentication.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/basic.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting succeeds with no credentials.\r\n\tconn, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n}\r\n\r\n// Ensure Reload supports token authentication config changes. Test this by\r\n// starting a server with token authentication enabled, connect to it to\r\n// verify, reload config using a different token, ensure reconnect fails, then\r\n// ensure reconnect succeeds when using the correct token.\r\nfunc TestConfigReloadRotateTokenAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/token_authentication_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\tdisconnected := make(chan struct{})\r\n\tasyncErr := make(chan error)\r\n\teh := func(nc *nats.Conn, sub *nats.Subscription, err error) { asyncErr <- err }\r\n\tdh := func(*nats.Conn) { disconnected <- struct{}{} }\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.Token(\"T0pS3cr3t\"), nats.ErrorHandler(eh), nats.DisconnectHandler(dh))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Change authentication token.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/token_authentication_2.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr, nats.Token(\"T0pS3cr3t\")); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when using new credentials.\r\n\tconn, err := nats.Connect(addr, nats.Token(\"passw0rd\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the previous connection received an authorization error.\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatal(\"Expected authorization error\")\r\n\t}\r\n\r\n\t// Ensure the previous connection was disconnected.\r\n\tselect {\r\n\tcase <-disconnected:\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports enabling token authentication. Test this by starting\r\n// a server with authentication disabled, connect to it to verify, reload\r\n// config using with a token, ensure reconnect fails, then ensure reconnect\r\n// succeeds when using the correct token.\r\nfunc TestConfigReloadEnableTokenAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tdisconnected := make(chan struct{}, 1)\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\tnc.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tdisconnected <- struct{}{}\r\n\t})\r\n\r\n\t// Enable authentication.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/token_authentication_1.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when using new credentials.\r\n\tconn, err := nats.Connect(addr, nats.Token(\"T0pS3cr3t\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the previous connection received an authorization error.\r\n\t// Note that it is possible that client gets EOF and not able to\r\n\t// process async error, so don't fail if we don't get it.\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(time.Second):\r\n\t}\r\n\r\n\t// Ensure the previous connection was disconnected.\r\n\tselect {\r\n\tcase <-disconnected:\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports disabling single token authentication. Test this by\r\n// starting a server with authentication enabled, connect to it to verify,\r\n// reload config using with authentication disabled, then ensure connecting\r\n// with no token succeeds.\r\nfunc TestConfigReloadDisableTokenAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/token_authentication_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.Token(\"T0pS3cr3t\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tt.Fatalf(\"Client received an unexpected error: %v\", err)\r\n\t})\r\n\r\n\t// Disable authentication.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/basic.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting succeeds with no credentials.\r\n\tconn, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n}\r\n\r\n// Ensure Reload supports users authentication config changes. Test this by\r\n// starting a server with users authentication enabled, connect to it to\r\n// verify, reload config using a different user, ensure reconnect fails, then\r\n// ensure reconnect succeeds when using the correct credentials.\r\nfunc TestConfigReloadRotateUsersAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/multiple_users_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.UserInfo(\"alice\", \"foo\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tdisconnected := make(chan struct{}, 1)\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\tnc.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tdisconnected <- struct{}{}\r\n\t})\r\n\r\n\t// These credentials won't change.\r\n\tnc2, err := nats.Connect(addr, nats.UserInfo(\"bob\", \"bar\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\tsub, err := nc2.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tdefer sub.Unsubscribe()\r\n\r\n\t// Change users credentials.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/multiple_users_2.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr, nats.UserInfo(\"alice\", \"foo\")); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when using new credentials.\r\n\tconn, err := nats.Connect(addr, nats.UserInfo(\"alice\", \"baz\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the previous connection received an authorization error.\r\n\t// Note that it is possible that client gets EOF and not able to\r\n\t// process async error, so don't fail if we don't get it.\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(time.Second):\r\n\t}\r\n\r\n\t// Ensure the previous connection was disconnected.\r\n\tselect {\r\n\tcase <-disconnected:\r\n\tcase <-time.After(2 * time.Second):\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n\r\n\t// Ensure the connection using unchanged credentials can still\r\n\t// publish/receive.\r\n\tif err := nc2.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tnc2.Flush()\r\n\tmsg, err := sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports enabling users authentication. Test this by starting\r\n// a server with authentication disabled, connect to it to verify, reload\r\n// config using with users, ensure reconnect fails, then ensure reconnect\r\n// succeeds when using the correct credentials.\r\nfunc TestConfigReloadEnableUsersAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tdisconnected := make(chan struct{}, 1)\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\tnc.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tdisconnected <- struct{}{}\r\n\t})\r\n\r\n\t// Enable authentication.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/multiple_users_1.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting fails.\r\n\tif _, err := nats.Connect(addr); err == nil {\r\n\t\tt.Fatal(\"Expected connect to fail\")\r\n\t}\r\n\r\n\t// Ensure connecting succeeds when using new credentials.\r\n\tconn, err := nats.Connect(addr, nats.UserInfo(\"alice\", \"foo\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n\r\n\t// Ensure the previous connection received an authorization error.\r\n\t// Note that it is possible that client gets EOF and not able to\r\n\t// process async error, so don't fail if we don't get it.\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif err != nats.ErrAuthorization {\r\n\t\t\tt.Fatalf(\"Expected ErrAuthorization, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(time.Second):\r\n\t}\r\n\r\n\t// Ensure the previous connection was disconnected.\r\n\tselect {\r\n\tcase <-disconnected:\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatal(\"Expected connection to be disconnected\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports disabling users authentication. Test this by starting\r\n// a server with authentication enabled, connect to it to verify,\r\n// reload config using with authentication disabled, then ensure connecting\r\n// with no credentials succeeds.\r\nfunc TestConfigReloadDisableUsersAuthentication(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/multiple_users_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.UserInfo(\"alice\", \"foo\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tt.Fatalf(\"Client received an unexpected error: %v\", err)\r\n\t})\r\n\r\n\t// Disable authentication.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/basic.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure connecting succeeds with no credentials.\r\n\tconn, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tconn.Close()\r\n}\r\n\r\n// Ensure Reload supports changing permissions. Test this by starting a server\r\n// with a user configured with certain permissions, test publish and subscribe,\r\n// reload config with new permissions, ensure the previous subscription was\r\n// closed and publishes fail, then ensure the new permissions succeed.\r\nfunc TestConfigReloadChangePermissions(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/authorization_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(addr, nats.UserInfo(\"bob\", \"bar\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tasyncErr := make(chan error, 1)\r\n\tnc.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr <- err\r\n\t})\r\n\t// Ensure we can publish and receive messages as a sanity check.\r\n\tsub, err := nc.SubscribeSync(\"_INBOX.>\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\r\n\tconn, err := nats.Connect(addr, nats.UserInfo(\"alice\", \"foo\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer conn.Close()\r\n\r\n\tsub2, err := conn.SubscribeSync(\"req.foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tif err := conn.Publish(\"_INBOX.foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\tmsg, err := sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n\r\n\tif err := nc.Publish(\"req.foo\", []byte(\"world\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\r\n\tmsg, err = sub2.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"world\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"world\"), msg.Data)\r\n\t}\r\n\r\n\t// Susan will subscribe to two subjects, both will succeed but a send to foo.bar should not succeed\r\n\t// however PUBLIC.foo should.\r\n\tsconn, err := nats.Connect(addr, nats.UserInfo(\"susan\", \"baz\"))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer sconn.Close()\r\n\r\n\tasyncErr2 := make(chan error, 1)\r\n\tsconn.SetErrorHandler(func(nc *nats.Conn, sub *nats.Subscription, err error) {\r\n\t\tasyncErr2 <- err\r\n\t})\r\n\r\n\tfooSub, err := sconn.SubscribeSync(\"foo.*\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tsconn.Flush()\r\n\r\n\t// Publishing from bob on foo.bar should not come through.\r\n\tif err := conn.Publish(\"foo.bar\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\t_, err = fooSub.NextMsg(100 * time.Millisecond)\r\n\tif err != nats.ErrTimeout {\r\n\t\tt.Fatalf(\"Received a message we shouldn't have\")\r\n\t}\r\n\r\n\tpubSub, err := sconn.SubscribeSync(\"PUBLIC.*\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tsconn.Flush()\r\n\r\n\tselect {\r\n\tcase err := <-asyncErr2:\r\n\t\tt.Fatalf(\"Received unexpected error for susan: %v\", err)\r\n\tdefault:\r\n\t}\r\n\r\n\t// This should work ok with original config.\r\n\tif err := conn.Publish(\"PUBLIC.foo\", []byte(\"hello monkey\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\tmsg, err = pubSub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello monkey\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %q\\ngot: %q\", \"hello monkey\", msg.Data)\r\n\t}\r\n\r\n\t///////////////////////////////////////////\r\n\t// Change permissions.\r\n\t///////////////////////////////////////////\r\n\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/authorization_2.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure we receive an error for the subscription that is no longer authorized.\r\n\t// In this test, since connection is not closed by the server,\r\n\t// the client must receive an -ERR\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif !strings.Contains(strings.ToLower(err.Error()), \"permissions violation for subscription to \\\"_inbox.>\\\"\") {\r\n\t\t\tt.Fatalf(\"Expected permissions violation error, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatal(\"Expected permissions violation error\")\r\n\t}\r\n\r\n\t// Ensure we receive an error when publishing to req.foo and we no longer\r\n\t// receive messages on _INBOX.>.\r\n\tif err := nc.Publish(\"req.foo\", []byte(\"hola\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\tif err := conn.Publish(\"_INBOX.foo\", []byte(\"mundo\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tif !strings.Contains(strings.ToLower(err.Error()), \"permissions violation for publish to \\\"req.foo\\\"\") {\r\n\t\t\tt.Fatalf(\"Expected permissions violation error, got %v\", err)\r\n\t\t}\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatal(\"Expected permissions violation error\")\r\n\t}\r\n\r\n\tqueued, _, err := sub2.Pending()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Failed to get pending messaged: %v\", err)\r\n\t}\r\n\tif queued != 0 {\r\n\t\tt.Fatalf(\"Pending is incorrect.\\nexpected: 0\\ngot: %d\", queued)\r\n\t}\r\n\r\n\tqueued, _, err = sub.Pending()\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Failed to get pending messaged: %v\", err)\r\n\t}\r\n\tif queued != 0 {\r\n\t\tt.Fatalf(\"Pending is incorrect.\\nexpected: 0\\ngot: %d\", queued)\r\n\t}\r\n\r\n\t// Ensure we can publish to _INBOX.foo.bar and subscribe to _INBOX.foo.>.\r\n\tsub, err = nc.SubscribeSync(\"_INBOX.foo.>\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\tif err := nc.Publish(\"_INBOX.foo.bar\", []byte(\"testing\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\tmsg, err = sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"testing\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"testing\"), msg.Data)\r\n\t}\r\n\r\n\tselect {\r\n\tcase err := <-asyncErr:\r\n\t\tt.Fatalf(\"Received unexpected error: %v\", err)\r\n\tdefault:\r\n\t}\r\n\r\n\t// Now check susan again.\r\n\t//\r\n\t// This worked ok with original config but should not deliver a message now.\r\n\tif err := conn.Publish(\"PUBLIC.foo\", []byte(\"hello monkey\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\t_, err = pubSub.NextMsg(100 * time.Millisecond)\r\n\tif err != nats.ErrTimeout {\r\n\t\tt.Fatalf(\"Received a message we shouldn't have\")\r\n\t}\r\n\r\n\t// Now check foo.bar, which did not work before but should work now..\r\n\tif err := conn.Publish(\"foo.bar\", []byte(\"hello?\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing message: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\tmsg, err = fooSub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving msg: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello?\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %q\\ngot: %q\", \"hello?\", msg.Data)\r\n\t}\r\n\r\n\t// Once last check for no errors.\r\n\tsconn.Flush()\r\n\r\n\tselect {\r\n\tcase err := <-asyncErr2:\r\n\t\tt.Fatalf(\"Received unexpected error for susan: %v\", err)\r\n\tdefault:\r\n\t}\r\n}\r\n\r\n// Ensure Reload returns an error when attempting to change cluster address\r\n// host.\r\nfunc TestConfigReloadClusterHostUnsupported(t *testing.T) {\r\n\tserver, _, config := runReloadServerWithConfig(t, \"./configs/reload/srv_a_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Attempt to change cluster listen host.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/srv_c_1.conf\")\r\n\r\n\t// This should fail because cluster address cannot be changed.\r\n\tif err := server.Reload(); err == nil {\r\n\t\tt.Fatal(\"Expected Reload to return an error\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload returns an error when attempting to change cluster address\r\n// port.\r\nfunc TestConfigReloadClusterPortUnsupported(t *testing.T) {\r\n\tserver, _, config := runReloadServerWithConfig(t, \"./configs/reload/srv_a_1.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Attempt to change cluster listen port.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/srv_b_1.conf\")\r\n\r\n\t// This should fail because cluster address cannot be changed.\r\n\tif err := server.Reload(); err == nil {\r\n\t\tt.Fatal(\"Expected Reload to return an error\")\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports enabling route authorization. Test this by starting\r\n// two servers in a cluster without authorization, ensuring messages flow\r\n// between them, then reloading with authorization and ensuring messages no\r\n// longer flow until reloading with the correct credentials.\r\nfunc TestConfigReloadEnableClusterAuthorization(t *testing.T) {\r\n\tsrvb, srvbOpts, srvbConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_b_1.conf\")\r\n\tdefer os.Remove(srvbConfig)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tsrva, srvaOpts, srvaConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_a_1.conf\")\r\n\tdefer os.Remove(srvaConfig)\r\n\tdefer srva.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tsrvaAddr := fmt.Sprintf(\"nats://%s:%d\", srvaOpts.Host, srvaOpts.Port)\r\n\tsrvaConn, err := nats.Connect(srvaAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvaConn.Close()\r\n\tsub, err := srvaConn.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tdefer sub.Unsubscribe()\r\n\tif err := srvaConn.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error flushing: %v\", err)\r\n\t}\r\n\r\n\tsrvbAddr := fmt.Sprintf(\"nats://%s:%d\", srvbOpts.Host, srvbOpts.Port)\r\n\tsrvbConn, err := nats.Connect(srvbAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvbConn.Close()\r\n\r\n\tif numRoutes := srvb.NumRoutes(); numRoutes != 1 {\r\n\t\tt.Fatalf(\"Expected 1 route, got %d\", numRoutes)\r\n\t}\r\n\r\n\t// Ensure messages flow through the cluster as a sanity check.\r\n\tif err := srvbConn.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tsrvbConn.Flush()\r\n\tmsg, err := sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n\r\n\t// Enable route authorization.\r\n\tchangeCurrentConfigContent(t, srvbConfig, \"./configs/reload/srv_b_2.conf\")\r\n\tif err := srvb.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\tcheckNumRoutes(t, srvb, 0)\r\n\r\n\t// Ensure messages no longer flow through the cluster.\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tif err := srvbConn.Publish(\"foo\", []byte(\"world\")); err != nil {\r\n\t\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t\t}\r\n\t\tsrvbConn.Flush()\r\n\t}\r\n\tif _, err := sub.NextMsg(50 * time.Millisecond); err != nats.ErrTimeout {\r\n\t\tt.Fatalf(\"Expected ErrTimeout, got %v\", err)\r\n\t}\r\n\r\n\t// Reload Server A with correct route credentials.\r\n\tchangeCurrentConfigContent(t, srvaConfig, \"./configs/reload/srv_a_2.conf\")\r\n\tdefer os.Remove(srvaConfig)\r\n\tif err := srva.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tif numRoutes := srvb.NumRoutes(); numRoutes != 1 {\r\n\t\tt.Fatalf(\"Expected 1 route, got %d\", numRoutes)\r\n\t}\r\n\r\n\t// Ensure messages flow through the cluster now.\r\n\tif err := srvbConn.Publish(\"foo\", []byte(\"hola\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tsrvbConn.Flush()\r\n\tmsg, err = sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hola\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hola\"), msg.Data)\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports disabling route authorization. Test this by starting\r\n// two servers in a cluster with authorization, ensuring messages flow\r\n// between them, then reloading without authorization and ensuring messages\r\n// still flow.\r\nfunc TestConfigReloadDisableClusterAuthorization(t *testing.T) {\r\n\tsrvb, srvbOpts, srvbConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_b_2.conf\")\r\n\tdefer os.Remove(srvbConfig)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tsrva, srvaOpts, srvaConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_a_2.conf\")\r\n\tdefer os.Remove(srvaConfig)\r\n\tdefer srva.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tsrvaAddr := fmt.Sprintf(\"nats://%s:%d\", srvaOpts.Host, srvaOpts.Port)\r\n\tsrvaConn, err := nats.Connect(srvaAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvaConn.Close()\r\n\r\n\tsub, err := srvaConn.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tdefer sub.Unsubscribe()\r\n\tif err := srvaConn.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error flushing: %v\", err)\r\n\t}\r\n\r\n\tsrvbAddr := fmt.Sprintf(\"nats://%s:%d\", srvbOpts.Host, srvbOpts.Port)\r\n\tsrvbConn, err := nats.Connect(srvbAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvbConn.Close()\r\n\r\n\tif numRoutes := srvb.NumRoutes(); numRoutes != 1 {\r\n\t\tt.Fatalf(\"Expected 1 route, got %d\", numRoutes)\r\n\t}\r\n\r\n\t// Ensure messages flow through the cluster as a sanity check.\r\n\tif err := srvbConn.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tsrvbConn.Flush()\r\n\tmsg, err := sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n\r\n\t// Disable route authorization.\r\n\tchangeCurrentConfigContent(t, srvbConfig, \"./configs/reload/srv_b_1.conf\")\r\n\tif err := srvb.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tif numRoutes := srvb.NumRoutes(); numRoutes != 1 {\r\n\t\tt.Fatalf(\"Expected 1 route, got %d\", numRoutes)\r\n\t}\r\n\r\n\t// Ensure messages still flow through the cluster.\r\n\tif err := srvbConn.Publish(\"foo\", []byte(\"hola\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tsrvbConn.Flush()\r\n\tmsg, err = sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hola\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hola\"), msg.Data)\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports changing cluster routes. Test this by starting\r\n// two servers in a cluster, ensuring messages flow between them, then\r\n// reloading with a different route and ensuring messages flow through the new\r\n// cluster.\r\nfunc TestConfigReloadClusterRoutes(t *testing.T) {\r\n\tsrvb, srvbOpts, srvbConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_b_1.conf\")\r\n\tdefer os.Remove(srvbConfig)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tsrva, srvaOpts, srvaConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_a_1.conf\")\r\n\tdefer os.Remove(srvaConfig)\r\n\tdefer srva.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tsrvcOpts, err := ProcessConfigFile(\"./configs/reload/srv_c_1.conf\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing config file: %v\", err)\r\n\t}\r\n\tsrvcOpts.NoLog = true\r\n\tsrvcOpts.NoSigs = true\r\n\r\n\tsrvc := RunServer(srvcOpts)\r\n\tdefer srvc.Shutdown()\r\n\r\n\tsrvaAddr := fmt.Sprintf(\"nats://%s:%d\", srvaOpts.Host, srvaOpts.Port)\r\n\tsrvaConn, err := nats.Connect(srvaAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvaConn.Close()\r\n\r\n\tsub, err := srvaConn.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tdefer sub.Unsubscribe()\r\n\tif err := srvaConn.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error flushing: %v\", err)\r\n\t}\r\n\r\n\tsrvbAddr := fmt.Sprintf(\"nats://%s:%d\", srvbOpts.Host, srvbOpts.Port)\r\n\tsrvbConn, err := nats.Connect(srvbAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvbConn.Close()\r\n\r\n\tif numRoutes := srvb.NumRoutes(); numRoutes != 1 {\r\n\t\tt.Fatalf(\"Expected 1 route, got %d\", numRoutes)\r\n\t}\r\n\r\n\t// Ensure consumer on srvA is propagated to srvB\r\n\tcheckExpectedSubs(t, 1, srvb)\r\n\r\n\t// Ensure messages flow through the cluster as a sanity check.\r\n\tif err := srvbConn.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tsrvbConn.Flush()\r\n\tmsg, err := sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n\r\n\t// Reload cluster routes.\r\n\tchangeCurrentConfigContent(t, srvaConfig, \"./configs/reload/srv_a_3.conf\")\r\n\tif err := srva.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Kill old route server.\r\n\tsrvbConn.Close()\r\n\tsrvb.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvc)\r\n\r\n\tsrvcAddr := fmt.Sprintf(\"nats://%s:%d\", srvcOpts.Host, srvcOpts.Port)\r\n\tsrvcConn, err := nats.Connect(srvcAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvcConn.Close()\r\n\r\n\t// Ensure messages flow through the new cluster.\r\n\tfor i := 0; i < 5; i++ {\r\n\t\tif err := srvcConn.Publish(\"foo\", []byte(\"hola\")); err != nil {\r\n\t\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t\t}\r\n\t\tsrvcConn.Flush()\r\n\t}\r\n\tmsg, err = sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hola\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hola\"), msg.Data)\r\n\t}\r\n}\r\n\r\n// Ensure Reload supports removing a solicited route. In this case from A->B\r\n// Test this by starting two servers in a cluster, ensuring messages flow between them.\r\n// Then stop server B, and have server A continue to try to connect. Reload A with a config\r\n// that removes the route and make sure it does not connect to server B when its restarted.\r\nfunc TestConfigReloadClusterRemoveSolicitedRoutes(t *testing.T) {\r\n\tsrvb, srvbOpts := RunServerWithConfig(\"./configs/reload/srv_b_1.conf\")\r\n\tdefer srvb.Shutdown()\r\n\r\n\tsrva, srvaOpts, srvaConfig := runReloadServerWithConfig(t, \"./configs/reload/srv_a_1.conf\")\r\n\tdefer os.Remove(srvaConfig)\r\n\tdefer srva.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tsrvaAddr := fmt.Sprintf(\"nats://%s:%d\", srvaOpts.Host, srvaOpts.Port)\r\n\tsrvaConn, err := nats.Connect(srvaAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvaConn.Close()\r\n\tsub, err := srvaConn.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tdefer sub.Unsubscribe()\r\n\tif err := srvaConn.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error flushing: %v\", err)\r\n\t}\r\n\tcheckExpectedSubs(t, 1, srvb)\r\n\r\n\tsrvbAddr := fmt.Sprintf(\"nats://%s:%d\", srvbOpts.Host, srvbOpts.Port)\r\n\tsrvbConn, err := nats.Connect(srvbAddr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer srvbConn.Close()\r\n\r\n\tif err := srvbConn.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tsrvbConn.Flush()\r\n\tmsg, err := sub.NextMsg(5 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\tif string(msg.Data) != \"hello\" {\r\n\t\tt.Fatalf(\"Msg is incorrect.\\nexpected: %+v\\ngot: %+v\", []byte(\"hello\"), msg.Data)\r\n\t}\r\n\r\n\t// Now stop server B.\r\n\tsrvb.Shutdown()\r\n\r\n\t// Wait til route is dropped.\r\n\tcheckNumRoutes(t, srva, 0)\r\n\r\n\t// Now change config for server A to not solicit a route to server B.\r\n\tchangeCurrentConfigContent(t, srvaConfig, \"./configs/reload/srv_a_4.conf\")\r\n\tdefer os.Remove(srvaConfig)\r\n\tif err := srva.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Restart server B.\r\n\tsrvb, _ = RunServerWithConfig(\"./configs/reload/srv_b_1.conf\")\r\n\tdefer srvb.Shutdown()\r\n\r\n\t// We should not have a cluster formed here.\r\n\tnumRoutes := 0\r\n\tdeadline := time.Now().Add(2 * DEFAULT_ROUTE_RECONNECT)\r\n\tfor time.Now().Before(deadline) {\r\n\t\tif numRoutes = srva.NumRoutes(); numRoutes != 0 {\r\n\t\t\tbreak\r\n\t\t} else {\r\n\t\t\ttime.Sleep(100 * time.Millisecond)\r\n\t\t}\r\n\t}\r\n\tif numRoutes != 0 {\r\n\t\tt.Fatalf(\"Expected 0 routes for server A, got %d\", numRoutes)\r\n\t}\r\n}\r\n\r\nfunc reloadUpdateConfig(t *testing.T, s *Server, conf, content string) {\r\n\tt.Helper()\r\n\tif err := ioutil.WriteFile(conf, []byte(content), 0666); err != nil {\r\n\t\tt.Fatalf(\"Error creating config file: %v\", err)\r\n\t}\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error on reload: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadClusterAdvertise(t *testing.T) {\r\n\ts, _, conf := runReloadServerWithContent(t, []byte(`\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t\tcluster: {\r\n\t\t\tlisten: \"0.0.0.0:-1\"\r\n\t\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\torgClusterPort := s.ClusterAddr().Port\r\n\r\n\tverify := func(expectedHost string, expectedPort int, expectedIP string) {\r\n\t\ts.mu.Lock()\r\n\t\trouteInfo := s.routeInfo\r\n\t\trouteInfoJSON := Info{}\r\n\t\terr := json.Unmarshal(s.routeInfoJSON[5:], &routeInfoJSON) // Skip \"INFO \"\r\n\t\ts.mu.Unlock()\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on Unmarshal: %v\", err)\r\n\t\t}\r\n\t\tif routeInfo.Host != expectedHost || routeInfo.Port != expectedPort || routeInfo.IP != expectedIP {\r\n\t\t\tt.Fatalf(\"Expected host/port/IP to be %s:%v, %q, got %s:%d, %q\",\r\n\t\t\t\texpectedHost, expectedPort, expectedIP, routeInfo.Host, routeInfo.Port, routeInfo.IP)\r\n\t\t}\r\n\t\t// Check that server routeInfoJSON was updated too\r\n\t\tif !reflect.DeepEqual(routeInfo, routeInfoJSON) {\r\n\t\t\tt.Fatalf(\"Expected routeInfoJSON to be %+v, got %+v\", routeInfo, routeInfoJSON)\r\n\t\t}\r\n\t}\r\n\r\n\t// Update config with cluster_advertise\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tcluster: {\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t\tcluster_advertise: \"me:1\"\r\n\t}\r\n\t`)\r\n\tverify(\"me\", 1, \"nats-route://me:1/\")\r\n\r\n\t// Update config with cluster_advertise (no port specified)\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tcluster: {\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t\tcluster_advertise: \"me\"\r\n\t}\r\n\t`)\r\n\tverify(\"me\", orgClusterPort, fmt.Sprintf(\"nats-route://me:%d/\", orgClusterPort))\r\n\r\n\t// Update config with cluster_advertise (-1 port specified)\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tcluster: {\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t\tcluster_advertise: \"me:-1\"\r\n\t}\r\n\t`)\r\n\tverify(\"me\", orgClusterPort, fmt.Sprintf(\"nats-route://me:%d/\", orgClusterPort))\r\n\r\n\t// Update to remove cluster_advertise\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tcluster: {\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t}\r\n\t`)\r\n\tverify(\"0.0.0.0\", orgClusterPort, \"\")\r\n}\r\n\r\nfunc TestConfigReloadClusterNoAdvertise(t *testing.T) {\r\n\ts, _, conf := runReloadServerWithContent(t, []byte(`\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t\tclient_advertise: \"me:1\"\r\n\t\tcluster: {\r\n\t\t\tlisten: \"0.0.0.0:-1\"\r\n\t\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\ts.mu.Lock()\r\n\tccurls := s.routeInfo.ClientConnectURLs\r\n\ts.mu.Unlock()\r\n\tif len(ccurls) != 1 && ccurls[0] != \"me:1\" {\r\n\t\tt.Fatalf(\"Unexpected routeInfo.ClientConnectURLS: %v\", ccurls)\r\n\t}\r\n\r\n\t// Update config with no_advertise\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tclient_advertise: \"me:1\"\r\n\tcluster: {\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t\tno_advertise: true\r\n\t}\r\n\t`)\r\n\r\n\ts.mu.Lock()\r\n\tccurls = s.routeInfo.ClientConnectURLs\r\n\ts.mu.Unlock()\r\n\tif len(ccurls) != 0 {\r\n\t\tt.Fatalf(\"Unexpected routeInfo.ClientConnectURLS: %v\", ccurls)\r\n\t}\r\n\r\n\t// Update config with cluster_advertise (no port specified)\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tclient_advertise: \"me:1\"\r\n\tcluster: {\r\n\t\tlisten: \"0.0.0.0:-1\"\r\n\t}\r\n\t`)\r\n\ts.mu.Lock()\r\n\tccurls = s.routeInfo.ClientConnectURLs\r\n\ts.mu.Unlock()\r\n\tif len(ccurls) != 1 && ccurls[0] != \"me:1\" {\r\n\t\tt.Fatalf(\"Unexpected routeInfo.ClientConnectURLS: %v\", ccurls)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadMaxSubsUnsupported(t *testing.T) {\r\n\ts, _, conf := runReloadServerWithContent(t, []byte(`max_subs: 1`))\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tif err := ioutil.WriteFile(conf, []byte(`max_subs: 10`), 0666); err != nil {\r\n\t\tt.Fatalf(\"Error writing config file: %v\", err)\r\n\t}\r\n\tif err := s.Reload(); err == nil {\r\n\t\tt.Fatal(\"Expected Reload to return an error\")\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadClientAdvertise(t *testing.T) {\r\n\ts, _, conf := runReloadServerWithContent(t, []byte(`listen: \"0.0.0.0:-1\"`))\r\n\tdefer os.Remove(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\torgPort := s.Addr().(*net.TCPAddr).Port\r\n\r\n\tverify := func(expectedHost string, expectedPort int) {\r\n\t\ts.mu.Lock()\r\n\t\tinfo := s.info\r\n\t\ts.mu.Unlock()\r\n\t\tif info.Host != expectedHost || info.Port != expectedPort {\r\n\t\t\tstackFatalf(t, \"Expected host/port to be %s:%d, got %s:%d\",\r\n\t\t\t\texpectedHost, expectedPort, info.Host, info.Port)\r\n\t\t}\r\n\t}\r\n\r\n\t// Update config with ClientAdvertise (port specified)\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tclient_advertise: \"me:1\"\r\n\t`)\r\n\tverify(\"me\", 1)\r\n\r\n\t// Update config with ClientAdvertise (no port specified)\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tclient_advertise: \"me\"\r\n\t`)\r\n\tverify(\"me\", orgPort)\r\n\r\n\t// Update config with ClientAdvertise (-1 port specified)\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"0.0.0.0:-1\"\r\n\tclient_advertise: \"me:-1\"\r\n\t`)\r\n\tverify(\"me\", orgPort)\r\n\r\n\t// Now remove ClientAdvertise to check that original values\r\n\t// are restored.\r\n\treloadUpdateConfig(t, s, conf, `listen: \"0.0.0.0:-1\"`)\r\n\tverify(\"0.0.0.0\", orgPort)\r\n}\r\n\r\n// Ensure Reload supports changing the max connections. Test this by starting a\r\n// server with no max connections, connecting two clients, reloading with a\r\n// max connections of one, and ensuring one client is disconnected.\r\nfunc TestConfigReloadMaxConnections(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Make two connections.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, server.Addr().(*net.TCPAddr).Port)\r\n\tnc1, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc1.Close()\r\n\tclosed := make(chan struct{}, 1)\r\n\tnc1.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tclosed <- struct{}{}\r\n\t})\r\n\tnc2, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\tnc2.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tclosed <- struct{}{}\r\n\t})\r\n\r\n\tif numClients := server.NumClients(); numClients != 2 {\r\n\t\tt.Fatalf(\"Expected 2 clients, got %d\", numClients)\r\n\t}\r\n\r\n\t// Set max connections to one.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/max_connections.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure one connection was closed.\r\n\tselect {\r\n\tcase <-closed:\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatal(\"Expected to be disconnected\")\r\n\t}\r\n\r\n\tcheckClientsCount(t, server, 1)\r\n\r\n\t// Ensure new connections fail.\r\n\t_, err = nats.Connect(addr)\r\n\tif err == nil {\r\n\t\tt.Fatal(\"Expected error on connect\")\r\n\t}\r\n}\r\n\r\n// Ensure reload supports changing the max payload size. Test this by starting\r\n// a server with the default size limit, ensuring publishes work, reloading\r\n// with a restrictive limit, and ensuring publishing an oversized message fails\r\n// and disconnects the client.\r\nfunc TestConfigReloadMaxPayload(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, server.Addr().(*net.TCPAddr).Port)\r\n\tnc, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tclosed := make(chan struct{})\r\n\tnc.SetDisconnectHandler(func(*nats.Conn) {\r\n\t\tclosed <- struct{}{}\r\n\t})\r\n\r\n\tconn, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer conn.Close()\r\n\tsub, err := conn.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error subscribing: %v\", err)\r\n\t}\r\n\tconn.Flush()\r\n\r\n\t// Ensure we can publish as a sanity check.\r\n\tif err := nc.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\t_, err = sub.NextMsg(2 * time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error receiving message: %v\", err)\r\n\t}\r\n\r\n\t// Set max payload to one.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/max_payload.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Ensure oversized messages don't get delivered and the client is\r\n\t// disconnected.\r\n\tif err := nc.Publish(\"foo\", []byte(\"hello\")); err != nil {\r\n\t\tt.Fatalf(\"Error publishing: %v\", err)\r\n\t}\r\n\tnc.Flush()\r\n\t_, err = sub.NextMsg(20 * time.Millisecond)\r\n\tif err != nats.ErrTimeout {\r\n\t\tt.Fatalf(\"Expected ErrTimeout, got: %v\", err)\r\n\t}\r\n\r\n\tselect {\r\n\tcase <-closed:\r\n\tcase <-time.After(5 * time.Second):\r\n\t\tt.Fatal(\"Expected to be disconnected\")\r\n\t}\r\n}\r\n\r\n// Ensure reload supports rotating out files. Test this by starting\r\n// a server with log and pid files, reloading new ones, then check that\r\n// we can rename and delete the old log/pid files.\r\nfunc TestConfigReloadRotateFiles(t *testing.T) {\r\n\tserver, _, config := runReloadServerWithConfig(t, \"./configs/reload/file_rotate.conf\")\r\n\tdefer func() {\r\n\t\tos.Remove(config)\r\n\t\tos.Remove(\"log.txt\")\r\n\t\tos.Remove(\"nats-server.pid\")\r\n\t\tos.Remove(\"log1.txt\")\r\n\t\tos.Remove(\"nats-server1.pid\")\r\n\t}()\r\n\tdefer server.Shutdown()\r\n\r\n\t// Configure the logger to enable actual logging\r\n\topts := server.getOpts()\r\n\topts.NoLog = false\r\n\tserver.ConfigureLogger()\r\n\r\n\t// Load a config that renames the files.\r\n\tchangeCurrentConfigContent(t, config, \"./configs/reload/file_rotate1.conf\")\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config: %v\", err)\r\n\t}\r\n\r\n\t// Make sure the new files exist.\r\n\tif _, err := os.Stat(\"log1.txt\"); os.IsNotExist(err) {\r\n\t\tt.Fatalf(\"Error reloading config, no new file: %v\", err)\r\n\t}\r\n\tif _, err := os.Stat(\"nats-server1.pid\"); os.IsNotExist(err) {\r\n\t\tt.Fatalf(\"Error reloading config, no new file: %v\", err)\r\n\t}\r\n\r\n\t// Check that old file can be renamed.\r\n\tif err := os.Rename(\"log.txt\", \"log_old.txt\"); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config, cannot rename file: %v\", err)\r\n\t}\r\n\tif err := os.Rename(\"nats-server.pid\", \"nats-server_old.pid\"); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config, cannot rename file: %v\", err)\r\n\t}\r\n\r\n\t// Check that the old files can be removed after rename.\r\n\tif err := os.Remove(\"log_old.txt\"); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config, cannot delete file: %v\", err)\r\n\t}\r\n\tif err := os.Remove(\"nats-server_old.pid\"); err != nil {\r\n\t\tt.Fatalf(\"Error reloading config, cannot delete file: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadClusterWorks(t *testing.T) {\r\n\tconfBTemplate := `\r\n\t\tlisten: -1\r\n\t\tcluster: {\r\n\t\t\tlisten: 127.0.0.1:7244\r\n\t\t\tauthorization {\r\n\t\t\t\tuser: ruser\r\n\t\t\t\tpassword: pwd\r\n\t\t\t\ttimeout: %d\r\n\t\t\t}\r\n\t\t\troutes = [\r\n\t\t\t\tnats-route://ruser:pwd@127.0.0.1:7246\r\n\t\t\t]\r\n\t\t}`\r\n\tconfB := createConfFile(t, []byte(fmt.Sprintf(confBTemplate, 3)))\r\n\tdefer os.Remove(confB)\r\n\r\n\tconfATemplate := `\r\n\t\tlisten: -1\r\n\t\tcluster: {\r\n\t\t\tlisten: 127.0.0.1:7246\r\n\t\t\tauthorization {\r\n\t\t\t\tuser: ruser\r\n\t\t\t\tpassword: pwd\r\n\t\t\t\ttimeout: %d\r\n\t\t\t}\r\n\t\t\troutes = [\r\n\t\t\t\tnats-route://ruser:pwd@127.0.0.1:7244\r\n\t\t\t]\r\n\t\t}`\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(confATemplate, 3)))\r\n\tdefer os.Remove(confA)\r\n\r\n\tsrvb, _ := RunServerWithConfig(confB)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tsrva, _ := RunServerWithConfig(confA)\r\n\tdefer srva.Shutdown()\r\n\r\n\t// Wait for the cluster to form and capture the connection IDs of each route\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\tgetCID := func(s *Server) uint64 {\r\n\t\ts.mu.Lock()\r\n\t\tdefer s.mu.Unlock()\r\n\t\tfor _, r := range s.routes {\r\n\t\t\treturn r.cid\r\n\t\t}\r\n\t\treturn 0\r\n\t}\r\n\tacid := getCID(srva)\r\n\tbcid := getCID(srvb)\r\n\r\n\t// Update auth timeout to force a check of the connected route auth\r\n\treloadUpdateConfig(t, srvb, confB, fmt.Sprintf(confBTemplate, 5))\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, 5))\r\n\r\n\t// Wait a little bit to ensure that there is no issue with connection\r\n\t// breaking at this point (this was an issue before).\r\n\ttime.Sleep(100 * time.Millisecond)\r\n\r\n\t// Cluster should still exist\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\t// Check that routes were not re-created\r\n\tnewacid := getCID(srva)\r\n\tnewbcid := getCID(srvb)\r\n\r\n\tif newacid != acid {\r\n\t\tt.Fatalf(\"Expected server A route ID to be %v, got %v\", acid, newacid)\r\n\t}\r\n\tif newbcid != bcid {\r\n\t\tt.Fatalf(\"Expected server B route ID to be %v, got %v\", bcid, newbcid)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadClusterPerms(t *testing.T) {\r\n\tconfATemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\timport {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t\texport {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(confATemplate, `\"foo\"`, `\"foo\"`)))\r\n\tdefer os.Remove(confA)\r\n\tsrva, _ := RunServerWithConfig(confA)\r\n\tdefer srva.Shutdown()\r\n\r\n\tconfBTemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\timport {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t\texport {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\troutes = [\r\n\t\t\t\t\"nats://127.0.0.1:%d\"\r\n\t\t\t]\r\n\t\t}\r\n\t`\r\n\tconfB := createConfFile(t, []byte(fmt.Sprintf(confBTemplate, `\"foo\"`, `\"foo\"`, srva.ClusterAddr().Port)))\r\n\tdefer os.Remove(confB)\r\n\tsrvb, _ := RunServerWithConfig(confB)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\t// Create a connection on A\r\n\tnca, err := nats.Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", srva.Addr().(*net.TCPAddr).Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nca.Close()\r\n\t// Create a subscription on \"foo\" and \"bar\", only \"foo\" will be also on server B.\r\n\tsubFooOnA, err := nca.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tsubBarOnA, err := nca.SubscribeSync(\"bar\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\t// Connect on B and do the same\r\n\tncb, err := nats.Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", srvb.Addr().(*net.TCPAddr).Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncb.Close()\r\n\t// Create a subscription on \"foo\" and \"bar\", only \"foo\" will be also on server B.\r\n\tsubFooOnB, err := ncb.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tsubBarOnB, err := ncb.SubscribeSync(\"bar\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\t// Check subscriptions on each server. There should be 3 on each server,\r\n\t// foo and bar locally and foo from remote server.\r\n\tcheckExpectedSubs(t, 3, srva, srvb)\r\n\r\n\tsendMsg := func(t *testing.T, subj string, nc *nats.Conn) {\r\n\t\tt.Helper()\r\n\t\tif err := nc.Publish(subj, []byte(\"msg\")); err != nil {\r\n\t\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\tcheckSub := func(t *testing.T, sub *nats.Subscription, shouldReceive bool) {\r\n\t\tt.Helper()\r\n\t\t_, err := sub.NextMsg(100 * time.Millisecond)\r\n\t\tif shouldReceive && err != nil {\r\n\t\t\tt.Fatalf(\"Expected message on %q, got %v\", sub.Subject, err)\r\n\t\t} else if !shouldReceive && err == nil {\r\n\t\t\tt.Fatalf(\"Expected no message on %q, got one\", sub.Subject)\r\n\t\t}\r\n\t}\r\n\r\n\t// Produce from A and check received on both sides\r\n\tsendMsg(t, \"foo\", nca)\r\n\tcheckSub(t, subFooOnA, true)\r\n\tcheckSub(t, subFooOnB, true)\r\n\t// Now from B:\r\n\tsendMsg(t, \"foo\", ncb)\r\n\tcheckSub(t, subFooOnA, true)\r\n\tcheckSub(t, subFooOnB, true)\r\n\r\n\t// Publish on bar from A and make sure only local sub receives\r\n\tsendMsg(t, \"bar\", nca)\r\n\tcheckSub(t, subBarOnA, true)\r\n\tcheckSub(t, subBarOnB, false)\r\n\r\n\t// Publish on bar from B and make sure only local sub receives\r\n\tsendMsg(t, \"bar\", ncb)\r\n\tcheckSub(t, subBarOnA, false)\r\n\tcheckSub(t, subBarOnB, true)\r\n\r\n\t// We will now both import/export foo and bar. Start with reloading A.\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`, `[\"foo\", \"bar\"]`))\r\n\r\n\t// Since B has not been updated yet, the state should remain the same,\r\n\t// that is 3 subs on each server.\r\n\tcheckExpectedSubs(t, 3, srva, srvb)\r\n\r\n\t// Now update and reload B. Add \"baz\" for another test down below\r\n\treloadUpdateConfig(t, srvb, confB, fmt.Sprintf(confBTemplate, `[\"foo\", \"bar\", \"baz\"]`, `[\"foo\", \"bar\", \"baz\"]`, srva.ClusterAddr().Port))\r\n\r\n\t// Now 4 on each server\r\n\tcheckExpectedSubs(t, 4, srva, srvb)\r\n\r\n\t// Make sure that we can receive all messages\r\n\tsendMsg(t, \"foo\", nca)\r\n\tcheckSub(t, subFooOnA, true)\r\n\tcheckSub(t, subFooOnB, true)\r\n\tsendMsg(t, \"foo\", ncb)\r\n\tcheckSub(t, subFooOnA, true)\r\n\tcheckSub(t, subFooOnB, true)\r\n\r\n\tsendMsg(t, \"bar\", nca)\r\n\tcheckSub(t, subBarOnA, true)\r\n\tcheckSub(t, subBarOnB, true)\r\n\tsendMsg(t, \"bar\", ncb)\r\n\tcheckSub(t, subBarOnA, true)\r\n\tcheckSub(t, subBarOnB, true)\r\n\r\n\t// Create subscription on baz on server B.\r\n\tsubBazOnB, err := ncb.SubscribeSync(\"baz\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\t// Check subscriptions count\r\n\tcheckExpectedSubs(t, 5, srvb)\r\n\tcheckExpectedSubs(t, 4, srva)\r\n\r\n\tsendMsg(t, \"baz\", nca)\r\n\tcheckSub(t, subBazOnB, false)\r\n\tsendMsg(t, \"baz\", ncb)\r\n\tcheckSub(t, subBazOnB, true)\r\n\r\n\t// Test UNSUB by denying something that was previously imported\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"foo\"`, `[\"foo\", \"bar\"]`))\r\n\t// Since A no longer imports \"bar\", we should have one less subscription\r\n\t// on B (B will have received an UNSUB for bar)\r\n\tcheckExpectedSubs(t, 4, srvb)\r\n\t// A, however, should still have same number of subs.\r\n\tcheckExpectedSubs(t, 4, srva)\r\n\r\n\t// Remove all permissions from A.\r\n\treloadUpdateConfig(t, srva, confA, `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t}\r\n\t`)\r\n\t// Server A should now have baz sub\r\n\tcheckExpectedSubs(t, 5, srvb)\r\n\tcheckExpectedSubs(t, 5, srva)\r\n\r\n\tsendMsg(t, \"baz\", nca)\r\n\tcheckSub(t, subBazOnB, true)\r\n\tsendMsg(t, \"baz\", ncb)\r\n\tcheckSub(t, subBazOnB, true)\r\n\r\n\t// Finally, remove permissions from B\r\n\treloadUpdateConfig(t, srvb, confB, fmt.Sprintf(`\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\troutes = [\r\n\t\t\t\t\"nats://127.0.0.1:%d\"\r\n\t\t\t]\r\n\t\t}\r\n\t`, srva.ClusterAddr().Port))\r\n\t// Check expected subscriptions count.\r\n\tcheckExpectedSubs(t, 5, srvb)\r\n\tcheckExpectedSubs(t, 5, srva)\r\n}\r\n\r\nfunc TestConfigReloadClusterPermsImport(t *testing.T) {\r\n\tconfATemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\timport: {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`)))\r\n\tdefer os.Remove(confA)\r\n\tsrva, _ := RunServerWithConfig(confA)\r\n\tdefer srva.Shutdown()\r\n\r\n\tconfBTemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\troutes = [\r\n\t\t\t\t\"nats://127.0.0.1:%d\"\r\n\t\t\t]\r\n\t\t}\r\n\t`\r\n\tconfB := createConfFile(t, []byte(fmt.Sprintf(confBTemplate, srva.ClusterAddr().Port)))\r\n\tdefer os.Remove(confB)\r\n\tsrvb, _ := RunServerWithConfig(confB)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\t// Create a connection on A\r\n\tnca, err := nats.Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", srva.Addr().(*net.TCPAddr).Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nca.Close()\r\n\t// Create a subscription on \"foo\" and \"bar\"\r\n\tif _, err := nca.SubscribeSync(\"foo\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tif _, err := nca.SubscribeSync(\"bar\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\tcheckExpectedSubs(t, 2, srva, srvb)\r\n\r\n\t// Drop foo\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"bar\"`))\r\n\tcheckExpectedSubs(t, 2, srva)\r\n\tcheckExpectedSubs(t, 1, srvb)\r\n\r\n\t// Add it back\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`))\r\n\tcheckExpectedSubs(t, 2, srva, srvb)\r\n\r\n\t// Empty Import means implicit allow\r\n\treloadUpdateConfig(t, srva, confA, `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\texport: \">\"\r\n\t\t\t}\r\n\t\t}\r\n\t`)\r\n\tcheckExpectedSubs(t, 2, srva, srvb)\r\n\r\n\tconfATemplate = `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\timport: {\r\n\t\t\t\t\tallow: [\"foo\", \"bar\"]\r\n\t\t\t\t\tdeny: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\t// Now deny all:\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`))\r\n\tcheckExpectedSubs(t, 2, srva)\r\n\tcheckExpectedSubs(t, 0, srvb)\r\n\r\n\t// Drop foo from the deny list\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"bar\"`))\r\n\tcheckExpectedSubs(t, 2, srva)\r\n\tcheckExpectedSubs(t, 1, srvb)\r\n}\r\n\r\nfunc TestConfigReloadClusterPermsExport(t *testing.T) {\r\n\tconfATemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\texport: {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`)))\r\n\tdefer os.Remove(confA)\r\n\tsrva, _ := RunServerWithConfig(confA)\r\n\tdefer srva.Shutdown()\r\n\r\n\tconfBTemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\troutes = [\r\n\t\t\t\t\"nats://127.0.0.1:%d\"\r\n\t\t\t]\r\n\t\t}\r\n\t`\r\n\tconfB := createConfFile(t, []byte(fmt.Sprintf(confBTemplate, srva.ClusterAddr().Port)))\r\n\tdefer os.Remove(confB)\r\n\tsrvb, _ := RunServerWithConfig(confB)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\t// Create a connection on B\r\n\tncb, err := nats.Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", srvb.Addr().(*net.TCPAddr).Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncb.Close()\r\n\t// Create a subscription on \"foo\" and \"bar\"\r\n\tif _, err := ncb.SubscribeSync(\"foo\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tif _, err := ncb.SubscribeSync(\"bar\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\tcheckExpectedSubs(t, 2, srva, srvb)\r\n\r\n\t// Drop foo\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"bar\"`))\r\n\tcheckExpectedSubs(t, 2, srvb)\r\n\tcheckExpectedSubs(t, 1, srva)\r\n\r\n\t// Add it back\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`))\r\n\tcheckExpectedSubs(t, 2, srva, srvb)\r\n\r\n\t// Empty Export means implicit allow\r\n\treloadUpdateConfig(t, srva, confA, `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\timport: \">\"\r\n\t\t\t}\r\n\t\t}\r\n\t`)\r\n\tcheckExpectedSubs(t, 2, srva, srvb)\r\n\r\n\tconfATemplate = `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\texport: {\r\n\t\t\t\t\tallow: [\"foo\", \"bar\"]\r\n\t\t\t\t\tdeny: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\t// Now deny all:\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`))\r\n\tcheckExpectedSubs(t, 0, srva)\r\n\tcheckExpectedSubs(t, 2, srvb)\r\n\r\n\t// Drop foo from the deny list\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"bar\"`))\r\n\tcheckExpectedSubs(t, 1, srva)\r\n\tcheckExpectedSubs(t, 2, srvb)\r\n}\r\n\r\nfunc TestConfigReloadClusterPermsOldServer(t *testing.T) {\r\n\tconfATemplate := `\r\n\t\tport: -1\r\n\t\tcluster {\r\n\t\t\tlisten: 127.0.0.1:-1\r\n\t\t\tpermissions {\r\n\t\t\t\texport: {\r\n\t\t\t\t\tallow: %s\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\tconfA := createConfFile(t, []byte(fmt.Sprintf(confATemplate, `[\"foo\", \"bar\"]`)))\r\n\tdefer os.Remove(confA)\r\n\tsrva, _ := RunServerWithConfig(confA)\r\n\tdefer srva.Shutdown()\r\n\r\n\toptsB := DefaultOptions()\r\n\toptsB.Routes = RoutesFromStr(fmt.Sprintf(\"nats://127.0.0.1:%d\", srva.ClusterAddr().Port))\r\n\t// Make server B behave like an old server\r\n\toptsB.routeProto = setRouteProtoForTest(RouteProtoZero)\r\n\tsrvb := RunServer(optsB)\r\n\tdefer srvb.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb)\r\n\r\n\t// Get the route's connection ID\r\n\tgetRouteRID := func() uint64 {\r\n\t\trid := uint64(0)\r\n\t\tsrvb.mu.Lock()\r\n\t\tfor _, r := range srvb.routes {\r\n\t\t\tr.mu.Lock()\r\n\t\t\trid = r.cid\r\n\t\t\tr.mu.Unlock()\r\n\t\t\tbreak\r\n\t\t}\r\n\t\tsrvb.mu.Unlock()\r\n\t\treturn rid\r\n\t}\r\n\torgRID := getRouteRID()\r\n\r\n\t// Cause a config reload on A\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"bar\"`))\r\n\r\n\t// Check that new route gets created\r\n\tcheck := func(t *testing.T) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, 3*time.Second, 15*time.Millisecond, func() error {\r\n\t\t\tif rid := getRouteRID(); rid == orgRID {\r\n\t\t\t\treturn fmt.Errorf(\"Route does not seem to have been recreated\")\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\tcheck(t)\r\n\r\n\t// Save the current value\r\n\torgRID = getRouteRID()\r\n\r\n\t// Add another server that supports INFO updates\r\n\r\n\toptsC := DefaultOptions()\r\n\toptsC.Routes = RoutesFromStr(fmt.Sprintf(\"nats://127.0.0.1:%d\", srva.ClusterAddr().Port))\r\n\tsrvc := RunServer(optsC)\r\n\tdefer srvc.Shutdown()\r\n\r\n\tcheckClusterFormed(t, srva, srvb, srvc)\r\n\r\n\t// Cause a config reload on A\r\n\treloadUpdateConfig(t, srva, confA, fmt.Sprintf(confATemplate, `\"foo\"`))\r\n\t// Check that new route gets created\r\n\tcheck(t)\r\n}\r\n\r\nfunc TestConfigReloadAccountUsers(t *testing.T) {\r\n\tconf := createConfFile(t, []byte(`\r\n\tlisten: \"127.0.0.1:-1\"\r\n\taccounts {\r\n\t\tsynadia {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: derek, password: derek}\r\n\t\t\t\t{user: foo, password: foo}\r\n\t\t\t]\r\n\t\t}\r\n\t\tnats.io {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: ivan, password: ivan}\r\n\t\t\t\t{user: bar, password: bar}\r\n\t\t\t]\r\n\t\t}\r\n\t\tacc_deleted_after_reload {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: gone, password: soon}\r\n\t\t\t\t{user: baz, password: baz}\r\n\t\t\t\t{user: bat, password: bat}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Connect as exisiting users, should work.\r\n\tnc, err := nats.Connect(fmt.Sprintf(\"nats://derek:derek@%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\tch := make(chan bool, 2)\r\n\tcb := func(_ *nats.Conn) {\r\n\t\tch <- true\r\n\t}\r\n\tnc2, err := nats.Connect(\r\n\t\tfmt.Sprintf(\"nats://ivan:ivan@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.NoReconnect(),\r\n\t\tnats.ClosedHandler(cb))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\tnc3, err := nats.Connect(\r\n\t\tfmt.Sprintf(\"nats://gone:soon@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.NoReconnect(),\r\n\t\tnats.ClosedHandler(cb))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc3.Close()\r\n\t// These users will be moved from an account to another (to a specific or to global account)\r\n\t// We will create subscriptions to ensure that they are moved to proper sublists too.\r\n\trch := make(chan bool, 4)\r\n\trcb := func(_ *nats.Conn) {\r\n\t\trch <- true\r\n\t}\r\n\tnc4, err := nats.Connect(fmt.Sprintf(\"nats://foo:foo@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.ReconnectWait(50*time.Millisecond), nats.ReconnectHandler(rcb))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc4.Close()\r\n\tif _, err := nc4.SubscribeSync(\"foo\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tnc5, err := nats.Connect(fmt.Sprintf(\"nats://bar:bar@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.ReconnectWait(50*time.Millisecond), nats.ReconnectHandler(rcb))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc5.Close()\r\n\tif _, err := nc5.SubscribeSync(\"bar\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tnc6, err := nats.Connect(fmt.Sprintf(\"nats://baz:baz@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.ReconnectWait(50*time.Millisecond), nats.ReconnectHandler(rcb))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc6.Close()\r\n\tif _, err := nc6.SubscribeSync(\"baz\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tnc7, err := nats.Connect(fmt.Sprintf(\"nats://bat:bat@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.ReconnectWait(50*time.Millisecond), nats.ReconnectHandler(rcb))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc7.Close()\r\n\tif _, err := nc7.SubscribeSync(\"bat\"); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\t// Remove user from account and whole account\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\tauthorization {\r\n\t\tusers = [\r\n\t\t\t{user: foo, password: foo}\r\n\t\t\t{user: baz, password: baz}\r\n\t\t]\r\n\t}\r\n\taccounts {\r\n\t\tsynadia {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: derek, password: derek}\r\n\t\t\t\t{user: bar, password: bar}\r\n\t\t\t]\r\n\t\t}\r\n\t\tnats.io {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: bat, password: bat}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`)\r\n\t// nc2 and nc3 should be closed\r\n\tif err := wait(ch); err != nil {\r\n\t\tt.Fatal(\"Did not get the closed callback\")\r\n\t}\r\n\tif err := wait(ch); err != nil {\r\n\t\tt.Fatal(\"Did not get the closed callback\")\r\n\t}\r\n\t// And first connection should still be connected\r\n\tif !nc.IsConnected() {\r\n\t\tt.Fatal(\"First connection should still be connected\")\r\n\t}\r\n\r\n\t// Old account should be gone\r\n\tif _, err := s.LookupAccount(\"acc_deleted_after_reload\"); err == nil {\r\n\t\tt.Fatal(\"old account should be gone\")\r\n\t}\r\n\r\n\t// Check subscriptions. Since most of the users have been\r\n\t// moving accounts, make sure we account for the reconnect\r\n\tfor i := 0; i < 4; i++ {\r\n\t\tif err := wait(rch); err != nil {\r\n\t\t\tt.Fatal(\"Did not get the reconnect cb\")\r\n\t\t}\r\n\t}\r\n\t// Still need to do the tests in a checkFor() because clients\r\n\t// being reconnected does not mean that resent of subscriptions\r\n\t// has already been processed.\r\n\tcheckFor(t, 2*time.Second, 100*time.Millisecond, func() error {\r\n\t\tgAcc, _ := s.LookupAccount(globalAccountName)\r\n\t\tgAcc.mu.RLock()\r\n\t\tn := gAcc.sl.Count()\r\n\t\tfooMatch := gAcc.sl.Match(\"foo\")\r\n\t\tbazMatch := gAcc.sl.Match(\"baz\")\r\n\t\tgAcc.mu.RUnlock()\r\n\t\tif n != 2 {\r\n\t\t\treturn fmt.Errorf(\"Global account should have 2 subs, got %v\", n)\r\n\t\t}\r\n\t\tif len(fooMatch.psubs) != 1 {\r\n\t\t\treturn fmt.Errorf(\"Global account should have foo sub\")\r\n\t\t}\r\n\t\tif len(bazMatch.psubs) != 1 {\r\n\t\t\treturn fmt.Errorf(\"Global account should have baz sub\")\r\n\t\t}\r\n\r\n\t\tsAcc, _ := s.LookupAccount(\"synadia\")\r\n\t\tsAcc.mu.RLock()\r\n\t\tn = sAcc.sl.Count()\r\n\t\tbarMatch := sAcc.sl.Match(\"bar\")\r\n\t\tsAcc.mu.RUnlock()\r\n\t\tif n != 1 {\r\n\t\t\treturn fmt.Errorf(\"Synadia account should have 1 sub, got %v\", n)\r\n\t\t}\r\n\t\tif len(barMatch.psubs) != 1 {\r\n\t\t\treturn fmt.Errorf(\"Synadia account should have bar sub\")\r\n\t\t}\r\n\r\n\t\tnAcc, _ := s.LookupAccount(\"nats.io\")\r\n\t\tnAcc.mu.RLock()\r\n\t\tn = nAcc.sl.Count()\r\n\t\tbatMatch := nAcc.sl.Match(\"bat\")\r\n\t\tnAcc.mu.RUnlock()\r\n\t\tif n != 1 {\r\n\t\t\treturn fmt.Errorf(\"Nats.io account should have 1 sub, got %v\", n)\r\n\t\t}\r\n\t\tif len(batMatch.psubs) != 1 {\r\n\t\t\treturn fmt.Errorf(\"Synadia account should have bar sub\")\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n}\r\n\r\nfunc TestConfigReloadAccountNKeyUsers(t *testing.T) {\r\n\tconf := createConfFile(t, []byte(`\r\n\tlisten: \"127.0.0.1:-1\"\r\n\taccounts {\r\n\t\tsynadia {\r\n\t\t\tusers = [\r\n\t\t\t\t# Derek\r\n\t\t\t\t{nkey : UCNGL4W5QX66CFX6A6DCBVDH5VOHMI7B2UZZU7TXAUQQSI2JPHULCKBR}\r\n\t\t\t]\r\n\t\t}\r\n\t\tnats.io {\r\n\t\t\tusers = [\r\n\t\t\t\t# Ivan\r\n\t\t\t\t{nkey : UDPGQVFIWZ7Q5UH4I5E6DBCZULQS6VTVBG6CYBD7JV3G3N2GMQOMNAUH}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tsynadia, _ := s.LookupAccount(\"synadia\")\r\n\tnats, _ := s.LookupAccount(\"nats.io\")\r\n\r\n\tseed1 := []byte(\"SUAPM67TC4RHQLKBX55NIQXSMATZDOZK6FNEOSS36CAYA7F7TY66LP4BOM\")\r\n\tseed2 := []byte(\"SUAIS5JPX4X4GJ7EIIJEQ56DH2GWPYJRPWN5XJEDENJOZHCBLI7SEPUQDE\")\r\n\r\n\tkp, _ := nkeys.FromSeed(seed1)\r\n\tpubKey, _ := kp.PublicKey()\r\n\r\n\tc, cr, l := newClientForServer(s)\r\n\tdefer c.close()\r\n\t// Check for Nonce\r\n\tvar info nonceInfo\r\n\tif err := json.Unmarshal([]byte(l[5:]), &info); err != nil {\r\n\t\tt.Fatalf(\"Could not parse INFO json: %v\\n\", err)\r\n\t}\r\n\tif info.Nonce == \"\" {\r\n\t\tt.Fatalf(\"Expected a non-empty nonce with nkeys defined\")\r\n\t}\r\n\tsigraw, err := kp.Sign([]byte(info.Nonce))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Failed signing nonce: %v\", err)\r\n\t}\r\n\tsig := base64.RawURLEncoding.EncodeToString(sigraw)\r\n\r\n\t// PING needed to flush the +OK to us.\r\n\tcs := fmt.Sprintf(\"CONNECT {\\\"nkey\\\":%q,\\\"sig\\\":\\\"%s\\\",\\\"verbose\\\":true,\\\"pedantic\\\":true}\\r\\nPING\\r\\n\", pubKey, sig)\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"+OK\") {\r\n\t\tt.Fatalf(\"Expected an OK, got: %v\", l)\r\n\t}\r\n\tif c.acc != synadia {\r\n\t\tt.Fatalf(\"Expected the nkey client's account to match 'synadia', got %v\", c.acc)\r\n\t}\r\n\r\n\t// Now nats account nkey user.\r\n\tkp, _ = nkeys.FromSeed(seed2)\r\n\tpubKey, _ = kp.PublicKey()\r\n\r\n\tc, cr, l = newClientForServer(s)\r\n\tdefer c.close()\r\n\t// Check for Nonce\r\n\terr = json.Unmarshal([]byte(l[5:]), &info)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Could not parse INFO json: %v\\n\", err)\r\n\t}\r\n\tif info.Nonce == \"\" {\r\n\t\tt.Fatalf(\"Expected a non-empty nonce with nkeys defined\")\r\n\t}\r\n\tsigraw, err = kp.Sign([]byte(info.Nonce))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Failed signing nonce: %v\", err)\r\n\t}\r\n\tsig = base64.RawURLEncoding.EncodeToString(sigraw)\r\n\r\n\t// PING needed to flush the +OK to us.\r\n\tcs = fmt.Sprintf(\"CONNECT {\\\"nkey\\\":%q,\\\"sig\\\":\\\"%s\\\",\\\"verbose\\\":true,\\\"pedantic\\\":true}\\r\\nPING\\r\\n\", pubKey, sig)\r\n\tc.parseAsync(cs)\r\n\tl, _ = cr.ReadString('\\n')\r\n\tif !strings.HasPrefix(l, \"+OK\") {\r\n\t\tt.Fatalf(\"Expected an OK, got: %v\", l)\r\n\t}\r\n\tif c.acc != nats {\r\n\t\tt.Fatalf(\"Expected the nkey client's account to match 'nats', got %v\", c.acc)\r\n\t}\r\n\r\n\t// Remove user from account and whole account\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\tauthorization {\r\n\t\tusers = [\r\n\t\t\t# Ivan\r\n\t\t\t{nkey : UDPGQVFIWZ7Q5UH4I5E6DBCZULQS6VTVBG6CYBD7JV3G3N2GMQOMNAUH}\r\n\t\t]\r\n\t}\r\n\taccounts {\r\n\t\tnats.io {\r\n\t\t\tusers = [\r\n\t\t\t\t# Derek\r\n\t\t\t\t{nkey : UCNGL4W5QX66CFX6A6DCBVDH5VOHMI7B2UZZU7TXAUQQSI2JPHULCKBR}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`)\r\n\r\n\ts.mu.Lock()\r\n\tnkeys := s.nkeys\r\n\tglobalAcc := s.gacc\r\n\ts.mu.Unlock()\r\n\r\n\tif n := len(nkeys); n != 2 {\r\n\t\tt.Fatalf(\"NKeys map should have 2 users, got %v\", n)\r\n\t}\r\n\tderek := nkeys[\"UCNGL4W5QX66CFX6A6DCBVDH5VOHMI7B2UZZU7TXAUQQSI2JPHULCKBR\"]\r\n\tif derek == nil {\r\n\t\tt.Fatal(\"NKey for user Derek not found\")\r\n\t}\r\n\tif derek.Account == nil || derek.Account.Name != \"nats.io\" {\r\n\t\tt.Fatalf(\"Invalid account for user Derek: %#v\", derek.Account)\r\n\t}\r\n\tivan := nkeys[\"UDPGQVFIWZ7Q5UH4I5E6DBCZULQS6VTVBG6CYBD7JV3G3N2GMQOMNAUH\"]\r\n\tif ivan == nil {\r\n\t\tt.Fatal(\"NKey for user Ivan not found\")\r\n\t}\r\n\tif ivan.Account != globalAcc {\r\n\t\tt.Fatalf(\"Invalid account for user Ivan: %#v\", ivan.Account)\r\n\t}\r\n\tif _, err := s.LookupAccount(\"synadia\"); err == nil {\r\n\t\tt.Fatal(\"Account Synadia should have been removed\")\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadAccountStreamsImportExport(t *testing.T) {\r\n\ttemplate := `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\taccounts {\r\n\t\tsynadia {\r\n\t\t\tusers [{user: derek, password: foo}]\r\n\t\t\texports = [\r\n\t\t\t\t{stream: \"private.>\", accounts: [nats.io]}\r\n\t\t\t\t{stream: %s}\r\n\t\t\t]\r\n\t\t}\r\n\t\tnats.io {\r\n\t\t\tusers [\r\n\t\t\t\t{user: ivan, password: bar, permissions: {subscribe: {deny: %s}}}\r\n\t\t\t]\r\n\t\t\timports = [\r\n\t\t\t\t{stream: {account: \"synadia\", subject: %s}}\r\n\t\t\t\t{stream: {account: \"synadia\", subject: \"private.natsio.*\"}, prefix: %s}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`\r\n\t// synadia account exports \"private.>\" to nats.io\r\n\t// synadia account exports \"foo.*\"\r\n\t// user ivan denies subscription on \"xxx\"\r\n\t// nats.io account imports \"foo.*\" from synadia\r\n\t// nats.io account imports \"private.natsio.*\" from synadia with prefix \"ivan\"\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(template, `\"foo.*\"`, `\"xxx\"`, `\"foo.*\"`, `\"ivan\"`)))\r\n\tdefer os.Remove(conf)\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tderek, err := nats.Connect(fmt.Sprintf(\"nats://derek:foo@%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer derek.Close()\r\n\tcheckClientsCount(t, s, 1)\r\n\r\n\tch := make(chan bool, 1)\r\n\tivan, err := nats.Connect(fmt.Sprintf(\"nats://ivan:bar@%s:%d\", opts.Host, opts.Port),\r\n\t\tnats.ErrorHandler(func(_ *nats.Conn, _ *nats.Subscription, err error) {\r\n\t\t\tif strings.Contains(strings.ToLower(err.Error()), \"permissions violation\") {\r\n\t\t\t\tch <- true\r\n\t\t\t}\r\n\t\t}))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ivan.Close()\r\n\tcheckClientsCount(t, s, 2)\r\n\r\n\tsubscribe := func(t *testing.T, nc *nats.Conn, subj string) *nats.Subscription {\r\n\t\tt.Helper()\r\n\t\ts, err := nc.SubscribeSync(subj)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t\t}\r\n\t\treturn s\r\n\t}\r\n\r\n\tsubFooBar := subscribe(t, ivan, \"foo.bar\")\r\n\tsubFooBaz := subscribe(t, ivan, \"foo.baz\")\r\n\tsubFooBat := subscribe(t, ivan, \"foo.bat\")\r\n\tsubPriv := subscribe(t, ivan, \"ivan.private.natsio.*\")\r\n\tivan.Flush()\r\n\r\n\tpublish := func(t *testing.T, nc *nats.Conn, subj string) {\r\n\t\tt.Helper()\r\n\t\tif err := nc.Publish(subj, []byte(\"hello\")); err != nil {\r\n\t\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\tnextMsg := func(t *testing.T, sub *nats.Subscription, expected bool) {\r\n\t\tt.Helper()\r\n\t\tdur := 100 * time.Millisecond\r\n\t\tif expected {\r\n\t\t\tdur = time.Second\r\n\t\t}\r\n\t\t_, err := sub.NextMsg(dur)\r\n\t\tif expected && err != nil {\r\n\t\t\tt.Fatalf(\"Expected a message on %s, got %v\", sub.Subject, err)\r\n\t\t} else if !expected && err != nats.ErrTimeout {\r\n\t\t\tt.Fatalf(\"Expected a timeout on %s, got %v\", sub.Subject, err)\r\n\t\t}\r\n\t}\r\n\r\n\t// Checks the derek's user sublist for presence of given subject\r\n\t// interest. Boolean says if interest is expected or not.\r\n\tcheckSublist := func(t *testing.T, subject string, shouldBeThere bool) {\r\n\t\tt.Helper()\r\n\t\tdcli := s.getClient(1)\r\n\t\tdcli.mu.Lock()\r\n\t\tr := dcli.acc.sl.Match(subject)\r\n\t\tdcli.mu.Unlock()\r\n\t\tif shouldBeThere && len(r.psubs) != 1 {\r\n\t\t\tt.Fatalf(\"%s should have 1 match in derek's sublist, got %v\", subject, len(r.psubs))\r\n\t\t} else if !shouldBeThere && len(r.psubs) > 0 {\r\n\t\t\tt.Fatalf(\"%s should not be in derek's sublist\", subject)\r\n\t\t}\r\n\t}\r\n\r\n\t// Publish on all subjects and the subs should receive and\r\n\t// subjects should be in sublist\r\n\tpublish(t, derek, \"foo.bar\")\r\n\tnextMsg(t, subFooBar, true)\r\n\tcheckSublist(t, \"foo.bar\", true)\r\n\r\n\tpublish(t, derek, \"foo.baz\")\r\n\tnextMsg(t, subFooBaz, true)\r\n\tcheckSublist(t, \"foo.baz\", true)\r\n\r\n\tpublish(t, derek, \"foo.bat\")\r\n\tnextMsg(t, subFooBat, true)\r\n\tcheckSublist(t, \"foo.bat\", true)\r\n\r\n\tpublish(t, derek, \"private.natsio.foo\")\r\n\tnextMsg(t, subPriv, true)\r\n\tcheckSublist(t, \"private.natsio.foo\", true)\r\n\r\n\t// Also make sure that intra-account subscription works OK\r\n\tivanSub := subscribe(t, ivan, \"ivan.sub\")\r\n\tpublish(t, ivan, \"ivan.sub\")\r\n\tnextMsg(t, ivanSub, true)\r\n\tderekSub := subscribe(t, derek, \"derek.sub\")\r\n\tpublish(t, derek, \"derek.sub\")\r\n\tnextMsg(t, derekSub, true)\r\n\r\n\t// synadia account exports \"private.>\" to nats.io\r\n\t// synadia account exports \"foo.*\"\r\n\t// user ivan denies subscription on \"foo.bat\"\r\n\t// nats.io account imports \"foo.baz\" from synadia\r\n\t// nats.io account imports \"private.natsio.*\" from synadia with prefix \"yyyy\"\r\n\treloadUpdateConfig(t, s, conf, fmt.Sprintf(template, `\"foo.*\"`, `\"foo.bat\"`, `\"foo.baz\"`, `\"yyyy\"`))\r\n\r\n\t// Sub on foo.bar should now fail to receive\r\n\tpublish(t, derek, \"foo.bar\")\r\n\tnextMsg(t, subFooBar, false)\r\n\tcheckSublist(t, \"foo.bar\", false)\r\n\t// But foo.baz should be received\r\n\tpublish(t, derek, \"foo.baz\")\r\n\tnextMsg(t, subFooBaz, true)\r\n\tcheckSublist(t, \"foo.baz\", true)\r\n\t// Due to permissions, foo.bat should not\r\n\tpublish(t, derek, \"foo.bat\")\r\n\tnextMsg(t, subFooBat, false)\r\n\tcheckSublist(t, \"foo.bat\", false)\r\n\t// Prefix changed, so should not be received\r\n\tpublish(t, derek, \"private.natsio.foo\")\r\n\tnextMsg(t, subPriv, false)\r\n\tcheckSublist(t, \"private.natsio.foo\", false)\r\n\r\n\t// Wait for client notification of permissions error\r\n\tif err := wait(ch); err != nil {\r\n\t\tt.Fatal(\"Did not the permissions error\")\r\n\t}\r\n\r\n\tpublish(t, ivan, \"ivan.sub\")\r\n\tnextMsg(t, ivanSub, true)\r\n\tpublish(t, derek, \"derek.sub\")\r\n\tnextMsg(t, derekSub, true)\r\n\r\n\t// Change export so that foo.* is no longer exported\r\n\t// synadia account exports \"private.>\" to nats.io\r\n\t// synadia account exports \"xxx\"\r\n\t// user ivan denies subscription on \"foo.bat\"\r\n\t// nats.io account imports \"xxx\" from synadia\r\n\t// nats.io account imports \"private.natsio.*\" from synadia with prefix \"ivan\"\r\n\treloadUpdateConfig(t, s, conf, fmt.Sprintf(template, `\"xxx\"`, `\"foo.bat\"`, `\"xxx\"`, `\"ivan\"`))\r\n\r\n\tpublish(t, derek, \"foo.bar\")\r\n\tnextMsg(t, subFooBar, false)\r\n\tcheckSublist(t, \"foo.bar\", false)\r\n\r\n\tpublish(t, derek, \"foo.baz\")\r\n\tnextMsg(t, subFooBaz, false)\r\n\tcheckSublist(t, \"foo.baz\", false)\r\n\r\n\tpublish(t, derek, \"foo.bat\")\r\n\tnextMsg(t, subFooBat, false)\r\n\tcheckSublist(t, \"foo.bat\", false)\r\n\r\n\t// Prefix changed back, so should receive\r\n\tpublish(t, derek, \"private.natsio.foo\")\r\n\tnextMsg(t, subPriv, true)\r\n\tcheckSublist(t, \"private.natsio.foo\", true)\r\n\r\n\tpublish(t, ivan, \"ivan.sub\")\r\n\tnextMsg(t, ivanSub, true)\r\n\tpublish(t, derek, \"derek.sub\")\r\n\tnextMsg(t, derekSub, true)\r\n}\r\n\r\nfunc TestConfigReloadAccountServicesImportExport(t *testing.T) {\r\n\tconf := createConfFile(t, []byte(`\r\n\tlisten: \"127.0.0.1:-1\"\r\n\taccounts {\r\n\t\tsynadia {\r\n\t\t\tusers [{user: derek, password: foo}]\r\n\t\t\texports = [\r\n\t\t\t\t{service: \"pub.request\"}\r\n\t\t\t\t{service: \"pub.special.request\", accounts: [nats.io]}\r\n\t\t\t]\r\n\t\t}\r\n\t\tnats.io {\r\n\t\t\tusers [{user: ivan, password: bar}]\r\n\t\t\timports = [\r\n\t\t\t\t{service: {account: \"synadia\", subject: \"pub.special.request\"}, to: \"foo\"}\r\n\t\t\t\t{service: {account: \"synadia\", subject: \"pub.request\"}, to: \"bar\"}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tderek, err := nats.Connect(fmt.Sprintf(\"nats://derek:foo@%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer derek.Close()\r\n\tcheckClientsCount(t, s, 1)\r\n\r\n\tivan, err := nats.Connect(fmt.Sprintf(\"nats://ivan:bar@%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ivan.Close()\r\n\tcheckClientsCount(t, s, 2)\r\n\r\n\tif _, err := derek.Subscribe(\"pub.special.request\", func(m *nats.Msg) {\r\n\t\tderek.Publish(m.Reply, []byte(\"reply1\"))\r\n\t}); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tif _, err := derek.Subscribe(\"pub.request\", func(m *nats.Msg) {\r\n\t\tderek.Publish(m.Reply, []byte(\"reply2\"))\r\n\t}); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tif _, err := derek.Subscribe(\"pub.special.request.new\", func(m *nats.Msg) {\r\n\t\tderek.Publish(m.Reply, []byte(\"reply3\"))\r\n\t}); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\t// Also create one that will be used for intra-account communication\r\n\tif _, err := derek.Subscribe(\"derek.sub\", func(m *nats.Msg) {\r\n\t\tderek.Publish(m.Reply, []byte(\"private\"))\r\n\t}); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tderek.Flush()\r\n\r\n\t// Create an intra-account sub for ivan too\r\n\tif _, err := ivan.Subscribe(\"ivan.sub\", func(m *nats.Msg) {\r\n\t\tivan.Publish(m.Reply, []byte(\"private\"))\r\n\t}); err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\r\n\treq := func(t *testing.T, nc *nats.Conn, subj string, reply string) {\r\n\t\tt.Helper()\r\n\t\tvar timeout time.Duration\r\n\t\tif reply != \"\" {\r\n\t\t\ttimeout = time.Second\r\n\t\t} else {\r\n\t\t\ttimeout = 100 * time.Millisecond\r\n\t\t}\r\n\t\tmsg, err := nc.Request(subj, []byte(\"request\"), timeout)\r\n\t\tif reply != \"\" {\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Expected reply %s on subject %s, got %v\", reply, subj, err)\r\n\t\t\t}\r\n\t\t\tif string(msg.Data) != reply {\r\n\t\t\t\tt.Fatalf(\"Expected reply %s on subject %s, got %s\", reply, subj, msg.Data)\r\n\t\t\t}\r\n\t\t} else if err != nats.ErrTimeout {\r\n\t\t\tt.Fatalf(\"Expected timeout on subject %s, got %v\", subj, err)\r\n\t\t}\r\n\t}\r\n\r\n\treq(t, ivan, \"foo\", \"reply1\")\r\n\treq(t, ivan, \"bar\", \"reply2\")\r\n\t// This not exported/imported, so should timeout\r\n\treq(t, ivan, \"baz\", \"\")\r\n\r\n\t// Check intra-account communication\r\n\treq(t, ivan, \"ivan.sub\", \"private\")\r\n\treq(t, derek, \"derek.sub\", \"private\")\r\n\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\taccounts {\r\n\t\tsynadia {\r\n\t\t\tusers [{user: derek, password: foo}]\r\n\t\t\texports = [\r\n\t\t\t\t{service: \"pub.request\"}\r\n\t\t\t\t{service: \"pub.special.request\", accounts: [nats.io]}\r\n\t\t\t\t{service: \"pub.special.request.new\", accounts: [nats.io]}\r\n\t\t\t]\r\n\t\t}\r\n\t\tnats.io {\r\n\t\t\tusers [{user: ivan, password: bar}]\r\n\t\t\timports = [\r\n\t\t\t\t{service: {account: \"synadia\", subject: \"pub.special.request\"}, to: \"foo\"}\r\n\t\t\t\t{service: {account: \"synadia\", subject: \"pub.special.request.new\"}, to: \"baz\"}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`)\r\n\t// This still should work\r\n\treq(t, ivan, \"foo\", \"reply1\")\r\n\t// This should not\r\n\treq(t, ivan, \"bar\", \"\")\r\n\t// This now should work\r\n\treq(t, ivan, \"baz\", \"reply3\")\r\n\r\n\t// Check intra-account communication\r\n\treq(t, ivan, \"ivan.sub\", \"private\")\r\n\treq(t, derek, \"derek.sub\", \"private\")\r\n}\r\n\r\n// As of now, config reload does not support changes for gateways.\r\n// However, ensure that if a gateway is defined, one can still\r\n// do reload as long as we don't change the gateway spec.\r\nfunc TestConfigReloadNotPreventedByGateways(t *testing.T) {\r\n\tconfTemplate := `\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t\t%s\r\n\t\tgateway {\r\n\t\t\tname: \"A\"\r\n\t\t\tlisten: \"127.0.0.1:-1\"\r\n\t\t\ttls {\r\n\t\t\t\tcert_file: \"configs/certs/server.pem\"\r\n\t\t\t\tkey_file: \"configs/certs/key.pem\"\r\n\t\t\t\ttimeout: %s\r\n\t\t\t}\r\n\t\t\tgateways [\r\n\t\t\t\t{\r\n\t\t\t\t\tname: \"B\"\r\n\t\t\t\t\turl: \"nats://localhost:8888\"\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t`\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(confTemplate, \"\", \"5\")))\r\n\tdefer os.Remove(conf)\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\t// Cause reload with adding a param that is supported\r\n\treloadUpdateConfig(t, s, conf, fmt.Sprintf(confTemplate, \"max_payload: 100000\", \"5\"))\r\n\r\n\t// Now update gateway, should fail to reload.\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(confTemplate, \"max_payload: 100000\", \"3\")))\r\n\tif err := s.Reload(); err == nil || !strings.Contains(err.Error(), \"not supported for Gateway\") {\r\n\t\tt.Fatalf(\"Expected Reload to return a not supported error, got %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadBoolFlags(t *testing.T) {\r\n\tdefer func() { FlagSnapshot = nil }()\r\n\r\n\tlogfile := \"logtime.log\"\r\n\tdefer os.Remove(logfile)\r\n\ttemplate := `\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t\tlogfile: \"logtime.log\"\r\n\t\t%s\r\n\t`\r\n\r\n\tvar opts *Options\r\n\tvar err error\r\n\r\n\tfor _, test := range []struct {\r\n\t\tname     string\r\n\t\tcontent  string\r\n\t\tcmdLine  []string\r\n\t\texpected bool\r\n\t\tval      func() bool\r\n\t}{\r\n\t\t// Logtime\r\n\t\t{\r\n\t\t\t\"logtime_not_in_config_no_override\",\r\n\t\t\t\"\",\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_not_in_config_override_short_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-T\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_not_in_config_override_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-logtime\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_false_in_config_no_override\",\r\n\t\t\t\"logtime: false\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_false_in_config_override_short_true\",\r\n\t\t\t\"logtime: false\",\r\n\t\t\t[]string{\"-T\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_false_in_config_override_true\",\r\n\t\t\t\"logtime: false\",\r\n\t\t\t[]string{\"-logtime\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_true_in_config_no_override\",\r\n\t\t\t\"logtime: true\",\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_true_in_config_override_short_false\",\r\n\t\t\t\"logtime: true\",\r\n\t\t\t[]string{\"-T=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"logtime_true_in_config_override_false\",\r\n\t\t\t\"logtime: true\",\r\n\t\t\t[]string{\"-logtime=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Logtime },\r\n\t\t},\r\n\t\t// Debug\r\n\t\t{\r\n\t\t\t\"debug_not_in_config_no_override\",\r\n\t\t\t\"\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_not_in_config_override_short_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-D\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_not_in_config_override_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-debug\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_false_in_config_no_override\",\r\n\t\t\t\"debug: false\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_false_in_config_override_short_true\",\r\n\t\t\t\"debug: false\",\r\n\t\t\t[]string{\"-D\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_false_in_config_override_true\",\r\n\t\t\t\"debug: false\",\r\n\t\t\t[]string{\"-debug\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_true_in_config_no_override\",\r\n\t\t\t\"debug: true\",\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_true_in_config_override_short_false\",\r\n\t\t\t\"debug: true\",\r\n\t\t\t[]string{\"-D=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_true_in_config_override_false\",\r\n\t\t\t\"debug: true\",\r\n\t\t\t[]string{\"-debug=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Debug },\r\n\t\t},\r\n\t\t// Trace\r\n\t\t{\r\n\t\t\t\"trace_not_in_config_no_override\",\r\n\t\t\t\"\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_not_in_config_override_short_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-V\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_not_in_config_override_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-trace\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_false_in_config_no_override\",\r\n\t\t\t\"trace: false\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_false_in_config_override_short_true\",\r\n\t\t\t\"trace: false\",\r\n\t\t\t[]string{\"-V\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_false_in_config_override_true\",\r\n\t\t\t\"trace: false\",\r\n\t\t\t[]string{\"-trace\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_true_in_config_no_override\",\r\n\t\t\t\"trace: true\",\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_true_in_config_override_short_false\",\r\n\t\t\t\"trace: true\",\r\n\t\t\t[]string{\"-V=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_true_in_config_override_false\",\r\n\t\t\t\"trace: true\",\r\n\t\t\t[]string{\"-trace=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Trace },\r\n\t\t},\r\n\t\t// Syslog\r\n\t\t{\r\n\t\t\t\"syslog_not_in_config_no_override\",\r\n\t\t\t\"\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_not_in_config_override_short_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-s\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_not_in_config_override_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-syslog\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_false_in_config_no_override\",\r\n\t\t\t\"syslog: false\",\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_false_in_config_override_short_true\",\r\n\t\t\t\"syslog: false\",\r\n\t\t\t[]string{\"-s\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_false_in_config_override_true\",\r\n\t\t\t\"syslog: false\",\r\n\t\t\t[]string{\"-syslog\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_true_in_config_no_override\",\r\n\t\t\t\"syslog: true\",\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_true_in_config_override_short_false\",\r\n\t\t\t\"syslog: true\",\r\n\t\t\t[]string{\"-s=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"syslog_true_in_config_override_false\",\r\n\t\t\t\"syslog: true\",\r\n\t\t\t[]string{\"-syslog=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t// Cluster.NoAdvertise\r\n\t\t{\r\n\t\t\t\"cluster_no_advertise_not_in_config_no_override\",\r\n\t\t\t`cluster {\r\n\t\t\t\tport: -1\r\n\t\t\t}`,\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Cluster.NoAdvertise },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"cluster_no_advertise_not_in_config_override_true\",\r\n\t\t\t`cluster {\r\n\t\t\t\tport: -1\r\n\t\t\t}`,\r\n\t\t\t[]string{\"-no_advertise\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Cluster.NoAdvertise },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"cluster_no_advertise_false_in_config_no_override\",\r\n\t\t\t`cluster {\r\n\t\t\t\tport: -1\r\n\t\t\t\tno_advertise: false\r\n\t\t\t}`,\r\n\t\t\tnil,\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Cluster.NoAdvertise },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"cluster_no_advertise_false_in_config_override_true\",\r\n\t\t\t`cluster {\r\n\t\t\t\tport: -1\r\n\t\t\t\tno_advertise: false\r\n\t\t\t}`,\r\n\t\t\t[]string{\"-no_advertise\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Cluster.NoAdvertise },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"cluster_no_advertise_true_in_config_no_override\",\r\n\t\t\t`cluster {\r\n\t\t\t\tport: -1\r\n\t\t\t\tno_advertise: true\r\n\t\t\t}`,\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Cluster.NoAdvertise },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"cluster_no_advertise_true_in_config_override_false\",\r\n\t\t\t`cluster {\r\n\t\t\t\tport: -1\r\n\t\t\t\tno_advertise: true\r\n\t\t\t}`,\r\n\t\t\t[]string{\"-no_advertise=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Syslog },\r\n\t\t},\r\n\t\t// -DV override\r\n\t\t{\r\n\t\t\t\"debug_trace_not_in_config_dv_override_true\",\r\n\t\t\t\"\",\r\n\t\t\t[]string{\"-DV\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug && opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_trace_false_in_config_dv_override_true\",\r\n\t\t\t`debug: false\r\n\t\t     trace: false\r\n\t\t\t`,\r\n\t\t\t[]string{\"-DV\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Debug && opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"debug_trace_true_in_config_dv_override_false\",\r\n\t\t\t`debug: true\r\n\t\t     trace: true\r\n\t\t\t`,\r\n\t\t\t[]string{\"-DV=false\"},\r\n\t\t\tfalse,\r\n\t\t\tfunc() bool { return opts.Debug && opts.Trace },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_verbose_true_in_config_override_true\",\r\n\t\t\t`trace_verbose: true\r\n\t\t\t`,\r\n\t\t\tnil,\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.Trace && opts.TraceVerbose },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_verbose_true_in_config_override_false\",\r\n\t\t\t`trace_verbose: true\r\n\t\t\t`,\r\n\t\t\t[]string{\"--VV=false\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return !opts.TraceVerbose },\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"trace_verbose_true_in_config_override_false\",\r\n\t\t\t`trace_verbose: false\r\n\t\t\t`,\r\n\t\t\t[]string{\"--VV=true\"},\r\n\t\t\ttrue,\r\n\t\t\tfunc() bool { return opts.TraceVerbose },\r\n\t\t},\r\n\t} {\r\n\t\tt.Run(test.name, func(t *testing.T) {\r\n\t\t\tconf := createConfFile(t, []byte(fmt.Sprintf(template, test.content)))\r\n\t\t\tdefer os.Remove(conf)\r\n\r\n\t\t\tfs := flag.NewFlagSet(\"test\", flag.ContinueOnError)\r\n\t\t\tvar args []string\r\n\t\t\targs = append(args, \"-c\", conf)\r\n\t\t\tif test.cmdLine != nil {\r\n\t\t\t\targs = append(args, test.cmdLine...)\r\n\t\t\t}\r\n\t\t\topts, err = ConfigureOptions(fs, args, nil, nil, nil)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error processing config: %v\", err)\r\n\t\t\t}\r\n\t\t\topts.NoSigs = true\r\n\t\t\ts := RunServer(opts)\r\n\t\t\tdefer s.Shutdown()\r\n\r\n\t\t\tif test.val() != test.expected {\r\n\t\t\t\tt.Fatalf(\"Expected to be set to %v, got %v\", test.expected, test.val())\r\n\t\t\t}\r\n\t\t\tif err := s.Reload(); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error on reload: %v\", err)\r\n\t\t\t}\r\n\t\t\tif test.val() != test.expected {\r\n\t\t\t\tt.Fatalf(\"Expected to be set to %v, got %v\", test.expected, test.val())\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadMaxControlLineWithClients(t *testing.T) {\r\n\tserver, opts, config := runReloadServerWithConfig(t, \"./configs/reload/basic.conf\")\r\n\tdefer os.Remove(config)\r\n\tdefer server.Shutdown()\r\n\r\n\t// Ensure we can connect as a sanity check.\r\n\taddr := fmt.Sprintf(\"nats://%s:%d\", opts.Host, server.Addr().(*net.TCPAddr).Port)\r\n\tnc, err := nats.Connect(addr)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\t// Now grab server's internal client that matches.\r\n\tcid, _ := nc.GetClientID()\r\n\tc := server.getClient(cid)\r\n\tif c == nil {\r\n\t\tt.Fatalf(\"Could not look up internal client\")\r\n\t}\r\n\r\n\t// Check that we have the correct mcl snapshotted into the connected client.\r\n\tgetMcl := func(c *client) int32 {\r\n\t\tc.mu.Lock()\r\n\t\tdefer c.mu.Unlock()\r\n\t\treturn c.mcl\r\n\t}\r\n\tif mcl := getMcl(c); mcl != opts.MaxControlLine {\r\n\t\tt.Fatalf(\"Expected snapshot in client for mcl to be same as opts.MaxControlLine, got %d vs %d\",\r\n\t\t\tmcl, opts.MaxControlLine)\r\n\t}\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, config, []byte(\"listen: 127.0.0.1:-1; max_control_line: 222\"))\r\n\tif err := server.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Expected Reload to succeed, got %v\", err)\r\n\t}\r\n\r\n\t// Refresh properly.\r\n\topts = server.getOpts()\r\n\r\n\tif mcl := getMcl(c); mcl != opts.MaxControlLine {\r\n\t\tt.Fatalf(\"Expected snapshot in client for mcl to be same as new opts.MaxControlLine, got %d vs %d\",\r\n\t\t\tmcl, opts.MaxControlLine)\r\n\t}\r\n}\r\n\r\ntype testCustomAuth struct{}\r\n\r\nfunc (ca *testCustomAuth) Check(c ClientAuthentication) bool { return true }\r\n\r\nfunc TestConfigReloadIgnoreCustomAuth(t *testing.T) {\r\n\tconf := createConfFile(t, []byte(`\r\n\t\tport: -1\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\topts := LoadConfig(conf)\r\n\r\n\tca := &testCustomAuth{}\r\n\topts.CustomClientAuthentication = ca\r\n\topts.CustomRouterAuthentication = ca\r\n\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n\r\n\tif s.getOpts().CustomClientAuthentication != ca || s.getOpts().CustomRouterAuthentication != ca {\r\n\t\tt.Fatalf(\"Custom auth missing\")\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadLeafNodeRandomPort(t *testing.T) {\r\n\tconf := createConfFile(t, []byte(`\r\n\t\tport: -1\r\n\t\tleafnodes {\r\n\t\t\tport: -1\r\n\t\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\ts.mu.Lock()\r\n\tlnPortBefore := s.leafNodeListener.Addr().(*net.TCPAddr).Port\r\n\ts.mu.Unlock()\r\n\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n\r\n\ts.mu.Lock()\r\n\tlnPortAfter := s.leafNodeListener.Addr().(*net.TCPAddr).Port\r\n\ts.mu.Unlock()\r\n\r\n\tif lnPortBefore != lnPortAfter {\r\n\t\tt.Fatalf(\"Expected leafnodes listen port to be same, was %v is now %v\", lnPortBefore, lnPortAfter)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadLeafNodeWithTLS(t *testing.T) {\r\n\ttemplate := `\r\n\t\tport: -1\r\n\t\t%s\r\n\t\tleaf {\r\n\t\t\tlisten: \"127.0.0.1:-1\"\r\n\t\t\ttls: {\r\n\t\t\t\tca_file: \"../test/configs/certs/tlsauth/ca.pem\"\r\n\t\t\t\tcert_file: \"../test/configs/certs/tlsauth/server.pem\"\r\n\t\t\t\tkey_file: \"../test/configs/certs/tlsauth/server-key.pem\"\r\n\t\t\t\ttimeout: 3\r\n\t\t\t}\r\n\t\t}\r\n\t`\r\n\tconf1 := createConfFile(t, []byte(fmt.Sprintf(template, \"\")))\r\n\tdefer os.Remove(conf1)\r\n\ts1, o1 := RunServerWithConfig(conf1)\r\n\tdefer s1.Shutdown()\r\n\r\n\tu, err := url.Parse(fmt.Sprintf(\"nats://localhost:%d\", o1.LeafNode.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating url: %v\", err)\r\n\t}\r\n\tconf2 := createConfFile(t, []byte(fmt.Sprintf(`\r\n\t\tport: -1\r\n\t\tleaf {\r\n\t\t\tremotes [\r\n\t\t\t\t{\r\n\t\t\t\t\turl: \"%s\"\r\n\t\t\t\t\ttls {\r\n\t\t\t\t\t\tca_file: \"../test/configs/certs/tlsauth/ca.pem\"\r\n\t\t\t\t\t\tcert_file: \"../test/configs/certs/tlsauth/client.pem\"\r\n\t\t\t\t\t\tkey_file:  \"../test/configs/certs/tlsauth/client-key.pem\"\r\n\t\t\t\t\t\ttimeout: 2\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t]\r\n\t\t}\r\n\t`, u.String())))\r\n\tdefer os.Remove(conf2)\r\n\to2, err := ProcessConfigFile(conf2)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error processing config file: %v\", err)\r\n\t}\r\n\to2.NoLog, o2.NoSigs = true, true\r\n\to2.LeafNode.resolver = &testLoopbackResolver{}\r\n\ts2 := RunServer(o2)\r\n\tdefer s2.Shutdown()\r\n\r\n\tcheckFor(t, 3*time.Second, 15*time.Millisecond, func() error {\r\n\t\tif n := s1.NumLeafNodes(); n != 1 {\r\n\t\t\treturn fmt.Errorf(\"Expected 1 leaf node, got %v\", n)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, conf1, []byte(fmt.Sprintf(template, \"debug: false\")))\r\n\r\n\tif err := s1.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadAndVarz(t *testing.T) {\r\n\ttemplate := `\r\n\t\tport: -1\r\n\t\t%s\r\n\t`\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(template, \"\")))\r\n\tdefer os.Remove(conf)\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\ts.mu.Lock()\r\n\tinitConfigTime := s.configTime\r\n\ts.mu.Unlock()\r\n\r\n\tv, _ := s.Varz(nil)\r\n\tif !v.ConfigLoadTime.Equal(initConfigTime) {\r\n\t\tt.Fatalf(\"ConfigLoadTime should be %v, got %v\", initConfigTime, v.ConfigLoadTime)\r\n\t}\r\n\tif v.MaxConn != DEFAULT_MAX_CONNECTIONS {\r\n\t\tt.Fatalf(\"MaxConn should be %v, got %v\", DEFAULT_MAX_CONNECTIONS, v.MaxConn)\r\n\t}\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(template, \"max_connections: 10\")))\r\n\r\n\t// Make sure we wait a bit so config load time has a chance to change.\r\n\ttime.Sleep(15 * time.Millisecond)\r\n\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n\r\n\tv, _ = s.Varz(nil)\r\n\tif v.ConfigLoadTime.Equal(initConfigTime) {\r\n\t\tt.Fatalf(\"ConfigLoadTime should be different from %v\", initConfigTime)\r\n\t}\r\n\tif v.MaxConn != 10 {\r\n\t\tt.Fatalf(\"MaxConn should be 10, got %v\", v.MaxConn)\r\n\t}\r\n}\r\n\r\nfunc TestConfigReloadConnectErrReports(t *testing.T) {\r\n\ttemplate := `\r\n\t\tport: -1\r\n\t\t%s\r\n\t\t%s\r\n\t`\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(template, \"\", \"\")))\r\n\tdefer os.Remove(conf)\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\topts := s.getOpts()\r\n\tif cer := opts.ConnectErrorReports; cer != DEFAULT_CONNECT_ERROR_REPORTS {\r\n\t\tt.Fatalf(\"Expected ConnectErrorReports to be %v, got %v\", DEFAULT_CONNECT_ERROR_REPORTS, cer)\r\n\t}\r\n\tif rer := opts.ReconnectErrorReports; rer != DEFAULT_RECONNECT_ERROR_REPORTS {\r\n\t\tt.Fatalf(\"Expected ReconnectErrorReports to be %v, got %v\", DEFAULT_RECONNECT_ERROR_REPORTS, rer)\r\n\t}\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, conf,\r\n\t\t[]byte(fmt.Sprintf(template, \"connect_error_reports: 2\", \"reconnect_error_reports: 3\")))\r\n\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n\r\n\topts = s.getOpts()\r\n\tif cer := opts.ConnectErrorReports; cer != 2 {\r\n\t\tt.Fatalf(\"Expected ConnectErrorReports to be %v, got %v\", 2, cer)\r\n\t}\r\n\tif rer := opts.ReconnectErrorReports; rer != 3 {\r\n\t\tt.Fatalf(\"Expected ReconnectErrorReports to be %v, got %v\", 3, rer)\r\n\t}\r\n}\r\n\r\nfunc TestAuthReloadDoesNotBreakRouteInterest(t *testing.T) {\r\n\ts, opts := RunServerWithConfig(\"./configs/seed_tls.conf\")\r\n\tdefer s.Shutdown()\r\n\r\n\t// Create client and sub interest on seed server.\r\n\turlSeed := fmt.Sprintf(\"nats://%s:%d/\", opts.Host, opts.Port)\r\n\tnc, err := nats.Connect(urlSeed)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\tdefer nc.Close()\r\n\r\n\tch := make(chan bool)\r\n\tnc.Subscribe(\"foo\", func(m *nats.Msg) { ch <- true })\r\n\tnc.Flush()\r\n\r\n\t// Use this to check for message.\r\n\tcheckForMsg := func() {\r\n\t\tt.Helper()\r\n\t\tselect {\r\n\t\tcase <-ch:\r\n\t\tcase <-time.After(2 * time.Second):\r\n\t\t\tt.Fatal(\"Timeout waiting for message across route\")\r\n\t\t}\r\n\t}\r\n\r\n\t// Create second server and form cluster. We will send from here.\r\n\turlRoute := fmt.Sprintf(\"nats://%s:%d\", opts.Cluster.Host, opts.Cluster.Port)\r\n\toptsA := nextServerOpts(opts)\r\n\toptsA.Routes = RoutesFromStr(urlRoute)\r\n\r\n\tsa := RunServer(optsA)\r\n\tdefer sa.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, sa)\r\n\tcheckSubInterest(t, sa, globalAccountName, \"foo\", time.Second)\r\n\r\n\t// Create second client and send message from this one. Interest should be here.\r\n\turlA := fmt.Sprintf(\"nats://%s:%d/\", optsA.Host, optsA.Port)\r\n\tnc2, err := nats.Connect(urlA)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Check that we can send messages.\r\n\tnc2.Publish(\"foo\", nil)\r\n\tcheckForMsg()\r\n\r\n\t// Now shutdown nc2 and srvA.\r\n\tnc2.Close()\r\n\tsa.Shutdown()\r\n\r\n\t// Now force reload on seed server of auth.\r\n\ts.reloadAuthorization()\r\n\r\n\t// Restart both server A and client 2.\r\n\tsa = RunServer(optsA)\r\n\tdefer sa.Shutdown()\r\n\r\n\tcheckClusterFormed(t, s, sa)\r\n\tcheckSubInterest(t, sa, globalAccountName, \"foo\", time.Second)\r\n\r\n\tnc2, err = nats.Connect(urlA)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\t// Check that we can still send messages.\r\n\tnc2.Publish(\"foo\", nil)\r\n\tcheckForMsg()\r\n}\r\n\r\nfunc TestConfigReloadAccountResolverTLSConfig(t *testing.T) {\r\n\tkp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tapub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(apub)\r\n\tajwt, err := nac.Encode(kp)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error generating account JWT: %v\", err)\r\n\t}\r\n\tpub, _ := kp.PublicKey()\r\n\r\n\tts := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n\t\tw.Write([]byte(ajwt))\r\n\t}))\r\n\tdefer ts.Close()\r\n\t// Set a dummy logger to prevent tls bad certificate output to stderr.\r\n\tts.Config.ErrorLog = log.New(&bytes.Buffer{}, \"\", 0)\r\n\r\n\tconfTemplate := `\r\n\t\t\t\tlisten: -1\r\n\t\t\t\ttrusted_keys: %s\r\n\t\t\t\tresolver: URL(\"%s/ngs/v1/accounts/jwt/\")\r\n\t\t\t\t%s\r\n\t`\r\n\tconf := createConfFile(t, []byte(fmt.Sprintf(confTemplate, pub, ts.URL, `\r\n\t\tresolver_tls {\r\n\t\t\tinsecure: true\r\n\t\t}\r\n\t`)))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, _ := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(confTemplate, pub, ts.URL, \"\")))\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n\r\n\tif _, err := s.LookupAccount(apub); err == nil {\r\n\t\tt.Fatal(\"Expected error during lookup, did not get one\")\r\n\t}\r\n\r\n\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(fmt.Sprintf(confTemplate, pub, ts.URL, `\r\n\t\tresolver_tls {\r\n\t\t\tinsecure: true\r\n\t\t}\r\n\t`)))\r\n\tif err := s.Reload(); err != nil {\r\n\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t}\r\n\r\n\tacc, err := s.LookupAccount(apub)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error during lookup: %v\", err)\r\n\t}\r\n\tif acc == nil {\r\n\t\tt.Fatalf(\"Expected to receive an account\")\r\n\t}\r\n\tif acc.Name != apub {\r\n\t\tt.Fatalf(\"Account name did not match claim key\")\r\n\t}\r\n}\r\n\r\nfunc TestLoggingReload(t *testing.T) {\r\n\t// This test basically starts a server and causes it's configuration to be reloaded 3 times.\r\n\t// Each time, a new log file is created and trace levels are turned, off - on - off.\r\n\r\n\t// At the end of the test, all 3 log files are inspected for certain traces.\r\n\tcountMatches := func(log []byte, stmts ...string) int {\r\n\t\tmatchCnt := 0\r\n\t\tfor _, stmt := range stmts {\r\n\t\t\tif strings.Contains(string(log), stmt) {\r\n\t\t\t\tmatchCnt++\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn matchCnt\r\n\t}\r\n\r\n\ttraces := []string{\"[TRC]\", \"[DBG]\", \"SYSTEM\", \"MSG_PAYLOAD\", \"$SYS.SERVER.ACCOUNT\"}\r\n\r\n\tdidTrace := func(log []byte) bool {\r\n\t\treturn countMatches(log, \"[INF] Reloaded server configuration\") == 1\r\n\t}\r\n\r\n\ttracingAbsent := func(log []byte) bool {\r\n\t\treturn countMatches(log, traces...) == 0 && didTrace(log)\r\n\t}\r\n\r\n\ttracingPresent := func(log []byte) bool {\r\n\t\treturn len(traces) == countMatches(log, traces...) && didTrace(log)\r\n\t}\r\n\r\n\tcheck := func(filename string, valid func([]byte) bool) {\r\n\t\tt.Helper()\r\n\t\tlog, err := ioutil.ReadFile(filename)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error reading log file %s: %v\\n\", filename, err)\r\n\t\t}\r\n\t\tif !valid(log) {\r\n\t\t\tt.Fatalf(\"%s is not valid: %s\", filename, log)\r\n\t\t}\r\n\t\t//t.Logf(\"%s contains: %s\\n\", filename, log)\r\n\t}\r\n\r\n\t// common configuration setting up system accounts. trace_verbose needs this to cause traces\r\n\tcommonCfg := `\r\n\t\tport: -1\r\n\t\tsystem_account: sys\r\n\t\taccounts {\r\n\t\t  sys { users = [ {user: sys, pass: \"\" } ] }\r\n\t\t  nats.io: { users = [ { user : bar, pass: \"pwd\" } ] }\r\n\t\t}\r\n\t`\r\n\r\n\tconf := createConfFile(t, []byte(commonCfg))\r\n\tdefer os.Remove(conf)\r\n\r\n\ts, opts := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\treload := func(change string) {\r\n\t\tt.Helper()\r\n\t\tchangeCurrentConfigContentWithNewContent(t, conf, []byte(commonCfg+`\r\n\t\t\t`+change+`\r\n\t\t`))\r\n\r\n\t\tif err := s.Reload(); err != nil {\r\n\t\t\tt.Fatalf(\"Error during reload: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\ttraffic := func(cnt int) {\r\n\t\tt.Helper()\r\n\t\t// Create client and sub interest on server and create traffic\r\n\t\turlSeed := fmt.Sprintf(\"nats://bar:pwd@%s:%d/\", opts.Host, opts.Port)\r\n\t\tnc, err := nats.Connect(urlSeed)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error creating client: %v\\n\", err)\r\n\t\t}\r\n\r\n\t\tmsgs := make(chan *nats.Msg, 1)\r\n\t\tdefer close(msgs)\r\n\r\n\t\tsub, err := nc.ChanSubscribe(\"foo\", msgs)\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error creating subscriber: %v\\n\", err)\r\n\t\t}\r\n\r\n\t\tnc.Flush()\r\n\r\n\t\tfor i := 0; i < cnt; i++ {\r\n\t\t\tif err := nc.Publish(\"foo\", []byte(\"bar\")); err == nil {\r\n\t\t\t\t<-msgs\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tsub.Unsubscribe()\r\n\t\tnc.Close()\r\n\t}\r\n\r\n\tdefer os.Remove(\"off-pre.log\")\r\n\treload(\"log_file: off-pre.log\")\r\n\r\n\ttraffic(10) // generate NO trace/debug entries in off-pre.log\r\n\r\n\tdefer os.Remove(\"on.log\")\r\n\treload(`\r\n\t\tlog_file: on.log\r\n\t\tdebug: true\r\n\t\ttrace_verbose: true\r\n\t`)\r\n\r\n\ttraffic(10) // generate trace/debug entries in on.log\r\n\r\n\tdefer os.Remove(\"off-post.log\")\r\n\treload(`\r\n\t\tlog_file: off-post.log\r\n\t\tdebug: false\r\n\t\ttrace_verbose: false\r\n\t`)\r\n\r\n\ttraffic(10) // generate trace/debug entries in off-post.log\r\n\r\n\t// check resulting log files for expected content\r\n\tcheck(\"off-pre.log\", tracingAbsent)\r\n\tcheck(\"on.log\", tracingPresent)\r\n\tcheck(\"off-post.log\", tracingAbsent)\r\n}\r\n\r\nfunc TestReloadValidate(t *testing.T) {\r\n\tconfFileName := createConfFile(t, []byte(`\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t\tno_auth_user: a\r\n\t\tauthorization {\r\n\t\t\tusers [\r\n\t\t\t\t{user: \"a\", password: \"a\"},\r\n\t\t\t\t{user: \"b\", password: \"b\"}\r\n\t\t\t]\r\n\t\t}\r\n\t`))\r\n\tdefer os.Remove(confFileName)\r\n\tsrv, _ := RunServerWithConfig(confFileName)\r\n\tif srv == nil {\r\n\t\tt.Fatal(\"Server did not start\")\r\n\t}\r\n\t// Induce error by removing the user no_auth_user points to\r\n\tchangeCurrentConfigContentWithNewContent(t, confFileName, []byte(`\r\n\t\tlisten: \"127.0.0.1:-1\"\r\n\t\tno_auth_user: a\r\n\t\tauthorization {\r\n\t\t\tusers [\r\n\t\t\t\t{user: \"b\", password: \"b\"}\r\n\t\t\t]\r\n\t\t}\r\n\t`))\r\n\tif err := srv.Reload(); err == nil {\r\n\t\tt.Fatal(\"Expected error on reload, got none\")\r\n\t} else if strings.HasPrefix(err.Error(), \" no_auth_user:\") {\r\n\t\tt.Logf(\"Expected no_auth_user error, got different one %s\", err)\r\n\t}\r\n\tsrv.Shutdown()\r\n}\r\n\r\nfunc TestConfigReloadAccounts(t *testing.T) {\r\n\tconf := createConfFile(t, []byte(`\r\n\tlisten: \"127.0.0.1:-1\"\r\n\tsystem_account: SYS\r\n\taccounts {\r\n\t\tSYS {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: sys, password: pwd}\r\n\t\t\t]\r\n\t\t}\r\n\t\tACC {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: usr, password: pwd}\r\n\t\t\t]\r\n\t\t}\r\n\t\tacc_deleted_after_reload_will_trigger_reload_of_all_accounts {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: notused, password: soon}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`))\r\n\tdefer os.Remove(conf)\r\n\ts, o := RunServerWithConfig(conf)\r\n\tdefer s.Shutdown()\r\n\r\n\turlSys := fmt.Sprintf(\"nats://sys:pwd@%s:%d\", o.Host, o.Port)\r\n\turlUsr := fmt.Sprintf(\"nats://usr:pwd@%s:%d\", o.Host, o.Port)\r\n\toldAcc, ok := s.accounts.Load(\"SYS\")\r\n\tif !ok {\r\n\t\tt.Fatal(\"No SYS account\")\r\n\t}\r\n\r\n\ttestSrvState := func(oldAcc interface{}) {\r\n\t\tsysAcc := s.SystemAccount()\r\n\t\ts.mu.Lock()\r\n\t\tdefer s.mu.Unlock()\r\n\t\tif s.sys == nil || sysAcc == nil {\r\n\t\t\tt.Fatal(\"Expected sys.account to be non-nil\")\r\n\t\t}\r\n\t\tif sysAcc.Name != \"SYS\" {\r\n\t\t\tt.Fatal(\"Found wrong sys.account\")\r\n\t\t}\r\n\t\tif s.opts.SystemAccount != \"SYS\" {\r\n\t\t\tt.Fatal(\"Found wrong sys.account\")\r\n\t\t}\r\n\t\t// This will fail prior to system account reload\r\n\t\tif acc, ok := s.accounts.Load(s.opts.SystemAccount); !ok {\r\n\t\t\tt.Fatal(\"Found different sys.account pointer\")\r\n\t\t} else if acc == oldAcc {\r\n\t\t\tt.Fatal(\"System account is unaltered\")\r\n\t\t}\r\n\t\tif s.sys.client == nil {\r\n\t\t\tt.Fatal(\"Expected sys.client to be non-nil\")\r\n\t\t}\r\n\t\ts.sys.client.mu.Lock()\r\n\t\tdefer s.sys.client.mu.Unlock()\r\n\t\tif s.sys.client.acc.Name != \"SYS\" {\r\n\t\t\tt.Fatal(\"Found wrong sys.account\")\r\n\t\t}\r\n\t\tif s.sys.client.echo {\r\n\t\t\tt.Fatal(\"Internal clients should always have echo false\")\r\n\t\t}\r\n\t\ts.sys.account.mu.Lock()\r\n\t\tif _, ok := s.sys.account.clients[s.sys.client]; !ok {\r\n\t\t\ts.sys.account.mu.Unlock()\r\n\t\t\tt.Fatal(\"internal client not present\")\r\n\t\t}\r\n\t\ts.sys.account.mu.Unlock()\r\n\t}\r\n\r\n\t// Below tests use connection names so that they can be checked for.\r\n\t// The test subscribes to ACC only. This avoids receiving own messages.\r\n\tsubscribe := func(name string) (*nats.Conn, *nats.Subscription, *nats.Subscription) {\r\n\t\tt.Helper()\r\n\t\tc, err := nats.Connect(urlSys, nats.Name(name))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tsubCon, err := c.SubscribeSync(\"$SYS.ACCOUNT.ACC.CONNECT\")\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on subscribe CONNECT: %v\", err)\r\n\t\t}\r\n\t\tsubDis, err := c.SubscribeSync(\"$SYS.ACCOUNT.ACC.DISCONNECT\")\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on subscribe DISCONNECT: %v\", err)\r\n\t\t}\r\n\t\tc.Flush()\r\n\t\treturn c, subCon, subDis\r\n\t}\r\n\trecv := func(name string, sub *nats.Subscription) {\r\n\t\tt.Helper()\r\n\t\tif msg, err := sub.NextMsg(1 * time.Second); err != nil {\r\n\t\t\tt.Fatalf(\"%s Error on next: %v\", name, err)\r\n\t\t} else {\r\n\t\t\tcMsg := ConnectEventMsg{}\r\n\t\t\tjson.Unmarshal(msg.Data, &cMsg)\r\n\t\t\tif cMsg.Client.Name != name {\r\n\t\t\t\tt.Fatalf(\"%s wrong message: %s\", name, string(msg.Data))\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\ttriggerSysEvent := func(name string, subs []*nats.Subscription) {\r\n\t\tncs1, err := nats.Connect(urlUsr, nats.Name(name))\r\n\t\tif err != nil {\r\n\t\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t\t}\r\n\t\tncs1.Close()\r\n\t\tfor _, sub := range subs {\r\n\t\t\trecv(name, sub)\r\n\t\t\t// Make sure they are empty.\r\n\t\t\tif pending, _, _ := sub.Pending(); pending != 0 {\r\n\t\t\t\tt.Fatalf(\"Expected no pending, got %d for %+v\", pending, sub)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\ttestSrvState(nil)\r\n\tc1, s1C, s1D := subscribe(\"SYS1\")\r\n\tdefer c1.Close()\r\n\tdefer s1C.Unsubscribe()\r\n\tdefer s1D.Unsubscribe()\r\n\ttriggerSysEvent(\"BEFORE1\", []*nats.Subscription{s1C, s1D})\r\n\ttriggerSysEvent(\"BEFORE2\", []*nats.Subscription{s1C, s1D})\r\n\r\n\t// Remove account to trigger account reload\r\n\treloadUpdateConfig(t, s, conf, `\r\n\tlisten: \"127.0.0.1:-1\"\r\n\tsystem_account: SYS\r\n\taccounts {\r\n\t\tSYS {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: sys, password: pwd}\r\n\t\t\t]\r\n\t\t}\r\n\t\tACC {\r\n\t\t\tusers = [\r\n\t\t\t\t{user: usr, password: pwd}\r\n\t\t\t]\r\n\t\t}\r\n\t}\r\n\t`)\r\n\r\n\ttestSrvState(oldAcc)\r\n\tc2, s2C, s2D := subscribe(\"SYS2\")\r\n\tdefer c2.Close()\r\n\tdefer s2C.Unsubscribe()\r\n\tdefer s2D.Unsubscribe()\r\n\t// test new and existing subscriptions\r\n\ttriggerSysEvent(\"AFTER1\", []*nats.Subscription{s1C, s1D, s2C, s2D})\r\n\ttriggerSysEvent(\"AFTER2\", []*nats.Subscription{s1C, s1D, s2C, s2D})\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/reload_test.go b/server/gnatsd/server/reload_test.go
--- a/server/gnatsd/server/reload_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/reload_test.go	(date 1665399049827)
@@ -34,7 +34,7 @@
 	"testing"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 
 	"github.com/nats-io/nats.go"
 	"github.com/nats-io/nkeys"
Index: server/gnatsd/server/reload.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2017-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"crypto/tls\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\t\"net/url\"\r\n\t\"reflect\"\r\n\t\"sort\"\r\n\t\"strings\"\r\n\t\"sync/atomic\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n)\r\n\r\n// FlagSnapshot captures the server options as specified by CLI flags at\r\n// startup. This should not be modified once the server has started.\r\nvar FlagSnapshot *Options\r\n\r\ntype reloadContext struct {\r\n\toldClusterPerms *RoutePermissions\r\n}\r\n\r\n// option is a hot-swappable configuration setting.\r\ntype option interface {\r\n\t// Apply the server option.\r\n\tApply(server *Server)\r\n\r\n\t// IsLoggingChange indicates if this option requires reloading the logger.\r\n\tIsLoggingChange() bool\r\n\r\n\t// IsTraceLevelChange indicates if this option requires reloading cached trace level.\r\n\t// Clients store trace level separately.\r\n\tIsTraceLevelChange() bool\r\n\r\n\t// IsAuthChange indicates if this option requires reloading authorization.\r\n\tIsAuthChange() bool\r\n\r\n\t// IsClusterPermsChange indicates if this option requires reloading\r\n\t// cluster permissions.\r\n\tIsClusterPermsChange() bool\r\n}\r\n\r\n// noopOption is a base struct that provides default no-op behaviors.\r\ntype noopOption struct{}\r\n\r\nfunc (n noopOption) IsLoggingChange() bool {\r\n\treturn false\r\n}\r\n\r\nfunc (n noopOption) IsTraceLevelChange() bool {\r\n\treturn false\r\n}\r\n\r\nfunc (n noopOption) IsAuthChange() bool {\r\n\treturn false\r\n}\r\n\r\nfunc (n noopOption) IsClusterPermsChange() bool {\r\n\treturn false\r\n}\r\n\r\n// loggingOption is a base struct that provides default option behaviors for\r\n// logging-related options.\r\ntype loggingOption struct {\r\n\tnoopOption\r\n}\r\n\r\nfunc (l loggingOption) IsLoggingChange() bool {\r\n\treturn true\r\n}\r\n\r\n// traceLevelOption is a base struct that provides default option behaviors for\r\n// tracelevel-related options.\r\ntype traceLevelOption struct {\r\n\tloggingOption\r\n}\r\n\r\nfunc (l traceLevelOption) IsTraceLevelChange() bool {\r\n\treturn true\r\n}\r\n\r\n// traceOption implements the option interface for the `trace` setting.\r\ntype traceOption struct {\r\n\ttraceLevelOption\r\n\tnewValue bool\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (t *traceOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: trace = %v\", t.newValue)\r\n}\r\n\r\n// traceOption implements the option interface for the `trace` setting.\r\ntype traceVerboseOption struct {\r\n\ttraceLevelOption\r\n\tnewValue bool\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (t *traceVerboseOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: trace_verbose = %v\", t.newValue)\r\n}\r\n\r\n// debugOption implements the option interface for the `debug` setting.\r\ntype debugOption struct {\r\n\tloggingOption\r\n\tnewValue bool\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (d *debugOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: debug = %v\", d.newValue)\r\n}\r\n\r\n// logtimeOption implements the option interface for the `logtime` setting.\r\ntype logtimeOption struct {\r\n\tloggingOption\r\n\tnewValue bool\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (l *logtimeOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: logtime = %v\", l.newValue)\r\n}\r\n\r\n// logfileOption implements the option interface for the `log_file` setting.\r\ntype logfileOption struct {\r\n\tloggingOption\r\n\tnewValue string\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (l *logfileOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: log_file = %v\", l.newValue)\r\n}\r\n\r\n// syslogOption implements the option interface for the `syslog` setting.\r\ntype syslogOption struct {\r\n\tloggingOption\r\n\tnewValue bool\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (s *syslogOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: syslog = %v\", s.newValue)\r\n}\r\n\r\n// remoteSyslogOption implements the option interface for the `remote_syslog`\r\n// setting.\r\ntype remoteSyslogOption struct {\r\n\tloggingOption\r\n\tnewValue string\r\n}\r\n\r\n// Apply is a no-op because logging will be reloaded after options are applied.\r\nfunc (r *remoteSyslogOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: remote_syslog = %v\", r.newValue)\r\n}\r\n\r\n// tlsOption implements the option interface for the `tls` setting.\r\ntype tlsOption struct {\r\n\tnoopOption\r\n\tnewValue *tls.Config\r\n}\r\n\r\n// Apply the tls change.\r\nfunc (t *tlsOption) Apply(server *Server) {\r\n\tserver.mu.Lock()\r\n\ttlsRequired := t.newValue != nil\r\n\tserver.info.TLSRequired = tlsRequired\r\n\tmessage := \"disabled\"\r\n\tif tlsRequired {\r\n\t\tserver.info.TLSVerify = (t.newValue.ClientAuth == tls.RequireAndVerifyClientCert)\r\n\t\tmessage = \"enabled\"\r\n\t}\r\n\tserver.mu.Unlock()\r\n\tserver.Noticef(\"Reloaded: tls = %s\", message)\r\n}\r\n\r\n// tlsTimeoutOption implements the option interface for the tls `timeout`\r\n// setting.\r\ntype tlsTimeoutOption struct {\r\n\tnoopOption\r\n\tnewValue float64\r\n}\r\n\r\n// Apply is a no-op because the timeout will be reloaded after options are\r\n// applied.\r\nfunc (t *tlsTimeoutOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: tls timeout = %v\", t.newValue)\r\n}\r\n\r\n// authOption is a base struct that provides default option behaviors.\r\ntype authOption struct {\r\n\tnoopOption\r\n}\r\n\r\nfunc (o authOption) IsAuthChange() bool {\r\n\treturn true\r\n}\r\n\r\n// usernameOption implements the option interface for the `username` setting.\r\ntype usernameOption struct {\r\n\tauthOption\r\n}\r\n\r\n// Apply is a no-op because authorization will be reloaded after options are\r\n// applied.\r\nfunc (u *usernameOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: authorization username\")\r\n}\r\n\r\n// passwordOption implements the option interface for the `password` setting.\r\ntype passwordOption struct {\r\n\tauthOption\r\n}\r\n\r\n// Apply is a no-op because authorization will be reloaded after options are\r\n// applied.\r\nfunc (p *passwordOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: authorization password\")\r\n}\r\n\r\n// authorizationOption implements the option interface for the `token`\r\n// authorization setting.\r\ntype authorizationOption struct {\r\n\tauthOption\r\n}\r\n\r\n// Apply is a no-op because authorization will be reloaded after options are\r\n// applied.\r\nfunc (a *authorizationOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: authorization token\")\r\n}\r\n\r\n// authTimeoutOption implements the option interface for the authorization\r\n// `timeout` setting.\r\ntype authTimeoutOption struct {\r\n\tnoopOption // Not authOption because this is a no-op; will be reloaded with options.\r\n\tnewValue   float64\r\n}\r\n\r\n// Apply is a no-op because the timeout will be reloaded after options are\r\n// applied.\r\nfunc (a *authTimeoutOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: authorization timeout = %v\", a.newValue)\r\n}\r\n\r\n// usersOption implements the option interface for the authorization `users`\r\n// setting.\r\ntype usersOption struct {\r\n\tauthOption\r\n}\r\n\r\nfunc (u *usersOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: authorization users\")\r\n}\r\n\r\n// nkeysOption implements the option interface for the authorization `users`\r\n// setting.\r\ntype nkeysOption struct {\r\n\tauthOption\r\n}\r\n\r\nfunc (u *nkeysOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: authorization nkey users\")\r\n}\r\n\r\n// clusterOption implements the option interface for the `cluster` setting.\r\ntype clusterOption struct {\r\n\tauthOption\r\n\tnewValue     ClusterOpts\r\n\tpermsChanged bool\r\n}\r\n\r\n// Apply the cluster change.\r\nfunc (c *clusterOption) Apply(server *Server) {\r\n\t// TODO: support enabling/disabling clustering.\r\n\tserver.mu.Lock()\r\n\ttlsRequired := c.newValue.TLSConfig != nil\r\n\tserver.routeInfo.TLSRequired = tlsRequired\r\n\tserver.routeInfo.TLSVerify = tlsRequired\r\n\tserver.routeInfo.AuthRequired = c.newValue.Username != \"\"\r\n\tif c.newValue.NoAdvertise {\r\n\t\tserver.routeInfo.ClientConnectURLs = nil\r\n\t} else {\r\n\t\tserver.routeInfo.ClientConnectURLs = server.clientConnectURLs\r\n\t}\r\n\tserver.setRouteInfoHostPortAndIP()\r\n\tserver.mu.Unlock()\r\n\tserver.Noticef(\"Reloaded: cluster\")\r\n\tif tlsRequired && c.newValue.TLSConfig.InsecureSkipVerify {\r\n\t\tserver.Warnf(clusterTLSInsecureWarning)\r\n\t}\r\n}\r\n\r\nfunc (c *clusterOption) IsClusterPermsChange() bool {\r\n\treturn c.permsChanged\r\n}\r\n\r\n// routesOption implements the option interface for the cluster `routes`\r\n// setting.\r\ntype routesOption struct {\r\n\tnoopOption\r\n\tadd    []*url.URL\r\n\tremove []*url.URL\r\n}\r\n\r\n// Apply the route changes by adding and removing the necessary routes.\r\nfunc (r *routesOption) Apply(server *Server) {\r\n\tserver.mu.Lock()\r\n\troutes := make([]*client, len(server.routes))\r\n\ti := 0\r\n\tfor _, client := range server.routes {\r\n\t\troutes[i] = client\r\n\t\ti++\r\n\t}\r\n\t// If there was a change, notify monitoring code that it should\r\n\t// update the route URLs if /varz endpoint is inspected.\r\n\tif len(r.add)+len(r.remove) > 0 {\r\n\t\tserver.varzUpdateRouteURLs = true\r\n\t}\r\n\tserver.mu.Unlock()\r\n\r\n\t// Remove routes.\r\n\tfor _, remove := range r.remove {\r\n\t\tfor _, client := range routes {\r\n\t\t\tvar url *url.URL\r\n\t\t\tclient.mu.Lock()\r\n\t\t\tif client.route != nil {\r\n\t\t\t\turl = client.route.url\r\n\t\t\t}\r\n\t\t\tclient.mu.Unlock()\r\n\t\t\tif url != nil && urlsAreEqual(url, remove) {\r\n\t\t\t\t// Do not attempt to reconnect when route is removed.\r\n\t\t\t\tclient.setNoReconnect()\r\n\t\t\t\tclient.closeConnection(RouteRemoved)\r\n\t\t\t\tserver.Noticef(\"Removed route %v\", remove)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Add routes.\r\n\tserver.solicitRoutes(r.add)\r\n\r\n\tserver.Noticef(\"Reloaded: cluster routes\")\r\n}\r\n\r\n// maxConnOption implements the option interface for the `max_connections`\r\n// setting.\r\ntype maxConnOption struct {\r\n\tnoopOption\r\n\tnewValue int\r\n}\r\n\r\n// Apply the max connections change by closing random connections til we are\r\n// below the limit if necessary.\r\nfunc (m *maxConnOption) Apply(server *Server) {\r\n\tserver.mu.Lock()\r\n\tvar (\r\n\t\tclients = make([]*client, len(server.clients))\r\n\t\ti       = 0\r\n\t)\r\n\t// Map iteration is random, which allows us to close random connections.\r\n\tfor _, client := range server.clients {\r\n\t\tclients[i] = client\r\n\t\ti++\r\n\t}\r\n\tserver.mu.Unlock()\r\n\r\n\tif m.newValue > 0 && len(clients) > m.newValue {\r\n\t\t// Close connections til we are within the limit.\r\n\t\tvar (\r\n\t\t\tnumClose = len(clients) - m.newValue\r\n\t\t\tclosed   = 0\r\n\t\t)\r\n\t\tfor _, client := range clients {\r\n\t\t\tclient.maxConnExceeded()\r\n\t\t\tclosed++\r\n\t\t\tif closed >= numClose {\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t}\r\n\t\tserver.Noticef(\"Closed %d connections to fall within max_connections\", closed)\r\n\t}\r\n\tserver.Noticef(\"Reloaded: max_connections = %v\", m.newValue)\r\n}\r\n\r\n// pidFileOption implements the option interface for the `pid_file` setting.\r\ntype pidFileOption struct {\r\n\tnoopOption\r\n\tnewValue string\r\n}\r\n\r\n// Apply the setting by logging the pid to the new file.\r\nfunc (p *pidFileOption) Apply(server *Server) {\r\n\tif p.newValue == \"\" {\r\n\t\treturn\r\n\t}\r\n\tif err := server.logPid(); err != nil {\r\n\t\tserver.Errorf(\"Failed to write pidfile: %v\", err)\r\n\t}\r\n\tserver.Noticef(\"Reloaded: pid_file = %v\", p.newValue)\r\n}\r\n\r\n// portsFileDirOption implements the option interface for the `portFileDir` setting.\r\ntype portsFileDirOption struct {\r\n\tnoopOption\r\n\toldValue string\r\n\tnewValue string\r\n}\r\n\r\nfunc (p *portsFileDirOption) Apply(server *Server) {\r\n\tserver.deletePortsFile(p.oldValue)\r\n\tserver.logPorts()\r\n\tserver.Noticef(\"Reloaded: ports_file_dir = %v\", p.newValue)\r\n}\r\n\r\n// maxControlLineOption implements the option interface for the\r\n// `max_control_line` setting.\r\ntype maxControlLineOption struct {\r\n\tnoopOption\r\n\tnewValue int32\r\n}\r\n\r\n// Apply the setting by updating each client.\r\nfunc (m *maxControlLineOption) Apply(server *Server) {\r\n\tmcl := int32(m.newValue)\r\n\tserver.mu.Lock()\r\n\tfor _, client := range server.clients {\r\n\t\tatomic.StoreInt32(&client.mcl, mcl)\r\n\t}\r\n\tserver.mu.Unlock()\r\n\tserver.Noticef(\"Reloaded: max_control_line = %d\", mcl)\r\n}\r\n\r\n// maxPayloadOption implements the option interface for the `max_payload`\r\n// setting.\r\ntype maxPayloadOption struct {\r\n\tnoopOption\r\n\tnewValue int32\r\n}\r\n\r\n// Apply the setting by updating the server info and each client.\r\nfunc (m *maxPayloadOption) Apply(server *Server) {\r\n\tserver.mu.Lock()\r\n\tserver.info.MaxPayload = m.newValue\r\n\tfor _, client := range server.clients {\r\n\t\tatomic.StoreInt32(&client.mpay, int32(m.newValue))\r\n\t}\r\n\tserver.mu.Unlock()\r\n\tserver.Noticef(\"Reloaded: max_payload = %d\", m.newValue)\r\n}\r\n\r\n// pingIntervalOption implements the option interface for the `ping_interval`\r\n// setting.\r\ntype pingIntervalOption struct {\r\n\tnoopOption\r\n\tnewValue time.Duration\r\n}\r\n\r\n// Apply is a no-op because the ping interval will be reloaded after options\r\n// are applied.\r\nfunc (p *pingIntervalOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: ping_interval = %s\", p.newValue)\r\n}\r\n\r\n// maxPingsOutOption implements the option interface for the `ping_max`\r\n// setting.\r\ntype maxPingsOutOption struct {\r\n\tnoopOption\r\n\tnewValue int\r\n}\r\n\r\n// Apply is a no-op because the ping interval will be reloaded after options\r\n// are applied.\r\nfunc (m *maxPingsOutOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: ping_max = %d\", m.newValue)\r\n}\r\n\r\n// writeDeadlineOption implements the option interface for the `write_deadline`\r\n// setting.\r\ntype writeDeadlineOption struct {\r\n\tnoopOption\r\n\tnewValue time.Duration\r\n}\r\n\r\n// Apply is a no-op because the write deadline will be reloaded after options\r\n// are applied.\r\nfunc (w *writeDeadlineOption) Apply(server *Server) {\r\n\tserver.Noticef(\"Reloaded: write_deadline = %s\", w.newValue)\r\n}\r\n\r\n// clientAdvertiseOption implements the option interface for the `client_advertise` setting.\r\ntype clientAdvertiseOption struct {\r\n\tnoopOption\r\n\tnewValue string\r\n}\r\n\r\n// Apply the setting by updating the server info and regenerate the infoJSON byte array.\r\nfunc (c *clientAdvertiseOption) Apply(server *Server) {\r\n\tserver.mu.Lock()\r\n\tserver.setInfoHostPortAndGenerateJSON()\r\n\tserver.mu.Unlock()\r\n\tserver.Noticef(\"Reload: client_advertise = %s\", c.newValue)\r\n}\r\n\r\n// accountsOption implements the option interface.\r\n// Ensure that authorization code is executed if any change in accounts\r\ntype accountsOption struct {\r\n\tauthOption\r\n}\r\n\r\n// Apply is a no-op. Changes will be applied in reloadAuthorization\r\nfunc (a *accountsOption) Apply(s *Server) {\r\n\ts.Noticef(\"Reloaded: accounts\")\r\n}\r\n\r\n// connectErrorReports implements the option interface for the `connect_error_reports`\r\n// setting.\r\ntype connectErrorReports struct {\r\n\tnoopOption\r\n\tnewValue int\r\n}\r\n\r\n// Apply is a no-op because the value will be reloaded after options are applied.\r\nfunc (c *connectErrorReports) Apply(s *Server) {\r\n\ts.Noticef(\"Reloaded: connect_error_reports = %v\", c.newValue)\r\n}\r\n\r\n// connectErrorReports implements the option interface for the `connect_error_reports`\r\n// setting.\r\ntype reconnectErrorReports struct {\r\n\tnoopOption\r\n\tnewValue int\r\n}\r\n\r\n// Apply is a no-op because the value will be reloaded after options are applied.\r\nfunc (r *reconnectErrorReports) Apply(s *Server) {\r\n\ts.Noticef(\"Reloaded: reconnect_error_reports = %v\", r.newValue)\r\n}\r\n\r\n// maxTracedMsgLenOption implements the option interface for the `max_traced_msg_len` setting.\r\ntype maxTracedMsgLenOption struct {\r\n\tnoopOption\r\n\tnewValue int\r\n}\r\n\r\n// Apply the setting by updating the maximum traced message length.\r\nfunc (m *maxTracedMsgLenOption) Apply(server *Server) {\r\n\tserver.mu.Lock()\r\n\tdefer server.mu.Unlock()\r\n\tserver.opts.MaxTracedMsgLen = m.newValue\r\n\tserver.Noticef(\"Reloaded: max_traced_msg_len = %d\", m.newValue)\r\n}\r\n\r\n// Reload reads the current configuration file and applies any supported\r\n// changes. This returns an error if the server was not started with a config\r\n// file or an option which doesn't support hot-swapping was changed.\r\nfunc (s *Server) Reload() error {\r\n\ts.mu.Lock()\r\n\tif s.configFile == \"\" {\r\n\t\ts.mu.Unlock()\r\n\t\treturn errors.New(\"can only reload config when a file is provided using -c or --config\")\r\n\t}\r\n\r\n\tnewOpts, err := ProcessConfigFile(s.configFile)\r\n\tif err != nil {\r\n\t\ts.mu.Unlock()\r\n\t\t// TODO: Dump previous good config to a .bak file?\r\n\t\treturn err\r\n\t}\r\n\r\n\tcurOpts := s.getOpts()\r\n\r\n\t// Wipe trusted keys if needed when we have an operator.\r\n\tif len(curOpts.TrustedOperators) > 0 && len(curOpts.TrustedKeys) > 0 {\r\n\t\tcurOpts.TrustedKeys = nil\r\n\t}\r\n\r\n\tclientOrgPort := curOpts.Port\r\n\tclusterOrgPort := curOpts.Cluster.Port\r\n\tgatewayOrgPort := curOpts.Gateway.Port\r\n\tleafnodesOrgPort := curOpts.LeafNode.Port\r\n\r\n\ts.mu.Unlock()\r\n\r\n\t// Apply flags over config file settings.\r\n\tnewOpts = MergeOptions(newOpts, FlagSnapshot)\r\n\r\n\t// Need more processing for boolean flags...\r\n\tif FlagSnapshot != nil {\r\n\t\tapplyBoolFlags(newOpts, FlagSnapshot)\r\n\t}\r\n\r\n\tsetBaselineOptions(newOpts)\r\n\r\n\t// setBaselineOptions sets Port to 0 if set to -1 (RANDOM port)\r\n\t// If that's the case, set it to the saved value when the accept loop was\r\n\t// created.\r\n\tif newOpts.Port == 0 {\r\n\t\tnewOpts.Port = clientOrgPort\r\n\t}\r\n\t// We don't do that for cluster, so check against -1.\r\n\tif newOpts.Cluster.Port == -1 {\r\n\t\tnewOpts.Cluster.Port = clusterOrgPort\r\n\t}\r\n\tif newOpts.Gateway.Port == -1 {\r\n\t\tnewOpts.Gateway.Port = gatewayOrgPort\r\n\t}\r\n\tif newOpts.LeafNode.Port == -1 {\r\n\t\tnewOpts.LeafNode.Port = leafnodesOrgPort\r\n\t}\r\n\r\n\tif err := s.reloadOptions(curOpts, newOpts); err != nil {\r\n\t\treturn err\r\n\t}\r\n\ts.mu.Lock()\r\n\ts.configTime = time.Now()\r\n\ts.updateVarzConfigReloadableFields(s.varz)\r\n\ts.mu.Unlock()\r\n\treturn nil\r\n}\r\n\r\nfunc applyBoolFlags(newOpts, flagOpts *Options) {\r\n\t// Reset fields that may have been set to `true` in\r\n\t// MergeOptions() when some of the flags default to `true`\r\n\t// but have not been explicitly set and therefore value\r\n\t// from config file should take precedence.\r\n\tfor name, val := range newOpts.inConfig {\r\n\t\tf := reflect.ValueOf(newOpts).Elem()\r\n\t\tnames := strings.Split(name, \".\")\r\n\t\tfor _, name := range names {\r\n\t\t\tf = f.FieldByName(name)\r\n\t\t}\r\n\t\tf.SetBool(val)\r\n\t}\r\n\t// Now apply value (true or false) from flags that have\r\n\t// been explicitly set in command line\r\n\tfor name, val := range flagOpts.inCmdLine {\r\n\t\tf := reflect.ValueOf(newOpts).Elem()\r\n\t\tnames := strings.Split(name, \".\")\r\n\t\tfor _, name := range names {\r\n\t\t\tf = f.FieldByName(name)\r\n\t\t}\r\n\t\tf.SetBool(val)\r\n\t}\r\n}\r\n\r\n// reloadOptions reloads the server config with the provided options. If an\r\n// option that doesn't support hot-swapping is changed, this returns an error.\r\nfunc (s *Server) reloadOptions(curOpts, newOpts *Options) error {\r\n\t// Apply to the new options some of the options that may have been set\r\n\t// that can't be configured in the config file (this can happen in\r\n\t// applications starting NATS Server programmatically).\r\n\tnewOpts.CustomClientAuthentication = curOpts.CustomClientAuthentication\r\n\tnewOpts.CustomRouterAuthentication = curOpts.CustomRouterAuthentication\r\n\r\n\tchanged, err := s.diffOptions(newOpts)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif len(changed) != 0 {\r\n\t\tif err := validateOptions(newOpts); err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t}\r\n\r\n\t// Create a context that is used to pass special info that we may need\r\n\t// while applying the new options.\r\n\tctx := reloadContext{oldClusterPerms: curOpts.Cluster.Permissions}\r\n\ts.setOpts(newOpts)\r\n\ts.applyOptions(&ctx, changed)\r\n\treturn nil\r\n}\r\n\r\n// For the purpose of comparing, impose a order on slice data types where order does not matter\r\nfunc imposeOrder(value interface{}) error {\r\n\tswitch value := value.(type) {\r\n\tcase []*Account:\r\n\t\tsort.Slice(value, func(i, j int) bool {\r\n\t\t\treturn value[i].Name < value[j].Name\r\n\t\t})\r\n\t\tfor _, a := range value {\r\n\t\t\tsort.Slice(a.imports.streams, func(i, j int) bool {\r\n\t\t\t\treturn a.imports.streams[i].acc.Name < a.imports.streams[j].acc.Name\r\n\t\t\t})\r\n\t\t}\r\n\tcase []*User:\r\n\t\tsort.Slice(value, func(i, j int) bool {\r\n\t\t\treturn value[i].Username < value[j].Username\r\n\t\t})\r\n\tcase []*NkeyUser:\r\n\t\tsort.Slice(value, func(i, j int) bool {\r\n\t\t\treturn value[i].Nkey < value[j].Nkey\r\n\t\t})\r\n\tcase []*url.URL:\r\n\t\tsort.Slice(value, func(i, j int) bool {\r\n\t\t\treturn value[i].String() < value[j].String()\r\n\t\t})\r\n\tcase []string:\r\n\t\tsort.Slice(value, func(i, j int) bool {\r\n\t\t\treturn value[i] < value[j]\r\n\t\t})\r\n\tcase []*jwt.OperatorClaims:\r\n\t\tsort.Slice(value, func(i, j int) bool {\r\n\t\t\treturn value[i].Issuer < value[j].Issuer\r\n\t\t})\r\n\tcase GatewayOpts:\r\n\t\tsort.Slice(value.Gateways, func(i, j int) bool {\r\n\t\t\treturn value.Gateways[i].Name < value.Gateways[j].Name\r\n\t\t})\r\n\tcase string, bool, int, int32, int64, time.Duration, float64, nil,\r\n\t\tLeafNodeOpts, ClusterOpts, *tls.Config, *URLAccResolver, *MemAccResolver, Authentication:\r\n\t\t// explicitly skipped types\r\n\tdefault:\r\n\t\t// this will fail during unit tests\r\n\t\treturn fmt.Errorf(\"OnReload, sort or explicitly skip type: %s\",\r\n\t\t\treflect.TypeOf(value))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// diffOptions returns a slice containing options which have been changed. If\r\n// an option that doesn't support hot-swapping is changed, this returns an\r\n// error.\r\nfunc (s *Server) diffOptions(newOpts *Options) ([]option, error) {\r\n\tvar (\r\n\t\toldConfig = reflect.ValueOf(s.getOpts()).Elem()\r\n\t\tnewConfig = reflect.ValueOf(newOpts).Elem()\r\n\t\tdiffOpts  = []option{}\r\n\t)\r\n\tfor i := 0; i < oldConfig.NumField(); i++ {\r\n\t\tfield := oldConfig.Type().Field(i)\r\n\t\t// field.PkgPath is empty for exported fields, and is not for unexported ones.\r\n\t\t// We skip the unexported fields.\r\n\t\tif field.PkgPath != \"\" {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tvar (\r\n\t\t\toldValue = oldConfig.Field(i).Interface()\r\n\t\t\tnewValue = newConfig.Field(i).Interface()\r\n\t\t)\r\n\t\tif err := imposeOrder(oldValue); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tif err := imposeOrder(newValue); err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tif changed := !reflect.DeepEqual(oldValue, newValue); !changed {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tswitch strings.ToLower(field.Name) {\r\n\t\tcase \"traceverbose\":\r\n\t\t\tdiffOpts = append(diffOpts, &traceVerboseOption{newValue: newValue.(bool)})\r\n\t\tcase \"trace\":\r\n\t\t\tdiffOpts = append(diffOpts, &traceOption{newValue: newValue.(bool)})\r\n\t\tcase \"debug\":\r\n\t\t\tdiffOpts = append(diffOpts, &debugOption{newValue: newValue.(bool)})\r\n\t\tcase \"logtime\":\r\n\t\t\tdiffOpts = append(diffOpts, &logtimeOption{newValue: newValue.(bool)})\r\n\t\tcase \"logfile\":\r\n\t\t\tdiffOpts = append(diffOpts, &logfileOption{newValue: newValue.(string)})\r\n\t\tcase \"syslog\":\r\n\t\t\tdiffOpts = append(diffOpts, &syslogOption{newValue: newValue.(bool)})\r\n\t\tcase \"remotesyslog\":\r\n\t\t\tdiffOpts = append(diffOpts, &remoteSyslogOption{newValue: newValue.(string)})\r\n\t\tcase \"tlsconfig\":\r\n\t\t\tdiffOpts = append(diffOpts, &tlsOption{newValue: newValue.(*tls.Config)})\r\n\t\tcase \"tlstimeout\":\r\n\t\t\tdiffOpts = append(diffOpts, &tlsTimeoutOption{newValue: newValue.(float64)})\r\n\t\tcase \"username\":\r\n\t\t\tdiffOpts = append(diffOpts, &usernameOption{})\r\n\t\tcase \"password\":\r\n\t\t\tdiffOpts = append(diffOpts, &passwordOption{})\r\n\t\tcase \"authorization\":\r\n\t\t\tdiffOpts = append(diffOpts, &authorizationOption{})\r\n\t\tcase \"authtimeout\":\r\n\t\t\tdiffOpts = append(diffOpts, &authTimeoutOption{newValue: newValue.(float64)})\r\n\t\tcase \"users\":\r\n\t\t\tdiffOpts = append(diffOpts, &usersOption{})\r\n\t\tcase \"nkeys\":\r\n\t\t\tdiffOpts = append(diffOpts, &nkeysOption{})\r\n\t\tcase \"cluster\":\r\n\t\t\tnewClusterOpts := newValue.(ClusterOpts)\r\n\t\t\toldClusterOpts := oldValue.(ClusterOpts)\r\n\t\t\tif err := validateClusterOpts(oldClusterOpts, newClusterOpts); err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\tpermsChanged := !reflect.DeepEqual(newClusterOpts.Permissions, oldClusterOpts.Permissions)\r\n\t\t\tdiffOpts = append(diffOpts, &clusterOption{newValue: newClusterOpts, permsChanged: permsChanged})\r\n\t\tcase \"routes\":\r\n\t\t\tadd, remove := diffRoutes(oldValue.([]*url.URL), newValue.([]*url.URL))\r\n\t\t\tdiffOpts = append(diffOpts, &routesOption{add: add, remove: remove})\r\n\t\tcase \"maxconn\":\r\n\t\t\tdiffOpts = append(diffOpts, &maxConnOption{newValue: newValue.(int)})\r\n\t\tcase \"pidfile\":\r\n\t\t\tdiffOpts = append(diffOpts, &pidFileOption{newValue: newValue.(string)})\r\n\t\tcase \"portsfiledir\":\r\n\t\t\tdiffOpts = append(diffOpts, &portsFileDirOption{newValue: newValue.(string), oldValue: oldValue.(string)})\r\n\t\tcase \"maxcontrolline\":\r\n\t\t\tdiffOpts = append(diffOpts, &maxControlLineOption{newValue: newValue.(int32)})\r\n\t\tcase \"maxpayload\":\r\n\t\t\tdiffOpts = append(diffOpts, &maxPayloadOption{newValue: newValue.(int32)})\r\n\t\tcase \"pinginterval\":\r\n\t\t\tdiffOpts = append(diffOpts, &pingIntervalOption{newValue: newValue.(time.Duration)})\r\n\t\tcase \"maxpingsout\":\r\n\t\t\tdiffOpts = append(diffOpts, &maxPingsOutOption{newValue: newValue.(int)})\r\n\t\tcase \"writedeadline\":\r\n\t\t\tdiffOpts = append(diffOpts, &writeDeadlineOption{newValue: newValue.(time.Duration)})\r\n\t\tcase \"clientadvertise\":\r\n\t\t\tcliAdv := newValue.(string)\r\n\t\t\tif cliAdv != \"\" {\r\n\t\t\t\t// Validate ClientAdvertise syntax\r\n\t\t\t\tif _, _, err := parseHostPort(cliAdv, 0); err != nil {\r\n\t\t\t\t\treturn nil, fmt.Errorf(\"invalid ClientAdvertise value of %s, err=%v\", cliAdv, err)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tdiffOpts = append(diffOpts, &clientAdvertiseOption{newValue: cliAdv})\r\n\t\tcase \"accounts\":\r\n\t\t\tdiffOpts = append(diffOpts, &accountsOption{})\r\n\t\tcase \"resolver\", \"accountresolver\", \"accountsresolver\":\r\n\t\t\t// We can't move from no resolver to one. So check for that.\r\n\t\t\tif (oldValue == nil && newValue != nil) ||\r\n\t\t\t\t(oldValue != nil && newValue == nil) {\r\n\t\t\t\treturn nil, fmt.Errorf(\"config reload does not support moving to or from an account resolver\")\r\n\t\t\t}\r\n\t\t\tdiffOpts = append(diffOpts, &accountsOption{})\r\n\t\tcase \"accountresolvertlsconfig\":\r\n\t\t\tdiffOpts = append(diffOpts, &accountsOption{})\r\n\t\tcase \"gateway\":\r\n\t\t\t// Not supported for now, but report warning if configuration of gateway\r\n\t\t\t// is actually changed so that user knows that it won't take effect.\r\n\r\n\t\t\t// Any deep-equal is likely to fail for when there is a TLSConfig. so\r\n\t\t\t// remove for the test.\r\n\t\t\ttmpOld := oldValue.(GatewayOpts)\r\n\t\t\ttmpNew := newValue.(GatewayOpts)\r\n\t\t\ttmpOld.TLSConfig = nil\r\n\t\t\ttmpNew.TLSConfig = nil\r\n\t\t\t// If there is really a change prevents reload.\r\n\t\t\tif !reflect.DeepEqual(tmpOld, tmpNew) {\r\n\t\t\t\t// See TODO(ik) note below about printing old/new values.\r\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\r\n\t\t\t\t\tfield.Name, oldValue, newValue)\r\n\t\t\t}\r\n\t\tcase \"leafnode\":\r\n\t\t\t// Similar to gateways\r\n\t\t\ttmpOld := oldValue.(LeafNodeOpts)\r\n\t\t\ttmpNew := newValue.(LeafNodeOpts)\r\n\t\t\ttmpOld.TLSConfig = nil\r\n\t\t\ttmpNew.TLSConfig = nil\r\n\t\t\t// If there is really a change prevents reload.\r\n\t\t\tif !reflect.DeepEqual(tmpOld, tmpNew) {\r\n\t\t\t\t// See TODO(ik) note below about printing old/new values.\r\n\t\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\r\n\t\t\t\t\tfield.Name, oldValue, newValue)\r\n\t\t\t}\r\n\t\tcase \"connecterrorreports\":\r\n\t\t\tdiffOpts = append(diffOpts, &connectErrorReports{newValue: newValue.(int)})\r\n\t\tcase \"reconnecterrorreports\":\r\n\t\t\tdiffOpts = append(diffOpts, &reconnectErrorReports{newValue: newValue.(int)})\r\n\t\tcase \"nolog\", \"nosigs\":\r\n\t\t\t// Ignore NoLog and NoSigs options since they are not parsed and only used in\r\n\t\t\t// testing.\r\n\t\t\tcontinue\r\n\t\tcase \"disableshortfirstping\":\r\n\t\t\tnewOpts.DisableShortFirstPing = oldValue.(bool)\r\n\t\t\tcontinue\r\n\t\tcase \"maxtracedmsglen\":\r\n\t\t\tdiffOpts = append(diffOpts, &maxTracedMsgLenOption{newValue: newValue.(int)})\r\n\t\tcase \"port\":\r\n\t\t\t// check to see if newValue == 0 and continue if so.\r\n\t\t\tif newValue == 0 {\r\n\t\t\t\t// ignore RANDOM_PORT\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tfallthrough\r\n\t\tdefault:\r\n\t\t\t// TODO(ik): Implement String() on those options to have a nice print.\r\n\t\t\t// %v is difficult to figure what's what, %+v print private fields and\r\n\t\t\t// would print passwords. Tried json.Marshal but it is too verbose for\r\n\t\t\t// the URL array.\r\n\r\n\t\t\t// Bail out if attempting to reload any unsupported options.\r\n\t\t\treturn nil, fmt.Errorf(\"config reload not supported for %s: old=%v, new=%v\",\r\n\t\t\t\tfield.Name, oldValue, newValue)\r\n\t\t}\r\n\t}\r\n\r\n\treturn diffOpts, nil\r\n}\r\n\r\nfunc (s *Server) applyOptions(ctx *reloadContext, opts []option) {\r\n\tvar (\r\n\t\treloadLogging      = false\r\n\t\treloadAuth         = false\r\n\t\treloadClusterPerms = false\r\n\t\treloadClientTrcLvl = false\r\n\t)\r\n\tfor _, opt := range opts {\r\n\t\topt.Apply(s)\r\n\t\tif opt.IsLoggingChange() {\r\n\t\t\treloadLogging = true\r\n\t\t}\r\n\t\tif opt.IsTraceLevelChange() {\r\n\t\t\treloadClientTrcLvl = true\r\n\t\t}\r\n\t\tif opt.IsAuthChange() {\r\n\t\t\treloadAuth = true\r\n\t\t}\r\n\t\tif opt.IsClusterPermsChange() {\r\n\t\t\treloadClusterPerms = true\r\n\t\t}\r\n\t}\r\n\r\n\tif reloadLogging {\r\n\t\ts.ConfigureLogger()\r\n\t}\r\n\tif reloadClientTrcLvl {\r\n\t\ts.reloadClientTraceLevel()\r\n\t}\r\n\tif reloadAuth {\r\n\t\ts.reloadAuthorization()\r\n\t}\r\n\tif reloadClusterPerms {\r\n\t\ts.reloadClusterPermissions(ctx.oldClusterPerms)\r\n\t}\r\n\r\n\ts.Noticef(\"Reloaded server configuration\")\r\n}\r\n\r\n// Update all cached debug and trace settings for every client\r\nfunc (s *Server) reloadClientTraceLevel() {\r\n\topts := s.getOpts()\r\n\r\n\tif opts.NoLog {\r\n\t\treturn\r\n\t}\r\n\r\n\t// Create a list of all clients.\r\n\t// Update their trace level when not holding server or gateway lock\r\n\r\n\ts.mu.Lock()\r\n\tclientCnt := 1 + len(s.clients) + len(s.grTmpClients) + len(s.routes) + len(s.leafs)\r\n\ts.mu.Unlock()\r\n\r\n\ts.gateway.RLock()\r\n\tclientCnt += len(s.gateway.in) + len(s.gateway.outo)\r\n\ts.gateway.RUnlock()\r\n\r\n\tclients := make([]*client, 0, clientCnt)\r\n\r\n\ts.mu.Lock()\r\n\tif s.eventsEnabled() {\r\n\t\tclients = append(clients, s.sys.client)\r\n\t}\r\n\r\n\tcMaps := []map[uint64]*client{s.clients, s.grTmpClients, s.routes, s.leafs}\r\n\tfor _, m := range cMaps {\r\n\t\tfor _, c := range m {\r\n\t\t\tclients = append(clients, c)\r\n\t\t}\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\ts.gateway.RLock()\r\n\tfor _, c := range s.gateway.in {\r\n\t\tclients = append(clients, c)\r\n\t}\r\n\tclients = append(clients, s.gateway.outo...)\r\n\ts.gateway.RUnlock()\r\n\r\n\tfor _, c := range clients {\r\n\t\t// client.trace is commonly read while holding the lock\r\n\t\tc.mu.Lock()\r\n\t\tc.setTraceLevel()\r\n\t\tc.mu.Unlock()\r\n\t}\r\n}\r\n\r\n// reloadAuthorization reconfigures the server authorization settings,\r\n// disconnects any clients who are no longer authorized, and removes any\r\n// unauthorized subscriptions.\r\nfunc (s *Server) reloadAuthorization() {\r\n\t// This map will contain the names of accounts that have their streams\r\n\t// import configuration changed.\r\n\tawcsti := make(map[string]struct{})\r\n\r\n\ts.mu.Lock()\r\n\r\n\t// This can not be changed for now so ok to check server's trustedKeys\r\n\tif s.trustedKeys == nil {\r\n\t\t// We need to drain the old accounts here since we have something\r\n\t\t// new configured. We do not want s.accounts to change since that would\r\n\t\t// mean adding a lock to lookupAccount which is what we are trying to\r\n\t\t// optimize for with the change from a map to a sync.Map.\r\n\t\toldAccounts := make(map[string]*Account)\r\n\t\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\t\tacc := v.(*Account)\r\n\t\t\tacc.mu.RLock()\r\n\t\t\toldAccounts[acc.Name] = acc\r\n\t\t\tacc.mu.RUnlock()\r\n\t\t\ts.accounts.Delete(k)\r\n\t\t\treturn true\r\n\t\t})\r\n\t\ts.gacc = nil\r\n\t\ts.configureAccounts()\r\n\t\ts.configureAuthorization()\r\n\r\n\t\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\t\tnewAcc := v.(*Account)\r\n\t\t\tif acc, ok := oldAccounts[newAcc.Name]; ok {\r\n\t\t\t\t// If account exist in latest config, \"transfer\" the account's\r\n\t\t\t\t// sublist and client map to the new account.\r\n\t\t\t\tacc.mu.RLock()\r\n\t\t\t\tif len(acc.clients) > 0 {\r\n\t\t\t\t\tnewAcc.clients = make(map[*client]struct{}, len(acc.clients))\r\n\t\t\t\t\tfor c := range acc.clients {\r\n\t\t\t\t\t\tnewAcc.clients[c] = struct{}{}\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tnewAcc.sl = acc.sl\r\n\t\t\t\tnewAcc.rm = acc.rm\r\n\t\t\t\tnewAcc.respMap = acc.respMap\r\n\t\t\t\tacc.mu.RUnlock()\r\n\r\n\t\t\t\t// Check if current and new config of this account are same\r\n\t\t\t\t// in term of stream imports.\r\n\t\t\t\tif !acc.checkStreamImportsEqual(newAcc) {\r\n\t\t\t\t\tawcsti[newAcc.Name] = struct{}{}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\treturn true\r\n\t\t})\r\n\t} else if s.opts.AccountResolver != nil {\r\n\t\ts.configureResolver()\r\n\t\tif _, ok := s.accResolver.(*MemAccResolver); ok {\r\n\t\t\t// Check preloads so we can issue warnings etc if needed.\r\n\t\t\ts.checkResolvePreloads()\r\n\t\t\t// With a memory resolver we want to do something similar to configured accounts.\r\n\t\t\t// We will walk the accounts and delete them if they are no longer present via fetch.\r\n\t\t\t// If they are present we will force a claim update to process changes.\r\n\t\t\ts.accounts.Range(func(k, v interface{}) bool {\r\n\t\t\t\tacc := v.(*Account)\r\n\t\t\t\t// Skip global account.\r\n\t\t\t\tif acc == s.gacc {\r\n\t\t\t\t\treturn true\r\n\t\t\t\t}\r\n\t\t\t\tacc.mu.RLock()\r\n\t\t\t\taccName := acc.Name\r\n\t\t\t\tacc.mu.RUnlock()\r\n\t\t\t\t// Release server lock for following actions\r\n\t\t\t\ts.mu.Unlock()\r\n\t\t\t\taccClaims, claimJWT, _ := s.fetchAccountClaims(accName)\r\n\t\t\t\tif accClaims != nil {\r\n\t\t\t\t\terr := s.updateAccountWithClaimJWT(acc, claimJWT)\r\n\t\t\t\t\tif err != nil && err != ErrAccountResolverSameClaims {\r\n\t\t\t\t\t\ts.Noticef(\"Reloaded: deleting account [bad claims]: %q\", accName)\r\n\t\t\t\t\t\ts.accounts.Delete(k)\r\n\t\t\t\t\t}\r\n\t\t\t\t} else {\r\n\t\t\t\t\ts.Noticef(\"Reloaded: deleting account [removed]: %q\", accName)\r\n\t\t\t\t\ts.accounts.Delete(k)\r\n\t\t\t\t}\r\n\t\t\t\t// Regrab server lock.\r\n\t\t\t\ts.mu.Lock()\r\n\t\t\t\treturn true\r\n\t\t\t})\r\n\t\t}\r\n\t}\r\n\r\n\t// Gather clients that changed accounts. We will close them and they\r\n\t// will reconnect, doing the right thing.\r\n\tvar (\r\n\t\tcclientsa [64]*client\r\n\t\tcclients  = cclientsa[:0]\r\n\t\tclientsa  [64]*client\r\n\t\tclients   = clientsa[:0]\r\n\t\troutesa   [64]*client\r\n\t\troutes    = routesa[:0]\r\n\t)\r\n\tfor _, client := range s.clients {\r\n\t\tif s.clientHasMovedToDifferentAccount(client) {\r\n\t\t\tcclients = append(cclients, client)\r\n\t\t} else {\r\n\t\t\tclients = append(clients, client)\r\n\t\t}\r\n\t}\r\n\tfor _, route := range s.routes {\r\n\t\troutes = append(routes, route)\r\n\t}\r\n\tvar resetCh chan struct{}\r\n\tif s.sys != nil {\r\n\t\t// can't hold the lock as go routine reading it may be waiting for lock as well\r\n\t\tresetCh = s.sys.resetCh\r\n\t}\r\n\ts.mu.Unlock()\r\n\r\n\tif resetCh != nil {\r\n\t\tresetCh <- struct{}{}\r\n\t}\r\n\r\n\t// Close clients that have moved accounts\r\n\tfor _, client := range cclients {\r\n\t\tclient.closeConnection(ClientClosed)\r\n\t}\r\n\r\n\tfor _, client := range clients {\r\n\t\t// Disconnect any unauthorized clients.\r\n\t\tif !s.isClientAuthorized(client) {\r\n\t\t\tclient.authViolation()\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\t// Remove any unauthorized subscriptions and check for account imports.\r\n\t\tclient.processSubsOnConfigReload(awcsti)\r\n\t}\r\n\r\n\tfor _, route := range routes {\r\n\t\t// Disconnect any unauthorized routes.\r\n\t\t// Do this only for routes that were accepted, not initiated\r\n\t\t// because in the later case, we don't have the user name/password\r\n\t\t// of the remote server.\r\n\t\tif !route.isSolicitedRoute() && !s.isRouterAuthorized(route) {\r\n\t\t\troute.setNoReconnect()\r\n\t\t\troute.authViolation()\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// Returns true if given client current account has changed (or user\r\n// no longer exist) in the new config, false if the user did not\r\n// change account.\r\n// Server lock is held on entry.\r\nfunc (s *Server) clientHasMovedToDifferentAccount(c *client) bool {\r\n\tvar (\r\n\t\tnu *NkeyUser\r\n\t\tu  *User\r\n\t)\r\n\tif c.opts.Nkey != \"\" {\r\n\t\tif s.nkeys != nil {\r\n\t\t\tnu = s.nkeys[c.opts.Nkey]\r\n\t\t}\r\n\t} else if c.opts.Username != \"\" {\r\n\t\tif s.users != nil {\r\n\t\t\tu = s.users[c.opts.Username]\r\n\t\t}\r\n\t} else {\r\n\t\treturn false\r\n\t}\r\n\t// Get the current account name\r\n\tc.mu.Lock()\r\n\tvar curAccName string\r\n\tif c.acc != nil {\r\n\t\tcurAccName = c.acc.Name\r\n\t}\r\n\tc.mu.Unlock()\r\n\tif nu != nil && nu.Account != nil {\r\n\t\treturn curAccName != nu.Account.Name\r\n\t} else if u != nil && u.Account != nil {\r\n\t\treturn curAccName != u.Account.Name\r\n\t}\r\n\t// user/nkey no longer exists.\r\n\treturn true\r\n}\r\n\r\n// reloadClusterPermissions reconfigures the cluster's permssions\r\n// and set the permissions to all existing routes, sending an\r\n// update INFO protocol so that remote can resend their local\r\n// subs if needed, and sending local subs matching cluster's\r\n// import subjects.\r\nfunc (s *Server) reloadClusterPermissions(oldPerms *RoutePermissions) {\r\n\ts.mu.Lock()\r\n\tvar (\r\n\t\tinfoJSON     []byte\r\n\t\tnewPerms     = s.opts.Cluster.Permissions\r\n\t\troutes       = make(map[uint64]*client, len(s.routes))\r\n\t\twithNewProto int\r\n\t)\r\n\t// Get all connected routes\r\n\tfor i, route := range s.routes {\r\n\t\t// Count the number of routes that can understand receiving INFO updates.\r\n\t\troute.mu.Lock()\r\n\t\tif route.opts.Protocol >= RouteProtoInfo {\r\n\t\t\twithNewProto++\r\n\t\t}\r\n\t\troute.mu.Unlock()\r\n\t\troutes[i] = route\r\n\t}\r\n\t// If new permissions is nil, then clear routeInfo import/export\r\n\tif newPerms == nil {\r\n\t\ts.routeInfo.Import = nil\r\n\t\ts.routeInfo.Export = nil\r\n\t} else {\r\n\t\ts.routeInfo.Import = newPerms.Import\r\n\t\ts.routeInfo.Export = newPerms.Export\r\n\t}\r\n\t// Regenerate route INFO\r\n\ts.generateRouteInfoJSON()\r\n\tinfoJSON = s.routeInfoJSON\r\n\tgacc := s.gacc\r\n\ts.mu.Unlock()\r\n\r\n\t// If there were no route, we are done\r\n\tif len(routes) == 0 {\r\n\t\treturn\r\n\t}\r\n\r\n\t// If only older servers, simply close all routes and they will do the right\r\n\t// thing on reconnect.\r\n\tif withNewProto == 0 {\r\n\t\tfor _, route := range routes {\r\n\t\t\troute.closeConnection(RouteRemoved)\r\n\t\t}\r\n\t\treturn\r\n\t}\r\n\r\n\t// Fake clients to test cluster permissions\r\n\toldPermsTester := &client{}\r\n\toldPermsTester.setRoutePermissions(oldPerms)\r\n\tnewPermsTester := &client{}\r\n\tnewPermsTester.setRoutePermissions(newPerms)\r\n\r\n\tvar (\r\n\t\t_localSubs       [4096]*subscription\r\n\t\tlocalSubs        = _localSubs[:0]\r\n\t\tsubsNeedSUB      []*subscription\r\n\t\tsubsNeedUNSUB    []*subscription\r\n\t\tdeleteRoutedSubs []*subscription\r\n\t)\r\n\t// FIXME(dlc) - Change for accounts.\r\n\tgacc.sl.localSubs(&localSubs)\r\n\r\n\t// Go through all local subscriptions\r\n\tfor _, sub := range localSubs {\r\n\t\t// Get all subs that can now be imported\r\n\t\tsubj := string(sub.subject)\r\n\t\tcouldImportThen := oldPermsTester.canImport(subj)\r\n\t\tcanImportNow := newPermsTester.canImport(subj)\r\n\t\tif canImportNow {\r\n\t\t\t// If we could not before, then will need to send a SUB protocol.\r\n\t\t\tif !couldImportThen {\r\n\t\t\t\tsubsNeedSUB = append(subsNeedSUB, sub)\r\n\t\t\t}\r\n\t\t} else if couldImportThen {\r\n\t\t\t// We were previously able to import this sub, but now\r\n\t\t\t// we can't so we need to send an UNSUB protocol\r\n\t\t\tsubsNeedUNSUB = append(subsNeedUNSUB, sub)\r\n\t\t}\r\n\t}\r\n\r\n\tfor _, route := range routes {\r\n\t\troute.mu.Lock()\r\n\t\t// If route is to older server, simply close connection.\r\n\t\tif route.opts.Protocol < RouteProtoInfo {\r\n\t\t\troute.mu.Unlock()\r\n\t\t\troute.closeConnection(RouteRemoved)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\troute.setRoutePermissions(newPerms)\r\n\t\tfor _, sub := range route.subs {\r\n\t\t\t// If we can't export, we need to drop the subscriptions that\r\n\t\t\t// we have on behalf of this route.\r\n\t\t\tsubj := string(sub.subject)\r\n\t\t\tif !route.canExport(subj) {\r\n\t\t\t\tdelete(route.subs, string(sub.sid))\r\n\t\t\t\tdeleteRoutedSubs = append(deleteRoutedSubs, sub)\r\n\t\t\t}\r\n\t\t}\r\n\t\t// Send an update INFO, which will allow remote server to show\r\n\t\t// our current route config in monitoring and resend subscriptions\r\n\t\t// that we now possibly allow with a change of Export permissions.\r\n\t\troute.enqueueProto(infoJSON)\r\n\t\t// Now send SUB and UNSUB protocols as needed.\r\n\t\troute.sendRouteSubProtos(subsNeedSUB, false, nil)\r\n\t\troute.sendRouteUnSubProtos(subsNeedUNSUB, false, nil)\r\n\t\troute.mu.Unlock()\r\n\t}\r\n\t// Remove as a batch all the subs that we have removed from each route.\r\n\t// FIXME(dlc) - Change for accounts.\r\n\tgacc.sl.RemoveBatch(deleteRoutedSubs)\r\n}\r\n\r\n// validateClusterOpts ensures the new ClusterOpts does not change host or\r\n// port, which do not support reload.\r\nfunc validateClusterOpts(old, new ClusterOpts) error {\r\n\tif old.Host != new.Host {\r\n\t\treturn fmt.Errorf(\"config reload not supported for cluster host: old=%s, new=%s\",\r\n\t\t\told.Host, new.Host)\r\n\t}\r\n\tif old.Port != new.Port {\r\n\t\treturn fmt.Errorf(\"config reload not supported for cluster port: old=%d, new=%d\",\r\n\t\t\told.Port, new.Port)\r\n\t}\r\n\t// Validate Cluster.Advertise syntax\r\n\tif new.Advertise != \"\" {\r\n\t\tif _, _, err := parseHostPort(new.Advertise, 0); err != nil {\r\n\t\t\treturn fmt.Errorf(\"invalid Cluster.Advertise value of %s, err=%v\", new.Advertise, err)\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// diffRoutes diffs the old routes and the new routes and returns the ones that\r\n// should be added and removed from the server.\r\nfunc diffRoutes(old, new []*url.URL) (add, remove []*url.URL) {\r\n\t// Find routes to remove.\r\nremoveLoop:\r\n\tfor _, oldRoute := range old {\r\n\t\tfor _, newRoute := range new {\r\n\t\t\tif urlsAreEqual(oldRoute, newRoute) {\r\n\t\t\t\tcontinue removeLoop\r\n\t\t\t}\r\n\t\t}\r\n\t\tremove = append(remove, oldRoute)\r\n\t}\r\n\r\n\t// Find routes to add.\r\naddLoop:\r\n\tfor _, newRoute := range new {\r\n\t\tfor _, oldRoute := range old {\r\n\t\t\tif urlsAreEqual(oldRoute, newRoute) {\r\n\t\t\t\tcontinue addLoop\r\n\t\t\t}\r\n\t\t}\r\n\t\tadd = append(add, newRoute)\r\n\t}\r\n\r\n\treturn add, remove\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/reload.go b/server/gnatsd/server/reload.go
--- a/server/gnatsd/server/reload.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/reload.go	(date 1665399050048)
@@ -24,7 +24,7 @@
 	"sync/atomic"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 )
 
 // FlagSnapshot captures the server options as specified by CLI flags at
Index: server/gnatsd/server/norace_test.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright 2018-2019 The NATS Authors\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n// http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\n// +build !race\r\n\r\npackage server\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"math/rand\"\r\n\t\"net\"\r\n\t\"runtime\"\r\n\t\"runtime/debug\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/nats-io/jwt\"\r\n\t\"github.com/nats-io/nats.go\"\r\n\t\"github.com/nats-io/nkeys\"\r\n)\r\n\r\n// IMPORTANT: Tests in this file are not executed when running with the -race flag.\r\n//            The test name should be prefixed with TestNoRace so we can run only\r\n//            those tests: go test -run=TestNoRace ...\r\n\r\nfunc TestNoRaceAvoidSlowConsumerBigMessages(t *testing.T) {\r\n\topts := DefaultOptions() // Use defaults to make sure they avoid pending slow consumer.\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tnc1, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc1.Close()\r\n\r\n\tnc2, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer nc2.Close()\r\n\r\n\tdata := make([]byte, 1024*1024) // 1MB payload\r\n\trand.Read(data)\r\n\r\n\texpected := int32(500)\r\n\treceived := int32(0)\r\n\r\n\tdone := make(chan bool)\r\n\r\n\t// Create Subscription.\r\n\tnc1.Subscribe(\"slow.consumer\", func(m *nats.Msg) {\r\n\t\t// Just eat it so that we are not measuring\r\n\t\t// code time, just delivery.\r\n\t\tatomic.AddInt32(&received, 1)\r\n\t\tif received >= expected {\r\n\t\t\tdone <- true\r\n\t\t}\r\n\t})\r\n\r\n\t// Create Error handler\r\n\tnc1.SetErrorHandler(func(c *nats.Conn, s *nats.Subscription, err error) {\r\n\t\tt.Fatalf(\"Received an error on the subscription's connection: %v\\n\", err)\r\n\t})\r\n\r\n\tnc1.Flush()\r\n\r\n\tfor i := 0; i < int(expected); i++ {\r\n\t\tnc2.Publish(\"slow.consumer\", data)\r\n\t}\r\n\tnc2.Flush()\r\n\r\n\tselect {\r\n\tcase <-done:\r\n\t\treturn\r\n\tcase <-time.After(10 * time.Second):\r\n\t\tr := atomic.LoadInt32(&received)\r\n\t\tif s.NumSlowConsumers() > 0 {\r\n\t\t\tt.Fatalf(\"Did not receive all large messages due to slow consumer status: %d of %d\", r, expected)\r\n\t\t}\r\n\t\tt.Fatalf(\"Failed to receive all large messages: %d of %d\\n\", r, expected)\r\n\t}\r\n}\r\n\r\nfunc TestNoRaceRoutedQueueAutoUnsubscribe(t *testing.T) {\r\n\toptsA, _ := ProcessConfigFile(\"./configs/seed.conf\")\r\n\toptsA.NoSigs, optsA.NoLog = true, true\r\n\tsrvA := RunServer(optsA)\r\n\tdefer srvA.Shutdown()\r\n\r\n\tsrvARouteURL := fmt.Sprintf(\"nats://%s:%d\", optsA.Cluster.Host, srvA.ClusterAddr().Port)\r\n\toptsB := nextServerOpts(optsA)\r\n\toptsB.Routes = RoutesFromStr(srvARouteURL)\r\n\r\n\tsrvB := RunServer(optsB)\r\n\tdefer srvB.Shutdown()\r\n\r\n\t// Wait for these 2 to connect to each other\r\n\tcheckClusterFormed(t, srvA, srvB)\r\n\r\n\t// Have a client connection to each server\r\n\tncA, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", optsA.Host, optsA.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncA.Close()\r\n\r\n\tncB, err := nats.Connect(fmt.Sprintf(\"nats://%s:%d\", optsB.Host, optsB.Port))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer ncB.Close()\r\n\r\n\trbar := int32(0)\r\n\tbarCb := func(m *nats.Msg) {\r\n\t\tatomic.AddInt32(&rbar, 1)\r\n\t}\r\n\trbaz := int32(0)\r\n\tbazCb := func(m *nats.Msg) {\r\n\t\tatomic.AddInt32(&rbaz, 1)\r\n\t}\r\n\r\n\t// Create 125 queue subs with auto-unsubscribe to each server for\r\n\t// group bar and group baz. So 250 total per queue group.\r\n\tcons := []*nats.Conn{ncA, ncB}\r\n\tfor _, c := range cons {\r\n\t\tfor i := 0; i < 100; i++ {\r\n\t\t\tqsub, err := c.QueueSubscribe(\"foo\", \"bar\", barCb)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t\t\t}\r\n\t\t\tif err := qsub.AutoUnsubscribe(1); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error on auto-unsubscribe: %v\", err)\r\n\t\t\t}\r\n\t\t\tqsub, err = c.QueueSubscribe(\"foo\", \"baz\", bazCb)\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t\t\t}\r\n\t\t\tif err := qsub.AutoUnsubscribe(1); err != nil {\r\n\t\t\t\tt.Fatalf(\"Error on auto-unsubscribe: %v\", err)\r\n\t\t\t}\r\n\t\t}\r\n\t\tc.Subscribe(\"TEST.COMPLETE\", func(m *nats.Msg) {})\r\n\t}\r\n\r\n\t// We coelasce now so for each server we will have all local (200) plus\r\n\t// two from the remote side for each queue group. We also create one more\r\n\t// and will wait til each server has 204 subscriptions, that will make sure\r\n\t// that we have everything setup.\r\n\tcheckFor(t, 10*time.Second, 100*time.Millisecond, func() error {\r\n\t\tsubsA := srvA.NumSubscriptions()\r\n\t\tsubsB := srvB.NumSubscriptions()\r\n\t\tif subsA != 204 || subsB != 204 {\r\n\t\t\treturn fmt.Errorf(\"Not all subs processed yet: %d and %d\", subsA, subsB)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\texpected := int32(200)\r\n\t// Now send messages from each server\r\n\tfor i := int32(0); i < expected; i++ {\r\n\t\tc := cons[i%2]\r\n\t\tc.Publish(\"foo\", []byte(\"Don't Drop Me!\"))\r\n\t}\r\n\tfor _, c := range cons {\r\n\t\tc.Flush()\r\n\t}\r\n\r\n\tcheckFor(t, 10*time.Second, 100*time.Millisecond, func() error {\r\n\t\tnbar := atomic.LoadInt32(&rbar)\r\n\t\tnbaz := atomic.LoadInt32(&rbaz)\r\n\t\tif nbar == expected && nbaz == expected {\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\treturn fmt.Errorf(\"Did not receive all %d queue messages, received %d for 'bar' and %d for 'baz'\",\r\n\t\t\texpected, atomic.LoadInt32(&rbar), atomic.LoadInt32(&rbaz))\r\n\t})\r\n}\r\n\r\nfunc TestNoRaceClosedSlowConsumerWriteDeadline(t *testing.T) {\r\n\topts := DefaultOptions()\r\n\topts.WriteDeadline = 10 * time.Millisecond // Make very small to trip.\r\n\topts.MaxPending = 500 * 1024 * 1024        // Set high so it will not trip here.\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tc, err := net.DialTimeout(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port), 3*time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer c.Close()\r\n\tif _, err := c.Write([]byte(\"CONNECT {}\\r\\nPING\\r\\nSUB foo 1\\r\\n\")); err != nil {\r\n\t\tt.Fatalf(\"Error sending protocols to server: %v\", err)\r\n\t}\r\n\t// Reduce socket buffer to increase reliability of data backing up in the server destined\r\n\t// for our subscribed client.\r\n\tc.(*net.TCPConn).SetReadBuffer(128)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tsender, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer sender.Close()\r\n\r\n\tpayload := make([]byte, 1024*1024)\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tif err := sender.Publish(\"foo\", payload); err != nil {\r\n\t\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\t// Flush sender connection to ensure that all data has been sent.\r\n\tif err := sender.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error on flush: %v\", err)\r\n\t}\r\n\r\n\t// At this point server should have closed connection c.\r\n\tcheckClosedConns(t, s, 1, 2*time.Second)\r\n\tconns := s.closedClients()\r\n\tif lc := len(conns); lc != 1 {\r\n\t\tt.Fatalf(\"len(conns) expected to be %d, got %d\\n\", 1, lc)\r\n\t}\r\n\tcheckReason(t, conns[0].Reason, SlowConsumerWriteDeadline)\r\n}\r\n\r\nfunc TestNoRaceClosedSlowConsumerPendingBytes(t *testing.T) {\r\n\topts := DefaultOptions()\r\n\topts.WriteDeadline = 30 * time.Second // Wait for long time so write deadline does not trigger slow consumer.\r\n\topts.MaxPending = 1 * 1024 * 1024     // Set to low value (1MB) to allow SC to trip.\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tc, err := net.DialTimeout(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port), 3*time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer c.Close()\r\n\tif _, err := c.Write([]byte(\"CONNECT {}\\r\\nPING\\r\\nSUB foo 1\\r\\n\")); err != nil {\r\n\t\tt.Fatalf(\"Error sending protocols to server: %v\", err)\r\n\t}\r\n\t// Reduce socket buffer to increase reliability of data backing up in the server destined\r\n\t// for our subscribed client.\r\n\tc.(*net.TCPConn).SetReadBuffer(128)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tsender, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer sender.Close()\r\n\r\n\tpayload := make([]byte, 1024*1024)\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tif err := sender.Publish(\"foo\", payload); err != nil {\r\n\t\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\t// Flush sender connection to ensure that all data has been sent.\r\n\tif err := sender.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error on flush: %v\", err)\r\n\t}\r\n\r\n\t// At this point server should have closed connection c.\r\n\tcheckClosedConns(t, s, 1, 2*time.Second)\r\n\tconns := s.closedClients()\r\n\tif lc := len(conns); lc != 1 {\r\n\t\tt.Fatalf(\"len(conns) expected to be %d, got %d\\n\", 1, lc)\r\n\t}\r\n\tcheckReason(t, conns[0].Reason, SlowConsumerPendingBytes)\r\n}\r\n\r\nfunc TestNoRaceSlowConsumerPendingBytes(t *testing.T) {\r\n\topts := DefaultOptions()\r\n\topts.WriteDeadline = 30 * time.Second // Wait for long time so write deadline does not trigger slow consumer.\r\n\topts.MaxPending = 1 * 1024 * 1024     // Set to low value (1MB) to allow SC to trip.\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tc, err := net.DialTimeout(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port), 3*time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer c.Close()\r\n\tif _, err := c.Write([]byte(\"CONNECT {}\\r\\nPING\\r\\nSUB foo 1\\r\\n\")); err != nil {\r\n\t\tt.Fatalf(\"Error sending protocols to server: %v\", err)\r\n\t}\r\n\t// Reduce socket buffer to increase reliability of data backing up in the server destined\r\n\t// for our subscribed client.\r\n\tc.(*net.TCPConn).SetReadBuffer(128)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tsender, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer sender.Close()\r\n\r\n\tpayload := make([]byte, 1024*1024)\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tif err := sender.Publish(\"foo\", payload); err != nil {\r\n\t\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t\t}\r\n\t}\r\n\r\n\t// Flush sender connection to ensure that all data has been sent.\r\n\tif err := sender.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error on flush: %v\", err)\r\n\t}\r\n\r\n\t// At this point server should have closed connection c.\r\n\r\n\t// On certain platforms, it may take more than one call before\r\n\t// getting the error.\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tif _, err := c.Write([]byte(\"PUB bar 5\\r\\nhello\\r\\n\")); err != nil {\r\n\t\t\t// ok\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n\tt.Fatal(\"Connection should have been closed\")\r\n}\r\n\r\nfunc TestNoRaceGatewayNoMissingReplies(t *testing.T) {\r\n\t// This test will have following setup:\r\n\t//\r\n\t// responder1\t\t         requestor\r\n\t//    |                          |\r\n\t//    v                          v\r\n\t//   [A1]<-------gw------------[B1]\r\n\t//    |  \\                      |\r\n\t//    |   \\______gw__________   | route\r\n\t//    |                     _\\| |\r\n\t//   [  ]--------gw----------->[  ]\r\n\t//   [A2]<-------gw------------[B2]\r\n\t//   [  ]                      [  ]\r\n\t//    ^\r\n\t//    |\r\n\t// responder2\r\n\t//\r\n\t// There is a possible race that when the requestor creates\r\n\t// a subscription on the reply subject, the subject interest\r\n\t// being sent from the inbound gateway, and B1 having none,\r\n\t// the SUB first goes to B2 before being sent to A1 from\r\n\t// B2's inbound GW. But the request can go from B1 to A1\r\n\t// right away and the responder1 connecting to A1 may send\r\n\t// back the reply before the interest on the reply makes it\r\n\t// to A1 (from B2).\r\n\t// This test will also verify that if the responder is instead\r\n\t// connected to A2, the reply is properly received by requestor\r\n\t// on B1.\r\n\r\n\t// For this test we want to be in interestOnly mode, so\r\n\t// make it happen quickly\r\n\tgatewayMaxRUnsubBeforeSwitch = 1\r\n\tdefer func() { gatewayMaxRUnsubBeforeSwitch = defaultGatewayMaxRUnsubBeforeSwitch }()\r\n\r\n\t// Start with setting up A2 and B2.\r\n\tob2 := testDefaultOptionsForGateway(\"B\")\r\n\tsb2 := runGatewayServer(ob2)\r\n\tdefer sb2.Shutdown()\r\n\r\n\toa2 := testGatewayOptionsFromToWithServers(t, \"A\", \"B\", sb2)\r\n\tsa2 := runGatewayServer(oa2)\r\n\tdefer sa2.Shutdown()\r\n\r\n\twaitForOutboundGateways(t, sa2, 1, time.Second)\r\n\twaitForInboundGateways(t, sa2, 1, time.Second)\r\n\twaitForOutboundGateways(t, sb2, 1, time.Second)\r\n\twaitForInboundGateways(t, sb2, 1, time.Second)\r\n\r\n\t// Now start A1 which will connect to B2\r\n\toa1 := testGatewayOptionsFromToWithServers(t, \"A\", \"B\", sb2)\r\n\toa1.Routes = RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", oa2.Cluster.Host, oa2.Cluster.Port))\r\n\tsa1 := runGatewayServer(oa1)\r\n\tdefer sa1.Shutdown()\r\n\r\n\twaitForOutboundGateways(t, sa1, 1, time.Second)\r\n\twaitForInboundGateways(t, sb2, 2, time.Second)\r\n\r\n\tcheckClusterFormed(t, sa1, sa2)\r\n\r\n\t// Finally, start B1 that will connect to A1.\r\n\tob1 := testGatewayOptionsFromToWithServers(t, \"B\", \"A\", sa1)\r\n\tob1.Routes = RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", ob2.Cluster.Host, ob2.Cluster.Port))\r\n\tsb1 := runGatewayServer(ob1)\r\n\tdefer sb1.Shutdown()\r\n\r\n\t// Check that we have the outbound gateway from B1 to A1\r\n\tcheckFor(t, 3*time.Second, 15*time.Millisecond, func() error {\r\n\t\tc := sb1.getOutboundGatewayConnection(\"A\")\r\n\t\tif c == nil {\r\n\t\t\treturn fmt.Errorf(\"Outbound connection to A not created yet\")\r\n\t\t}\r\n\t\tc.mu.Lock()\r\n\t\tname := c.opts.Name\r\n\t\tnc := c.nc\r\n\t\tc.mu.Unlock()\r\n\t\tif name != sa1.ID() {\r\n\t\t\t// Force a disconnect\r\n\t\t\tnc.Close()\r\n\t\t\treturn fmt.Errorf(\"Was unable to have B1 connect to A1\")\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\twaitForInboundGateways(t, sa1, 1, time.Second)\r\n\tcheckClusterFormed(t, sb1, sb2)\r\n\r\n\ta1URL := fmt.Sprintf(\"nats://%s:%d\", oa1.Host, oa1.Port)\r\n\ta2URL := fmt.Sprintf(\"nats://%s:%d\", oa2.Host, oa2.Port)\r\n\tb1URL := fmt.Sprintf(\"nats://%s:%d\", ob1.Host, ob1.Port)\r\n\tb2URL := fmt.Sprintf(\"nats://%s:%d\", ob2.Host, ob2.Port)\r\n\r\n\tncb1 := natsConnect(t, b1URL)\r\n\tdefer ncb1.Close()\r\n\r\n\tncb2 := natsConnect(t, b2URL)\r\n\tdefer ncb2.Close()\r\n\r\n\tnatsSubSync(t, ncb1, \"just.a.sub\")\r\n\tnatsSubSync(t, ncb2, \"just.a.sub\")\r\n\tcheckExpectedSubs(t, 2, sb1, sb2)\r\n\r\n\t// For this test, we want A to be checking B's interest in order\r\n\t// to send messages (which would cause replies to be dropped if\r\n\t// there is no interest registered on A). So from A servers,\r\n\t// send to various subjects and cause B's to switch to interestOnly\r\n\t// mode.\r\n\tnca1 := natsConnect(t, a1URL)\r\n\tdefer nca1.Close()\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnatsPub(t, nca1, fmt.Sprintf(\"reject.%d\", i), []byte(\"hello\"))\r\n\t}\r\n\tnca2 := natsConnect(t, a2URL)\r\n\tdefer nca2.Close()\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnatsPub(t, nca2, fmt.Sprintf(\"reject.%d\", i), []byte(\"hello\"))\r\n\t}\r\n\r\n\tcheckSwitchedMode := func(t *testing.T, s *Server) {\r\n\t\tt.Helper()\r\n\t\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\t\tvar switchedMode bool\r\n\t\t\tc := s.getOutboundGatewayConnection(\"B\")\r\n\t\t\tei, _ := c.gw.outsim.Load(globalAccountName)\r\n\t\t\tif ei != nil {\r\n\t\t\t\te := ei.(*outsie)\r\n\t\t\t\te.RLock()\r\n\t\t\t\tswitchedMode = e.ni == nil && e.mode == InterestOnly\r\n\t\t\t\te.RUnlock()\r\n\t\t\t}\r\n\t\t\tif !switchedMode {\r\n\t\t\t\treturn fmt.Errorf(\"Still not switched mode\")\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t})\r\n\t}\r\n\tcheckSwitchedMode(t, sa1)\r\n\tcheckSwitchedMode(t, sa2)\r\n\r\n\t// Setup a subscriber on _INBOX.> on each of A's servers.\r\n\ttotal := 1000\r\n\texpected := int32(total)\r\n\trcvOnA := int32(0)\r\n\tqrcvOnA := int32(0)\r\n\tnatsSub(t, nca1, \"myreply.>\", func(_ *nats.Msg) {\r\n\t\tatomic.AddInt32(&rcvOnA, 1)\r\n\t})\r\n\tnatsQueueSub(t, nca2, \"myreply.>\", \"bar\", func(_ *nats.Msg) {\r\n\t\tatomic.AddInt32(&qrcvOnA, 1)\r\n\t})\r\n\tcheckExpectedSubs(t, 2, sa1, sa2)\r\n\r\n\t// Ok.. so now we will run the actual test where we\r\n\t// create a responder on A1 and make sure that every\r\n\t// single request from B1 gets the reply. Will repeat\r\n\t// test with responder connected to A2.\r\n\tsendReqs := func(t *testing.T, subConn *nats.Conn) {\r\n\t\tt.Helper()\r\n\t\tresponder := natsSub(t, subConn, \"foo\", func(m *nats.Msg) {\r\n\t\t\tm.Respond([]byte(\"reply\"))\r\n\t\t})\r\n\t\tnatsFlush(t, subConn)\r\n\t\tcheckExpectedSubs(t, 3, sa1, sa2)\r\n\r\n\t\t// We are not going to use Request() because this sets\r\n\t\t// a wildcard subscription on an INBOX and less likely\r\n\t\t// to produce the race. Instead we will explicitly set\r\n\t\t// the subscription on the reply subject and create one\r\n\t\t// per request.\r\n\t\tfor i := 0; i < total/2; i++ {\r\n\t\t\treply := fmt.Sprintf(\"myreply.%d\", i)\r\n\t\t\treplySub := natsQueueSubSync(t, ncb1, reply, \"bar\")\r\n\t\t\tnatsFlush(t, ncb1)\r\n\r\n\t\t\t// Let's make sure we have interest on B2.\r\n\t\t\tif r := sb2.globalAccount().sl.Match(reply); len(r.qsubs) == 0 {\r\n\t\t\t\tcheckFor(t, time.Second, time.Millisecond, func() error {\r\n\t\t\t\t\tif r := sb2.globalAccount().sl.Match(reply); len(r.qsubs) == 0 {\r\n\t\t\t\t\t\treturn fmt.Errorf(\"B still not registered interest on %s\", reply)\r\n\t\t\t\t\t}\r\n\t\t\t\t\treturn nil\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t\tnatsPubReq(t, ncb1, \"foo\", reply, []byte(\"request\"))\r\n\t\t\tif _, err := replySub.NextMsg(time.Second); err != nil {\r\n\t\t\t\tt.Fatalf(\"Did not receive reply: %v\", err)\r\n\t\t\t}\r\n\t\t\tnatsUnsub(t, replySub)\r\n\t\t}\r\n\r\n\t\tresponder.Unsubscribe()\r\n\t\tnatsFlush(t, subConn)\r\n\t\tcheckExpectedSubs(t, 2, sa1, sa2)\r\n\t}\r\n\tsendReqs(t, nca1)\r\n\tsendReqs(t, nca2)\r\n\r\n\tcheckFor(t, time.Second, 15*time.Millisecond, func() error {\r\n\t\tif n := atomic.LoadInt32(&rcvOnA); n != expected {\r\n\t\t\treturn fmt.Errorf(\"Subs on A expected to get %v replies, got %v\", expected, n)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\t// We should not have received a single message on the queue sub\r\n\t// on cluster A because messages will have been delivered to\r\n\t// the member on cluster B.\r\n\tif n := atomic.LoadInt32(&qrcvOnA); n != 0 {\r\n\t\tt.Fatalf(\"Queue sub on A should not have received message, got %v\", n)\r\n\t}\r\n}\r\n\r\nfunc TestNoRaceRouteMemUsage(t *testing.T) {\r\n\toa := DefaultOptions()\r\n\tsa := RunServer(oa)\r\n\tdefer sa.Shutdown()\r\n\r\n\tob := DefaultOptions()\r\n\tob.Routes = RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", oa.Cluster.Host, oa.Cluster.Port))\r\n\tsb := RunServer(ob)\r\n\tdefer sb.Shutdown()\r\n\r\n\tcheckClusterFormed(t, sa, sb)\r\n\r\n\tresponder := natsConnect(t, fmt.Sprintf(\"nats://%s:%d\", oa.Host, oa.Port))\r\n\tdefer responder.Close()\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnatsSub(t, responder, \"foo\", func(m *nats.Msg) {\r\n\t\t\tm.Respond(m.Data)\r\n\t\t})\r\n\t}\r\n\tnatsFlush(t, responder)\r\n\r\n\tpayload := make([]byte, 50*1024)\r\n\r\n\tbURL := fmt.Sprintf(\"nats://%s:%d\", ob.Host, ob.Port)\r\n\r\n\t// Capture mem usage\r\n\tmem := runtime.MemStats{}\r\n\truntime.ReadMemStats(&mem)\r\n\tinUseBefore := mem.HeapInuse\r\n\r\n\tfor i := 0; i < 100; i++ {\r\n\t\trequestor := natsConnect(t, bURL)\r\n\t\tinbox := nats.NewInbox()\r\n\t\tsub := natsSubSync(t, requestor, inbox)\r\n\t\tnatsPubReq(t, requestor, \"foo\", inbox, payload)\r\n\t\tfor j := 0; j < 10; j++ {\r\n\t\t\tnatsNexMsg(t, sub, time.Second)\r\n\t\t}\r\n\t\trequestor.Close()\r\n\t}\r\n\r\n\truntime.GC()\r\n\tdebug.FreeOSMemory()\r\n\truntime.ReadMemStats(&mem)\r\n\tinUseNow := mem.HeapInuse\r\n\tif inUseNow > 3*inUseBefore {\r\n\t\tt.Fatalf(\"Heap in-use before was %v, now %v: too high\", inUseBefore, inUseNow)\r\n\t}\r\n}\r\n\r\nfunc TestNoRaceRouteCache(t *testing.T) {\r\n\tmaxPerAccountCacheSize = 20\r\n\tprunePerAccountCacheSize = 5\r\n\tclosedSubsCheckInterval = 250 * time.Millisecond\r\n\r\n\tdefer func() {\r\n\t\tmaxPerAccountCacheSize = defaultMaxPerAccountCacheSize\r\n\t\tprunePerAccountCacheSize = defaultPrunePerAccountCacheSize\r\n\t\tclosedSubsCheckInterval = defaultClosedSubsCheckInterval\r\n\t}()\r\n\r\n\tfor _, test := range []struct {\r\n\t\tname     string\r\n\t\tuseQueue bool\r\n\t}{\r\n\t\t{\"plain_sub\", false},\r\n\t\t{\"queue_sub\", true},\r\n\t} {\r\n\t\tt.Run(test.name, func(t *testing.T) {\r\n\r\n\t\t\toa := DefaultOptions()\r\n\t\t\tsa := RunServer(oa)\r\n\t\t\tdefer sa.Shutdown()\r\n\r\n\t\t\tob := DefaultOptions()\r\n\t\t\tob.Routes = RoutesFromStr(fmt.Sprintf(\"nats://%s:%d\", oa.Cluster.Host, oa.Cluster.Port))\r\n\t\t\tsb := RunServer(ob)\r\n\t\t\tdefer sb.Shutdown()\r\n\r\n\t\t\tcheckClusterFormed(t, sa, sb)\r\n\r\n\t\t\tresponder := natsConnect(t, fmt.Sprintf(\"nats://%s:%d\", oa.Host, oa.Port))\r\n\t\t\tdefer responder.Close()\r\n\t\t\tnatsSub(t, responder, \"foo\", func(m *nats.Msg) {\r\n\t\t\t\tm.Respond(m.Data)\r\n\t\t\t})\r\n\t\t\tnatsFlush(t, responder)\r\n\r\n\t\t\tcheckExpectedSubs(t, 1, sa)\r\n\t\t\tcheckExpectedSubs(t, 1, sb)\r\n\r\n\t\t\tbURL := fmt.Sprintf(\"nats://%s:%d\", ob.Host, ob.Port)\r\n\t\t\trequestor := natsConnect(t, bURL)\r\n\t\t\tdefer requestor.Close()\r\n\r\n\t\t\tch := make(chan struct{}, 1)\r\n\t\t\tcb := func(_ *nats.Msg) {\r\n\t\t\t\tselect {\r\n\t\t\t\tcase ch <- struct{}{}:\r\n\t\t\t\tdefault:\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\tsendReqs := func(t *testing.T, nc *nats.Conn, count int, unsub bool) {\r\n\t\t\t\tt.Helper()\r\n\t\t\t\tfor i := 0; i < count; i++ {\r\n\t\t\t\t\tinbox := nats.NewInbox()\r\n\t\t\t\t\tvar sub *nats.Subscription\r\n\t\t\t\t\tif test.useQueue {\r\n\t\t\t\t\t\tsub = natsQueueSub(t, nc, inbox, \"queue\", cb)\r\n\t\t\t\t\t} else {\r\n\t\t\t\t\t\tsub = natsSub(t, nc, inbox, cb)\r\n\t\t\t\t\t}\r\n\t\t\t\t\tnatsPubReq(t, nc, \"foo\", inbox, []byte(\"hello\"))\r\n\t\t\t\t\tselect {\r\n\t\t\t\t\tcase <-ch:\r\n\t\t\t\t\tcase <-time.After(time.Second):\r\n\t\t\t\t\t\tt.Fatalf(\"Failed to get reply\")\r\n\t\t\t\t\t}\r\n\t\t\t\t\tif unsub {\r\n\t\t\t\t\t\tnatsUnsub(t, sub)\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tsendReqs(t, requestor, maxPerAccountCacheSize+1, true)\r\n\r\n\t\t\tvar route *client\r\n\t\t\tsb.mu.Lock()\r\n\t\t\tfor _, r := range sb.routes {\r\n\t\t\t\troute = r\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t\tsb.mu.Unlock()\r\n\r\n\t\t\tcheckExpected := func(t *testing.T, expected int) {\r\n\t\t\t\tt.Helper()\r\n\t\t\t\tcheckFor(t, 2*time.Second, 15*time.Millisecond, func() error {\r\n\t\t\t\t\troute.mu.Lock()\r\n\t\t\t\t\tn := len(route.in.pacache)\r\n\t\t\t\t\troute.mu.Unlock()\r\n\t\t\t\t\tif n != expected {\r\n\t\t\t\t\t\treturn fmt.Errorf(\"Expected %v subs in the cache, got %v\", expected, n)\r\n\t\t\t\t\t}\r\n\t\t\t\t\treturn nil\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t\tcheckExpected(t, (maxPerAccountCacheSize+1)-(prunePerAccountCacheSize+1))\r\n\r\n\t\t\t// Wait for more than the orphan check\r\n\t\t\ttime.Sleep(2 * closedSubsCheckInterval)\r\n\r\n\t\t\t// Add a new subs up to point where new prune would occur\r\n\t\t\tsendReqs(t, requestor, prunePerAccountCacheSize+1, false)\r\n\r\n\t\t\t// Now closed subs should have been removed, so expected\r\n\t\t\t// subs in the cache should be the new ones.\r\n\t\t\tcheckExpected(t, prunePerAccountCacheSize+1)\r\n\r\n\t\t\t// Now try wil implicit unsubscribe (due to connection close)\r\n\t\t\tsendReqs(t, requestor, maxPerAccountCacheSize+1, false)\r\n\t\t\trequestor.Close()\r\n\r\n\t\t\tcheckExpected(t, maxPerAccountCacheSize-prunePerAccountCacheSize)\r\n\r\n\t\t\t// Wait for more than the orphan check\r\n\t\t\ttime.Sleep(2 * closedSubsCheckInterval)\r\n\r\n\t\t\t// Now create new connection and send prunePerAccountCacheSize+1\r\n\t\t\t// and that should cause all subs from previous connection to be\r\n\t\t\t// removed from cache\r\n\t\t\trequestor = natsConnect(t, bURL)\r\n\t\t\tdefer requestor.Close()\r\n\r\n\t\t\tsendReqs(t, requestor, prunePerAccountCacheSize+1, false)\r\n\t\t\tcheckExpected(t, prunePerAccountCacheSize+1)\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestNoRaceFetchAccountDoesNotRegisterAccountTwice(t *testing.T) {\r\n\tsa, oa, sb, ob, _ := runTrustedGateways(t)\r\n\tdefer sa.Shutdown()\r\n\tdefer sb.Shutdown()\r\n\r\n\t// Let's create a user account.\r\n\tokp, _ := nkeys.FromSeed(oSeed)\r\n\takp, _ := nkeys.CreateAccount()\r\n\tpub, _ := akp.PublicKey()\r\n\tnac := jwt.NewAccountClaims(pub)\r\n\tjwt, _ := nac.Encode(okp)\r\n\tuserAcc := pub\r\n\r\n\t// Replace B's account resolver with one that introduces\r\n\t// delay during the Fetch()\r\n\tsac := &slowAccResolver{AccountResolver: sb.AccountResolver()}\r\n\tsb.SetAccountResolver(sac)\r\n\r\n\t// Add the account in sa and sb\r\n\taddAccountToMemResolver(sa, userAcc, jwt)\r\n\taddAccountToMemResolver(sb, userAcc, jwt)\r\n\r\n\t// Tell the slow account resolver which account to slow down\r\n\tsac.Lock()\r\n\tsac.acc = userAcc\r\n\tsac.Unlock()\r\n\r\n\turlA := fmt.Sprintf(\"nats://%s:%d\", oa.Host, oa.Port)\r\n\turlB := fmt.Sprintf(\"nats://%s:%d\", ob.Host, ob.Port)\r\n\r\n\tnca, err := nats.Connect(urlA, createUserCreds(t, sa, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error connecting to A: %v\", err)\r\n\t}\r\n\tdefer nca.Close()\r\n\r\n\t// Since there is an optimistic send, this message will go to B\r\n\t// and on processing this message, B will lookup/fetch this\r\n\t// account, which can produce race with the fetch of this\r\n\t// account from A's system account that sent a notification\r\n\t// about this account, or with the client connect just after\r\n\t// that.\r\n\tnca.Publish(\"foo\", []byte(\"hello\"))\r\n\r\n\t// Now connect and create a subscription on B\r\n\tncb, err := nats.Connect(urlB, createUserCreds(t, sb, akp))\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error connecting to A: %v\", err)\r\n\t}\r\n\tdefer ncb.Close()\r\n\tsub, err := ncb.SubscribeSync(\"foo\")\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on subscribe: %v\", err)\r\n\t}\r\n\tncb.Flush()\r\n\r\n\t// Now send messages from A and B should ultimately start to receive\r\n\t// them (once the subscription has been correctly registered)\r\n\tok := false\r\n\tfor i := 0; i < 10; i++ {\r\n\t\tnca.Publish(\"foo\", []byte(\"hello\"))\r\n\t\tif _, err := sub.NextMsg(100 * time.Millisecond); err != nil {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tok = true\r\n\t\tbreak\r\n\t}\r\n\tif !ok {\r\n\t\tt.Fatalf(\"B should be able to receive messages\")\r\n\t}\r\n\r\n\tcheckTmpAccounts := func(t *testing.T, s *Server) {\r\n\t\tt.Helper()\r\n\t\tempty := true\r\n\t\ts.tmpAccounts.Range(func(_, _ interface{}) bool {\r\n\t\t\tempty = false\r\n\t\t\treturn false\r\n\t\t})\r\n\t\tif !empty {\r\n\t\t\tt.Fatalf(\"tmpAccounts is not empty\")\r\n\t\t}\r\n\t}\r\n\tcheckTmpAccounts(t, sa)\r\n\tcheckTmpAccounts(t, sb)\r\n}\r\n\r\nfunc TestNoRaceWriteDeadline(t *testing.T) {\r\n\topts := DefaultOptions()\r\n\topts.WriteDeadline = 30 * time.Millisecond\r\n\ts := RunServer(opts)\r\n\tdefer s.Shutdown()\r\n\r\n\tc, err := net.DialTimeout(\"tcp\", fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port), 3*time.Second)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer c.Close()\r\n\tif _, err := c.Write([]byte(\"CONNECT {}\\r\\nPING\\r\\nSUB foo 1\\r\\n\")); err != nil {\r\n\t\tt.Fatalf(\"Error sending protocols to server: %v\", err)\r\n\t}\r\n\t// Reduce socket buffer to increase reliability of getting\r\n\t// write deadline errors.\r\n\tc.(*net.TCPConn).SetReadBuffer(4)\r\n\r\n\turl := fmt.Sprintf(\"nats://%s:%d\", opts.Host, opts.Port)\r\n\tsender, err := nats.Connect(url)\r\n\tif err != nil {\r\n\t\tt.Fatalf(\"Error on connect: %v\", err)\r\n\t}\r\n\tdefer sender.Close()\r\n\r\n\tpayload := make([]byte, 1000000)\r\n\ttotal := 1000\r\n\tfor i := 0; i < total; i++ {\r\n\t\tif err := sender.Publish(\"foo\", payload); err != nil {\r\n\t\t\tt.Fatalf(\"Error on publish: %v\", err)\r\n\t\t}\r\n\t}\r\n\t// Flush sender connection to ensure that all data has been sent.\r\n\tif err := sender.Flush(); err != nil {\r\n\t\tt.Fatalf(\"Error on flush: %v\", err)\r\n\t}\r\n\r\n\t// At this point server should have closed connection c.\r\n\r\n\t// On certain platforms, it may take more than one call before\r\n\t// getting the error.\r\n\tfor i := 0; i < 100; i++ {\r\n\t\tif _, err := c.Write([]byte(\"PUB bar 5\\r\\nhello\\r\\n\")); err != nil {\r\n\t\t\t// ok\r\n\t\t\treturn\r\n\t\t}\r\n\t}\r\n\tt.Fatal(\"Connection should have been closed\")\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/server/gnatsd/server/norace_test.go b/server/gnatsd/server/norace_test.go
--- a/server/gnatsd/server/norace_test.go	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/server/gnatsd/server/norace_test.go	(date 1665399053590)
@@ -11,6 +11,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
+//go:build !race
 // +build !race
 
 package server
@@ -25,7 +26,7 @@
 	"testing"
 	"time"
 
-	"github.com/nats-io/jwt"
+	"github.com/nats-io/jwt/v2"
 	"github.com/nats-io/nats.go"
 	"github.com/nats-io/nkeys"
 )
Index: Taskfile.yml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># github.com/go-task/task\r\n\r\nversion: '3'\r\n\r\nvars:\r\n  BINARY_NAME: broker\r\n  VERSION: v1.5.0\r\n\r\ntasks:\r\n  default:\r\n    cmds:\r\n      - go build -buildmode=exe -o \"{{.BINARY_NAME}}.exe\"\r\n  check_update:\r\n    cmds:\r\n      - go list -u -m -json -mod=mod all | go-mod-outdated -update -direct\r\n  lint:\r\n    cmds:\r\n      - golangci-lint run --disable gocritic --enable misspell\r\n  test:\r\n    cmds:\r\n      - go test -timeout 30m ./...\r\n  build:\r\n    env:\r\n      GOOS: '{{OS}}'\r\n      GOARCH: '{{ARCH}}'\r\n    cmds:\r\n      - go mod tidy\r\n      - go mod vendor\r\n      - go build -buildmode=exe -o \"{{.BINARY_NAME}}.exe\" -ldflags \"-X main.version={{.VERSION}}\"\r\n  commit-modifed:\r\n    cmds:\r\n      - git add -A\r\n      - git commit -a -m \"release {{.VERSION}}\"\r\n      - git push origin master\r\n  tag:\r\n    cmds:\r\n      - git tag -a {{.VERSION}} -m {{.VERSION}}\r\n      - git push origin master --tags\r\n  release:\r\n    cmds:\r\n#      - task: lint\r\n      - task: commit-modifed\r\n      - task: tag\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Taskfile.yml b/Taskfile.yml
--- a/Taskfile.yml	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/Taskfile.yml	(date 1665398786874)
@@ -4,7 +4,7 @@
 
 vars:
   BINARY_NAME: broker
-  VERSION: v1.5.0
+  VERSION: v1.6.0
 
 tasks:
   default:
Index: go.mod
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>module github.com/kubemq-io/broker\r\n\r\ngo 1.15\r\n\r\nrequire (\r\n\tgithub.com/go-sql-driver/mysql v1.5.0\r\n\tgithub.com/gogo/protobuf v1.3.1\r\n\tgithub.com/golang/protobuf v1.4.3\r\n\tgithub.com/hashicorp/go-hclog v0.15.0\r\n\tgithub.com/hashicorp/go-msgpack v1.1.5\r\n\tgithub.com/hashicorp/raft v1.2.0\r\n\tgithub.com/lib/pq v1.9.0\r\n\tgithub.com/nats-io/jwt v1.2.2\r\n\tgithub.com/nats-io/nkeys v0.2.0\r\n\tgithub.com/nats-io/nuid v1.0.1\r\n\tgithub.com/prometheus/procfs v0.2.0\r\n\tgo.etcd.io/bbolt v1.3.5\r\n\tgolang.org/x/crypto v0.0.0-20201221181555-eec23a3978ad\r\n\tgolang.org/x/sys v0.0.0-20201223074533-0d417f636930\r\n)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/go.mod b/go.mod
--- a/go.mod	(revision c64cd80333379d168901ed529557d6d6f26f4b17)
+++ b/go.mod	(date 1665399350414)
@@ -1,20 +1,32 @@
 module github.com/kubemq-io/broker
 
-go 1.15
+go 1.18
 
 require (
-	github.com/go-sql-driver/mysql v1.5.0
-	github.com/gogo/protobuf v1.3.1
-	github.com/golang/protobuf v1.4.3
+	github.com/go-sql-driver/mysql v1.6.0
+	github.com/gogo/protobuf v1.3.2
+	github.com/golang/protobuf v1.5.2
 	github.com/hashicorp/go-hclog v0.15.0
 	github.com/hashicorp/go-msgpack v1.1.5
-	github.com/hashicorp/raft v1.2.0
-	github.com/lib/pq v1.9.0
-	github.com/nats-io/jwt v1.2.2
-	github.com/nats-io/nkeys v0.2.0
+	github.com/hashicorp/raft v1.3.11
+	github.com/lib/pq v1.10.7
+	github.com/nats-io/jwt/v2 v2.3.0
+	github.com/nats-io/nats.go v1.17.0
+	github.com/nats-io/nkeys v0.3.0
 	github.com/nats-io/nuid v1.0.1
 	github.com/prometheus/procfs v0.2.0
 	go.etcd.io/bbolt v1.3.5
-	golang.org/x/crypto v0.0.0-20201221181555-eec23a3978ad
-	golang.org/x/sys v0.0.0-20201223074533-0d417f636930
+	golang.org/x/crypto v0.0.0-20220926161630-eccd6366d1be
+	golang.org/x/sys v0.0.0-20220928140112-f11e5e49a4ec
+)
+
+require (
+	github.com/armon/go-metrics v0.0.0-20190430140413-ec5e00d3c878 // indirect
+	github.com/fatih/color v1.7.0 // indirect
+	github.com/hashicorp/go-immutable-radix v1.0.0 // indirect
+	github.com/hashicorp/golang-lru v0.5.0 // indirect
+	github.com/mattn/go-colorable v0.1.4 // indirect
+	github.com/mattn/go-isatty v0.0.10 // indirect
+	github.com/nats-io/nats-server/v2 v2.9.2 // indirect
+	google.golang.org/protobuf v1.26.0 // indirect
 )
